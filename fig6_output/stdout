Total number of parameters: 4733
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0284897
Train loss (w/o reg) on all data: 0.0137206
Test loss (w/o reg) on all data: 0.139353
Train acc on all data:  0.999274836838
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 6.20681e-07
Norm of the params: 17.1867
Orig loss: 0.13935. Accuracy: 0.976
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984458
Train loss (w/o reg) on all data: 0.0662637
Test loss (w/o reg) on all data: 0.259807
Train acc on all data:  0.986705342035
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.27733e-05
Norm of the params: 25.3701
Flipped loss: 0.25981. Accuracy: 0.936
### Flips: 206, rs: 0, checks: 206
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0480812
Train loss (w/o reg) on all data: 0.0301095
Test loss (w/o reg) on all data: 0.335971
Train acc on all data:  0.995407299976
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 3.91108e-06
Norm of the params: 18.9588
     Influence (LOO): fixed 146 labels. Loss 0.33597. Accuracy 0.973.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0485859
Train loss (w/o reg) on all data: 0.023943
Test loss (w/o reg) on all data: 0.182004
Train acc on all data:  0.999033115784
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 7.0408e-06
Norm of the params: 22.2004
                Loss: fixed 101 labels. Loss 0.18200. Accuracy 0.953.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0956434
Train loss (w/o reg) on all data: 0.0637432
Test loss (w/o reg) on all data: 0.247264
Train acc on all data:  0.987430505197
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 4.55671e-06
Norm of the params: 25.2588
              Random: fixed   8 labels. Loss 0.24726. Accuracy 0.942.
### Flips: 206, rs: 0, checks: 412
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0432929
Train loss (w/o reg) on all data: 0.0273417
Test loss (w/o reg) on all data: 0.284518
Train acc on all data:  0.995165578922
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.59981e-06
Norm of the params: 17.8613
     Influence (LOO): fixed 172 labels. Loss 0.28452. Accuracy 0.974.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0381968
Train loss (w/o reg) on all data: 0.0182571
Test loss (w/o reg) on all data: 0.199526
Train acc on all data:  0.999516557892
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.97916e-06
Norm of the params: 19.9699
                Loss: fixed 142 labels. Loss 0.19953. Accuracy 0.966.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0934253
Train loss (w/o reg) on all data: 0.062156
Test loss (w/o reg) on all data: 0.265157
Train acc on all data:  0.988155668359
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 1.30912e-05
Norm of the params: 25.0077
              Random: fixed  20 labels. Loss 0.26516. Accuracy 0.940.
### Flips: 206, rs: 0, checks: 618
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0425207
Train loss (w/o reg) on all data: 0.0268383
Test loss (w/o reg) on all data: 0.254056
Train acc on all data:  0.995165578922
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 4.23402e-07
Norm of the params: 17.7101
     Influence (LOO): fixed 179 labels. Loss 0.25406. Accuracy 0.976.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0351573
Train loss (w/o reg) on all data: 0.016736
Test loss (w/o reg) on all data: 0.212747
Train acc on all data:  0.999274836838
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 6.85321e-06
Norm of the params: 19.1944
                Loss: fixed 160 labels. Loss 0.21275. Accuracy 0.969.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090608
Train loss (w/o reg) on all data: 0.0600896
Test loss (w/o reg) on all data: 0.27191
Train acc on all data:  0.988639110467
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 7.25787e-06
Norm of the params: 24.7056
              Random: fixed  30 labels. Loss 0.27191. Accuracy 0.947.
### Flips: 206, rs: 0, checks: 824
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041444
Train loss (w/o reg) on all data: 0.0259145
Test loss (w/o reg) on all data: 0.239492
Train acc on all data:  0.99564902103
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.59626e-07
Norm of the params: 17.6235
     Influence (LOO): fixed 183 labels. Loss 0.23949. Accuracy 0.977.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0331822
Train loss (w/o reg) on all data: 0.0158448
Test loss (w/o reg) on all data: 0.18121
Train acc on all data:  0.999274836838
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.07927e-06
Norm of the params: 18.6212
                Loss: fixed 174 labels. Loss 0.18121. Accuracy 0.973.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883335
Train loss (w/o reg) on all data: 0.0584627
Test loss (w/o reg) on all data: 0.258056
Train acc on all data:  0.988639110467
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 4.34992e-06
Norm of the params: 24.4421
              Random: fixed  39 labels. Loss 0.25806. Accuracy 0.948.
### Flips: 206, rs: 0, checks: 1030
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0367237
Train loss (w/o reg) on all data: 0.0219582
Test loss (w/o reg) on all data: 0.150836
Train acc on all data:  0.996857626299
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 4.86275e-07
Norm of the params: 17.1846
     Influence (LOO): fixed 191 labels. Loss 0.15084. Accuracy 0.979.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0308794
Train loss (w/o reg) on all data: 0.0147685
Test loss (w/o reg) on all data: 0.177504
Train acc on all data:  0.999274836838
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.1005e-06
Norm of the params: 17.9504
                Loss: fixed 184 labels. Loss 0.17750. Accuracy 0.972.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0862071
Train loss (w/o reg) on all data: 0.0564185
Test loss (w/o reg) on all data: 0.224309
Train acc on all data:  0.989605994682
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 6.45741e-06
Norm of the params: 24.4084
              Random: fixed  47 labels. Loss 0.22431. Accuracy 0.945.
### Flips: 206, rs: 0, checks: 1236
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036061
Train loss (w/o reg) on all data: 0.0212163
Test loss (w/o reg) on all data: 0.160103
Train acc on all data:  0.997099347353
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 8.97861e-07
Norm of the params: 17.2306
     Influence (LOO): fixed 193 labels. Loss 0.16010. Accuracy 0.979.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0304657
Train loss (w/o reg) on all data: 0.0145791
Test loss (w/o reg) on all data: 0.17578
Train acc on all data:  0.999274836838
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.14308e-06
Norm of the params: 17.825
                Loss: fixed 188 labels. Loss 0.17578. Accuracy 0.973.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0821709
Train loss (w/o reg) on all data: 0.0526557
Test loss (w/o reg) on all data: 0.228738
Train acc on all data:  0.990572878898
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.54895e-05
Norm of the params: 24.2962
              Random: fixed  59 labels. Loss 0.22874. Accuracy 0.945.
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102139
Train loss (w/o reg) on all data: 0.0704658
Test loss (w/o reg) on all data: 0.273736
Train acc on all data:  0.984046410442
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 2.25566e-05
Norm of the params: 25.1686
Flipped loss: 0.27374. Accuracy: 0.922
### Flips: 206, rs: 1, checks: 206
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0543363
Train loss (w/o reg) on all data: 0.0361842
Test loss (w/o reg) on all data: 0.180572
Train acc on all data:  0.992748368383
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 3.06155e-06
Norm of the params: 19.0536
     Influence (LOO): fixed 131 labels. Loss 0.18057. Accuracy 0.962.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0477977
Train loss (w/o reg) on all data: 0.0232325
Test loss (w/o reg) on all data: 0.336958
Train acc on all data:  0.999516557892
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 7.26144e-06
Norm of the params: 22.1654
                Loss: fixed 100 labels. Loss 0.33696. Accuracy 0.944.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984134
Train loss (w/o reg) on all data: 0.0674881
Test loss (w/o reg) on all data: 0.262335
Train acc on all data:  0.985013294658
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.57629e-05
Norm of the params: 24.8698
              Random: fixed  12 labels. Loss 0.26234. Accuracy 0.931.
### Flips: 206, rs: 1, checks: 412
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049147
Train loss (w/o reg) on all data: 0.0328034
Test loss (w/o reg) on all data: 0.160723
Train acc on all data:  0.993473531545
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.56859e-06
Norm of the params: 18.0796
     Influence (LOO): fixed 162 labels. Loss 0.16072. Accuracy 0.975.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0383362
Train loss (w/o reg) on all data: 0.0183238
Test loss (w/o reg) on all data: 0.231381
Train acc on all data:  0.999516557892
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 2.04921e-06
Norm of the params: 20.0062
                Loss: fixed 143 labels. Loss 0.23138. Accuracy 0.962.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095359
Train loss (w/o reg) on all data: 0.0650047
Test loss (w/o reg) on all data: 0.247694
Train acc on all data:  0.98573845782
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.02812e-05
Norm of the params: 24.6391
              Random: fixed  24 labels. Loss 0.24769. Accuracy 0.931.
### Flips: 206, rs: 1, checks: 618
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0467897
Train loss (w/o reg) on all data: 0.0315601
Test loss (w/o reg) on all data: 0.0875094
Train acc on all data:  0.993473531545
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 1.75941e-06
Norm of the params: 17.4525
     Influence (LOO): fixed 174 labels. Loss 0.08751. Accuracy 0.978.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0346275
Train loss (w/o reg) on all data: 0.0165292
Test loss (w/o reg) on all data: 0.255796
Train acc on all data:  0.999516557892
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 7.049e-06
Norm of the params: 19.0254
                Loss: fixed 158 labels. Loss 0.25580. Accuracy 0.962.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0923811
Train loss (w/o reg) on all data: 0.0629133
Test loss (w/o reg) on all data: 0.219218
Train acc on all data:  0.985980178874
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.82225e-05
Norm of the params: 24.2767
              Random: fixed  35 labels. Loss 0.21922. Accuracy 0.935.
### Flips: 206, rs: 1, checks: 824
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0447047
Train loss (w/o reg) on all data: 0.0295428
Test loss (w/o reg) on all data: 0.110305
Train acc on all data:  0.994198694706
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 9.3861e-07
Norm of the params: 17.4138
     Influence (LOO): fixed 181 labels. Loss 0.11031. Accuracy 0.977.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0323521
Train loss (w/o reg) on all data: 0.0154627
Test loss (w/o reg) on all data: 0.216046
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.30948e-06
Norm of the params: 18.379
                Loss: fixed 172 labels. Loss 0.21605. Accuracy 0.972.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0890063
Train loss (w/o reg) on all data: 0.0602147
Test loss (w/o reg) on all data: 0.214006
Train acc on all data:  0.986947063089
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 2.12216e-05
Norm of the params: 23.9965
              Random: fixed  47 labels. Loss 0.21401. Accuracy 0.938.
### Flips: 206, rs: 1, checks: 1030
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044061
Train loss (w/o reg) on all data: 0.0289929
Test loss (w/o reg) on all data: 0.113412
Train acc on all data:  0.99444041576
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.51942e-07
Norm of the params: 17.3598
     Influence (LOO): fixed 184 labels. Loss 0.11341. Accuracy 0.977.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0313535
Train loss (w/o reg) on all data: 0.0149975
Test loss (w/o reg) on all data: 0.216407
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 2.34763e-06
Norm of the params: 18.0864
                Loss: fixed 181 labels. Loss 0.21641. Accuracy 0.974.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0860692
Train loss (w/o reg) on all data: 0.0575665
Test loss (w/o reg) on all data: 0.229257
Train acc on all data:  0.988155668359
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 2.38345e-05
Norm of the params: 23.8758
              Random: fixed  57 labels. Loss 0.22926. Accuracy 0.936.
### Flips: 206, rs: 1, checks: 1236
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044061
Train loss (w/o reg) on all data: 0.0289929
Test loss (w/o reg) on all data: 0.113394
Train acc on all data:  0.99444041576
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.27671e-07
Norm of the params: 17.3598
     Influence (LOO): fixed 184 labels. Loss 0.11339. Accuracy 0.977.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0310776
Train loss (w/o reg) on all data: 0.0148841
Test loss (w/o reg) on all data: 0.207989
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.2673e-06
Norm of the params: 17.9964
                Loss: fixed 186 labels. Loss 0.20799. Accuracy 0.974.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0816969
Train loss (w/o reg) on all data: 0.0537563
Test loss (w/o reg) on all data: 0.221976
Train acc on all data:  0.989605994682
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 4.60712e-06
Norm of the params: 23.6392
              Random: fixed  69 labels. Loss 0.22198. Accuracy 0.937.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105686
Train loss (w/o reg) on all data: 0.0732916
Test loss (w/o reg) on all data: 0.32984
Train acc on all data:  0.982837805173
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 8.80961e-06
Norm of the params: 25.4537
Flipped loss: 0.32984. Accuracy: 0.919
### Flips: 206, rs: 2, checks: 206
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0565887
Train loss (w/o reg) on all data: 0.0387645
Test loss (w/o reg) on all data: 0.154146
Train acc on all data:  0.992748368383
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 6.30499e-07
Norm of the params: 18.8808
     Influence (LOO): fixed 132 labels. Loss 0.15415. Accuracy 0.966.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0472406
Train loss (w/o reg) on all data: 0.0233302
Test loss (w/o reg) on all data: 0.261437
Train acc on all data:  0.999274836838
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 4.70683e-06
Norm of the params: 21.8679
                Loss: fixed 101 labels. Loss 0.26144. Accuracy 0.945.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101486
Train loss (w/o reg) on all data: 0.0698014
Test loss (w/o reg) on all data: 0.312835
Train acc on all data:  0.984046410442
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 1.56106e-05
Norm of the params: 25.1731
              Random: fixed  11 labels. Loss 0.31283. Accuracy 0.923.
### Flips: 206, rs: 2, checks: 412
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0497349
Train loss (w/o reg) on all data: 0.0337015
Test loss (w/o reg) on all data: 0.112989
Train acc on all data:  0.993956973652
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 5.49604e-06
Norm of the params: 17.9072
     Influence (LOO): fixed 167 labels. Loss 0.11299. Accuracy 0.973.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0381677
Train loss (w/o reg) on all data: 0.0185246
Test loss (w/o reg) on all data: 0.194717
Train acc on all data:  0.999274836838
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.57102e-06
Norm of the params: 19.8207
                Loss: fixed 145 labels. Loss 0.19472. Accuracy 0.968.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984181
Train loss (w/o reg) on all data: 0.0676832
Test loss (w/o reg) on all data: 0.295545
Train acc on all data:  0.985255015712
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 4.81593e-06
Norm of the params: 24.7931
              Random: fixed  18 labels. Loss 0.29554. Accuracy 0.921.
### Flips: 206, rs: 2, checks: 618
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0476921
Train loss (w/o reg) on all data: 0.0325084
Test loss (w/o reg) on all data: 0.121415
Train acc on all data:  0.994198694706
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.40282e-06
Norm of the params: 17.4263
     Influence (LOO): fixed 178 labels. Loss 0.12142. Accuracy 0.976.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0349532
Train loss (w/o reg) on all data: 0.0167509
Test loss (w/o reg) on all data: 0.164543
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 2.4454e-06
Norm of the params: 19.08
                Loss: fixed 161 labels. Loss 0.16454. Accuracy 0.971.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0941498
Train loss (w/o reg) on all data: 0.0645137
Test loss (w/o reg) on all data: 0.333917
Train acc on all data:  0.986221899927
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 5.58086e-06
Norm of the params: 24.3459
              Random: fixed  26 labels. Loss 0.33392. Accuracy 0.923.
### Flips: 206, rs: 2, checks: 824
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045842
Train loss (w/o reg) on all data: 0.0308698
Test loss (w/o reg) on all data: 0.114647
Train acc on all data:  0.994923857868
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 5.97893e-07
Norm of the params: 17.3045
     Influence (LOO): fixed 182 labels. Loss 0.11465. Accuracy 0.979.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0333487
Train loss (w/o reg) on all data: 0.0159687
Test loss (w/o reg) on all data: 0.156292
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 4.84376e-06
Norm of the params: 18.644
                Loss: fixed 171 labels. Loss 0.15629. Accuracy 0.971.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0874741
Train loss (w/o reg) on all data: 0.0589242
Test loss (w/o reg) on all data: 0.32106
Train acc on all data:  0.988397389413
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 4.30453e-06
Norm of the params: 23.8956
              Random: fixed  42 labels. Loss 0.32106. Accuracy 0.930.
### Flips: 206, rs: 2, checks: 1030
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0407799
Train loss (w/o reg) on all data: 0.026104
Test loss (w/o reg) on all data: 0.119592
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 2.74484e-06
Norm of the params: 17.1324
     Influence (LOO): fixed 189 labels. Loss 0.11959. Accuracy 0.975.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0314851
Train loss (w/o reg) on all data: 0.0151054
Test loss (w/o reg) on all data: 0.148145
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.50125e-06
Norm of the params: 18.0996
                Loss: fixed 179 labels. Loss 0.14815. Accuracy 0.975.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0859511
Train loss (w/o reg) on all data: 0.0580276
Test loss (w/o reg) on all data: 0.340237
Train acc on all data:  0.988639110467
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.18041e-05
Norm of the params: 23.632
              Random: fixed  52 labels. Loss 0.34024. Accuracy 0.936.
### Flips: 206, rs: 2, checks: 1236
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0390709
Train loss (w/o reg) on all data: 0.0244516
Test loss (w/o reg) on all data: 0.120148
Train acc on all data:  0.996615905245
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.82111e-06
Norm of the params: 17.0993
     Influence (LOO): fixed 191 labels. Loss 0.12015. Accuracy 0.975.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0298657
Train loss (w/o reg) on all data: 0.0143432
Test loss (w/o reg) on all data: 0.14644
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 4.93558e-07
Norm of the params: 17.6196
                Loss: fixed 188 labels. Loss 0.14644. Accuracy 0.974.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080318
Train loss (w/o reg) on all data: 0.0532545
Test loss (w/o reg) on all data: 0.261974
Train acc on all data:  0.99008943679
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.34001e-05
Norm of the params: 23.2652
              Random: fixed  68 labels. Loss 0.26197. Accuracy 0.936.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103103
Train loss (w/o reg) on all data: 0.0713897
Test loss (w/o reg) on all data: 0.31353
Train acc on all data:  0.984771573604
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.24574e-05
Norm of the params: 25.1845
Flipped loss: 0.31353. Accuracy: 0.941
### Flips: 206, rs: 3, checks: 206
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0509878
Train loss (w/o reg) on all data: 0.0328222
Test loss (w/o reg) on all data: 0.252631
Train acc on all data:  0.99444041576
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.84325e-06
Norm of the params: 19.0607
     Influence (LOO): fixed 135 labels. Loss 0.25263. Accuracy 0.974.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046098
Train loss (w/o reg) on all data: 0.0226022
Test loss (w/o reg) on all data: 0.217322
Train acc on all data:  0.999274836838
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 9.72203e-06
Norm of the params: 21.6775
                Loss: fixed 106 labels. Loss 0.21732. Accuracy 0.957.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0993875
Train loss (w/o reg) on all data: 0.068263
Test loss (w/o reg) on all data: 0.311907
Train acc on all data:  0.985980178874
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 3.58099e-05
Norm of the params: 24.9498
              Random: fixed  11 labels. Loss 0.31191. Accuracy 0.942.
### Flips: 206, rs: 3, checks: 412
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0464745
Train loss (w/o reg) on all data: 0.0301001
Test loss (w/o reg) on all data: 0.252108
Train acc on all data:  0.994923857868
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 1.12401e-06
Norm of the params: 18.0966
     Influence (LOO): fixed 167 labels. Loss 0.25211. Accuracy 0.980.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0390322
Train loss (w/o reg) on all data: 0.0187762
Test loss (w/o reg) on all data: 0.250608
Train acc on all data:  0.999033115784
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 4.94056e-06
Norm of the params: 20.1276
                Loss: fixed 138 labels. Loss 0.25061. Accuracy 0.960.
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0968487
Train loss (w/o reg) on all data: 0.0662019
Test loss (w/o reg) on all data: 0.291102
Train acc on all data:  0.986705342035
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 1.75741e-05
Norm of the params: 24.7576
              Random: fixed  21 labels. Loss 0.29110. Accuracy 0.943.
### Flips: 206, rs: 3, checks: 618
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0458751
Train loss (w/o reg) on all data: 0.029779
Test loss (w/o reg) on all data: 0.156004
Train acc on all data:  0.994923857868
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 2.91772e-06
Norm of the params: 17.9422
     Influence (LOO): fixed 172 labels. Loss 0.15600. Accuracy 0.980.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0355377
Train loss (w/o reg) on all data: 0.0170379
Test loss (w/o reg) on all data: 0.249916
Train acc on all data:  0.999274836838
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 9.11825e-06
Norm of the params: 19.2353
                Loss: fixed 162 labels. Loss 0.24992. Accuracy 0.968.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0919877
Train loss (w/o reg) on all data: 0.0623402
Test loss (w/o reg) on all data: 0.323491
Train acc on all data:  0.987672226251
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 1.75241e-05
Norm of the params: 24.3506
              Random: fixed  35 labels. Loss 0.32349. Accuracy 0.949.
### Flips: 206, rs: 3, checks: 824
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0427212
Train loss (w/o reg) on all data: 0.026946
Test loss (w/o reg) on all data: 0.166307
Train acc on all data:  0.995407299976
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 5.72134e-07
Norm of the params: 17.7624
     Influence (LOO): fixed 181 labels. Loss 0.16631. Accuracy 0.980.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0333402
Train loss (w/o reg) on all data: 0.016053
Test loss (w/o reg) on all data: 0.227298
Train acc on all data:  0.999274836838
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 3.64249e-06
Norm of the params: 18.5942
                Loss: fixed 174 labels. Loss 0.22730. Accuracy 0.977.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0880736
Train loss (w/o reg) on all data: 0.0585525
Test loss (w/o reg) on all data: 0.318766
Train acc on all data:  0.988639110467
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 1.2234e-05
Norm of the params: 24.2986
              Random: fixed  42 labels. Loss 0.31877. Accuracy 0.947.
### Flips: 206, rs: 3, checks: 1030
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0418165
Train loss (w/o reg) on all data: 0.0262651
Test loss (w/o reg) on all data: 0.167685
Train acc on all data:  0.99564902103
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 6.54816e-07
Norm of the params: 17.636
     Influence (LOO): fixed 185 labels. Loss 0.16769. Accuracy 0.979.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0322249
Train loss (w/o reg) on all data: 0.0155194
Test loss (w/o reg) on all data: 0.179723
Train acc on all data:  0.999274836838
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 6.39692e-06
Norm of the params: 18.2787
                Loss: fixed 181 labels. Loss 0.17972. Accuracy 0.978.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0856105
Train loss (w/o reg) on all data: 0.0566989
Test loss (w/o reg) on all data: 0.316119
Train acc on all data:  0.989122552574
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 2.7473e-05
Norm of the params: 24.0464
              Random: fixed  51 labels. Loss 0.31612. Accuracy 0.950.
### Flips: 206, rs: 3, checks: 1236
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0385116
Train loss (w/o reg) on all data: 0.02355
Test loss (w/o reg) on all data: 0.159988
Train acc on all data:  0.996857626299
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 1.42353e-06
Norm of the params: 17.2984
     Influence (LOO): fixed 192 labels. Loss 0.15999. Accuracy 0.979.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030373
Train loss (w/o reg) on all data: 0.0145769
Test loss (w/o reg) on all data: 0.201476
Train acc on all data:  0.999516557892
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 1.43035e-06
Norm of the params: 17.7742
                Loss: fixed 188 labels. Loss 0.20148. Accuracy 0.979.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0815544
Train loss (w/o reg) on all data: 0.0534906
Test loss (w/o reg) on all data: 0.269936
Train acc on all data:  0.99008943679
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.77185e-05
Norm of the params: 23.6913
              Random: fixed  60 labels. Loss 0.26994. Accuracy 0.955.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101739
Train loss (w/o reg) on all data: 0.0684525
Test loss (w/o reg) on all data: 0.328473
Train acc on all data:  0.985980178874
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.33948e-05
Norm of the params: 25.8018
Flipped loss: 0.32847. Accuracy: 0.929
### Flips: 206, rs: 4, checks: 206
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0471012
Train loss (w/o reg) on all data: 0.0288255
Test loss (w/o reg) on all data: 0.152264
Train acc on all data:  0.995890742084
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 6.90176e-06
Norm of the params: 19.1184
     Influence (LOO): fixed 135 labels. Loss 0.15226. Accuracy 0.974.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0487003
Train loss (w/o reg) on all data: 0.0238531
Test loss (w/o reg) on all data: 0.333629
Train acc on all data:  0.999516557892
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 2.65882e-06
Norm of the params: 22.2923
                Loss: fixed 103 labels. Loss 0.33363. Accuracy 0.952.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100294
Train loss (w/o reg) on all data: 0.0673947
Test loss (w/o reg) on all data: 0.327851
Train acc on all data:  0.986221899927
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 9.07776e-06
Norm of the params: 25.6512
              Random: fixed   6 labels. Loss 0.32785. Accuracy 0.934.
### Flips: 206, rs: 4, checks: 412
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430656
Train loss (w/o reg) on all data: 0.0263768
Test loss (w/o reg) on all data: 0.186209
Train acc on all data:  0.995890742084
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 9.72773e-07
Norm of the params: 18.2695
     Influence (LOO): fixed 166 labels. Loss 0.18621. Accuracy 0.972.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0393739
Train loss (w/o reg) on all data: 0.0189589
Test loss (w/o reg) on all data: 0.302604
Train acc on all data:  0.999516557892
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 3.88248e-06
Norm of the params: 20.2064
                Loss: fixed 142 labels. Loss 0.30260. Accuracy 0.958.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0980286
Train loss (w/o reg) on all data: 0.0657072
Test loss (w/o reg) on all data: 0.364685
Train acc on all data:  0.986947063089
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 5.60241e-06
Norm of the params: 25.4249
              Random: fixed  17 labels. Loss 0.36468. Accuracy 0.937.
### Flips: 206, rs: 4, checks: 618
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041693
Train loss (w/o reg) on all data: 0.0255566
Test loss (w/o reg) on all data: 0.156617
Train acc on all data:  0.995890742084
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 4.21105e-06
Norm of the params: 17.9646
     Influence (LOO): fixed 175 labels. Loss 0.15662. Accuracy 0.970.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0358626
Train loss (w/o reg) on all data: 0.0171571
Test loss (w/o reg) on all data: 0.181259
Train acc on all data:  0.999516557892
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.64143e-06
Norm of the params: 19.3419
                Loss: fixed 153 labels. Loss 0.18126. Accuracy 0.967.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0949492
Train loss (w/o reg) on all data: 0.063409
Test loss (w/o reg) on all data: 0.310034
Train acc on all data:  0.988155668359
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 4.82767e-06
Norm of the params: 25.1158
              Random: fixed  27 labels. Loss 0.31003. Accuracy 0.943.
### Flips: 206, rs: 4, checks: 824
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0396564
Train loss (w/o reg) on all data: 0.0237721
Test loss (w/o reg) on all data: 0.139019
Train acc on all data:  0.996615905245
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.49527e-06
Norm of the params: 17.8238
     Influence (LOO): fixed 182 labels. Loss 0.13902. Accuracy 0.971.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0342419
Train loss (w/o reg) on all data: 0.016379
Test loss (w/o reg) on all data: 0.155918
Train acc on all data:  0.999516557892
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.78147e-06
Norm of the params: 18.9013
                Loss: fixed 169 labels. Loss 0.15592. Accuracy 0.965.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0919431
Train loss (w/o reg) on all data: 0.0607637
Test loss (w/o reg) on all data: 0.342529
Train acc on all data:  0.988639110467
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 8.14398e-06
Norm of the params: 24.9718
              Random: fixed  36 labels. Loss 0.34253. Accuracy 0.940.
### Flips: 206, rs: 4, checks: 1030
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039306
Train loss (w/o reg) on all data: 0.0234645
Test loss (w/o reg) on all data: 0.118248
Train acc on all data:  0.997099347353
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 1.74953e-06
Norm of the params: 17.7997
     Influence (LOO): fixed 185 labels. Loss 0.11825. Accuracy 0.970.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0331778
Train loss (w/o reg) on all data: 0.0159106
Test loss (w/o reg) on all data: 0.183294
Train acc on all data:  0.999516557892
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.65466e-06
Norm of the params: 18.5834
                Loss: fixed 178 labels. Loss 0.18329. Accuracy 0.969.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0880335
Train loss (w/o reg) on all data: 0.0576291
Test loss (w/o reg) on all data: 0.358298
Train acc on all data:  0.989364273628
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.22337e-05
Norm of the params: 24.6595
              Random: fixed  46 labels. Loss 0.35830. Accuracy 0.941.
### Flips: 206, rs: 4, checks: 1236
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0381867
Train loss (w/o reg) on all data: 0.0225216
Test loss (w/o reg) on all data: 0.108376
Train acc on all data:  0.997341068407
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 3.66606e-06
Norm of the params: 17.7003
     Influence (LOO): fixed 190 labels. Loss 0.10838. Accuracy 0.971.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0315201
Train loss (w/o reg) on all data: 0.0150561
Test loss (w/o reg) on all data: 0.144767
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 3.9953e-06
Norm of the params: 18.1461
                Loss: fixed 186 labels. Loss 0.14477. Accuracy 0.973.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0833456
Train loss (w/o reg) on all data: 0.0537272
Test loss (w/o reg) on all data: 0.348635
Train acc on all data:  0.990331157844
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 1.87468e-05
Norm of the params: 24.3386
              Random: fixed  60 labels. Loss 0.34863. Accuracy 0.947.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108279
Train loss (w/o reg) on all data: 0.0763843
Test loss (w/o reg) on all data: 0.453398
Train acc on all data:  0.984771573604
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 3.22483e-05
Norm of the params: 25.2566
Flipped loss: 0.45340. Accuracy: 0.914
### Flips: 206, rs: 5, checks: 206
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0556614
Train loss (w/o reg) on all data: 0.0369049
Test loss (w/o reg) on all data: 0.138796
Train acc on all data:  0.993956973652
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 4.67986e-06
Norm of the params: 19.3682
     Influence (LOO): fixed 137 labels. Loss 0.13880. Accuracy 0.964.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049513
Train loss (w/o reg) on all data: 0.0244277
Test loss (w/o reg) on all data: 0.206906
Train acc on all data:  0.999516557892
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 8.06955e-06
Norm of the params: 22.3988
                Loss: fixed 103 labels. Loss 0.20691. Accuracy 0.947.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103418
Train loss (w/o reg) on all data: 0.0722146
Test loss (w/o reg) on all data: 0.443327
Train acc on all data:  0.98573845782
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 1.42756e-05
Norm of the params: 24.9812
              Random: fixed  13 labels. Loss 0.44333. Accuracy 0.919.
### Flips: 206, rs: 5, checks: 412
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0503999
Train loss (w/o reg) on all data: 0.0338735
Test loss (w/o reg) on all data: 0.141509
Train acc on all data:  0.99444041576
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 6.55099e-06
Norm of the params: 18.1804
     Influence (LOO): fixed 167 labels. Loss 0.14151. Accuracy 0.974.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0407302
Train loss (w/o reg) on all data: 0.019654
Test loss (w/o reg) on all data: 0.171466
Train acc on all data:  0.999274836838
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.60816e-05
Norm of the params: 20.531
                Loss: fixed 141 labels. Loss 0.17147. Accuracy 0.955.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0997288
Train loss (w/o reg) on all data: 0.0691966
Test loss (w/o reg) on all data: 0.434631
Train acc on all data:  0.986221899927
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 3.38024e-05
Norm of the params: 24.7112
              Random: fixed  25 labels. Loss 0.43463. Accuracy 0.928.
### Flips: 206, rs: 5, checks: 618
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0472648
Train loss (w/o reg) on all data: 0.0311791
Test loss (w/o reg) on all data: 0.167691
Train acc on all data:  0.994923857868
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 1.63597e-06
Norm of the params: 17.9364
     Influence (LOO): fixed 177 labels. Loss 0.16769. Accuracy 0.979.
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0380845
Train loss (w/o reg) on all data: 0.0182702
Test loss (w/o reg) on all data: 0.152758
Train acc on all data:  0.999274836838
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 8.62973e-06
Norm of the params: 19.9069
                Loss: fixed 155 labels. Loss 0.15276. Accuracy 0.958.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095401
Train loss (w/o reg) on all data: 0.065244
Test loss (w/o reg) on all data: 0.404039
Train acc on all data:  0.987672226251
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 1.58274e-05
Norm of the params: 24.5589
              Random: fixed  37 labels. Loss 0.40404. Accuracy 0.921.
### Flips: 206, rs: 5, checks: 824
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0448872
Train loss (w/o reg) on all data: 0.0290868
Test loss (w/o reg) on all data: 0.154978
Train acc on all data:  0.995407299976
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.32499e-06
Norm of the params: 17.7766
     Influence (LOO): fixed 184 labels. Loss 0.15498. Accuracy 0.977.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0352653
Train loss (w/o reg) on all data: 0.0169488
Test loss (w/o reg) on all data: 0.130537
Train acc on all data:  0.999274836838
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.96151e-06
Norm of the params: 19.1397
                Loss: fixed 169 labels. Loss 0.13054. Accuracy 0.968.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0930495
Train loss (w/o reg) on all data: 0.0635875
Test loss (w/o reg) on all data: 0.36364
Train acc on all data:  0.987430505197
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 7.57816e-06
Norm of the params: 24.2743
              Random: fixed  47 labels. Loss 0.36364. Accuracy 0.935.
### Flips: 206, rs: 5, checks: 1030
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0432395
Train loss (w/o reg) on all data: 0.0274508
Test loss (w/o reg) on all data: 0.144176
Train acc on all data:  0.99564902103
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.69806e-06
Norm of the params: 17.77
     Influence (LOO): fixed 187 labels. Loss 0.14418. Accuracy 0.977.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033594
Train loss (w/o reg) on all data: 0.0161107
Test loss (w/o reg) on all data: 0.177269
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 9.08926e-06
Norm of the params: 18.6994
                Loss: fixed 179 labels. Loss 0.17727. Accuracy 0.972.
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0899606
Train loss (w/o reg) on all data: 0.0612835
Test loss (w/o reg) on all data: 0.384825
Train acc on all data:  0.987913947305
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.53798e-05
Norm of the params: 23.9487
              Random: fixed  59 labels. Loss 0.38482. Accuracy 0.933.
### Flips: 206, rs: 5, checks: 1236
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0410696
Train loss (w/o reg) on all data: 0.0252521
Test loss (w/o reg) on all data: 0.153931
Train acc on all data:  0.996374184191
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 1.25415e-06
Norm of the params: 17.7862
     Influence (LOO): fixed 190 labels. Loss 0.15393. Accuracy 0.978.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0323739
Train loss (w/o reg) on all data: 0.015575
Test loss (w/o reg) on all data: 0.174928
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 7.49556e-06
Norm of the params: 18.3297
                Loss: fixed 185 labels. Loss 0.17493. Accuracy 0.974.
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0890454
Train loss (w/o reg) on all data: 0.0607096
Test loss (w/o reg) on all data: 0.346522
Train acc on all data:  0.988155668359
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 6.89505e-06
Norm of the params: 23.8058
              Random: fixed  63 labels. Loss 0.34652. Accuracy 0.933.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102714
Train loss (w/o reg) on all data: 0.0720968
Test loss (w/o reg) on all data: 0.242185
Train acc on all data:  0.983562968335
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 2.13554e-05
Norm of the params: 24.7454
Flipped loss: 0.24218. Accuracy: 0.934
### Flips: 206, rs: 6, checks: 206
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051526
Train loss (w/o reg) on all data: 0.0347378
Test loss (w/o reg) on all data: 0.158862
Train acc on all data:  0.993715252599
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.25009e-06
Norm of the params: 18.3238
     Influence (LOO): fixed 143 labels. Loss 0.15886. Accuracy 0.961.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0486023
Train loss (w/o reg) on all data: 0.0241844
Test loss (w/o reg) on all data: 0.173247
Train acc on all data:  0.999274836838
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 7.87234e-06
Norm of the params: 22.0988
                Loss: fixed 106 labels. Loss 0.17325. Accuracy 0.956.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0983902
Train loss (w/o reg) on all data: 0.0683257
Test loss (w/o reg) on all data: 0.232503
Train acc on all data:  0.985013294658
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 1.84358e-05
Norm of the params: 24.5212
              Random: fixed  12 labels. Loss 0.23250. Accuracy 0.932.
### Flips: 206, rs: 6, checks: 412
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0482722
Train loss (w/o reg) on all data: 0.0320248
Test loss (w/o reg) on all data: 0.132352
Train acc on all data:  0.994682136814
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 6.823e-06
Norm of the params: 18.0263
     Influence (LOO): fixed 163 labels. Loss 0.13235. Accuracy 0.964.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0383694
Train loss (w/o reg) on all data: 0.0184265
Test loss (w/o reg) on all data: 0.146578
Train acc on all data:  0.999274836838
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 2.95817e-06
Norm of the params: 19.9714
                Loss: fixed 141 labels. Loss 0.14658. Accuracy 0.967.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0951302
Train loss (w/o reg) on all data: 0.0655645
Test loss (w/o reg) on all data: 0.227264
Train acc on all data:  0.985496736766
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 7.57494e-06
Norm of the params: 24.3169
              Random: fixed  21 labels. Loss 0.22726. Accuracy 0.934.
### Flips: 206, rs: 6, checks: 618
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0443933
Train loss (w/o reg) on all data: 0.0291418
Test loss (w/o reg) on all data: 0.108013
Train acc on all data:  0.994923857868
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.15405e-06
Norm of the params: 17.4651
     Influence (LOO): fixed 173 labels. Loss 0.10801. Accuracy 0.972.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0342626
Train loss (w/o reg) on all data: 0.0162855
Test loss (w/o reg) on all data: 0.211751
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.89011e-05
Norm of the params: 18.9616
                Loss: fixed 159 labels. Loss 0.21175. Accuracy 0.972.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0895728
Train loss (w/o reg) on all data: 0.0611859
Test loss (w/o reg) on all data: 0.222514
Train acc on all data:  0.986947063089
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 6.89634e-06
Norm of the params: 23.8273
              Random: fixed  33 labels. Loss 0.22251. Accuracy 0.943.
### Flips: 206, rs: 6, checks: 824
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430228
Train loss (w/o reg) on all data: 0.0279293
Test loss (w/o reg) on all data: 0.0990458
Train acc on all data:  0.994923857868
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.76716e-06
Norm of the params: 17.3744
     Influence (LOO): fixed 177 labels. Loss 0.09905. Accuracy 0.972.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0320317
Train loss (w/o reg) on all data: 0.0152986
Test loss (w/o reg) on all data: 0.20702
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.82027e-06
Norm of the params: 18.2938
                Loss: fixed 175 labels. Loss 0.20702. Accuracy 0.976.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0871661
Train loss (w/o reg) on all data: 0.0595212
Test loss (w/o reg) on all data: 0.224795
Train acc on all data:  0.987430505197
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 1.20811e-05
Norm of the params: 23.5138
              Random: fixed  43 labels. Loss 0.22480. Accuracy 0.940.
### Flips: 206, rs: 6, checks: 1030
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04094
Train loss (w/o reg) on all data: 0.0261484
Test loss (w/o reg) on all data: 0.095952
Train acc on all data:  0.996132463138
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.12414e-06
Norm of the params: 17.1998
     Influence (LOO): fixed 182 labels. Loss 0.09595. Accuracy 0.977.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0313269
Train loss (w/o reg) on all data: 0.0149649
Test loss (w/o reg) on all data: 0.218849
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 5.35001e-06
Norm of the params: 18.0898
                Loss: fixed 180 labels. Loss 0.21885. Accuracy 0.974.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0855069
Train loss (w/o reg) on all data: 0.0584496
Test loss (w/o reg) on all data: 0.210022
Train acc on all data:  0.987188784143
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 1.15212e-05
Norm of the params: 23.2625
              Random: fixed  51 labels. Loss 0.21002. Accuracy 0.942.
### Flips: 206, rs: 6, checks: 1236
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0397727
Train loss (w/o reg) on all data: 0.0249672
Test loss (w/o reg) on all data: 0.102731
Train acc on all data:  0.996132463138
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 9.87382e-07
Norm of the params: 17.2079
     Influence (LOO): fixed 185 labels. Loss 0.10273. Accuracy 0.976.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0297626
Train loss (w/o reg) on all data: 0.0142349
Test loss (w/o reg) on all data: 0.193439
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.42439e-06
Norm of the params: 17.6226
                Loss: fixed 186 labels. Loss 0.19344. Accuracy 0.973.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0817562
Train loss (w/o reg) on all data: 0.0554852
Test loss (w/o reg) on all data: 0.200475
Train acc on all data:  0.987672226251
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 6.14485e-06
Norm of the params: 22.9221
              Random: fixed  65 labels. Loss 0.20048. Accuracy 0.947.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0974235
Train loss (w/o reg) on all data: 0.0678484
Test loss (w/o reg) on all data: 0.388817
Train acc on all data:  0.986705342035
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 8.22921e-06
Norm of the params: 24.3208
Flipped loss: 0.38882. Accuracy: 0.924
### Flips: 206, rs: 7, checks: 206
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0571319
Train loss (w/o reg) on all data: 0.0395521
Test loss (w/o reg) on all data: 0.206251
Train acc on all data:  0.992264926275
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 4.71106e-06
Norm of the params: 18.7509
     Influence (LOO): fixed 127 labels. Loss 0.20625. Accuracy 0.970.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0471053
Train loss (w/o reg) on all data: 0.023228
Test loss (w/o reg) on all data: 0.229644
Train acc on all data:  0.999033115784
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 2.98387e-06
Norm of the params: 21.8528
                Loss: fixed 103 labels. Loss 0.22964. Accuracy 0.962.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0936918
Train loss (w/o reg) on all data: 0.0647596
Test loss (w/o reg) on all data: 0.380451
Train acc on all data:  0.987430505197
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 6.57933e-06
Norm of the params: 24.055
              Random: fixed  15 labels. Loss 0.38045. Accuracy 0.924.
### Flips: 206, rs: 7, checks: 412
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051769
Train loss (w/o reg) on all data: 0.0362193
Test loss (w/o reg) on all data: 0.138261
Train acc on all data:  0.992506647329
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 6.5835e-07
Norm of the params: 17.635
     Influence (LOO): fixed 160 labels. Loss 0.13826. Accuracy 0.976.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0373785
Train loss (w/o reg) on all data: 0.0179752
Test loss (w/o reg) on all data: 0.192556
Train acc on all data:  0.999516557892
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.46189e-06
Norm of the params: 19.6994
                Loss: fixed 138 labels. Loss 0.19256. Accuracy 0.967.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0902178
Train loss (w/o reg) on all data: 0.0617997
Test loss (w/o reg) on all data: 0.377757
Train acc on all data:  0.988397389413
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 8.31875e-06
Norm of the params: 23.8404
              Random: fixed  28 labels. Loss 0.37776. Accuracy 0.928.
### Flips: 206, rs: 7, checks: 618
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0490332
Train loss (w/o reg) on all data: 0.0338739
Test loss (w/o reg) on all data: 0.14637
Train acc on all data:  0.993231810491
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 1.11001e-06
Norm of the params: 17.4122
     Influence (LOO): fixed 172 labels. Loss 0.14637. Accuracy 0.978.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0352358
Train loss (w/o reg) on all data: 0.0169136
Test loss (w/o reg) on all data: 0.200622
Train acc on all data:  0.999516557892
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 4.54419e-06
Norm of the params: 19.1427
                Loss: fixed 148 labels. Loss 0.20062. Accuracy 0.970.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0879122
Train loss (w/o reg) on all data: 0.0599882
Test loss (w/o reg) on all data: 0.372817
Train acc on all data:  0.988639110467
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 1.30964e-05
Norm of the params: 23.6322
              Random: fixed  40 labels. Loss 0.37282. Accuracy 0.928.
### Flips: 206, rs: 7, checks: 824
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0462874
Train loss (w/o reg) on all data: 0.0312824
Test loss (w/o reg) on all data: 0.133189
Train acc on all data:  0.994198694706
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 4.12788e-07
Norm of the params: 17.3234
     Influence (LOO): fixed 178 labels. Loss 0.13319. Accuracy 0.979.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0338509
Train loss (w/o reg) on all data: 0.0162464
Test loss (w/o reg) on all data: 0.168249
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 5.09553e-06
Norm of the params: 18.7641
                Loss: fixed 161 labels. Loss 0.16825. Accuracy 0.971.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0832268
Train loss (w/o reg) on all data: 0.0560994
Test loss (w/o reg) on all data: 0.361409
Train acc on all data:  0.990572878898
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 5.73817e-06
Norm of the params: 23.2927
              Random: fixed  53 labels. Loss 0.36141. Accuracy 0.924.
### Flips: 206, rs: 7, checks: 1030
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0424466
Train loss (w/o reg) on all data: 0.0276768
Test loss (w/o reg) on all data: 0.114832
Train acc on all data:  0.995407299976
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 1.58964e-06
Norm of the params: 17.1871
     Influence (LOO): fixed 186 labels. Loss 0.11483. Accuracy 0.979.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0324122
Train loss (w/o reg) on all data: 0.0155509
Test loss (w/o reg) on all data: 0.178732
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 3.35936e-06
Norm of the params: 18.3637
                Loss: fixed 173 labels. Loss 0.17873. Accuracy 0.973.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0802025
Train loss (w/o reg) on all data: 0.0541346
Test loss (w/o reg) on all data: 0.332356
Train acc on all data:  0.991298042059
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.31959e-05
Norm of the params: 22.8332
              Random: fixed  66 labels. Loss 0.33236. Accuracy 0.935.
### Flips: 206, rs: 7, checks: 1236
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0414411
Train loss (w/o reg) on all data: 0.0267351
Test loss (w/o reg) on all data: 0.119601
Train acc on all data:  0.99564902103
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 4.14292e-07
Norm of the params: 17.15
     Influence (LOO): fixed 188 labels. Loss 0.11960. Accuracy 0.978.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0310297
Train loss (w/o reg) on all data: 0.0148932
Test loss (w/o reg) on all data: 0.168125
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.85676e-06
Norm of the params: 17.9647
                Loss: fixed 185 labels. Loss 0.16812. Accuracy 0.976.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0764223
Train loss (w/o reg) on all data: 0.0507301
Test loss (w/o reg) on all data: 0.331349
Train acc on all data:  0.991539763113
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 4.56913e-06
Norm of the params: 22.6681
              Random: fixed  77 labels. Loss 0.33135. Accuracy 0.940.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0976661
Train loss (w/o reg) on all data: 0.0650726
Test loss (w/o reg) on all data: 0.336317
Train acc on all data:  0.986221899927
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.29983e-05
Norm of the params: 25.5317
Flipped loss: 0.33632. Accuracy: 0.933
### Flips: 206, rs: 8, checks: 206
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0467671
Train loss (w/o reg) on all data: 0.0291202
Test loss (w/o reg) on all data: 0.110889
Train acc on all data:  0.995890742084
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 2.71854e-06
Norm of the params: 18.7866
     Influence (LOO): fixed 150 labels. Loss 0.11089. Accuracy 0.967.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049727
Train loss (w/o reg) on all data: 0.0245546
Test loss (w/o reg) on all data: 0.206962
Train acc on all data:  0.999516557892
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 1.03934e-05
Norm of the params: 22.4377
                Loss: fixed  98 labels. Loss 0.20696. Accuracy 0.950.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0959424
Train loss (w/o reg) on all data: 0.0637869
Test loss (w/o reg) on all data: 0.281262
Train acc on all data:  0.986463620981
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 2.71171e-05
Norm of the params: 25.3596
              Random: fixed  10 labels. Loss 0.28126. Accuracy 0.929.
### Flips: 206, rs: 8, checks: 412
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0412588
Train loss (w/o reg) on all data: 0.0257735
Test loss (w/o reg) on all data: 0.103346
Train acc on all data:  0.996374184191
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 7.4871e-07
Norm of the params: 17.5985
     Influence (LOO): fixed 176 labels. Loss 0.10335. Accuracy 0.975.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0400768
Train loss (w/o reg) on all data: 0.0194779
Test loss (w/o reg) on all data: 0.156235
Train acc on all data:  0.999033115784
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.10202e-05
Norm of the params: 20.2972
                Loss: fixed 133 labels. Loss 0.15623. Accuracy 0.952.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0936995
Train loss (w/o reg) on all data: 0.0620415
Test loss (w/o reg) on all data: 0.268555
Train acc on all data:  0.987672226251
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 6.29751e-06
Norm of the params: 25.1627
              Random: fixed  21 labels. Loss 0.26855. Accuracy 0.930.
### Flips: 206, rs: 8, checks: 618
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0396668
Train loss (w/o reg) on all data: 0.0246996
Test loss (w/o reg) on all data: 0.095002
Train acc on all data:  0.996615905245
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 7.48714e-07
Norm of the params: 17.3015
     Influence (LOO): fixed 188 labels. Loss 0.09500. Accuracy 0.977.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0359813
Train loss (w/o reg) on all data: 0.0172966
Test loss (w/o reg) on all data: 0.206539
Train acc on all data:  0.999274836838
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 6.34124e-06
Norm of the params: 19.3312
                Loss: fixed 153 labels. Loss 0.20654. Accuracy 0.961.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0912921
Train loss (w/o reg) on all data: 0.0605696
Test loss (w/o reg) on all data: 0.208284
Train acc on all data:  0.987188784143
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.02052e-05
Norm of the params: 24.7881
              Random: fixed  32 labels. Loss 0.20828. Accuracy 0.933.
### Flips: 206, rs: 8, checks: 824
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0382392
Train loss (w/o reg) on all data: 0.0234066
Test loss (w/o reg) on all data: 0.0935099
Train acc on all data:  0.997099347353
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 2.95719e-06
Norm of the params: 17.2236
     Influence (LOO): fixed 193 labels. Loss 0.09351. Accuracy 0.979.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0331446
Train loss (w/o reg) on all data: 0.0158258
Test loss (w/o reg) on all data: 0.126981
Train acc on all data:  0.999516557892
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 1.801e-06
Norm of the params: 18.6112
                Loss: fixed 171 labels. Loss 0.12698. Accuracy 0.968.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0891681
Train loss (w/o reg) on all data: 0.0591505
Test loss (w/o reg) on all data: 0.209337
Train acc on all data:  0.986463620981
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 5.02618e-06
Norm of the params: 24.502
              Random: fixed  42 labels. Loss 0.20934. Accuracy 0.939.
### Flips: 206, rs: 8, checks: 1030
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0342292
Train loss (w/o reg) on all data: 0.0193655
Test loss (w/o reg) on all data: 0.0904944
Train acc on all data:  0.998066231569
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 1.11768e-06
Norm of the params: 17.2416
     Influence (LOO): fixed 197 labels. Loss 0.09049. Accuracy 0.980.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0321728
Train loss (w/o reg) on all data: 0.015397
Test loss (w/o reg) on all data: 0.154087
Train acc on all data:  0.999516557892
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 9.25585e-06
Norm of the params: 18.3171
                Loss: fixed 180 labels. Loss 0.15409. Accuracy 0.970.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0879814
Train loss (w/o reg) on all data: 0.0583944
Test loss (w/o reg) on all data: 0.19518
Train acc on all data:  0.986947063089
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 4.17536e-06
Norm of the params: 24.3257
              Random: fixed  48 labels. Loss 0.19518. Accuracy 0.942.
### Flips: 206, rs: 8, checks: 1236
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0341772
Train loss (w/o reg) on all data: 0.0193247
Test loss (w/o reg) on all data: 0.0898779
Train acc on all data:  0.998066231569
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 4.35906e-07
Norm of the params: 17.2352
     Influence (LOO): fixed 198 labels. Loss 0.08988. Accuracy 0.979.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0307746
Train loss (w/o reg) on all data: 0.0147544
Test loss (w/o reg) on all data: 0.170087
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 2.71194e-06
Norm of the params: 17.8998
                Loss: fixed 193 labels. Loss 0.17009. Accuracy 0.971.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084091
Train loss (w/o reg) on all data: 0.0555849
Test loss (w/o reg) on all data: 0.18547
Train acc on all data:  0.988155668359
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 1.33191e-05
Norm of the params: 23.8772
              Random: fixed  61 labels. Loss 0.18547. Accuracy 0.943.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104119
Train loss (w/o reg) on all data: 0.0719573
Test loss (w/o reg) on all data: 0.424938
Train acc on all data:  0.984288131496
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 4.192e-05
Norm of the params: 25.3619
Flipped loss: 0.42494. Accuracy: 0.929
### Flips: 206, rs: 9, checks: 206
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0550594
Train loss (w/o reg) on all data: 0.0364115
Test loss (w/o reg) on all data: 0.223367
Train acc on all data:  0.993473531545
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 1.5519e-06
Norm of the params: 19.3122
     Influence (LOO): fixed 137 labels. Loss 0.22337. Accuracy 0.962.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0497715
Train loss (w/o reg) on all data: 0.0247272
Test loss (w/o reg) on all data: 0.28116
Train acc on all data:  0.999516557892
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 1.54145e-05
Norm of the params: 22.3805
                Loss: fixed 104 labels. Loss 0.28116. Accuracy 0.950.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10227
Train loss (w/o reg) on all data: 0.0704077
Test loss (w/o reg) on all data: 0.442482
Train acc on all data:  0.985013294658
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 2.70501e-05
Norm of the params: 25.2437
              Random: fixed   6 labels. Loss 0.44248. Accuracy 0.929.
### Flips: 206, rs: 9, checks: 412
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0482391
Train loss (w/o reg) on all data: 0.0321501
Test loss (w/o reg) on all data: 0.19915
Train acc on all data:  0.993715252599
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.12101e-06
Norm of the params: 17.9382
     Influence (LOO): fixed 171 labels. Loss 0.19915. Accuracy 0.971.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0419817
Train loss (w/o reg) on all data: 0.0202339
Test loss (w/o reg) on all data: 0.217659
Train acc on all data:  0.999516557892
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 7.48407e-06
Norm of the params: 20.8556
                Loss: fixed 139 labels. Loss 0.21766. Accuracy 0.955.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0988712
Train loss (w/o reg) on all data: 0.0674006
Test loss (w/o reg) on all data: 0.419092
Train acc on all data:  0.98573845782
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 6.64335e-06
Norm of the params: 25.0881
              Random: fixed  18 labels. Loss 0.41909. Accuracy 0.933.
### Flips: 206, rs: 9, checks: 618
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0454604
Train loss (w/o reg) on all data: 0.0299256
Test loss (w/o reg) on all data: 0.189242
Train acc on all data:  0.994198694706
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.5372e-07
Norm of the params: 17.6266
     Influence (LOO): fixed 179 labels. Loss 0.18924. Accuracy 0.974.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037229
Train loss (w/o reg) on all data: 0.0178608
Test loss (w/o reg) on all data: 0.148757
Train acc on all data:  0.999516557892
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 9.33148e-06
Norm of the params: 19.6816
                Loss: fixed 161 labels. Loss 0.14876. Accuracy 0.962.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09631
Train loss (w/o reg) on all data: 0.0655473
Test loss (w/o reg) on all data: 0.397632
Train acc on all data:  0.985980178874
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 7.14191e-06
Norm of the params: 24.8043
              Random: fixed  28 labels. Loss 0.39763. Accuracy 0.932.
### Flips: 206, rs: 9, checks: 824
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044246
Train loss (w/o reg) on all data: 0.0291138
Test loss (w/o reg) on all data: 0.138817
Train acc on all data:  0.994198694706
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.23362e-07
Norm of the params: 17.3967
     Influence (LOO): fixed 184 labels. Loss 0.13882. Accuracy 0.975.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033446
Train loss (w/o reg) on all data: 0.0159917
Test loss (w/o reg) on all data: 0.160846
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 2.10215e-05
Norm of the params: 18.6838
                Loss: fixed 177 labels. Loss 0.16085. Accuracy 0.971.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0953143
Train loss (w/o reg) on all data: 0.0647192
Test loss (w/o reg) on all data: 0.382736
Train acc on all data:  0.986221899927
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.71199e-05
Norm of the params: 24.7366
              Random: fixed  34 labels. Loss 0.38274. Accuracy 0.934.
### Flips: 206, rs: 9, checks: 1030
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0421136
Train loss (w/o reg) on all data: 0.0269543
Test loss (w/o reg) on all data: 0.132396
Train acc on all data:  0.994923857868
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 3.9621e-07
Norm of the params: 17.4122
     Influence (LOO): fixed 188 labels. Loss 0.13240. Accuracy 0.975.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030864
Train loss (w/o reg) on all data: 0.0147994
Test loss (w/o reg) on all data: 0.159233
Train acc on all data:  0.999274836838
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 9.00895e-07
Norm of the params: 17.9246
                Loss: fixed 184 labels. Loss 0.15923. Accuracy 0.971.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0920977
Train loss (w/o reg) on all data: 0.061983
Test loss (w/o reg) on all data: 0.379937
Train acc on all data:  0.987430505197
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 8.11556e-06
Norm of the params: 24.5417
              Random: fixed  43 labels. Loss 0.37994. Accuracy 0.937.
### Flips: 206, rs: 9, checks: 1236
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0414445
Train loss (w/o reg) on all data: 0.0262816
Test loss (w/o reg) on all data: 0.136288
Train acc on all data:  0.995407299976
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.08828e-07
Norm of the params: 17.4143
     Influence (LOO): fixed 190 labels. Loss 0.13629. Accuracy 0.975.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0300588
Train loss (w/o reg) on all data: 0.0144084
Test loss (w/o reg) on all data: 0.140193
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.20261e-07
Norm of the params: 17.692
                Loss: fixed 191 labels. Loss 0.14019. Accuracy 0.975.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0879489
Train loss (w/o reg) on all data: 0.0588276
Test loss (w/o reg) on all data: 0.349541
Train acc on all data:  0.988155668359
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 7.06452e-06
Norm of the params: 24.1335
              Random: fixed  56 labels. Loss 0.34954. Accuracy 0.943.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106835
Train loss (w/o reg) on all data: 0.074244
Test loss (w/o reg) on all data: 0.315297
Train acc on all data:  0.983804689388
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 3.38392e-05
Norm of the params: 25.5307
Flipped loss: 0.31530. Accuracy: 0.933
### Flips: 206, rs: 10, checks: 206
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0513975
Train loss (w/o reg) on all data: 0.033006
Test loss (w/o reg) on all data: 0.205305
Train acc on all data:  0.995165578922
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 7.65145e-06
Norm of the params: 19.1789
     Influence (LOO): fixed 145 labels. Loss 0.20530. Accuracy 0.968.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0498673
Train loss (w/o reg) on all data: 0.0248567
Test loss (w/o reg) on all data: 0.240395
Train acc on all data:  0.999516557892
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 2.36569e-05
Norm of the params: 22.3654
                Loss: fixed 105 labels. Loss 0.24040. Accuracy 0.955.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10148
Train loss (w/o reg) on all data: 0.0693394
Test loss (w/o reg) on all data: 0.306245
Train acc on all data:  0.984771573604
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 8.87126e-06
Norm of the params: 25.3537
              Random: fixed  13 labels. Loss 0.30624. Accuracy 0.929.
### Flips: 206, rs: 10, checks: 412
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0467021
Train loss (w/o reg) on all data: 0.0302819
Test loss (w/o reg) on all data: 0.160824
Train acc on all data:  0.99564902103
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.26739e-06
Norm of the params: 18.1219
     Influence (LOO): fixed 167 labels. Loss 0.16082. Accuracy 0.971.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0363454
Train loss (w/o reg) on all data: 0.0172902
Test loss (w/o reg) on all data: 0.203386
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.76366e-06
Norm of the params: 19.5219
                Loss: fixed 152 labels. Loss 0.20339. Accuracy 0.971.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0971424
Train loss (w/o reg) on all data: 0.0661077
Test loss (w/o reg) on all data: 0.294685
Train acc on all data:  0.98573845782
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 1.83596e-05
Norm of the params: 24.9137
              Random: fixed  26 labels. Loss 0.29468. Accuracy 0.927.
### Flips: 206, rs: 10, checks: 618
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0445294
Train loss (w/o reg) on all data: 0.0286568
Test loss (w/o reg) on all data: 0.139121
Train acc on all data:  0.995890742084
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.60326e-06
Norm of the params: 17.8171
     Influence (LOO): fixed 176 labels. Loss 0.13912. Accuracy 0.972.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0349965
Train loss (w/o reg) on all data: 0.0166262
Test loss (w/o reg) on all data: 0.178972
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.16136e-05
Norm of the params: 19.1678
                Loss: fixed 160 labels. Loss 0.17897. Accuracy 0.972.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0954042
Train loss (w/o reg) on all data: 0.0648646
Test loss (w/o reg) on all data: 0.302971
Train acc on all data:  0.986463620981
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 2.23748e-05
Norm of the params: 24.7142
              Random: fixed  36 labels. Loss 0.30297. Accuracy 0.926.
### Flips: 206, rs: 10, checks: 824
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042639
Train loss (w/o reg) on all data: 0.0276604
Test loss (w/o reg) on all data: 0.117434
Train acc on all data:  0.995890742084
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.1257e-07
Norm of the params: 17.3081
     Influence (LOO): fixed 181 labels. Loss 0.11743. Accuracy 0.977.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0335005
Train loss (w/o reg) on all data: 0.0160637
Test loss (w/o reg) on all data: 0.181216
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.63812e-06
Norm of the params: 18.6745
                Loss: fixed 176 labels. Loss 0.18122. Accuracy 0.973.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0909401
Train loss (w/o reg) on all data: 0.0610928
Test loss (w/o reg) on all data: 0.316467
Train acc on all data:  0.988397389413
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.07945e-05
Norm of the params: 24.4325
              Random: fixed  49 labels. Loss 0.31647. Accuracy 0.935.
### Flips: 206, rs: 10, checks: 1030
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0418412
Train loss (w/o reg) on all data: 0.0273021
Test loss (w/o reg) on all data: 0.115877
Train acc on all data:  0.995890742084
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.39348e-07
Norm of the params: 17.0524
     Influence (LOO): fixed 186 labels. Loss 0.11588. Accuracy 0.975.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032209
Train loss (w/o reg) on all data: 0.0154747
Test loss (w/o reg) on all data: 0.111508
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.64039e-06
Norm of the params: 18.2944
                Loss: fixed 183 labels. Loss 0.11151. Accuracy 0.972.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0900651
Train loss (w/o reg) on all data: 0.0601733
Test loss (w/o reg) on all data: 0.315352
Train acc on all data:  0.98888083152
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 4.75543e-06
Norm of the params: 24.4507
              Random: fixed  52 labels. Loss 0.31535. Accuracy 0.933.
### Flips: 206, rs: 10, checks: 1236
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0387894
Train loss (w/o reg) on all data: 0.0240704
Test loss (w/o reg) on all data: 0.130057
Train acc on all data:  0.996857626299
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 9.69014e-07
Norm of the params: 17.1575
     Influence (LOO): fixed 193 labels. Loss 0.13006. Accuracy 0.977.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0315536
Train loss (w/o reg) on all data: 0.0151687
Test loss (w/o reg) on all data: 0.105455
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.26182e-07
Norm of the params: 18.1024
                Loss: fixed 186 labels. Loss 0.10546. Accuracy 0.975.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0865386
Train loss (w/o reg) on all data: 0.0570549
Test loss (w/o reg) on all data: 0.350065
Train acc on all data:  0.989605994682
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 6.66478e-06
Norm of the params: 24.2832
              Random: fixed  64 labels. Loss 0.35007. Accuracy 0.929.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108313
Train loss (w/o reg) on all data: 0.0765881
Test loss (w/o reg) on all data: 0.234277
Train acc on all data:  0.984046410442
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.34738e-05
Norm of the params: 25.1893
Flipped loss: 0.23428. Accuracy: 0.929
### Flips: 206, rs: 11, checks: 206
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0607756
Train loss (w/o reg) on all data: 0.0426205
Test loss (w/o reg) on all data: 0.133197
Train acc on all data:  0.991781484167
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.55708e-05
Norm of the params: 19.0552
     Influence (LOO): fixed 131 labels. Loss 0.13320. Accuracy 0.965.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050078
Train loss (w/o reg) on all data: 0.0247631
Test loss (w/o reg) on all data: 0.15362
Train acc on all data:  0.999274836838
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 7.47693e-06
Norm of the params: 22.5011
                Loss: fixed 101 labels. Loss 0.15362. Accuracy 0.957.
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104947
Train loss (w/o reg) on all data: 0.0736526
Test loss (w/o reg) on all data: 0.231354
Train acc on all data:  0.985013294658
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 5.97509e-05
Norm of the params: 25.0178
              Random: fixed  10 labels. Loss 0.23135. Accuracy 0.936.
### Flips: 206, rs: 11, checks: 412
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0513705
Train loss (w/o reg) on all data: 0.0362042
Test loss (w/o reg) on all data: 0.0988616
Train acc on all data:  0.992748368383
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.43847e-07
Norm of the params: 17.4162
     Influence (LOO): fixed 170 labels. Loss 0.09886. Accuracy 0.975.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0397627
Train loss (w/o reg) on all data: 0.0190298
Test loss (w/o reg) on all data: 0.136474
Train acc on all data:  0.999274836838
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 3.04484e-06
Norm of the params: 20.3631
                Loss: fixed 141 labels. Loss 0.13647. Accuracy 0.958.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101918
Train loss (w/o reg) on all data: 0.0716414
Test loss (w/o reg) on all data: 0.215184
Train acc on all data:  0.985255015712
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 2.36756e-05
Norm of the params: 24.6077
              Random: fixed  21 labels. Loss 0.21518. Accuracy 0.943.
### Flips: 206, rs: 11, checks: 618
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0496186
Train loss (w/o reg) on all data: 0.0347812
Test loss (w/o reg) on all data: 0.0977648
Train acc on all data:  0.993231810491
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.90003e-06
Norm of the params: 17.2263
     Influence (LOO): fixed 178 labels. Loss 0.09776. Accuracy 0.977.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036319
Train loss (w/o reg) on all data: 0.0173751
Test loss (w/o reg) on all data: 0.117745
Train acc on all data:  0.999516557892
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 3.84926e-06
Norm of the params: 19.4648
                Loss: fixed 159 labels. Loss 0.11774. Accuracy 0.966.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984902
Train loss (w/o reg) on all data: 0.0686321
Test loss (w/o reg) on all data: 0.202593
Train acc on all data:  0.986463620981
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 6.609e-06
Norm of the params: 24.4369
              Random: fixed  29 labels. Loss 0.20259. Accuracy 0.946.
### Flips: 206, rs: 11, checks: 824
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0490882
Train loss (w/o reg) on all data: 0.0344354
Test loss (w/o reg) on all data: 0.118181
Train acc on all data:  0.993231810491
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 9.24927e-07
Norm of the params: 17.1189
     Influence (LOO): fixed 180 labels. Loss 0.11818. Accuracy 0.976.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0328183
Train loss (w/o reg) on all data: 0.0156992
Test loss (w/o reg) on all data: 0.122737
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 4.29119e-06
Norm of the params: 18.5035
                Loss: fixed 178 labels. Loss 0.12274. Accuracy 0.974.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0965014
Train loss (w/o reg) on all data: 0.0674832
Test loss (w/o reg) on all data: 0.202503
Train acc on all data:  0.98573845782
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 1.12834e-05
Norm of the params: 24.0908
              Random: fixed  38 labels. Loss 0.20250. Accuracy 0.942.
### Flips: 206, rs: 11, checks: 1030
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0460208
Train loss (w/o reg) on all data: 0.0312238
Test loss (w/o reg) on all data: 0.116699
Train acc on all data:  0.99444041576
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.13609e-06
Norm of the params: 17.2029
     Influence (LOO): fixed 185 labels. Loss 0.11670. Accuracy 0.977.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0319055
Train loss (w/o reg) on all data: 0.0152738
Test loss (w/o reg) on all data: 0.13952
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.08795e-05
Norm of the params: 18.2383
                Loss: fixed 185 labels. Loss 0.13952. Accuracy 0.976.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0904184
Train loss (w/o reg) on all data: 0.0621615
Test loss (w/o reg) on all data: 0.193317
Train acc on all data:  0.987672226251
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 5.70472e-06
Norm of the params: 23.7726
              Random: fixed  51 labels. Loss 0.19332. Accuracy 0.946.
### Flips: 206, rs: 11, checks: 1236
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0453822
Train loss (w/o reg) on all data: 0.030571
Test loss (w/o reg) on all data: 0.114811
Train acc on all data:  0.994682136814
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.05208e-06
Norm of the params: 17.2112
     Influence (LOO): fixed 186 labels. Loss 0.11481. Accuracy 0.977.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0295413
Train loss (w/o reg) on all data: 0.0141946
Test loss (w/o reg) on all data: 0.157336
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 4.44457e-07
Norm of the params: 17.5195
                Loss: fixed 191 labels. Loss 0.15734. Accuracy 0.971.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883477
Train loss (w/o reg) on all data: 0.0604268
Test loss (w/o reg) on all data: 0.173273
Train acc on all data:  0.988397389413
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 1.27258e-05
Norm of the params: 23.6309
              Random: fixed  56 labels. Loss 0.17327. Accuracy 0.948.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107128
Train loss (w/o reg) on all data: 0.075053
Test loss (w/o reg) on all data: 0.220813
Train acc on all data:  0.983079526227
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 3.64745e-05
Norm of the params: 25.3279
Flipped loss: 0.22081. Accuracy: 0.930
### Flips: 206, rs: 12, checks: 206
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0532514
Train loss (w/o reg) on all data: 0.0358947
Test loss (w/o reg) on all data: 0.208973
Train acc on all data:  0.993715252599
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.68958e-06
Norm of the params: 18.6316
     Influence (LOO): fixed 135 labels. Loss 0.20897. Accuracy 0.968.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04779
Train loss (w/o reg) on all data: 0.023528
Test loss (w/o reg) on all data: 0.194819
Train acc on all data:  0.999274836838
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 9.43914e-06
Norm of the params: 22.0281
                Loss: fixed 109 labels. Loss 0.19482. Accuracy 0.954.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105121
Train loss (w/o reg) on all data: 0.0734078
Test loss (w/o reg) on all data: 0.223789
Train acc on all data:  0.983562968335
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 2.60539e-05
Norm of the params: 25.1848
              Random: fixed   9 labels. Loss 0.22379. Accuracy 0.936.
### Flips: 206, rs: 12, checks: 412
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0486549
Train loss (w/o reg) on all data: 0.0332397
Test loss (w/o reg) on all data: 0.106623
Train acc on all data:  0.993715252599
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.29072e-06
Norm of the params: 17.5586
     Influence (LOO): fixed 163 labels. Loss 0.10662. Accuracy 0.973.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0367384
Train loss (w/o reg) on all data: 0.0174487
Test loss (w/o reg) on all data: 0.191225
Train acc on all data:  0.999274836838
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.27065e-06
Norm of the params: 19.6416
                Loss: fixed 142 labels. Loss 0.19122. Accuracy 0.966.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101554
Train loss (w/o reg) on all data: 0.0700886
Test loss (w/o reg) on all data: 0.217061
Train acc on all data:  0.98452985255
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 4.59071e-06
Norm of the params: 25.0859
              Random: fixed  18 labels. Loss 0.21706. Accuracy 0.938.
### Flips: 206, rs: 12, checks: 618
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047108
Train loss (w/o reg) on all data: 0.0322449
Test loss (w/o reg) on all data: 0.117499
Train acc on all data:  0.993715252599
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 2.01698e-06
Norm of the params: 17.2413
     Influence (LOO): fixed 172 labels. Loss 0.11750. Accuracy 0.974.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0321273
Train loss (w/o reg) on all data: 0.0153053
Test loss (w/o reg) on all data: 0.185177
Train acc on all data:  0.999274836838
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.767e-06
Norm of the params: 18.3423
                Loss: fixed 171 labels. Loss 0.18518. Accuracy 0.971.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0952165
Train loss (w/o reg) on all data: 0.0648769
Test loss (w/o reg) on all data: 0.20216
Train acc on all data:  0.986463620981
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 8.0027e-06
Norm of the params: 24.6332
              Random: fixed  33 labels. Loss 0.20216. Accuracy 0.944.
### Flips: 206, rs: 12, checks: 824
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0461191
Train loss (w/o reg) on all data: 0.0312059
Test loss (w/o reg) on all data: 0.103401
Train acc on all data:  0.993956973652
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 4.88363e-07
Norm of the params: 17.2703
     Influence (LOO): fixed 175 labels. Loss 0.10340. Accuracy 0.975.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0313163
Train loss (w/o reg) on all data: 0.01494
Test loss (w/o reg) on all data: 0.181512
Train acc on all data:  0.999274836838
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.42943e-06
Norm of the params: 18.0977
                Loss: fixed 176 labels. Loss 0.18151. Accuracy 0.972.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0937766
Train loss (w/o reg) on all data: 0.0636752
Test loss (w/o reg) on all data: 0.194413
Train acc on all data:  0.986463620981
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 7.85135e-06
Norm of the params: 24.5363
              Random: fixed  37 labels. Loss 0.19441. Accuracy 0.947.
### Flips: 206, rs: 12, checks: 1030
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042366
Train loss (w/o reg) on all data: 0.0276596
Test loss (w/o reg) on all data: 0.141913
Train acc on all data:  0.995165578922
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 3.9737e-07
Norm of the params: 17.1502
     Influence (LOO): fixed 183 labels. Loss 0.14191. Accuracy 0.973.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0309231
Train loss (w/o reg) on all data: 0.0148683
Test loss (w/o reg) on all data: 0.107208
Train acc on all data:  0.999274836838
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.08175e-06
Norm of the params: 17.9192
                Loss: fixed 184 labels. Loss 0.10721. Accuracy 0.975.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888038
Train loss (w/o reg) on all data: 0.0596361
Test loss (w/o reg) on all data: 0.185551
Train acc on all data:  0.987913947305
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 7.65621e-06
Norm of the params: 24.1527
              Random: fixed  52 labels. Loss 0.18555. Accuracy 0.950.
### Flips: 206, rs: 12, checks: 1236
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0405109
Train loss (w/o reg) on all data: 0.0257562
Test loss (w/o reg) on all data: 0.148592
Train acc on all data:  0.995890742084
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 4.47162e-06
Norm of the params: 17.1783
     Influence (LOO): fixed 187 labels. Loss 0.14859. Accuracy 0.974.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0306138
Train loss (w/o reg) on all data: 0.0147186
Test loss (w/o reg) on all data: 0.125104
Train acc on all data:  0.999274836838
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.03664e-06
Norm of the params: 17.8298
                Loss: fixed 187 labels. Loss 0.12510. Accuracy 0.974.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0845401
Train loss (w/o reg) on all data: 0.0559423
Test loss (w/o reg) on all data: 0.177341
Train acc on all data:  0.98888083152
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 1.96137e-05
Norm of the params: 23.9156
              Random: fixed  61 labels. Loss 0.17734. Accuracy 0.951.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105792
Train loss (w/o reg) on all data: 0.0732624
Test loss (w/o reg) on all data: 0.403446
Train acc on all data:  0.98452985255
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 9.40004e-06
Norm of the params: 25.5067
Flipped loss: 0.40345. Accuracy: 0.936
### Flips: 206, rs: 13, checks: 206
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0501726
Train loss (w/o reg) on all data: 0.0327728
Test loss (w/o reg) on all data: 0.18703
Train acc on all data:  0.99444041576
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 1.4256e-06
Norm of the params: 18.6546
     Influence (LOO): fixed 144 labels. Loss 0.18703. Accuracy 0.970.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049318
Train loss (w/o reg) on all data: 0.0244938
Test loss (w/o reg) on all data: 0.178472
Train acc on all data:  0.999516557892
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 3.55321e-06
Norm of the params: 22.2819
                Loss: fixed 109 labels. Loss 0.17847. Accuracy 0.957.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103275
Train loss (w/o reg) on all data: 0.0711189
Test loss (w/o reg) on all data: 0.411316
Train acc on all data:  0.984288131496
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 4.78216e-06
Norm of the params: 25.3599
              Random: fixed   8 labels. Loss 0.41132. Accuracy 0.933.
### Flips: 206, rs: 13, checks: 412
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0461112
Train loss (w/o reg) on all data: 0.0299454
Test loss (w/o reg) on all data: 0.136351
Train acc on all data:  0.995407299976
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.20644e-06
Norm of the params: 17.981
     Influence (LOO): fixed 167 labels. Loss 0.13635. Accuracy 0.976.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0409648
Train loss (w/o reg) on all data: 0.0197934
Test loss (w/o reg) on all data: 0.152567
Train acc on all data:  0.999516557892
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.15615e-06
Norm of the params: 20.5774
                Loss: fixed 141 labels. Loss 0.15257. Accuracy 0.970.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101669
Train loss (w/o reg) on all data: 0.0699151
Test loss (w/o reg) on all data: 0.422919
Train acc on all data:  0.985255015712
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 3.91654e-06
Norm of the params: 25.2008
              Random: fixed  18 labels. Loss 0.42292. Accuracy 0.940.
### Flips: 206, rs: 13, checks: 618
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0437778
Train loss (w/o reg) on all data: 0.0281537
Test loss (w/o reg) on all data: 0.141459
Train acc on all data:  0.995407299976
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 8.83742e-07
Norm of the params: 17.6772
     Influence (LOO): fixed 178 labels. Loss 0.14146. Accuracy 0.973.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0354277
Train loss (w/o reg) on all data: 0.0170112
Test loss (w/o reg) on all data: 0.14463
Train acc on all data:  0.999516557892
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 6.46851e-06
Norm of the params: 19.1919
                Loss: fixed 163 labels. Loss 0.14463. Accuracy 0.978.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0983118
Train loss (w/o reg) on all data: 0.0672215
Test loss (w/o reg) on all data: 0.396232
Train acc on all data:  0.985980178874
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 3.49418e-06
Norm of the params: 24.936
              Random: fixed  28 labels. Loss 0.39623. Accuracy 0.938.
### Flips: 206, rs: 13, checks: 824
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0407722
Train loss (w/o reg) on all data: 0.0255698
Test loss (w/o reg) on all data: 0.135229
Train acc on all data:  0.996132463138
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 7.68848e-07
Norm of the params: 17.437
     Influence (LOO): fixed 187 labels. Loss 0.13523. Accuracy 0.976.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033462
Train loss (w/o reg) on all data: 0.0160963
Test loss (w/o reg) on all data: 0.15279
Train acc on all data:  0.999516557892
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 3.30987e-06
Norm of the params: 18.6364
                Loss: fixed 177 labels. Loss 0.15279. Accuracy 0.977.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0942379
Train loss (w/o reg) on all data: 0.0637411
Test loss (w/o reg) on all data: 0.381079
Train acc on all data:  0.987188784143
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.15929e-05
Norm of the params: 24.6969
              Random: fixed  39 labels. Loss 0.38108. Accuracy 0.935.
### Flips: 206, rs: 13, checks: 1030
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0378088
Train loss (w/o reg) on all data: 0.022953
Test loss (w/o reg) on all data: 0.145987
Train acc on all data:  0.996857626299
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 8.62138e-07
Norm of the params: 17.2371
     Influence (LOO): fixed 193 labels. Loss 0.14599. Accuracy 0.975.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0317527
Train loss (w/o reg) on all data: 0.0152793
Test loss (w/o reg) on all data: 0.15417
Train acc on all data:  0.999516557892
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 5.38342e-07
Norm of the params: 18.1513
                Loss: fixed 184 labels. Loss 0.15417. Accuracy 0.978.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0900532
Train loss (w/o reg) on all data: 0.0603527
Test loss (w/o reg) on all data: 0.347888
Train acc on all data:  0.988155668359
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 1.10895e-05
Norm of the params: 24.3723
              Random: fixed  49 labels. Loss 0.34789. Accuracy 0.944.
### Flips: 206, rs: 13, checks: 1236
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0358844
Train loss (w/o reg) on all data: 0.0211344
Test loss (w/o reg) on all data: 0.147397
Train acc on all data:  0.997341068407
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.76662e-07
Norm of the params: 17.1755
     Influence (LOO): fixed 196 labels. Loss 0.14740. Accuracy 0.977.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0306607
Train loss (w/o reg) on all data: 0.0147264
Test loss (w/o reg) on all data: 0.156671
Train acc on all data:  0.999516557892
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 1.32504e-06
Norm of the params: 17.8517
                Loss: fixed 190 labels. Loss 0.15667. Accuracy 0.980.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0865569
Train loss (w/o reg) on all data: 0.0573921
Test loss (w/o reg) on all data: 0.330842
Train acc on all data:  0.989605994682
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 1.09328e-05
Norm of the params: 24.1515
              Random: fixed  57 labels. Loss 0.33084. Accuracy 0.944.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11183
Train loss (w/o reg) on all data: 0.0790811
Test loss (w/o reg) on all data: 0.431909
Train acc on all data:  0.980904036742
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 6.63701e-06
Norm of the params: 25.5924
Flipped loss: 0.43191. Accuracy: 0.936
### Flips: 206, rs: 14, checks: 206
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0534608
Train loss (w/o reg) on all data: 0.0355836
Test loss (w/o reg) on all data: 0.189367
Train acc on all data:  0.993231810491
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.33347e-06
Norm of the params: 18.9089
     Influence (LOO): fixed 142 labels. Loss 0.18937. Accuracy 0.969.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049308
Train loss (w/o reg) on all data: 0.0243041
Test loss (w/o reg) on all data: 0.238888
Train acc on all data:  0.999516557892
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 1.06595e-05
Norm of the params: 22.3624
                Loss: fixed 105 labels. Loss 0.23889. Accuracy 0.953.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109494
Train loss (w/o reg) on all data: 0.076955
Test loss (w/o reg) on all data: 0.436399
Train acc on all data:  0.981870920957
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 4.30472e-06
Norm of the params: 25.5103
              Random: fixed   7 labels. Loss 0.43640. Accuracy 0.938.
### Flips: 206, rs: 14, checks: 412
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0491126
Train loss (w/o reg) on all data: 0.0329118
Test loss (w/o reg) on all data: 0.198113
Train acc on all data:  0.994198694706
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.46262e-06
Norm of the params: 18.0004
     Influence (LOO): fixed 167 labels. Loss 0.19811. Accuracy 0.974.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0412025
Train loss (w/o reg) on all data: 0.0198463
Test loss (w/o reg) on all data: 0.275709
Train acc on all data:  0.999274836838
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 7.50548e-06
Norm of the params: 20.667
                Loss: fixed 137 labels. Loss 0.27571. Accuracy 0.961.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105329
Train loss (w/o reg) on all data: 0.0736715
Test loss (w/o reg) on all data: 0.412324
Train acc on all data:  0.983804689388
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 2.76175e-05
Norm of the params: 25.1625
              Random: fixed  19 labels. Loss 0.41232. Accuracy 0.940.
### Flips: 206, rs: 14, checks: 618
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0439246
Train loss (w/o reg) on all data: 0.0286194
Test loss (w/o reg) on all data: 0.118701
Train acc on all data:  0.995165578922
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 5.07035e-07
Norm of the params: 17.4959
     Influence (LOO): fixed 178 labels. Loss 0.11870. Accuracy 0.973.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0356937
Train loss (w/o reg) on all data: 0.0171789
Test loss (w/o reg) on all data: 0.176801
Train acc on all data:  0.999516557892
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.73949e-06
Norm of the params: 19.2431
                Loss: fixed 160 labels. Loss 0.17680. Accuracy 0.969.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102915
Train loss (w/o reg) on all data: 0.071599
Test loss (w/o reg) on all data: 0.395596
Train acc on all data:  0.98452985255
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 2.1679e-05
Norm of the params: 25.0264
              Random: fixed  27 labels. Loss 0.39560. Accuracy 0.936.
### Flips: 206, rs: 14, checks: 824
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0424647
Train loss (w/o reg) on all data: 0.0274433
Test loss (w/o reg) on all data: 0.162081
Train acc on all data:  0.995407299976
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 7.07174e-07
Norm of the params: 17.3328
     Influence (LOO): fixed 184 labels. Loss 0.16208. Accuracy 0.974.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033866
Train loss (w/o reg) on all data: 0.016229
Test loss (w/o reg) on all data: 0.106728
Train acc on all data:  0.999274836838
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 5.09334e-06
Norm of the params: 18.7814
                Loss: fixed 170 labels. Loss 0.10673. Accuracy 0.974.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0997976
Train loss (w/o reg) on all data: 0.0688196
Test loss (w/o reg) on all data: 0.407868
Train acc on all data:  0.985013294658
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.44131e-05
Norm of the params: 24.8909
              Random: fixed  39 labels. Loss 0.40787. Accuracy 0.944.
### Flips: 206, rs: 14, checks: 1030
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0420447
Train loss (w/o reg) on all data: 0.0270801
Test loss (w/o reg) on all data: 0.158525
Train acc on all data:  0.99564902103
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.94013e-07
Norm of the params: 17.3
     Influence (LOO): fixed 186 labels. Loss 0.15852. Accuracy 0.975.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0323394
Train loss (w/o reg) on all data: 0.0154796
Test loss (w/o reg) on all data: 0.139266
Train acc on all data:  0.999274836838
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.41667e-06
Norm of the params: 18.3629
                Loss: fixed 175 labels. Loss 0.13927. Accuracy 0.973.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0975411
Train loss (w/o reg) on all data: 0.0673793
Test loss (w/o reg) on all data: 0.414811
Train acc on all data:  0.985255015712
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 4.70155e-06
Norm of the params: 24.5609
              Random: fixed  48 labels. Loss 0.41481. Accuracy 0.950.
### Flips: 206, rs: 14, checks: 1236
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0402344
Train loss (w/o reg) on all data: 0.0253168
Test loss (w/o reg) on all data: 0.165146
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.60113e-07
Norm of the params: 17.2729
     Influence (LOO): fixed 189 labels. Loss 0.16515. Accuracy 0.975.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0314192
Train loss (w/o reg) on all data: 0.0150301
Test loss (w/o reg) on all data: 0.13285
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.18506e-06
Norm of the params: 18.1048
                Loss: fixed 181 labels. Loss 0.13285. Accuracy 0.972.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0929248
Train loss (w/o reg) on all data: 0.0635542
Test loss (w/o reg) on all data: 0.438909
Train acc on all data:  0.987188784143
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 4.60222e-06
Norm of the params: 24.2366
              Random: fixed  59 labels. Loss 0.43891. Accuracy 0.949.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0973712
Train loss (w/o reg) on all data: 0.0666811
Test loss (w/o reg) on all data: 0.311801
Train acc on all data:  0.987430505197
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 7.05996e-06
Norm of the params: 24.7751
Flipped loss: 0.31180. Accuracy: 0.929
### Flips: 206, rs: 15, checks: 206
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0469576
Train loss (w/o reg) on all data: 0.0307389
Test loss (w/o reg) on all data: 0.169831
Train acc on all data:  0.995407299976
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.37274e-06
Norm of the params: 18.0104
     Influence (LOO): fixed 154 labels. Loss 0.16983. Accuracy 0.970.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0446925
Train loss (w/o reg) on all data: 0.0218398
Test loss (w/o reg) on all data: 0.159176
Train acc on all data:  0.999516557892
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 1.29134e-05
Norm of the params: 21.3788
                Loss: fixed 110 labels. Loss 0.15918. Accuracy 0.959.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0954284
Train loss (w/o reg) on all data: 0.0651264
Test loss (w/o reg) on all data: 0.33969
Train acc on all data:  0.987913947305
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 8.53153e-06
Norm of the params: 24.6179
              Random: fixed   7 labels. Loss 0.33969. Accuracy 0.930.
### Flips: 206, rs: 15, checks: 412
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0443712
Train loss (w/o reg) on all data: 0.0293791
Test loss (w/o reg) on all data: 0.157611
Train acc on all data:  0.995165578922
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 6.44259e-07
Norm of the params: 17.316
     Influence (LOO): fixed 173 labels. Loss 0.15761. Accuracy 0.972.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036932
Train loss (w/o reg) on all data: 0.017591
Test loss (w/o reg) on all data: 0.16202
Train acc on all data:  0.999516557892
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 4.99083e-06
Norm of the params: 19.6677
                Loss: fixed 143 labels. Loss 0.16202. Accuracy 0.961.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0923556
Train loss (w/o reg) on all data: 0.0629554
Test loss (w/o reg) on all data: 0.295225
Train acc on all data:  0.988397389413
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 4.47691e-06
Norm of the params: 24.2488
              Random: fixed  23 labels. Loss 0.29522. Accuracy 0.941.
### Flips: 206, rs: 15, checks: 618
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0427694
Train loss (w/o reg) on all data: 0.0278909
Test loss (w/o reg) on all data: 0.146069
Train acc on all data:  0.995407299976
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.98088e-07
Norm of the params: 17.2502
     Influence (LOO): fixed 180 labels. Loss 0.14607. Accuracy 0.975.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0346975
Train loss (w/o reg) on all data: 0.0165655
Test loss (w/o reg) on all data: 0.141834
Train acc on all data:  0.999516557892
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 2.77648e-06
Norm of the params: 19.0431
                Loss: fixed 159 labels. Loss 0.14183. Accuracy 0.966.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0898489
Train loss (w/o reg) on all data: 0.0611695
Test loss (w/o reg) on all data: 0.255945
Train acc on all data:  0.988639110467
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 4.83081e-06
Norm of the params: 23.9497
              Random: fixed  31 labels. Loss 0.25594. Accuracy 0.938.
### Flips: 206, rs: 15, checks: 824
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0410354
Train loss (w/o reg) on all data: 0.0260796
Test loss (w/o reg) on all data: 0.175699
Train acc on all data:  0.995890742084
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 4.29151e-07
Norm of the params: 17.295
     Influence (LOO): fixed 183 labels. Loss 0.17570. Accuracy 0.974.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0322687
Train loss (w/o reg) on all data: 0.0154149
Test loss (w/o reg) on all data: 0.122943
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 6.72632e-07
Norm of the params: 18.3596
                Loss: fixed 170 labels. Loss 0.12294. Accuracy 0.976.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0862306
Train loss (w/o reg) on all data: 0.0584032
Test loss (w/o reg) on all data: 0.254376
Train acc on all data:  0.989605994682
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 4.36256e-06
Norm of the params: 23.5913
              Random: fixed  42 labels. Loss 0.25438. Accuracy 0.940.
### Flips: 206, rs: 15, checks: 1030
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0404366
Train loss (w/o reg) on all data: 0.0254475
Test loss (w/o reg) on all data: 0.173299
Train acc on all data:  0.996615905245
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.16631e-06
Norm of the params: 17.3142
     Influence (LOO): fixed 186 labels. Loss 0.17330. Accuracy 0.974.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0314212
Train loss (w/o reg) on all data: 0.01503
Test loss (w/o reg) on all data: 0.117535
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.08025e-06
Norm of the params: 18.1059
                Loss: fixed 176 labels. Loss 0.11754. Accuracy 0.976.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0836047
Train loss (w/o reg) on all data: 0.0560223
Test loss (w/o reg) on all data: 0.239876
Train acc on all data:  0.99008943679
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 6.0711e-06
Norm of the params: 23.4872
              Random: fixed  47 labels. Loss 0.23988. Accuracy 0.939.
### Flips: 206, rs: 15, checks: 1236
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0365872
Train loss (w/o reg) on all data: 0.0215829
Test loss (w/o reg) on all data: 0.166845
Train acc on all data:  0.997099347353
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 6.39313e-07
Norm of the params: 17.323
     Influence (LOO): fixed 191 labels. Loss 0.16684. Accuracy 0.974.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0313004
Train loss (w/o reg) on all data: 0.0149869
Test loss (w/o reg) on all data: 0.112018
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 4.59271e-06
Norm of the params: 18.063
                Loss: fixed 178 labels. Loss 0.11202. Accuracy 0.975.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0808753
Train loss (w/o reg) on all data: 0.0540601
Test loss (w/o reg) on all data: 0.235439
Train acc on all data:  0.991056321006
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 9.38424e-06
Norm of the params: 23.1582
              Random: fixed  57 labels. Loss 0.23544. Accuracy 0.949.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104254
Train loss (w/o reg) on all data: 0.0719186
Test loss (w/o reg) on all data: 0.392172
Train acc on all data:  0.982596084119
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 1.00452e-05
Norm of the params: 25.4305
Flipped loss: 0.39217. Accuracy: 0.921
### Flips: 206, rs: 16, checks: 206
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0538834
Train loss (w/o reg) on all data: 0.0359951
Test loss (w/o reg) on all data: 0.259518
Train acc on all data:  0.992748368383
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.182e-06
Norm of the params: 18.9147
     Influence (LOO): fixed 132 labels. Loss 0.25952. Accuracy 0.967.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0478738
Train loss (w/o reg) on all data: 0.0236715
Test loss (w/o reg) on all data: 0.225486
Train acc on all data:  0.999274836838
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 4.11994e-06
Norm of the params: 22.0011
                Loss: fixed 106 labels. Loss 0.22549. Accuracy 0.956.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101955
Train loss (w/o reg) on all data: 0.0702555
Test loss (w/o reg) on all data: 0.3076
Train acc on all data:  0.983804689388
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 7.53501e-06
Norm of the params: 25.1793
              Random: fixed  10 labels. Loss 0.30760. Accuracy 0.921.
### Flips: 206, rs: 16, checks: 412
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0500479
Train loss (w/o reg) on all data: 0.0335416
Test loss (w/o reg) on all data: 0.253612
Train acc on all data:  0.993473531545
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 5.79695e-07
Norm of the params: 18.1694
     Influence (LOO): fixed 158 labels. Loss 0.25361. Accuracy 0.969.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0373787
Train loss (w/o reg) on all data: 0.0180258
Test loss (w/o reg) on all data: 0.185397
Train acc on all data:  0.999274836838
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 3.89897e-06
Norm of the params: 19.6738
                Loss: fixed 142 labels. Loss 0.18540. Accuracy 0.961.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100051
Train loss (w/o reg) on all data: 0.0686119
Test loss (w/o reg) on all data: 0.311871
Train acc on all data:  0.984046410442
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 5.06027e-06
Norm of the params: 25.0754
              Random: fixed  18 labels. Loss 0.31187. Accuracy 0.925.
### Flips: 206, rs: 16, checks: 618
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0475754
Train loss (w/o reg) on all data: 0.0317064
Test loss (w/o reg) on all data: 0.163729
Train acc on all data:  0.993956973652
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 8.24389e-07
Norm of the params: 17.8152
     Influence (LOO): fixed 167 labels. Loss 0.16373. Accuracy 0.971.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0360037
Train loss (w/o reg) on all data: 0.0173817
Test loss (w/o reg) on all data: 0.204572
Train acc on all data:  0.999033115784
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.08472e-06
Norm of the params: 19.2987
                Loss: fixed 156 labels. Loss 0.20457. Accuracy 0.965.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0975823
Train loss (w/o reg) on all data: 0.0669419
Test loss (w/o reg) on all data: 0.302657
Train acc on all data:  0.98452985255
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 6.57535e-06
Norm of the params: 24.755
              Random: fixed  28 labels. Loss 0.30266. Accuracy 0.929.
### Flips: 206, rs: 16, checks: 824
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0437851
Train loss (w/o reg) on all data: 0.0289853
Test loss (w/o reg) on all data: 0.0860539
Train acc on all data:  0.994682136814
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 1.22574e-06
Norm of the params: 17.2045
     Influence (LOO): fixed 177 labels. Loss 0.08605. Accuracy 0.978.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0338413
Train loss (w/o reg) on all data: 0.0162926
Test loss (w/o reg) on all data: 0.173254
Train acc on all data:  0.999274836838
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 2.73252e-06
Norm of the params: 18.7343
                Loss: fixed 171 labels. Loss 0.17325. Accuracy 0.971.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0941503
Train loss (w/o reg) on all data: 0.0637477
Test loss (w/o reg) on all data: 0.29895
Train acc on all data:  0.985496736766
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 8.10886e-06
Norm of the params: 24.6587
              Random: fixed  35 labels. Loss 0.29895. Accuracy 0.931.
### Flips: 206, rs: 16, checks: 1030
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042586
Train loss (w/o reg) on all data: 0.0277281
Test loss (w/o reg) on all data: 0.0857615
Train acc on all data:  0.995165578922
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.24959e-07
Norm of the params: 17.2383
     Influence (LOO): fixed 180 labels. Loss 0.08576. Accuracy 0.977.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032605
Train loss (w/o reg) on all data: 0.0157078
Test loss (w/o reg) on all data: 0.138344
Train acc on all data:  0.999274836838
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.0943e-06
Norm of the params: 18.3833
                Loss: fixed 179 labels. Loss 0.13834. Accuracy 0.976.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0919723
Train loss (w/o reg) on all data: 0.0619075
Test loss (w/o reg) on all data: 0.308098
Train acc on all data:  0.986947063089
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 5.15724e-06
Norm of the params: 24.5213
              Random: fixed  44 labels. Loss 0.30810. Accuracy 0.928.
### Flips: 206, rs: 16, checks: 1236
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0409722
Train loss (w/o reg) on all data: 0.0262405
Test loss (w/o reg) on all data: 0.0831382
Train acc on all data:  0.99564902103
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.89283e-06
Norm of the params: 17.1649
     Influence (LOO): fixed 185 labels. Loss 0.08314. Accuracy 0.976.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0301653
Train loss (w/o reg) on all data: 0.0144333
Test loss (w/o reg) on all data: 0.120555
Train acc on all data:  0.999516557892
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.05295e-07
Norm of the params: 17.7381
                Loss: fixed 185 labels. Loss 0.12056. Accuracy 0.977.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0885545
Train loss (w/o reg) on all data: 0.0591665
Test loss (w/o reg) on all data: 0.297651
Train acc on all data:  0.987430505197
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 5.83823e-06
Norm of the params: 24.2438
              Random: fixed  54 labels. Loss 0.29765. Accuracy 0.932.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107051
Train loss (w/o reg) on all data: 0.0755607
Test loss (w/o reg) on all data: 0.306264
Train acc on all data:  0.981629199903
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 2.57387e-05
Norm of the params: 25.0961
Flipped loss: 0.30626. Accuracy: 0.923
### Flips: 206, rs: 17, checks: 206
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0584371
Train loss (w/o reg) on all data: 0.0407339
Test loss (w/o reg) on all data: 0.156403
Train acc on all data:  0.991056321006
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 5.87487e-06
Norm of the params: 18.8166
     Influence (LOO): fixed 137 labels. Loss 0.15640. Accuracy 0.974.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0497759
Train loss (w/o reg) on all data: 0.0246767
Test loss (w/o reg) on all data: 0.214274
Train acc on all data:  0.999274836838
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 2.46506e-06
Norm of the params: 22.405
                Loss: fixed 109 labels. Loss 0.21427. Accuracy 0.952.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103744
Train loss (w/o reg) on all data: 0.0729513
Test loss (w/o reg) on all data: 0.301933
Train acc on all data:  0.983562968335
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.00163e-05
Norm of the params: 24.8165
              Random: fixed  12 labels. Loss 0.30193. Accuracy 0.930.
### Flips: 206, rs: 17, checks: 412
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0513978
Train loss (w/o reg) on all data: 0.0358733
Test loss (w/o reg) on all data: 0.120848
Train acc on all data:  0.992748368383
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 7.17511e-07
Norm of the params: 17.6207
     Influence (LOO): fixed 168 labels. Loss 0.12085. Accuracy 0.978.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037793
Train loss (w/o reg) on all data: 0.0182951
Test loss (w/o reg) on all data: 0.204301
Train acc on all data:  0.999274836838
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 3.32223e-06
Norm of the params: 19.7473
                Loss: fixed 154 labels. Loss 0.20430. Accuracy 0.958.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100296
Train loss (w/o reg) on all data: 0.0697212
Test loss (w/o reg) on all data: 0.299609
Train acc on all data:  0.984288131496
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.17547e-05
Norm of the params: 24.7284
              Random: fixed  19 labels. Loss 0.29961. Accuracy 0.931.
### Flips: 206, rs: 17, checks: 618
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0495395
Train loss (w/o reg) on all data: 0.0344257
Test loss (w/o reg) on all data: 0.124972
Train acc on all data:  0.992990089437
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.48147e-07
Norm of the params: 17.3861
     Influence (LOO): fixed 177 labels. Loss 0.12497. Accuracy 0.975.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034794
Train loss (w/o reg) on all data: 0.0168575
Test loss (w/o reg) on all data: 0.158955
Train acc on all data:  0.999274836838
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.89145e-06
Norm of the params: 18.9402
                Loss: fixed 169 labels. Loss 0.15895. Accuracy 0.965.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984802
Train loss (w/o reg) on all data: 0.0684762
Test loss (w/o reg) on all data: 0.279973
Train acc on all data:  0.985255015712
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.19369e-05
Norm of the params: 24.4965
              Random: fixed  28 labels. Loss 0.27997. Accuracy 0.935.
### Flips: 206, rs: 17, checks: 824
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0445424
Train loss (w/o reg) on all data: 0.0297207
Test loss (w/o reg) on all data: 0.114777
Train acc on all data:  0.994682136814
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.7315e-07
Norm of the params: 17.2173
     Influence (LOO): fixed 187 labels. Loss 0.11478. Accuracy 0.977.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0328752
Train loss (w/o reg) on all data: 0.0158389
Test loss (w/o reg) on all data: 0.169294
Train acc on all data:  0.999274836838
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 3.38636e-06
Norm of the params: 18.4587
                Loss: fixed 179 labels. Loss 0.16929. Accuracy 0.964.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0953094
Train loss (w/o reg) on all data: 0.065516
Test loss (w/o reg) on all data: 0.270336
Train acc on all data:  0.986221899927
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 6.29097e-06
Norm of the params: 24.4104
              Random: fixed  36 labels. Loss 0.27034. Accuracy 0.929.
### Flips: 206, rs: 17, checks: 1030
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0425064
Train loss (w/o reg) on all data: 0.0274499
Test loss (w/o reg) on all data: 0.111975
Train acc on all data:  0.995407299976
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.64847e-07
Norm of the params: 17.3531
     Influence (LOO): fixed 190 labels. Loss 0.11197. Accuracy 0.977.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0317091
Train loss (w/o reg) on all data: 0.0152804
Test loss (w/o reg) on all data: 0.170038
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.34132e-05
Norm of the params: 18.1266
                Loss: fixed 187 labels. Loss 0.17004. Accuracy 0.973.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090416
Train loss (w/o reg) on all data: 0.060919
Test loss (w/o reg) on all data: 0.259942
Train acc on all data:  0.988155668359
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 5.68015e-06
Norm of the params: 24.2887
              Random: fixed  46 labels. Loss 0.25994. Accuracy 0.940.
### Flips: 206, rs: 17, checks: 1236
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040688
Train loss (w/o reg) on all data: 0.0256863
Test loss (w/o reg) on all data: 0.10984
Train acc on all data:  0.995890742084
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.77541e-07
Norm of the params: 17.3215
     Influence (LOO): fixed 192 labels. Loss 0.10984. Accuracy 0.977.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0307612
Train loss (w/o reg) on all data: 0.0147715
Test loss (w/o reg) on all data: 0.157928
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.98759e-06
Norm of the params: 17.8828
                Loss: fixed 195 labels. Loss 0.15793. Accuracy 0.976.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0865791
Train loss (w/o reg) on all data: 0.058141
Test loss (w/o reg) on all data: 0.240926
Train acc on all data:  0.987913947305
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 4.59924e-06
Norm of the params: 23.8487
              Random: fixed  58 labels. Loss 0.24093. Accuracy 0.944.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0983535
Train loss (w/o reg) on all data: 0.0675359
Test loss (w/o reg) on all data: 0.344528
Train acc on all data:  0.986463620981
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 2.35784e-05
Norm of the params: 24.8264
Flipped loss: 0.34453. Accuracy: 0.935
### Flips: 206, rs: 18, checks: 206
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0550642
Train loss (w/o reg) on all data: 0.0378864
Test loss (w/o reg) on all data: 0.209539
Train acc on all data:  0.992023205221
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 7.19764e-06
Norm of the params: 18.5352
     Influence (LOO): fixed 131 labels. Loss 0.20954. Accuracy 0.978.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0493589
Train loss (w/o reg) on all data: 0.0242896
Test loss (w/o reg) on all data: 0.206237
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 3.61655e-06
Norm of the params: 22.3917
                Loss: fixed 102 labels. Loss 0.20624. Accuracy 0.951.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09486
Train loss (w/o reg) on all data: 0.0644351
Test loss (w/o reg) on all data: 0.32961
Train acc on all data:  0.987430505197
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 8.692e-06
Norm of the params: 24.6677
              Random: fixed  14 labels. Loss 0.32961. Accuracy 0.934.
### Flips: 206, rs: 18, checks: 412
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0495766
Train loss (w/o reg) on all data: 0.0339418
Test loss (w/o reg) on all data: 0.149677
Train acc on all data:  0.992990089437
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.3878e-05
Norm of the params: 17.6832
     Influence (LOO): fixed 162 labels. Loss 0.14968. Accuracy 0.975.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0383005
Train loss (w/o reg) on all data: 0.0183002
Test loss (w/o reg) on all data: 0.217972
Train acc on all data:  0.999274836838
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 3.79284e-06
Norm of the params: 20.0001
                Loss: fixed 144 labels. Loss 0.21797. Accuracy 0.963.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0929494
Train loss (w/o reg) on all data: 0.0630309
Test loss (w/o reg) on all data: 0.314926
Train acc on all data:  0.987672226251
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 4.55369e-06
Norm of the params: 24.4616
              Random: fixed  23 labels. Loss 0.31493. Accuracy 0.937.
### Flips: 206, rs: 18, checks: 618
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04709
Train loss (w/o reg) on all data: 0.0318755
Test loss (w/o reg) on all data: 0.132524
Train acc on all data:  0.993473531545
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 2.22781e-06
Norm of the params: 17.444
     Influence (LOO): fixed 171 labels. Loss 0.13252. Accuracy 0.978.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0360286
Train loss (w/o reg) on all data: 0.0172318
Test loss (w/o reg) on all data: 0.207198
Train acc on all data:  0.999274836838
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 3.90519e-06
Norm of the params: 19.3891
                Loss: fixed 158 labels. Loss 0.20720. Accuracy 0.967.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0908119
Train loss (w/o reg) on all data: 0.06171
Test loss (w/o reg) on all data: 0.289737
Train acc on all data:  0.987913947305
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 7.47919e-06
Norm of the params: 24.1255
              Random: fixed  34 labels. Loss 0.28974. Accuracy 0.940.
### Flips: 206, rs: 18, checks: 824
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0460692
Train loss (w/o reg) on all data: 0.0309098
Test loss (w/o reg) on all data: 0.122211
Train acc on all data:  0.994198694706
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.50434e-06
Norm of the params: 17.4123
     Influence (LOO): fixed 178 labels. Loss 0.12221. Accuracy 0.971.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0334612
Train loss (w/o reg) on all data: 0.0159831
Test loss (w/o reg) on all data: 0.177432
Train acc on all data:  0.999274836838
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.96443e-05
Norm of the params: 18.6966
                Loss: fixed 168 labels. Loss 0.17743. Accuracy 0.970.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0878193
Train loss (w/o reg) on all data: 0.0589383
Test loss (w/o reg) on all data: 0.294726
Train acc on all data:  0.989364273628
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.00994e-05
Norm of the params: 24.0337
              Random: fixed  42 labels. Loss 0.29473. Accuracy 0.944.
### Flips: 206, rs: 18, checks: 1030
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0436214
Train loss (w/o reg) on all data: 0.0284574
Test loss (w/o reg) on all data: 0.106644
Train acc on all data:  0.995165578922
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 3.93045e-06
Norm of the params: 17.4149
     Influence (LOO): fixed 184 labels. Loss 0.10664. Accuracy 0.973.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0323704
Train loss (w/o reg) on all data: 0.0154656
Test loss (w/o reg) on all data: 0.168374
Train acc on all data:  0.999516557892
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.302e-06
Norm of the params: 18.3874
                Loss: fixed 177 labels. Loss 0.16837. Accuracy 0.968.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0865758
Train loss (w/o reg) on all data: 0.0580851
Test loss (w/o reg) on all data: 0.316399
Train acc on all data:  0.989122552574
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.38088e-05
Norm of the params: 23.8708
              Random: fixed  50 labels. Loss 0.31640. Accuracy 0.944.
### Flips: 206, rs: 18, checks: 1236
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0419633
Train loss (w/o reg) on all data: 0.0266927
Test loss (w/o reg) on all data: 0.103228
Train acc on all data:  0.99564902103
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.91904e-06
Norm of the params: 17.476
     Influence (LOO): fixed 186 labels. Loss 0.10323. Accuracy 0.974.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0309967
Train loss (w/o reg) on all data: 0.0148957
Test loss (w/o reg) on all data: 0.173793
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.5468e-06
Norm of the params: 17.9449
                Loss: fixed 189 labels. Loss 0.17379. Accuracy 0.974.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0824862
Train loss (w/o reg) on all data: 0.0546039
Test loss (w/o reg) on all data: 0.33375
Train acc on all data:  0.989847715736
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 4.99006e-06
Norm of the params: 23.6145
              Random: fixed  60 labels. Loss 0.33375. Accuracy 0.945.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0983855
Train loss (w/o reg) on all data: 0.064676
Test loss (w/o reg) on all data: 0.269015
Train acc on all data:  0.986947063089
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 9.50895e-06
Norm of the params: 25.9652
Flipped loss: 0.26901. Accuracy: 0.936
### Flips: 206, rs: 19, checks: 206
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0471272
Train loss (w/o reg) on all data: 0.0286751
Test loss (w/o reg) on all data: 0.23112
Train acc on all data:  0.996132463138
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 1.33322e-06
Norm of the params: 19.2105
     Influence (LOO): fixed 140 labels. Loss 0.23112. Accuracy 0.968.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0480776
Train loss (w/o reg) on all data: 0.0237232
Test loss (w/o reg) on all data: 0.279717
Train acc on all data:  0.999274836838
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 8.75027e-06
Norm of the params: 22.0701
                Loss: fixed  97 labels. Loss 0.27972. Accuracy 0.958.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0962609
Train loss (w/o reg) on all data: 0.0632617
Test loss (w/o reg) on all data: 0.30729
Train acc on all data:  0.987188784143
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 2.82745e-05
Norm of the params: 25.6902
              Random: fixed  11 labels. Loss 0.30729. Accuracy 0.932.
### Flips: 206, rs: 19, checks: 412
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0432641
Train loss (w/o reg) on all data: 0.0263641
Test loss (w/o reg) on all data: 0.301259
Train acc on all data:  0.996857626299
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.06468e-06
Norm of the params: 18.3848
     Influence (LOO): fixed 167 labels. Loss 0.30126. Accuracy 0.974.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0396087
Train loss (w/o reg) on all data: 0.0192341
Test loss (w/o reg) on all data: 0.272881
Train acc on all data:  0.999274836838
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 3.79333e-06
Norm of the params: 20.1865
                Loss: fixed 128 labels. Loss 0.27288. Accuracy 0.971.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0954522
Train loss (w/o reg) on all data: 0.0625576
Test loss (w/o reg) on all data: 0.325787
Train acc on all data:  0.987188784143
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 3.40308e-06
Norm of the params: 25.6494
              Random: fixed  17 labels. Loss 0.32579. Accuracy 0.929.
### Flips: 206, rs: 19, checks: 618
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0409185
Train loss (w/o reg) on all data: 0.0251289
Test loss (w/o reg) on all data: 0.231125
Train acc on all data:  0.996857626299
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 5.6133e-07
Norm of the params: 17.7705
     Influence (LOO): fixed 181 labels. Loss 0.23113. Accuracy 0.978.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0342335
Train loss (w/o reg) on all data: 0.0163154
Test loss (w/o reg) on all data: 0.269841
Train acc on all data:  0.999516557892
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 7.49074e-06
Norm of the params: 18.9304
                Loss: fixed 158 labels. Loss 0.26984. Accuracy 0.977.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0910111
Train loss (w/o reg) on all data: 0.0591041
Test loss (w/o reg) on all data: 0.262572
Train acc on all data:  0.988155668359
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 8.42093e-06
Norm of the params: 25.2614
              Random: fixed  28 labels. Loss 0.26257. Accuracy 0.940.
### Flips: 206, rs: 19, checks: 824
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0402764
Train loss (w/o reg) on all data: 0.0246656
Test loss (w/o reg) on all data: 0.219301
Train acc on all data:  0.997099347353
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.97003e-07
Norm of the params: 17.6697
     Influence (LOO): fixed 187 labels. Loss 0.21930. Accuracy 0.977.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0326808
Train loss (w/o reg) on all data: 0.0156664
Test loss (w/o reg) on all data: 0.250597
Train acc on all data:  0.999274836838
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.04641e-06
Norm of the params: 18.4469
                Loss: fixed 167 labels. Loss 0.25060. Accuracy 0.973.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088365
Train loss (w/o reg) on all data: 0.0574782
Test loss (w/o reg) on all data: 0.289282
Train acc on all data:  0.988639110467
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 2.86468e-06
Norm of the params: 24.8543
              Random: fixed  36 labels. Loss 0.28928. Accuracy 0.934.
### Flips: 206, rs: 19, checks: 1030
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0380051
Train loss (w/o reg) on all data: 0.0224249
Test loss (w/o reg) on all data: 0.222547
Train acc on all data:  0.997582789461
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 3.88397e-07
Norm of the params: 17.6523
     Influence (LOO): fixed 190 labels. Loss 0.22255. Accuracy 0.976.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0318599
Train loss (w/o reg) on all data: 0.0152973
Test loss (w/o reg) on all data: 0.221944
Train acc on all data:  0.999274836838
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 8.56362e-07
Norm of the params: 18.2004
                Loss: fixed 177 labels. Loss 0.22194. Accuracy 0.973.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0850857
Train loss (w/o reg) on all data: 0.0545978
Test loss (w/o reg) on all data: 0.282674
Train acc on all data:  0.989122552574
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.77879e-06
Norm of the params: 24.6933
              Random: fixed  45 labels. Loss 0.28267. Accuracy 0.935.
### Flips: 206, rs: 19, checks: 1236
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0362712
Train loss (w/o reg) on all data: 0.021132
Test loss (w/o reg) on all data: 0.218806
Train acc on all data:  0.997824510515
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 6.51893e-07
Norm of the params: 17.4007
     Influence (LOO): fixed 195 labels. Loss 0.21881. Accuracy 0.976.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0312969
Train loss (w/o reg) on all data: 0.0150539
Test loss (w/o reg) on all data: 0.214244
Train acc on all data:  0.999274836838
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.47552e-06
Norm of the params: 18.0239
                Loss: fixed 182 labels. Loss 0.21424. Accuracy 0.975.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0810261
Train loss (w/o reg) on all data: 0.0519578
Test loss (w/o reg) on all data: 0.19956
Train acc on all data:  0.989847715736
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 3.94203e-06
Norm of the params: 24.1115
              Random: fixed  58 labels. Loss 0.19956. Accuracy 0.944.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106775
Train loss (w/o reg) on all data: 0.0742616
Test loss (w/o reg) on all data: 0.513097
Train acc on all data:  0.982596084119
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 3.29259e-05
Norm of the params: 25.5002
Flipped loss: 0.51310. Accuracy: 0.928
### Flips: 206, rs: 20, checks: 206
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0551899
Train loss (w/o reg) on all data: 0.0367052
Test loss (w/o reg) on all data: 0.140733
Train acc on all data:  0.993473531545
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 3.55499e-06
Norm of the params: 19.2274
     Influence (LOO): fixed 135 labels. Loss 0.14073. Accuracy 0.961.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0478384
Train loss (w/o reg) on all data: 0.023622
Test loss (w/o reg) on all data: 0.305562
Train acc on all data:  0.99879139473
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 9.19666e-06
Norm of the params: 22.0074
                Loss: fixed 105 labels. Loss 0.30556. Accuracy 0.950.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103141
Train loss (w/o reg) on all data: 0.0712115
Test loss (w/o reg) on all data: 0.495768
Train acc on all data:  0.984046410442
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 6.55626e-06
Norm of the params: 25.2703
              Random: fixed   9 labels. Loss 0.49577. Accuracy 0.923.
### Flips: 206, rs: 20, checks: 412
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0477549
Train loss (w/o reg) on all data: 0.0324041
Test loss (w/o reg) on all data: 0.141507
Train acc on all data:  0.994198694706
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 4.64282e-06
Norm of the params: 17.5219
     Influence (LOO): fixed 169 labels. Loss 0.14151. Accuracy 0.972.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0376247
Train loss (w/o reg) on all data: 0.0180261
Test loss (w/o reg) on all data: 0.249875
Train acc on all data:  0.999033115784
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 3.85255e-06
Norm of the params: 19.7983
                Loss: fixed 140 labels. Loss 0.24988. Accuracy 0.959.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0991583
Train loss (w/o reg) on all data: 0.0680485
Test loss (w/o reg) on all data: 0.506655
Train acc on all data:  0.985496736766
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 1.58014e-05
Norm of the params: 24.9439
              Random: fixed  23 labels. Loss 0.50666. Accuracy 0.928.
### Flips: 206, rs: 20, checks: 618
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0449077
Train loss (w/o reg) on all data: 0.029894
Test loss (w/o reg) on all data: 0.143045
Train acc on all data:  0.994682136814
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.53478e-06
Norm of the params: 17.3285
     Influence (LOO): fixed 180 labels. Loss 0.14304. Accuracy 0.977.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0353147
Train loss (w/o reg) on all data: 0.0168811
Test loss (w/o reg) on all data: 0.234161
Train acc on all data:  0.999274836838
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 2.04338e-06
Norm of the params: 19.2008
                Loss: fixed 154 labels. Loss 0.23416. Accuracy 0.962.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0952828
Train loss (w/o reg) on all data: 0.0644308
Test loss (w/o reg) on all data: 0.523641
Train acc on all data:  0.986463620981
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 3.89312e-06
Norm of the params: 24.8403
              Random: fixed  32 labels. Loss 0.52364. Accuracy 0.933.
### Flips: 206, rs: 20, checks: 824
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0437382
Train loss (w/o reg) on all data: 0.029007
Test loss (w/o reg) on all data: 0.159506
Train acc on all data:  0.995165578922
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 2.19245e-06
Norm of the params: 17.1646
     Influence (LOO): fixed 184 labels. Loss 0.15951. Accuracy 0.975.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034176
Train loss (w/o reg) on all data: 0.0163672
Test loss (w/o reg) on all data: 0.247255
Train acc on all data:  0.999274836838
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 2.57137e-06
Norm of the params: 18.8726
                Loss: fixed 165 labels. Loss 0.24725. Accuracy 0.963.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0918917
Train loss (w/o reg) on all data: 0.061772
Test loss (w/o reg) on all data: 0.514766
Train acc on all data:  0.987188784143
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.08071e-05
Norm of the params: 24.5437
              Random: fixed  41 labels. Loss 0.51477. Accuracy 0.934.
### Flips: 206, rs: 20, checks: 1030
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430159
Train loss (w/o reg) on all data: 0.0282153
Test loss (w/o reg) on all data: 0.164593
Train acc on all data:  0.995407299976
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.04056e-07
Norm of the params: 17.205
     Influence (LOO): fixed 187 labels. Loss 0.16459. Accuracy 0.975.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0323309
Train loss (w/o reg) on all data: 0.0155422
Test loss (w/o reg) on all data: 0.226171
Train acc on all data:  0.999274836838
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 5.7938e-06
Norm of the params: 18.3242
                Loss: fixed 175 labels. Loss 0.22617. Accuracy 0.965.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0894491
Train loss (w/o reg) on all data: 0.0598851
Test loss (w/o reg) on all data: 0.5057
Train acc on all data:  0.987913947305
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.45372e-05
Norm of the params: 24.3162
              Random: fixed  48 labels. Loss 0.50570. Accuracy 0.941.
### Flips: 206, rs: 20, checks: 1236
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0418616
Train loss (w/o reg) on all data: 0.0270506
Test loss (w/o reg) on all data: 0.138217
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 8.97837e-07
Norm of the params: 17.2111
     Influence (LOO): fixed 190 labels. Loss 0.13822. Accuracy 0.975.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0314493
Train loss (w/o reg) on all data: 0.015153
Test loss (w/o reg) on all data: 0.202224
Train acc on all data:  0.999274836838
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 4.32197e-06
Norm of the params: 18.0534
                Loss: fixed 182 labels. Loss 0.20222. Accuracy 0.968.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0832301
Train loss (w/o reg) on all data: 0.0549864
Test loss (w/o reg) on all data: 0.591797
Train acc on all data:  0.989605994682
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.47731e-05
Norm of the params: 23.7671
              Random: fixed  65 labels. Loss 0.59180. Accuracy 0.934.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0963046
Train loss (w/o reg) on all data: 0.0643689
Test loss (w/o reg) on all data: 0.370258
Train acc on all data:  0.987672226251
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 4.51643e-06
Norm of the params: 25.2728
Flipped loss: 0.37026. Accuracy: 0.937
### Flips: 206, rs: 21, checks: 206
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0514296
Train loss (w/o reg) on all data: 0.0334997
Test loss (w/o reg) on all data: 0.158107
Train acc on all data:  0.994682136814
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.688e-06
Norm of the params: 18.9367
     Influence (LOO): fixed 138 labels. Loss 0.15811. Accuracy 0.966.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0500387
Train loss (w/o reg) on all data: 0.0248237
Test loss (w/o reg) on all data: 0.219008
Train acc on all data:  0.999033115784
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 8.15234e-06
Norm of the params: 22.4566
                Loss: fixed  94 labels. Loss 0.21901. Accuracy 0.950.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0945302
Train loss (w/o reg) on all data: 0.0632616
Test loss (w/o reg) on all data: 0.362066
Train acc on all data:  0.987672226251
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.15777e-05
Norm of the params: 25.0074
              Random: fixed  10 labels. Loss 0.36207. Accuracy 0.934.
### Flips: 206, rs: 21, checks: 412
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046362
Train loss (w/o reg) on all data: 0.03007
Test loss (w/o reg) on all data: 0.156073
Train acc on all data:  0.995165578922
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 9.35881e-07
Norm of the params: 18.051
     Influence (LOO): fixed 162 labels. Loss 0.15607. Accuracy 0.970.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0412071
Train loss (w/o reg) on all data: 0.0200622
Test loss (w/o reg) on all data: 0.166659
Train acc on all data:  0.999274836838
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 5.51113e-06
Norm of the params: 20.5645
                Loss: fixed 134 labels. Loss 0.16666. Accuracy 0.966.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0915784
Train loss (w/o reg) on all data: 0.0612429
Test loss (w/o reg) on all data: 0.311729
Train acc on all data:  0.987672226251
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 1.89391e-05
Norm of the params: 24.6315
              Random: fixed  24 labels. Loss 0.31173. Accuracy 0.937.
### Flips: 206, rs: 21, checks: 618
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0434015
Train loss (w/o reg) on all data: 0.0273853
Test loss (w/o reg) on all data: 0.145402
Train acc on all data:  0.99564902103
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 8.60269e-07
Norm of the params: 17.8976
     Influence (LOO): fixed 172 labels. Loss 0.14540. Accuracy 0.973.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0340209
Train loss (w/o reg) on all data: 0.0163515
Test loss (w/o reg) on all data: 0.149625
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 3.84828e-06
Norm of the params: 18.7986
                Loss: fixed 161 labels. Loss 0.14962. Accuracy 0.972.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087155
Train loss (w/o reg) on all data: 0.0577544
Test loss (w/o reg) on all data: 0.293204
Train acc on all data:  0.989605994682
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 1.77862e-05
Norm of the params: 24.2489
              Random: fixed  39 labels. Loss 0.29320. Accuracy 0.939.
### Flips: 206, rs: 21, checks: 824
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0421034
Train loss (w/o reg) on all data: 0.0265795
Test loss (w/o reg) on all data: 0.164925
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 4.6377e-07
Norm of the params: 17.6203
     Influence (LOO): fixed 181 labels. Loss 0.16492. Accuracy 0.975.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0321786
Train loss (w/o reg) on all data: 0.0154285
Test loss (w/o reg) on all data: 0.174459
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 4.7719e-06
Norm of the params: 18.3031
                Loss: fixed 175 labels. Loss 0.17446. Accuracy 0.973.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0858273
Train loss (w/o reg) on all data: 0.056687
Test loss (w/o reg) on all data: 0.284003
Train acc on all data:  0.990331157844
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 3.41147e-06
Norm of the params: 24.1414
              Random: fixed  45 labels. Loss 0.28400. Accuracy 0.946.
### Flips: 206, rs: 21, checks: 1030
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0418079
Train loss (w/o reg) on all data: 0.0263711
Test loss (w/o reg) on all data: 0.16866
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.58915e-07
Norm of the params: 17.5708
     Influence (LOO): fixed 186 labels. Loss 0.16866. Accuracy 0.975.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0315863
Train loss (w/o reg) on all data: 0.0151262
Test loss (w/o reg) on all data: 0.161959
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 2.58099e-06
Norm of the params: 18.144
                Loss: fixed 179 labels. Loss 0.16196. Accuracy 0.971.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0821357
Train loss (w/o reg) on all data: 0.0540859
Test loss (w/o reg) on all data: 0.273327
Train acc on all data:  0.989605994682
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 2.07151e-05
Norm of the params: 23.6854
              Random: fixed  55 labels. Loss 0.27333. Accuracy 0.949.
### Flips: 206, rs: 21, checks: 1236
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0393766
Train loss (w/o reg) on all data: 0.0243159
Test loss (w/o reg) on all data: 0.163757
Train acc on all data:  0.996374184191
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 4.36235e-07
Norm of the params: 17.3555
     Influence (LOO): fixed 190 labels. Loss 0.16376. Accuracy 0.974.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0310154
Train loss (w/o reg) on all data: 0.0148706
Test loss (w/o reg) on all data: 0.136771
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 4.67348e-06
Norm of the params: 17.9693
                Loss: fixed 183 labels. Loss 0.13677. Accuracy 0.974.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0791321
Train loss (w/o reg) on all data: 0.0518675
Test loss (w/o reg) on all data: 0.295064
Train acc on all data:  0.99008943679
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 8.11881e-06
Norm of the params: 23.3515
              Random: fixed  66 labels. Loss 0.29506. Accuracy 0.946.
Using normal model
LBFGS training took [351] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103091
Train loss (w/o reg) on all data: 0.0682029
Test loss (w/o reg) on all data: 0.278644
Train acc on all data:  0.985980178874
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 7.87434e-06
Norm of the params: 26.4153
Flipped loss: 0.27864. Accuracy: 0.924
### Flips: 206, rs: 22, checks: 206
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0476333
Train loss (w/o reg) on all data: 0.0290518
Test loss (w/o reg) on all data: 0.155679
Train acc on all data:  0.996132463138
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 7.19253e-06
Norm of the params: 19.2777
     Influence (LOO): fixed 145 labels. Loss 0.15568. Accuracy 0.960.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052057
Train loss (w/o reg) on all data: 0.0259282
Test loss (w/o reg) on all data: 0.164902
Train acc on all data:  0.99879139473
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.03576e-05
Norm of the params: 22.8599
                Loss: fixed  94 labels. Loss 0.16490. Accuracy 0.952.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100705
Train loss (w/o reg) on all data: 0.0663512
Test loss (w/o reg) on all data: 0.348308
Train acc on all data:  0.987188784143
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 7.85776e-06
Norm of the params: 26.2121
              Random: fixed   7 labels. Loss 0.34831. Accuracy 0.925.
### Flips: 206, rs: 22, checks: 412
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430603
Train loss (w/o reg) on all data: 0.026367
Test loss (w/o reg) on all data: 0.116764
Train acc on all data:  0.996132463138
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 7.66957e-07
Norm of the params: 18.272
     Influence (LOO): fixed 167 labels. Loss 0.11676. Accuracy 0.968.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0428121
Train loss (w/o reg) on all data: 0.0206952
Test loss (w/o reg) on all data: 0.142561
Train acc on all data:  0.999033115784
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 4.02703e-06
Norm of the params: 21.0318
                Loss: fixed 131 labels. Loss 0.14256. Accuracy 0.957.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0977734
Train loss (w/o reg) on all data: 0.064209
Test loss (w/o reg) on all data: 0.32729
Train acc on all data:  0.987188784143
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 9.59925e-06
Norm of the params: 25.9092
              Random: fixed  16 labels. Loss 0.32729. Accuracy 0.926.
### Flips: 206, rs: 22, checks: 618
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0423841
Train loss (w/o reg) on all data: 0.0259452
Test loss (w/o reg) on all data: 0.11302
Train acc on all data:  0.995890742084
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 5.78805e-07
Norm of the params: 18.1322
     Influence (LOO): fixed 178 labels. Loss 0.11302. Accuracy 0.969.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0373093
Train loss (w/o reg) on all data: 0.01775
Test loss (w/o reg) on all data: 0.124377
Train acc on all data:  0.999516557892
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 1.61562e-05
Norm of the params: 19.7784
                Loss: fixed 151 labels. Loss 0.12438. Accuracy 0.968.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0953341
Train loss (w/o reg) on all data: 0.0625125
Test loss (w/o reg) on all data: 0.316079
Train acc on all data:  0.987430505197
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 8.04821e-06
Norm of the params: 25.6209
              Random: fixed  28 labels. Loss 0.31608. Accuracy 0.935.
### Flips: 206, rs: 22, checks: 824
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0419356
Train loss (w/o reg) on all data: 0.0256585
Test loss (w/o reg) on all data: 0.110797
Train acc on all data:  0.996374184191
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 7.69529e-07
Norm of the params: 18.0428
     Influence (LOO): fixed 183 labels. Loss 0.11080. Accuracy 0.973.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0324215
Train loss (w/o reg) on all data: 0.0154774
Test loss (w/o reg) on all data: 0.169683
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 8.64055e-07
Norm of the params: 18.4087
                Loss: fixed 167 labels. Loss 0.16968. Accuracy 0.973.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0924586
Train loss (w/o reg) on all data: 0.0600041
Test loss (w/o reg) on all data: 0.31387
Train acc on all data:  0.988155668359
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 2.1841e-05
Norm of the params: 25.4772
              Random: fixed  38 labels. Loss 0.31387. Accuracy 0.935.
### Flips: 206, rs: 22, checks: 1030
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0378678
Train loss (w/o reg) on all data: 0.0229022
Test loss (w/o reg) on all data: 0.116023
Train acc on all data:  0.997099347353
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.79893e-07
Norm of the params: 17.3006
     Influence (LOO): fixed 191 labels. Loss 0.11602. Accuracy 0.977.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0309072
Train loss (w/o reg) on all data: 0.0148289
Test loss (w/o reg) on all data: 0.157827
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 6.24764e-06
Norm of the params: 17.9323
                Loss: fixed 178 labels. Loss 0.15783. Accuracy 0.974.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0917322
Train loss (w/o reg) on all data: 0.0596798
Test loss (w/o reg) on all data: 0.315066
Train acc on all data:  0.988397389413
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 7.98497e-06
Norm of the params: 25.3189
              Random: fixed  43 labels. Loss 0.31507. Accuracy 0.940.
### Flips: 206, rs: 22, checks: 1236
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0374999
Train loss (w/o reg) on all data: 0.0225186
Test loss (w/o reg) on all data: 0.107992
Train acc on all data:  0.997341068407
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 3.83875e-07
Norm of the params: 17.3097
     Influence (LOO): fixed 192 labels. Loss 0.10799. Accuracy 0.976.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030578
Train loss (w/o reg) on all data: 0.014675
Test loss (w/o reg) on all data: 0.16019
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 2.23543e-06
Norm of the params: 17.8343
                Loss: fixed 184 labels. Loss 0.16019. Accuracy 0.975.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0906103
Train loss (w/o reg) on all data: 0.0589317
Test loss (w/o reg) on all data: 0.293121
Train acc on all data:  0.988639110467
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 6.15285e-06
Norm of the params: 25.1709
              Random: fixed  50 labels. Loss 0.29312. Accuracy 0.936.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105939
Train loss (w/o reg) on all data: 0.0747982
Test loss (w/o reg) on all data: 0.236769
Train acc on all data:  0.983804689388
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 7.22853e-06
Norm of the params: 24.9562
Flipped loss: 0.23677. Accuracy: 0.930
### Flips: 206, rs: 23, checks: 206
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0589955
Train loss (w/o reg) on all data: 0.0410263
Test loss (w/o reg) on all data: 0.180691
Train acc on all data:  0.991781484167
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 7.47187e-07
Norm of the params: 18.9574
     Influence (LOO): fixed 130 labels. Loss 0.18069. Accuracy 0.971.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0475525
Train loss (w/o reg) on all data: 0.0232787
Test loss (w/o reg) on all data: 0.229176
Train acc on all data:  0.999274836838
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 1.23795e-05
Norm of the params: 22.0335
                Loss: fixed 105 labels. Loss 0.22918. Accuracy 0.949.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100802
Train loss (w/o reg) on all data: 0.0704774
Test loss (w/o reg) on all data: 0.259307
Train acc on all data:  0.98452985255
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 5.92231e-06
Norm of the params: 24.627
              Random: fixed  11 labels. Loss 0.25931. Accuracy 0.936.
### Flips: 206, rs: 23, checks: 412
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0538945
Train loss (w/o reg) on all data: 0.0377626
Test loss (w/o reg) on all data: 0.0999341
Train acc on all data:  0.992506647329
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.27629e-06
Norm of the params: 17.9621
     Influence (LOO): fixed 163 labels. Loss 0.09993. Accuracy 0.972.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0381449
Train loss (w/o reg) on all data: 0.0182334
Test loss (w/o reg) on all data: 0.189652
Train acc on all data:  0.999516557892
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 7.44066e-06
Norm of the params: 19.9557
                Loss: fixed 139 labels. Loss 0.18965. Accuracy 0.968.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0954008
Train loss (w/o reg) on all data: 0.0658573
Test loss (w/o reg) on all data: 0.253457
Train acc on all data:  0.985980178874
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.8766e-05
Norm of the params: 24.3078
              Random: fixed  25 labels. Loss 0.25346. Accuracy 0.934.
### Flips: 206, rs: 23, checks: 618
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0488689
Train loss (w/o reg) on all data: 0.0335167
Test loss (w/o reg) on all data: 0.105128
Train acc on all data:  0.993473531545
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.19257e-06
Norm of the params: 17.5227
     Influence (LOO): fixed 176 labels. Loss 0.10513. Accuracy 0.973.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034256
Train loss (w/o reg) on all data: 0.0163254
Test loss (w/o reg) on all data: 0.14625
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.86849e-06
Norm of the params: 18.937
                Loss: fixed 166 labels. Loss 0.14625. Accuracy 0.973.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0925868
Train loss (w/o reg) on all data: 0.0637002
Test loss (w/o reg) on all data: 0.251714
Train acc on all data:  0.986463620981
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 5.74562e-06
Norm of the params: 24.036
              Random: fixed  34 labels. Loss 0.25171. Accuracy 0.939.
### Flips: 206, rs: 23, checks: 824
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0481803
Train loss (w/o reg) on all data: 0.0329021
Test loss (w/o reg) on all data: 0.106317
Train acc on all data:  0.993715252599
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.91476e-06
Norm of the params: 17.4804
     Influence (LOO): fixed 179 labels. Loss 0.10632. Accuracy 0.976.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0327929
Train loss (w/o reg) on all data: 0.0156104
Test loss (w/o reg) on all data: 0.0988016
Train acc on all data:  0.999516557892
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 2.44639e-06
Norm of the params: 18.5378
                Loss: fixed 170 labels. Loss 0.09880. Accuracy 0.977.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0891785
Train loss (w/o reg) on all data: 0.0605129
Test loss (w/o reg) on all data: 0.231072
Train acc on all data:  0.987430505197
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 1.28019e-05
Norm of the params: 23.9439
              Random: fixed  42 labels. Loss 0.23107. Accuracy 0.937.
### Flips: 206, rs: 23, checks: 1030
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0462983
Train loss (w/o reg) on all data: 0.0311714
Test loss (w/o reg) on all data: 0.103603
Train acc on all data:  0.994198694706
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 8.39716e-07
Norm of the params: 17.3936
     Influence (LOO): fixed 182 labels. Loss 0.10360. Accuracy 0.976.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0320431
Train loss (w/o reg) on all data: 0.015288
Test loss (w/o reg) on all data: 0.100336
Train acc on all data:  0.999516557892
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 3.27816e-06
Norm of the params: 18.3058
                Loss: fixed 176 labels. Loss 0.10034. Accuracy 0.979.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0878143
Train loss (w/o reg) on all data: 0.0595222
Test loss (w/o reg) on all data: 0.298936
Train acc on all data:  0.987913947305
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.67897e-06
Norm of the params: 23.7874
              Random: fixed  48 labels. Loss 0.29894. Accuracy 0.935.
### Flips: 206, rs: 23, checks: 1236
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0429026
Train loss (w/o reg) on all data: 0.0275504
Test loss (w/o reg) on all data: 0.100175
Train acc on all data:  0.995407299976
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 1.3059e-06
Norm of the params: 17.5227
     Influence (LOO): fixed 187 labels. Loss 0.10017. Accuracy 0.978.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0307577
Train loss (w/o reg) on all data: 0.0147048
Test loss (w/o reg) on all data: 0.12856
Train acc on all data:  0.999516557892
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 9.0876e-07
Norm of the params: 17.9181
                Loss: fixed 182 labels. Loss 0.12856. Accuracy 0.980.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0827419
Train loss (w/o reg) on all data: 0.0551757
Test loss (w/o reg) on all data: 0.329267
Train acc on all data:  0.989364273628
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 1.94885e-05
Norm of the params: 23.4803
              Random: fixed  61 labels. Loss 0.32927. Accuracy 0.938.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10001
Train loss (w/o reg) on all data: 0.0680274
Test loss (w/o reg) on all data: 0.344515
Train acc on all data:  0.985255015712
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 3.41399e-05
Norm of the params: 25.2913
Flipped loss: 0.34452. Accuracy: 0.919
### Flips: 206, rs: 24, checks: 206
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0493962
Train loss (w/o reg) on all data: 0.0313385
Test loss (w/o reg) on all data: 0.26693
Train acc on all data:  0.994682136814
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 2.28354e-06
Norm of the params: 19.0041
     Influence (LOO): fixed 135 labels. Loss 0.26693. Accuracy 0.964.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0473044
Train loss (w/o reg) on all data: 0.0231828
Test loss (w/o reg) on all data: 0.290686
Train acc on all data:  0.999274836838
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 1.3525e-05
Norm of the params: 21.9643
                Loss: fixed 100 labels. Loss 0.29069. Accuracy 0.951.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0958946
Train loss (w/o reg) on all data: 0.0643011
Test loss (w/o reg) on all data: 0.337377
Train acc on all data:  0.985496736766
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.93677e-05
Norm of the params: 25.137
              Random: fixed   7 labels. Loss 0.33738. Accuracy 0.920.
### Flips: 206, rs: 24, checks: 412
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0446227
Train loss (w/o reg) on all data: 0.0280814
Test loss (w/o reg) on all data: 0.185128
Train acc on all data:  0.995407299976
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 7.92494e-07
Norm of the params: 18.1886
     Influence (LOO): fixed 168 labels. Loss 0.18513. Accuracy 0.971.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0386558
Train loss (w/o reg) on all data: 0.0185512
Test loss (w/o reg) on all data: 0.219244
Train acc on all data:  0.999516557892
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 4.03651e-06
Norm of the params: 20.0522
                Loss: fixed 134 labels. Loss 0.21924. Accuracy 0.961.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0920221
Train loss (w/o reg) on all data: 0.0612065
Test loss (w/o reg) on all data: 0.284765
Train acc on all data:  0.986947063089
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 2.09239e-05
Norm of the params: 24.8256
              Random: fixed  19 labels. Loss 0.28477. Accuracy 0.928.
### Flips: 206, rs: 24, checks: 618
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0385067
Train loss (w/o reg) on all data: 0.0234254
Test loss (w/o reg) on all data: 0.215451
Train acc on all data:  0.996615905245
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 5.67841e-07
Norm of the params: 17.3674
     Influence (LOO): fixed 182 labels. Loss 0.21545. Accuracy 0.974.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0345925
Train loss (w/o reg) on all data: 0.0165644
Test loss (w/o reg) on all data: 0.192928
Train acc on all data:  0.999516557892
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.64902e-06
Norm of the params: 18.9885
                Loss: fixed 157 labels. Loss 0.19293. Accuracy 0.966.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0914541
Train loss (w/o reg) on all data: 0.0608736
Test loss (w/o reg) on all data: 0.302151
Train acc on all data:  0.986705342035
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 5.84543e-06
Norm of the params: 24.7307
              Random: fixed  26 labels. Loss 0.30215. Accuracy 0.928.
### Flips: 206, rs: 24, checks: 824
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0383461
Train loss (w/o reg) on all data: 0.023293
Test loss (w/o reg) on all data: 0.239926
Train acc on all data:  0.996615905245
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 8.42752e-07
Norm of the params: 17.3511
     Influence (LOO): fixed 185 labels. Loss 0.23993. Accuracy 0.975.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0328832
Train loss (w/o reg) on all data: 0.0157081
Test loss (w/o reg) on all data: 0.220677
Train acc on all data:  0.999516557892
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.323e-06
Norm of the params: 18.5338
                Loss: fixed 166 labels. Loss 0.22068. Accuracy 0.970.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0867011
Train loss (w/o reg) on all data: 0.0567642
Test loss (w/o reg) on all data: 0.296479
Train acc on all data:  0.988639110467
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.94169e-06
Norm of the params: 24.4691
              Random: fixed  39 labels. Loss 0.29648. Accuracy 0.931.
### Flips: 206, rs: 24, checks: 1030
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0364894
Train loss (w/o reg) on all data: 0.0216946
Test loss (w/o reg) on all data: 0.190229
Train acc on all data:  0.996857626299
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.66505e-07
Norm of the params: 17.2016
     Influence (LOO): fixed 189 labels. Loss 0.19023. Accuracy 0.975.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0318305
Train loss (w/o reg) on all data: 0.0151801
Test loss (w/o reg) on all data: 0.187589
Train acc on all data:  0.999274836838
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.80874e-06
Norm of the params: 18.2485
                Loss: fixed 173 labels. Loss 0.18759. Accuracy 0.973.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0815072
Train loss (w/o reg) on all data: 0.0532244
Test loss (w/o reg) on all data: 0.30563
Train acc on all data:  0.989364273628
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 2.37126e-06
Norm of the params: 23.7835
              Random: fixed  51 labels. Loss 0.30563. Accuracy 0.931.
### Flips: 206, rs: 24, checks: 1236
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0356326
Train loss (w/o reg) on all data: 0.0209917
Test loss (w/o reg) on all data: 0.143569
Train acc on all data:  0.997341068407
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 3.26527e-07
Norm of the params: 17.1119
     Influence (LOO): fixed 192 labels. Loss 0.14357. Accuracy 0.976.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0309042
Train loss (w/o reg) on all data: 0.0147802
Test loss (w/o reg) on all data: 0.175378
Train acc on all data:  0.999274836838
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.64746e-06
Norm of the params: 17.9577
                Loss: fixed 181 labels. Loss 0.17538. Accuracy 0.970.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0788378
Train loss (w/o reg) on all data: 0.0514137
Test loss (w/o reg) on all data: 0.315949
Train acc on all data:  0.990331157844
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 2.9959e-06
Norm of the params: 23.4197
              Random: fixed  63 labels. Loss 0.31595. Accuracy 0.935.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103493
Train loss (w/o reg) on all data: 0.0720121
Test loss (w/o reg) on all data: 0.230753
Train acc on all data:  0.985013294658
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.13022e-05
Norm of the params: 25.0923
Flipped loss: 0.23075. Accuracy: 0.935
### Flips: 206, rs: 25, checks: 206
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0555245
Train loss (w/o reg) on all data: 0.0373153
Test loss (w/o reg) on all data: 0.123715
Train acc on all data:  0.992748368383
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 5.67555e-06
Norm of the params: 19.0836
     Influence (LOO): fixed 134 labels. Loss 0.12371. Accuracy 0.973.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0491407
Train loss (w/o reg) on all data: 0.0241665
Test loss (w/o reg) on all data: 0.175231
Train acc on all data:  0.999516557892
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 8.99768e-06
Norm of the params: 22.3491
                Loss: fixed 103 labels. Loss 0.17523. Accuracy 0.946.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101238
Train loss (w/o reg) on all data: 0.0704892
Test loss (w/o reg) on all data: 0.221524
Train acc on all data:  0.985255015712
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 6.62734e-06
Norm of the params: 24.7988
              Random: fixed   8 labels. Loss 0.22152. Accuracy 0.939.
### Flips: 206, rs: 25, checks: 412
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0507971
Train loss (w/o reg) on all data: 0.0346025
Test loss (w/o reg) on all data: 0.168996
Train acc on all data:  0.993231810491
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.72179e-06
Norm of the params: 17.997
     Influence (LOO): fixed 162 labels. Loss 0.16900. Accuracy 0.972.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0392524
Train loss (w/o reg) on all data: 0.0189397
Test loss (w/o reg) on all data: 0.112544
Train acc on all data:  0.999274836838
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 1.94874e-05
Norm of the params: 20.1557
                Loss: fixed 139 labels. Loss 0.11254. Accuracy 0.968.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0976314
Train loss (w/o reg) on all data: 0.0676739
Test loss (w/o reg) on all data: 0.216234
Train acc on all data:  0.98573845782
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 6.97833e-06
Norm of the params: 24.4775
              Random: fixed  17 labels. Loss 0.21623. Accuracy 0.938.
### Flips: 206, rs: 25, checks: 618
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0480472
Train loss (w/o reg) on all data: 0.0322061
Test loss (w/o reg) on all data: 0.152841
Train acc on all data:  0.993715252599
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 7.39096e-06
Norm of the params: 17.7995
     Influence (LOO): fixed 172 labels. Loss 0.15284. Accuracy 0.977.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0370149
Train loss (w/o reg) on all data: 0.0178332
Test loss (w/o reg) on all data: 0.10404
Train acc on all data:  0.999274836838
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 7.50831e-06
Norm of the params: 19.5866
                Loss: fixed 153 labels. Loss 0.10404. Accuracy 0.972.
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0952476
Train loss (w/o reg) on all data: 0.0660053
Test loss (w/o reg) on all data: 0.198631
Train acc on all data:  0.986705342035
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 2.48628e-05
Norm of the params: 24.1836
              Random: fixed  26 labels. Loss 0.19863. Accuracy 0.935.
### Flips: 206, rs: 25, checks: 824
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0467288
Train loss (w/o reg) on all data: 0.0315315
Test loss (w/o reg) on all data: 0.130081
Train acc on all data:  0.993715252599
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 6.80359e-07
Norm of the params: 17.4341
     Influence (LOO): fixed 176 labels. Loss 0.13008. Accuracy 0.976.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0344974
Train loss (w/o reg) on all data: 0.0165708
Test loss (w/o reg) on all data: 0.0986459
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.11989e-06
Norm of the params: 18.9349
                Loss: fixed 167 labels. Loss 0.09865. Accuracy 0.975.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0934909
Train loss (w/o reg) on all data: 0.0646508
Test loss (w/o reg) on all data: 0.209121
Train acc on all data:  0.986947063089
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.24964e-05
Norm of the params: 24.0167
              Random: fixed  36 labels. Loss 0.20912. Accuracy 0.933.
### Flips: 206, rs: 25, checks: 1030
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0447021
Train loss (w/o reg) on all data: 0.0295763
Test loss (w/o reg) on all data: 0.138828
Train acc on all data:  0.99444041576
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.67787e-06
Norm of the params: 17.393
     Influence (LOO): fixed 184 labels. Loss 0.13883. Accuracy 0.976.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0334063
Train loss (w/o reg) on all data: 0.0160394
Test loss (w/o reg) on all data: 0.115141
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.38156e-06
Norm of the params: 18.637
                Loss: fixed 177 labels. Loss 0.11514. Accuracy 0.975.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0898205
Train loss (w/o reg) on all data: 0.0612805
Test loss (w/o reg) on all data: 0.218055
Train acc on all data:  0.987913947305
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.20274e-05
Norm of the params: 23.8914
              Random: fixed  46 labels. Loss 0.21805. Accuracy 0.936.
### Flips: 206, rs: 25, checks: 1236
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04254
Train loss (w/o reg) on all data: 0.0273549
Test loss (w/o reg) on all data: 0.135158
Train acc on all data:  0.995165578922
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.96813e-07
Norm of the params: 17.427
     Influence (LOO): fixed 187 labels. Loss 0.13516. Accuracy 0.975.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0318089
Train loss (w/o reg) on all data: 0.0153434
Test loss (w/o reg) on all data: 0.103822
Train acc on all data:  0.999274836838
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.85336e-06
Norm of the params: 18.1469
                Loss: fixed 187 labels. Loss 0.10382. Accuracy 0.974.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0878166
Train loss (w/o reg) on all data: 0.0599722
Test loss (w/o reg) on all data: 0.181485
Train acc on all data:  0.988397389413
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 2.45684e-05
Norm of the params: 23.5985
              Random: fixed  56 labels. Loss 0.18149. Accuracy 0.939.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102163
Train loss (w/o reg) on all data: 0.071254
Test loss (w/o reg) on all data: 0.289903
Train acc on all data:  0.985013294658
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.25257e-05
Norm of the params: 24.8631
Flipped loss: 0.28990. Accuracy: 0.930
### Flips: 206, rs: 26, checks: 206
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0548812
Train loss (w/o reg) on all data: 0.0374771
Test loss (w/o reg) on all data: 0.170509
Train acc on all data:  0.993231810491
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.54461e-06
Norm of the params: 18.657
     Influence (LOO): fixed 134 labels. Loss 0.17051. Accuracy 0.973.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0469494
Train loss (w/o reg) on all data: 0.0230426
Test loss (w/o reg) on all data: 0.289857
Train acc on all data:  0.999516557892
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.0032e-06
Norm of the params: 21.8663
                Loss: fixed  99 labels. Loss 0.28986. Accuracy 0.944.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0983348
Train loss (w/o reg) on all data: 0.0682369
Test loss (w/o reg) on all data: 0.233862
Train acc on all data:  0.986463620981
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 1.87174e-05
Norm of the params: 24.5348
              Random: fixed  12 labels. Loss 0.23386. Accuracy 0.938.
### Flips: 206, rs: 26, checks: 412
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0485798
Train loss (w/o reg) on all data: 0.0335533
Test loss (w/o reg) on all data: 0.161749
Train acc on all data:  0.993956973652
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 6.94583e-07
Norm of the params: 17.3358
     Influence (LOO): fixed 168 labels. Loss 0.16175. Accuracy 0.973.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038389
Train loss (w/o reg) on all data: 0.0186187
Test loss (w/o reg) on all data: 0.194656
Train acc on all data:  0.999516557892
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 2.56775e-06
Norm of the params: 19.8849
                Loss: fixed 136 labels. Loss 0.19466. Accuracy 0.958.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0937271
Train loss (w/o reg) on all data: 0.0645592
Test loss (w/o reg) on all data: 0.200454
Train acc on all data:  0.987672226251
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 1.06779e-05
Norm of the params: 24.1528
              Random: fixed  24 labels. Loss 0.20045. Accuracy 0.943.
### Flips: 206, rs: 26, checks: 618
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0456931
Train loss (w/o reg) on all data: 0.0308398
Test loss (w/o reg) on all data: 0.129424
Train acc on all data:  0.994923857868
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.40024e-07
Norm of the params: 17.2356
     Influence (LOO): fixed 180 labels. Loss 0.12942. Accuracy 0.977.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0350569
Train loss (w/o reg) on all data: 0.0168454
Test loss (w/o reg) on all data: 0.183482
Train acc on all data:  0.999516557892
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.52617e-06
Norm of the params: 19.0848
                Loss: fixed 152 labels. Loss 0.18348. Accuracy 0.965.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0899385
Train loss (w/o reg) on all data: 0.0612672
Test loss (w/o reg) on all data: 0.189238
Train acc on all data:  0.988155668359
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 6.37675e-06
Norm of the params: 23.9464
              Random: fixed  36 labels. Loss 0.18924. Accuracy 0.947.
### Flips: 206, rs: 26, checks: 824
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0443162
Train loss (w/o reg) on all data: 0.0295388
Test loss (w/o reg) on all data: 0.135874
Train acc on all data:  0.994923857868
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.45927e-07
Norm of the params: 17.1915
     Influence (LOO): fixed 186 labels. Loss 0.13587. Accuracy 0.977.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0329757
Train loss (w/o reg) on all data: 0.015842
Test loss (w/o reg) on all data: 0.19145
Train acc on all data:  0.999516557892
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 1.6717e-06
Norm of the params: 18.5114
                Loss: fixed 166 labels. Loss 0.19145. Accuracy 0.968.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0857266
Train loss (w/o reg) on all data: 0.0575122
Test loss (w/o reg) on all data: 0.198005
Train acc on all data:  0.988397389413
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 5.47818e-06
Norm of the params: 23.7547
              Random: fixed  48 labels. Loss 0.19800. Accuracy 0.941.
### Flips: 206, rs: 26, checks: 1030
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0417839
Train loss (w/o reg) on all data: 0.0268407
Test loss (w/o reg) on all data: 0.140428
Train acc on all data:  0.995890742084
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.4379e-07
Norm of the params: 17.2877
     Influence (LOO): fixed 191 labels. Loss 0.14043. Accuracy 0.977.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0310451
Train loss (w/o reg) on all data: 0.0148523
Test loss (w/o reg) on all data: 0.154504
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 8.9301e-07
Norm of the params: 17.996
                Loss: fixed 176 labels. Loss 0.15450. Accuracy 0.974.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0835464
Train loss (w/o reg) on all data: 0.0556394
Test loss (w/o reg) on all data: 0.204371
Train acc on all data:  0.989605994682
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 1.34878e-05
Norm of the params: 23.625
              Random: fixed  55 labels. Loss 0.20437. Accuracy 0.943.
### Flips: 206, rs: 26, checks: 1236
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0402458
Train loss (w/o reg) on all data: 0.0252971
Test loss (w/o reg) on all data: 0.142367
Train acc on all data:  0.996132463138
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 3.74283e-07
Norm of the params: 17.2909
     Influence (LOO): fixed 193 labels. Loss 0.14237. Accuracy 0.978.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0301863
Train loss (w/o reg) on all data: 0.0144594
Test loss (w/o reg) on all data: 0.169671
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 2.54079e-06
Norm of the params: 17.7352
                Loss: fixed 184 labels. Loss 0.16967. Accuracy 0.975.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0815753
Train loss (w/o reg) on all data: 0.0542059
Test loss (w/o reg) on all data: 0.187749
Train acc on all data:  0.989605994682
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 3.62935e-05
Norm of the params: 23.3963
              Random: fixed  61 labels. Loss 0.18775. Accuracy 0.949.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0994127
Train loss (w/o reg) on all data: 0.0671329
Test loss (w/o reg) on all data: 0.462178
Train acc on all data:  0.986947063089
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.44599e-05
Norm of the params: 25.4086
Flipped loss: 0.46218. Accuracy: 0.913
### Flips: 206, rs: 27, checks: 206
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052382
Train loss (w/o reg) on all data: 0.0340467
Test loss (w/o reg) on all data: 0.171891
Train acc on all data:  0.994198694706
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.66243e-06
Norm of the params: 19.1496
     Influence (LOO): fixed 134 labels. Loss 0.17189. Accuracy 0.971.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0487771
Train loss (w/o reg) on all data: 0.0239278
Test loss (w/o reg) on all data: 0.32188
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 1.23794e-05
Norm of the params: 22.2932
                Loss: fixed  99 labels. Loss 0.32188. Accuracy 0.951.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0966348
Train loss (w/o reg) on all data: 0.0652068
Test loss (w/o reg) on all data: 0.460187
Train acc on all data:  0.987672226251
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 1.18013e-05
Norm of the params: 25.0711
              Random: fixed  10 labels. Loss 0.46019. Accuracy 0.921.
### Flips: 206, rs: 27, checks: 412
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047732
Train loss (w/o reg) on all data: 0.0312617
Test loss (w/o reg) on all data: 0.181356
Train acc on all data:  0.99444041576
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 1.06054e-06
Norm of the params: 18.1496
     Influence (LOO): fixed 165 labels. Loss 0.18136. Accuracy 0.980.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0395252
Train loss (w/o reg) on all data: 0.0189372
Test loss (w/o reg) on all data: 0.223876
Train acc on all data:  0.999516557892
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 1.92062e-06
Norm of the params: 20.2919
                Loss: fixed 139 labels. Loss 0.22388. Accuracy 0.960.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0919156
Train loss (w/o reg) on all data: 0.0608728
Test loss (w/o reg) on all data: 0.445511
Train acc on all data:  0.988639110467
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 1.33397e-05
Norm of the params: 24.917
              Random: fixed  24 labels. Loss 0.44551. Accuracy 0.926.
### Flips: 206, rs: 27, checks: 618
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0437864
Train loss (w/o reg) on all data: 0.0283596
Test loss (w/o reg) on all data: 0.13065
Train acc on all data:  0.995165578922
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 8.1668e-07
Norm of the params: 17.5652
     Influence (LOO): fixed 179 labels. Loss 0.13065. Accuracy 0.977.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0347913
Train loss (w/o reg) on all data: 0.0165603
Test loss (w/o reg) on all data: 0.228575
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 4.55474e-06
Norm of the params: 19.095
                Loss: fixed 163 labels. Loss 0.22857. Accuracy 0.972.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0895734
Train loss (w/o reg) on all data: 0.0589736
Test loss (w/o reg) on all data: 0.418317
Train acc on all data:  0.988639110467
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.18901e-05
Norm of the params: 24.7385
              Random: fixed  31 labels. Loss 0.41832. Accuracy 0.930.
### Flips: 206, rs: 27, checks: 824
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0424438
Train loss (w/o reg) on all data: 0.0271744
Test loss (w/o reg) on all data: 0.130146
Train acc on all data:  0.995407299976
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.53477e-07
Norm of the params: 17.4754
     Influence (LOO): fixed 184 labels. Loss 0.13015. Accuracy 0.977.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0322434
Train loss (w/o reg) on all data: 0.0153563
Test loss (w/o reg) on all data: 0.207615
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 3.82675e-06
Norm of the params: 18.3778
                Loss: fixed 176 labels. Loss 0.20761. Accuracy 0.976.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0876689
Train loss (w/o reg) on all data: 0.057609
Test loss (w/o reg) on all data: 0.383909
Train acc on all data:  0.989122552574
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.29284e-05
Norm of the params: 24.5193
              Random: fixed  39 labels. Loss 0.38391. Accuracy 0.934.
### Flips: 206, rs: 27, checks: 1030
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0408618
Train loss (w/o reg) on all data: 0.0258032
Test loss (w/o reg) on all data: 0.122337
Train acc on all data:  0.995890742084
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 4.90281e-07
Norm of the params: 17.3543
     Influence (LOO): fixed 187 labels. Loss 0.12234. Accuracy 0.976.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0309315
Train loss (w/o reg) on all data: 0.0147909
Test loss (w/o reg) on all data: 0.161124
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.02583e-05
Norm of the params: 17.9669
                Loss: fixed 183 labels. Loss 0.16112. Accuracy 0.975.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0846742
Train loss (w/o reg) on all data: 0.0554931
Test loss (w/o reg) on all data: 0.378224
Train acc on all data:  0.99008943679
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 4.91267e-05
Norm of the params: 24.1583
              Random: fixed  48 labels. Loss 0.37822. Accuracy 0.939.
### Flips: 206, rs: 27, checks: 1236
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039379
Train loss (w/o reg) on all data: 0.0242782
Test loss (w/o reg) on all data: 0.125619
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 3.82733e-07
Norm of the params: 17.3786
     Influence (LOO): fixed 189 labels. Loss 0.12562. Accuracy 0.975.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0308353
Train loss (w/o reg) on all data: 0.0147505
Test loss (w/o reg) on all data: 0.159734
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 6.61383e-06
Norm of the params: 17.9359
                Loss: fixed 186 labels. Loss 0.15973. Accuracy 0.972.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0834505
Train loss (w/o reg) on all data: 0.0548508
Test loss (w/o reg) on all data: 0.389656
Train acc on all data:  0.989605994682
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 5.07827e-06
Norm of the params: 23.9164
              Random: fixed  57 labels. Loss 0.38966. Accuracy 0.941.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0992488
Train loss (w/o reg) on all data: 0.0679659
Test loss (w/o reg) on all data: 0.303431
Train acc on all data:  0.98573845782
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 9.22097e-06
Norm of the params: 25.0131
Flipped loss: 0.30343. Accuracy: 0.930
### Flips: 206, rs: 28, checks: 206
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0531948
Train loss (w/o reg) on all data: 0.0351119
Test loss (w/o reg) on all data: 0.155806
Train acc on all data:  0.99444041576
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.39093e-06
Norm of the params: 19.0173
     Influence (LOO): fixed 133 labels. Loss 0.15581. Accuracy 0.972.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048718
Train loss (w/o reg) on all data: 0.0241216
Test loss (w/o reg) on all data: 0.158298
Train acc on all data:  0.999516557892
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 4.02942e-06
Norm of the params: 22.1794
                Loss: fixed  99 labels. Loss 0.15830. Accuracy 0.955.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0954596
Train loss (w/o reg) on all data: 0.0647528
Test loss (w/o reg) on all data: 0.310071
Train acc on all data:  0.986221899927
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 3.86343e-06
Norm of the params: 24.7818
              Random: fixed  13 labels. Loss 0.31007. Accuracy 0.935.
### Flips: 206, rs: 28, checks: 412
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0483357
Train loss (w/o reg) on all data: 0.0323865
Test loss (w/o reg) on all data: 0.109641
Train acc on all data:  0.99444041576
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.25905e-07
Norm of the params: 17.8601
     Influence (LOO): fixed 167 labels. Loss 0.10964. Accuracy 0.975.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039096
Train loss (w/o reg) on all data: 0.0187582
Test loss (w/o reg) on all data: 0.132057
Train acc on all data:  0.999274836838
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 3.11114e-06
Norm of the params: 20.1682
                Loss: fixed 140 labels. Loss 0.13206. Accuracy 0.965.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0934829
Train loss (w/o reg) on all data: 0.0632225
Test loss (w/o reg) on all data: 0.305719
Train acc on all data:  0.986705342035
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 5.45801e-06
Norm of the params: 24.601
              Random: fixed  23 labels. Loss 0.30572. Accuracy 0.931.
### Flips: 206, rs: 28, checks: 618
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0460918
Train loss (w/o reg) on all data: 0.031028
Test loss (w/o reg) on all data: 0.10606
Train acc on all data:  0.994682136814
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 7.10292e-07
Norm of the params: 17.3573
     Influence (LOO): fixed 176 labels. Loss 0.10606. Accuracy 0.976.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034059
Train loss (w/o reg) on all data: 0.01624
Test loss (w/o reg) on all data: 0.106798
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 9.45741e-07
Norm of the params: 18.878
                Loss: fixed 164 labels. Loss 0.10680. Accuracy 0.976.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0908558
Train loss (w/o reg) on all data: 0.061449
Test loss (w/o reg) on all data: 0.312938
Train acc on all data:  0.986463620981
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.47198e-05
Norm of the params: 24.2515
              Random: fixed  34 labels. Loss 0.31294. Accuracy 0.936.
### Flips: 206, rs: 28, checks: 824
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0429236
Train loss (w/o reg) on all data: 0.0281844
Test loss (w/o reg) on all data: 0.0981443
Train acc on all data:  0.995407299976
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.22538e-07
Norm of the params: 17.1692
     Influence (LOO): fixed 186 labels. Loss 0.09814. Accuracy 0.977.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0326502
Train loss (w/o reg) on all data: 0.0155739
Test loss (w/o reg) on all data: 0.0957896
Train acc on all data:  0.999516557892
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 1.11754e-06
Norm of the params: 18.4804
                Loss: fixed 170 labels. Loss 0.09579. Accuracy 0.978.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0900121
Train loss (w/o reg) on all data: 0.0607461
Test loss (w/o reg) on all data: 0.322353
Train acc on all data:  0.986705342035
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 7.25347e-06
Norm of the params: 24.1934
              Random: fixed  38 labels. Loss 0.32235. Accuracy 0.934.
### Flips: 206, rs: 28, checks: 1030
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0421357
Train loss (w/o reg) on all data: 0.0273634
Test loss (w/o reg) on all data: 0.0958602
Train acc on all data:  0.99564902103
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 9.58431e-07
Norm of the params: 17.1885
     Influence (LOO): fixed 187 labels. Loss 0.09586. Accuracy 0.977.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0305984
Train loss (w/o reg) on all data: 0.0146077
Test loss (w/o reg) on all data: 0.144078
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.28889e-06
Norm of the params: 17.8834
                Loss: fixed 180 labels. Loss 0.14408. Accuracy 0.973.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088252
Train loss (w/o reg) on all data: 0.0592574
Test loss (w/o reg) on all data: 0.347905
Train acc on all data:  0.987188784143
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 9.75851e-06
Norm of the params: 24.0809
              Random: fixed  46 labels. Loss 0.34790. Accuracy 0.934.
### Flips: 206, rs: 28, checks: 1236
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0400036
Train loss (w/o reg) on all data: 0.0252108
Test loss (w/o reg) on all data: 0.0946961
Train acc on all data:  0.996374184191
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.10205e-06
Norm of the params: 17.2005
     Influence (LOO): fixed 190 labels. Loss 0.09470. Accuracy 0.977.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0296038
Train loss (w/o reg) on all data: 0.0141716
Test loss (w/o reg) on all data: 0.164168
Train acc on all data:  0.999274836838
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 7.74032e-07
Norm of the params: 17.5683
                Loss: fixed 185 labels. Loss 0.16417. Accuracy 0.975.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0861292
Train loss (w/o reg) on all data: 0.0576517
Test loss (w/o reg) on all data: 0.362004
Train acc on all data:  0.988155668359
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 3.85757e-06
Norm of the params: 23.8652
              Random: fixed  54 labels. Loss 0.36200. Accuracy 0.935.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102859
Train loss (w/o reg) on all data: 0.0716214
Test loss (w/o reg) on all data: 0.336958
Train acc on all data:  0.98452985255
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.06204e-05
Norm of the params: 24.9952
Flipped loss: 0.33696. Accuracy: 0.933
### Flips: 206, rs: 29, checks: 206
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0576221
Train loss (w/o reg) on all data: 0.0401167
Test loss (w/o reg) on all data: 0.236414
Train acc on all data:  0.992023205221
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 5.28185e-06
Norm of the params: 18.7112
     Influence (LOO): fixed 133 labels. Loss 0.23641. Accuracy 0.963.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0491162
Train loss (w/o reg) on all data: 0.0242391
Test loss (w/o reg) on all data: 0.258415
Train acc on all data:  0.999033115784
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 3.22481e-06
Norm of the params: 22.3057
                Loss: fixed 105 labels. Loss 0.25842. Accuracy 0.947.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0971916
Train loss (w/o reg) on all data: 0.0672031
Test loss (w/o reg) on all data: 0.340873
Train acc on all data:  0.985013294658
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 3.19413e-06
Norm of the params: 24.4902
              Random: fixed  16 labels. Loss 0.34087. Accuracy 0.931.
### Flips: 206, rs: 29, checks: 412
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0497093
Train loss (w/o reg) on all data: 0.0344736
Test loss (w/o reg) on all data: 0.127881
Train acc on all data:  0.992748368383
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 9.74558e-07
Norm of the params: 17.4561
     Influence (LOO): fixed 167 labels. Loss 0.12788. Accuracy 0.975.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0390382
Train loss (w/o reg) on all data: 0.0188346
Test loss (w/o reg) on all data: 0.17331
Train acc on all data:  0.999033115784
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 3.86669e-06
Norm of the params: 20.1015
                Loss: fixed 138 labels. Loss 0.17331. Accuracy 0.961.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0952607
Train loss (w/o reg) on all data: 0.065799
Test loss (w/o reg) on all data: 0.339728
Train acc on all data:  0.985255015712
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.32341e-05
Norm of the params: 24.2742
              Random: fixed  25 labels. Loss 0.33973. Accuracy 0.930.
### Flips: 206, rs: 29, checks: 618
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0478286
Train loss (w/o reg) on all data: 0.0328135
Test loss (w/o reg) on all data: 0.127026
Train acc on all data:  0.993956973652
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 5.40273e-07
Norm of the params: 17.3292
     Influence (LOO): fixed 180 labels. Loss 0.12703. Accuracy 0.976.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0353022
Train loss (w/o reg) on all data: 0.0170164
Test loss (w/o reg) on all data: 0.174194
Train acc on all data:  0.999274836838
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.27382e-06
Norm of the params: 19.1237
                Loss: fixed 154 labels. Loss 0.17419. Accuracy 0.961.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0914912
Train loss (w/o reg) on all data: 0.0630921
Test loss (w/o reg) on all data: 0.307285
Train acc on all data:  0.986463620981
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 4.67681e-06
Norm of the params: 23.8324
              Random: fixed  35 labels. Loss 0.30728. Accuracy 0.937.
### Flips: 206, rs: 29, checks: 824
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045708
Train loss (w/o reg) on all data: 0.030925
Test loss (w/o reg) on all data: 0.117731
Train acc on all data:  0.994198694706
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 8.76588e-07
Norm of the params: 17.1948
     Influence (LOO): fixed 183 labels. Loss 0.11773. Accuracy 0.973.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0339454
Train loss (w/o reg) on all data: 0.0163565
Test loss (w/o reg) on all data: 0.14825
Train acc on all data:  0.999274836838
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.78474e-06
Norm of the params: 18.7558
                Loss: fixed 164 labels. Loss 0.14825. Accuracy 0.965.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0885238
Train loss (w/o reg) on all data: 0.0607382
Test loss (w/o reg) on all data: 0.276096
Train acc on all data:  0.987430505197
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.42934e-05
Norm of the params: 23.5736
              Random: fixed  43 labels. Loss 0.27610. Accuracy 0.935.
### Flips: 206, rs: 29, checks: 1030
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0449706
Train loss (w/o reg) on all data: 0.0301945
Test loss (w/o reg) on all data: 0.118726
Train acc on all data:  0.99444041576
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.26559e-06
Norm of the params: 17.1908
     Influence (LOO): fixed 184 labels. Loss 0.11873. Accuracy 0.973.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0330194
Train loss (w/o reg) on all data: 0.0159011
Test loss (w/o reg) on all data: 0.149657
Train acc on all data:  0.999033115784
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 5.64103e-06
Norm of the params: 18.5032
                Loss: fixed 178 labels. Loss 0.14966. Accuracy 0.968.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0859018
Train loss (w/o reg) on all data: 0.058678
Test loss (w/o reg) on all data: 0.293033
Train acc on all data:  0.988155668359
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 2.7685e-06
Norm of the params: 23.334
              Random: fixed  52 labels. Loss 0.29303. Accuracy 0.932.
### Flips: 206, rs: 29, checks: 1236
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0406016
Train loss (w/o reg) on all data: 0.0260122
Test loss (w/o reg) on all data: 0.12392
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.56608e-06
Norm of the params: 17.0818
     Influence (LOO): fixed 191 labels. Loss 0.12392. Accuracy 0.975.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0322913
Train loss (w/o reg) on all data: 0.0155329
Test loss (w/o reg) on all data: 0.140659
Train acc on all data:  0.999274836838
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 9.48786e-07
Norm of the params: 18.3076
                Loss: fixed 188 labels. Loss 0.14066. Accuracy 0.976.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0818013
Train loss (w/o reg) on all data: 0.0549817
Test loss (w/o reg) on all data: 0.282296
Train acc on all data:  0.989605994682
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 3.92119e-06
Norm of the params: 23.1601
              Random: fixed  64 labels. Loss 0.28230. Accuracy 0.937.
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102276
Train loss (w/o reg) on all data: 0.0708689
Test loss (w/o reg) on all data: 0.400379
Train acc on all data:  0.98573845782
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 3.958e-05
Norm of the params: 25.0627
Flipped loss: 0.40038. Accuracy: 0.935
### Flips: 206, rs: 30, checks: 206
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0545734
Train loss (w/o reg) on all data: 0.0365394
Test loss (w/o reg) on all data: 0.111585
Train acc on all data:  0.993715252599
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 6.69995e-06
Norm of the params: 18.9915
     Influence (LOO): fixed 131 labels. Loss 0.11158. Accuracy 0.973.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0478093
Train loss (w/o reg) on all data: 0.0235641
Test loss (w/o reg) on all data: 0.187952
Train acc on all data:  0.999516557892
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 1.80288e-05
Norm of the params: 22.0206
                Loss: fixed 101 labels. Loss 0.18795. Accuracy 0.956.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0982524
Train loss (w/o reg) on all data: 0.0672276
Test loss (w/o reg) on all data: 0.406629
Train acc on all data:  0.986947063089
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 7.89456e-06
Norm of the params: 24.9098
              Random: fixed  11 labels. Loss 0.40663. Accuracy 0.937.
### Flips: 206, rs: 30, checks: 412
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0490557
Train loss (w/o reg) on all data: 0.0333527
Test loss (w/o reg) on all data: 0.146582
Train acc on all data:  0.993715252599
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.30475e-06
Norm of the params: 17.7218
     Influence (LOO): fixed 163 labels. Loss 0.14658. Accuracy 0.975.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0407443
Train loss (w/o reg) on all data: 0.0197658
Test loss (w/o reg) on all data: 0.221215
Train acc on all data:  0.999516557892
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 5.72984e-06
Norm of the params: 20.4834
                Loss: fixed 142 labels. Loss 0.22122. Accuracy 0.959.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093975
Train loss (w/o reg) on all data: 0.063622
Test loss (w/o reg) on all data: 0.419832
Train acc on all data:  0.988397389413
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 1.74092e-05
Norm of the params: 24.6386
              Random: fixed  25 labels. Loss 0.41983. Accuracy 0.944.
### Flips: 206, rs: 30, checks: 618
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0479233
Train loss (w/o reg) on all data: 0.032772
Test loss (w/o reg) on all data: 0.167606
Train acc on all data:  0.993715252599
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 7.98773e-07
Norm of the params: 17.4076
     Influence (LOO): fixed 171 labels. Loss 0.16761. Accuracy 0.975.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0374493
Train loss (w/o reg) on all data: 0.0179579
Test loss (w/o reg) on all data: 0.191697
Train acc on all data:  0.999516557892
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 2.02792e-06
Norm of the params: 19.7441
                Loss: fixed 158 labels. Loss 0.19170. Accuracy 0.959.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0899572
Train loss (w/o reg) on all data: 0.0602401
Test loss (w/o reg) on all data: 0.399438
Train acc on all data:  0.989605994682
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 3.26151e-05
Norm of the params: 24.3791
              Random: fixed  38 labels. Loss 0.39944. Accuracy 0.946.
### Flips: 206, rs: 30, checks: 824
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0438077
Train loss (w/o reg) on all data: 0.0288493
Test loss (w/o reg) on all data: 0.120552
Train acc on all data:  0.994923857868
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.3511e-06
Norm of the params: 17.2964
     Influence (LOO): fixed 178 labels. Loss 0.12055. Accuracy 0.972.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0337938
Train loss (w/o reg) on all data: 0.0160923
Test loss (w/o reg) on all data: 0.133143
Train acc on all data:  0.999516557892
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 6.57394e-07
Norm of the params: 18.8157
                Loss: fixed 172 labels. Loss 0.13314. Accuracy 0.966.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888054
Train loss (w/o reg) on all data: 0.0595415
Test loss (w/o reg) on all data: 0.405629
Train acc on all data:  0.989605994682
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 9.45358e-06
Norm of the params: 24.1925
              Random: fixed  44 labels. Loss 0.40563. Accuracy 0.943.
### Flips: 206, rs: 30, checks: 1030
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042442
Train loss (w/o reg) on all data: 0.0278004
Test loss (w/o reg) on all data: 0.121225
Train acc on all data:  0.995165578922
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 9.03289e-07
Norm of the params: 17.1124
     Influence (LOO): fixed 182 labels. Loss 0.12122. Accuracy 0.972.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0324907
Train loss (w/o reg) on all data: 0.0154953
Test loss (w/o reg) on all data: 0.171499
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 9.45167e-07
Norm of the params: 18.4366
                Loss: fixed 179 labels. Loss 0.17150. Accuracy 0.973.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0844402
Train loss (w/o reg) on all data: 0.0557277
Test loss (w/o reg) on all data: 0.373178
Train acc on all data:  0.99008943679
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 1.54892e-05
Norm of the params: 23.9635
              Random: fixed  55 labels. Loss 0.37318. Accuracy 0.946.
### Flips: 206, rs: 30, checks: 1236
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0399773
Train loss (w/o reg) on all data: 0.0252775
Test loss (w/o reg) on all data: 0.137022
Train acc on all data:  0.995890742084
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.39509e-06
Norm of the params: 17.1463
     Influence (LOO): fixed 187 labels. Loss 0.13702. Accuracy 0.976.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0301212
Train loss (w/o reg) on all data: 0.0144353
Test loss (w/o reg) on all data: 0.186033
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.82499e-06
Norm of the params: 17.7121
                Loss: fixed 188 labels. Loss 0.18603. Accuracy 0.973.
Using normal model
LBFGS training took [411] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0817356
Train loss (w/o reg) on all data: 0.0535919
Test loss (w/o reg) on all data: 0.380791
Train acc on all data:  0.991298042059
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 1.50857e-05
Norm of the params: 23.725
              Random: fixed  64 labels. Loss 0.38079. Accuracy 0.943.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984407
Train loss (w/o reg) on all data: 0.0653019
Test loss (w/o reg) on all data: 0.513691
Train acc on all data:  0.987672226251
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.69343e-05
Norm of the params: 25.7444
Flipped loss: 0.51369. Accuracy: 0.924
### Flips: 206, rs: 31, checks: 206
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0466606
Train loss (w/o reg) on all data: 0.0281394
Test loss (w/o reg) on all data: 0.147934
Train acc on all data:  0.996615905245
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.10986e-06
Norm of the params: 19.2464
     Influence (LOO): fixed 146 labels. Loss 0.14793. Accuracy 0.963.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0492561
Train loss (w/o reg) on all data: 0.0239986
Test loss (w/o reg) on all data: 0.187063
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 4.8795e-06
Norm of the params: 22.4756
                Loss: fixed  95 labels. Loss 0.18706. Accuracy 0.951.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0971177
Train loss (w/o reg) on all data: 0.0643533
Test loss (w/o reg) on all data: 0.460953
Train acc on all data:  0.987672226251
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 5.027e-05
Norm of the params: 25.5986
              Random: fixed   6 labels. Loss 0.46095. Accuracy 0.930.
### Flips: 206, rs: 31, checks: 412
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0414096
Train loss (w/o reg) on all data: 0.0254005
Test loss (w/o reg) on all data: 0.11459
Train acc on all data:  0.996615905245
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 1.71809e-06
Norm of the params: 17.8937
     Influence (LOO): fixed 176 labels. Loss 0.11459. Accuracy 0.970.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0390798
Train loss (w/o reg) on all data: 0.0186575
Test loss (w/o reg) on all data: 0.160524
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 3.61132e-06
Norm of the params: 20.2101
                Loss: fixed 138 labels. Loss 0.16052. Accuracy 0.951.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0947017
Train loss (w/o reg) on all data: 0.0624536
Test loss (w/o reg) on all data: 0.422009
Train acc on all data:  0.988397389413
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 4.48878e-05
Norm of the params: 25.3961
              Random: fixed  12 labels. Loss 0.42201. Accuracy 0.929.
### Flips: 206, rs: 31, checks: 618
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0396732
Train loss (w/o reg) on all data: 0.0243486
Test loss (w/o reg) on all data: 0.101024
Train acc on all data:  0.996857626299
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 1.86301e-06
Norm of the params: 17.5069
     Influence (LOO): fixed 188 labels. Loss 0.10102. Accuracy 0.978.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0355093
Train loss (w/o reg) on all data: 0.0169433
Test loss (w/o reg) on all data: 0.150069
Train acc on all data:  0.999516557892
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.22969e-05
Norm of the params: 19.2696
                Loss: fixed 160 labels. Loss 0.15007. Accuracy 0.961.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0905227
Train loss (w/o reg) on all data: 0.0592474
Test loss (w/o reg) on all data: 0.413036
Train acc on all data:  0.989847715736
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 7.66787e-06
Norm of the params: 25.0101
              Random: fixed  27 labels. Loss 0.41304. Accuracy 0.939.
### Flips: 206, rs: 31, checks: 824
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0370707
Train loss (w/o reg) on all data: 0.0217749
Test loss (w/o reg) on all data: 0.102659
Train acc on all data:  0.997824510515
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 9.51742e-07
Norm of the params: 17.4905
     Influence (LOO): fixed 195 labels. Loss 0.10266. Accuracy 0.979.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0342049
Train loss (w/o reg) on all data: 0.0163342
Test loss (w/o reg) on all data: 0.144862
Train acc on all data:  0.999516557892
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 5.05248e-06
Norm of the params: 18.9054
                Loss: fixed 169 labels. Loss 0.14486. Accuracy 0.965.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883465
Train loss (w/o reg) on all data: 0.0575902
Test loss (w/o reg) on all data: 0.348523
Train acc on all data:  0.99008943679
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 5.24633e-06
Norm of the params: 24.8017
              Random: fixed  35 labels. Loss 0.34852. Accuracy 0.936.
### Flips: 206, rs: 31, checks: 1030
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0344083
Train loss (w/o reg) on all data: 0.0190037
Test loss (w/o reg) on all data: 0.0968019
Train acc on all data:  0.998307952623
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 4.09551e-07
Norm of the params: 17.5526
     Influence (LOO): fixed 197 labels. Loss 0.09680. Accuracy 0.979.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0318181
Train loss (w/o reg) on all data: 0.0152246
Test loss (w/o reg) on all data: 0.112245
Train acc on all data:  0.999516557892
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 7.83013e-06
Norm of the params: 18.2173
                Loss: fixed 180 labels. Loss 0.11225. Accuracy 0.969.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0829687
Train loss (w/o reg) on all data: 0.0535991
Test loss (w/o reg) on all data: 0.311131
Train acc on all data:  0.990814599952
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 6.09632e-05
Norm of the params: 24.2362
              Random: fixed  55 labels. Loss 0.31113. Accuracy 0.948.
### Flips: 206, rs: 31, checks: 1236
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0334777
Train loss (w/o reg) on all data: 0.0185481
Test loss (w/o reg) on all data: 0.149585
Train acc on all data:  0.998066231569
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.74716e-07
Norm of the params: 17.2798
     Influence (LOO): fixed 199 labels. Loss 0.14959. Accuracy 0.977.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0313796
Train loss (w/o reg) on all data: 0.0150541
Test loss (w/o reg) on all data: 0.108646
Train acc on all data:  0.999516557892
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.65855e-06
Norm of the params: 18.0696
                Loss: fixed 185 labels. Loss 0.10865. Accuracy 0.972.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075751
Train loss (w/o reg) on all data: 0.0470089
Test loss (w/o reg) on all data: 0.308402
Train acc on all data:  0.992506647329
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 2.03665e-05
Norm of the params: 23.9758
              Random: fixed  72 labels. Loss 0.30840. Accuracy 0.954.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103303
Train loss (w/o reg) on all data: 0.0715073
Test loss (w/o reg) on all data: 0.281705
Train acc on all data:  0.983804689388
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 3.32183e-05
Norm of the params: 25.2174
Flipped loss: 0.28171. Accuracy: 0.919
### Flips: 206, rs: 32, checks: 206
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0516307
Train loss (w/o reg) on all data: 0.0334836
Test loss (w/o reg) on all data: 0.157037
Train acc on all data:  0.99444041576
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 2.44772e-06
Norm of the params: 19.051
     Influence (LOO): fixed 139 labels. Loss 0.15704. Accuracy 0.957.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0502995
Train loss (w/o reg) on all data: 0.0249291
Test loss (w/o reg) on all data: 0.215385
Train acc on all data:  0.999033115784
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 5.61199e-06
Norm of the params: 22.5257
                Loss: fixed  99 labels. Loss 0.21538. Accuracy 0.937.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0996926
Train loss (w/o reg) on all data: 0.0684198
Test loss (w/o reg) on all data: 0.259556
Train acc on all data:  0.985255015712
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 2.18231e-05
Norm of the params: 25.0091
              Random: fixed  14 labels. Loss 0.25956. Accuracy 0.919.
### Flips: 206, rs: 32, checks: 412
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0480226
Train loss (w/o reg) on all data: 0.0316062
Test loss (w/o reg) on all data: 0.193745
Train acc on all data:  0.994198694706
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.84428e-06
Norm of the params: 18.1198
     Influence (LOO): fixed 168 labels. Loss 0.19375. Accuracy 0.971.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0388124
Train loss (w/o reg) on all data: 0.0186917
Test loss (w/o reg) on all data: 0.145145
Train acc on all data:  0.999516557892
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 6.13159e-06
Norm of the params: 20.0602
                Loss: fixed 141 labels. Loss 0.14514. Accuracy 0.956.
Using normal model
LBFGS training took [376] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0964952
Train loss (w/o reg) on all data: 0.0657035
Test loss (w/o reg) on all data: 0.24494
Train acc on all data:  0.986221899927
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 9.1724e-06
Norm of the params: 24.816
              Random: fixed  26 labels. Loss 0.24494. Accuracy 0.930.
### Flips: 206, rs: 32, checks: 618
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0469237
Train loss (w/o reg) on all data: 0.0310178
Test loss (w/o reg) on all data: 0.224568
Train acc on all data:  0.99444041576
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 7.64521e-07
Norm of the params: 17.8358
     Influence (LOO): fixed 176 labels. Loss 0.22457. Accuracy 0.975.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0346937
Train loss (w/o reg) on all data: 0.016607
Test loss (w/o reg) on all data: 0.138934
Train acc on all data:  0.999516557892
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 4.75845e-06
Norm of the params: 19.0193
                Loss: fixed 162 labels. Loss 0.13893. Accuracy 0.965.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0930675
Train loss (w/o reg) on all data: 0.0629695
Test loss (w/o reg) on all data: 0.251503
Train acc on all data:  0.986705342035
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 3.0462e-05
Norm of the params: 24.5349
              Random: fixed  36 labels. Loss 0.25150. Accuracy 0.933.
### Flips: 206, rs: 32, checks: 824
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0453107
Train loss (w/o reg) on all data: 0.0296218
Test loss (w/o reg) on all data: 0.172992
Train acc on all data:  0.994682136814
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 3.91326e-06
Norm of the params: 17.7138
     Influence (LOO): fixed 182 labels. Loss 0.17299. Accuracy 0.980.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0336586
Train loss (w/o reg) on all data: 0.0161319
Test loss (w/o reg) on all data: 0.164325
Train acc on all data:  0.999274836838
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 9.94167e-06
Norm of the params: 18.7226
                Loss: fixed 171 labels. Loss 0.16432. Accuracy 0.965.
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09003
Train loss (w/o reg) on all data: 0.0600844
Test loss (w/o reg) on all data: 0.244301
Train acc on all data:  0.987672226251
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 7.02754e-06
Norm of the params: 24.4727
              Random: fixed  44 labels. Loss 0.24430. Accuracy 0.931.
### Flips: 206, rs: 32, checks: 1030
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430526
Train loss (w/o reg) on all data: 0.0276309
Test loss (w/o reg) on all data: 0.162686
Train acc on all data:  0.995407299976
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 1.00228e-06
Norm of the params: 17.5623
     Influence (LOO): fixed 188 labels. Loss 0.16269. Accuracy 0.980.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03225
Train loss (w/o reg) on all data: 0.0154332
Test loss (w/o reg) on all data: 0.158846
Train acc on all data:  0.999516557892
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.2241e-06
Norm of the params: 18.3395
                Loss: fixed 177 labels. Loss 0.15885. Accuracy 0.969.
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0870224
Train loss (w/o reg) on all data: 0.0575629
Test loss (w/o reg) on all data: 0.234016
Train acc on all data:  0.988639110467
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 6.47544e-06
Norm of the params: 24.2732
              Random: fixed  56 labels. Loss 0.23402. Accuracy 0.936.
### Flips: 206, rs: 32, checks: 1236
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039975
Train loss (w/o reg) on all data: 0.0245136
Test loss (w/o reg) on all data: 0.154121
Train acc on all data:  0.996374184191
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.35076e-06
Norm of the params: 17.5849
     Influence (LOO): fixed 191 labels. Loss 0.15412. Accuracy 0.977.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0317051
Train loss (w/o reg) on all data: 0.0151472
Test loss (w/o reg) on all data: 0.170598
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.44966e-06
Norm of the params: 18.1978
                Loss: fixed 182 labels. Loss 0.17060. Accuracy 0.971.
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0851517
Train loss (w/o reg) on all data: 0.0561588
Test loss (w/o reg) on all data: 0.212718
Train acc on all data:  0.989122552574
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 6.31197e-05
Norm of the params: 24.0802
              Random: fixed  63 labels. Loss 0.21272. Accuracy 0.940.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110125
Train loss (w/o reg) on all data: 0.0782356
Test loss (w/o reg) on all data: 0.214626
Train acc on all data:  0.983321247281
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 2.95379e-05
Norm of the params: 25.2546
Flipped loss: 0.21463. Accuracy: 0.929
### Flips: 206, rs: 33, checks: 206
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0547105
Train loss (w/o reg) on all data: 0.0369273
Test loss (w/o reg) on all data: 0.174252
Train acc on all data:  0.992748368383
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.33379e-06
Norm of the params: 18.8591
     Influence (LOO): fixed 146 labels. Loss 0.17425. Accuracy 0.973.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0483387
Train loss (w/o reg) on all data: 0.0237652
Test loss (w/o reg) on all data: 0.245567
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 6.38664e-06
Norm of the params: 22.1692
                Loss: fixed 108 labels. Loss 0.24557. Accuracy 0.951.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10627
Train loss (w/o reg) on all data: 0.0749836
Test loss (w/o reg) on all data: 0.212988
Train acc on all data:  0.983804689388
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.47627e-05
Norm of the params: 25.0145
              Random: fixed   9 labels. Loss 0.21299. Accuracy 0.935.
### Flips: 206, rs: 33, checks: 412
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0503378
Train loss (w/o reg) on all data: 0.034279
Test loss (w/o reg) on all data: 0.207231
Train acc on all data:  0.993231810491
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 2.03346e-06
Norm of the params: 17.9214
     Influence (LOO): fixed 167 labels. Loss 0.20723. Accuracy 0.974.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0399876
Train loss (w/o reg) on all data: 0.0193178
Test loss (w/o reg) on all data: 0.247962
Train acc on all data:  0.999516557892
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 1.52229e-05
Norm of the params: 20.3322
                Loss: fixed 138 labels. Loss 0.24796. Accuracy 0.957.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102638
Train loss (w/o reg) on all data: 0.0715054
Test loss (w/o reg) on all data: 0.211589
Train acc on all data:  0.985013294658
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 7.07232e-06
Norm of the params: 24.9528
              Random: fixed  16 labels. Loss 0.21159. Accuracy 0.934.
### Flips: 206, rs: 33, checks: 618
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0488127
Train loss (w/o reg) on all data: 0.0332343
Test loss (w/o reg) on all data: 0.203677
Train acc on all data:  0.993473531545
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.43495e-06
Norm of the params: 17.6513
     Influence (LOO): fixed 173 labels. Loss 0.20368. Accuracy 0.975.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036255
Train loss (w/o reg) on all data: 0.0173198
Test loss (w/o reg) on all data: 0.247137
Train acc on all data:  0.999516557892
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 2.33231e-06
Norm of the params: 19.4603
                Loss: fixed 151 labels. Loss 0.24714. Accuracy 0.960.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.099836
Train loss (w/o reg) on all data: 0.0691774
Test loss (w/o reg) on all data: 0.201517
Train acc on all data:  0.985496736766
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.95411e-05
Norm of the params: 24.7623
              Random: fixed  28 labels. Loss 0.20152. Accuracy 0.936.
### Flips: 206, rs: 33, checks: 824
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0476588
Train loss (w/o reg) on all data: 0.0322167
Test loss (w/o reg) on all data: 0.19494
Train acc on all data:  0.993956973652
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.78638e-06
Norm of the params: 17.5739
     Influence (LOO): fixed 178 labels. Loss 0.19494. Accuracy 0.975.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0341878
Train loss (w/o reg) on all data: 0.0163417
Test loss (w/o reg) on all data: 0.243643
Train acc on all data:  0.999516557892
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 4.70612e-06
Norm of the params: 18.8924
                Loss: fixed 164 labels. Loss 0.24364. Accuracy 0.967.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0989226
Train loss (w/o reg) on all data: 0.0685645
Test loss (w/o reg) on all data: 0.200594
Train acc on all data:  0.985496736766
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.29932e-06
Norm of the params: 24.6406
              Random: fixed  34 labels. Loss 0.20059. Accuracy 0.935.
### Flips: 206, rs: 33, checks: 1030
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0471651
Train loss (w/o reg) on all data: 0.0318025
Test loss (w/o reg) on all data: 0.194801
Train acc on all data:  0.994198694706
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.06093e-06
Norm of the params: 17.5286
     Influence (LOO): fixed 179 labels. Loss 0.19480. Accuracy 0.975.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0332219
Train loss (w/o reg) on all data: 0.0158954
Test loss (w/o reg) on all data: 0.184558
Train acc on all data:  0.999516557892
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.89802e-06
Norm of the params: 18.6153
                Loss: fixed 173 labels. Loss 0.18456. Accuracy 0.970.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0946988
Train loss (w/o reg) on all data: 0.0653336
Test loss (w/o reg) on all data: 0.204453
Train acc on all data:  0.986947063089
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 6.79624e-06
Norm of the params: 24.2343
              Random: fixed  46 labels. Loss 0.20445. Accuracy 0.942.
### Flips: 206, rs: 33, checks: 1236
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045163
Train loss (w/o reg) on all data: 0.0299061
Test loss (w/o reg) on all data: 0.19143
Train acc on all data:  0.994682136814
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.07481e-06
Norm of the params: 17.4682
     Influence (LOO): fixed 182 labels. Loss 0.19143. Accuracy 0.974.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0315179
Train loss (w/o reg) on all data: 0.01511
Test loss (w/o reg) on all data: 0.170387
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 6.97743e-06
Norm of the params: 18.1152
                Loss: fixed 183 labels. Loss 0.17039. Accuracy 0.974.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0896517
Train loss (w/o reg) on all data: 0.0605425
Test loss (w/o reg) on all data: 0.18829
Train acc on all data:  0.988155668359
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.53929e-05
Norm of the params: 24.1285
              Random: fixed  61 labels. Loss 0.18829. Accuracy 0.945.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103434
Train loss (w/o reg) on all data: 0.0728675
Test loss (w/o reg) on all data: 0.256688
Train acc on all data:  0.985496736766
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 9.79746e-06
Norm of the params: 24.7251
Flipped loss: 0.25669. Accuracy: 0.930
### Flips: 206, rs: 34, checks: 206
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0508861
Train loss (w/o reg) on all data: 0.0333522
Test loss (w/o reg) on all data: 0.161079
Train acc on all data:  0.994198694706
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.2037e-06
Norm of the params: 18.7264
     Influence (LOO): fixed 136 labels. Loss 0.16108. Accuracy 0.963.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0479657
Train loss (w/o reg) on all data: 0.0236383
Test loss (w/o reg) on all data: 0.173294
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 2.11315e-06
Norm of the params: 22.0579
                Loss: fixed  96 labels. Loss 0.17329. Accuracy 0.951.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10053
Train loss (w/o reg) on all data: 0.070453
Test loss (w/o reg) on all data: 0.249076
Train acc on all data:  0.986463620981
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.2081e-05
Norm of the params: 24.5262
              Random: fixed   7 labels. Loss 0.24908. Accuracy 0.933.
### Flips: 206, rs: 34, checks: 412
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0472484
Train loss (w/o reg) on all data: 0.0313359
Test loss (w/o reg) on all data: 0.182303
Train acc on all data:  0.994682136814
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 3.80732e-06
Norm of the params: 17.8395
     Influence (LOO): fixed 164 labels. Loss 0.18230. Accuracy 0.971.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0375959
Train loss (w/o reg) on all data: 0.0179712
Test loss (w/o reg) on all data: 0.129315
Train acc on all data:  0.999274836838
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 5.21502e-06
Norm of the params: 19.8115
                Loss: fixed 141 labels. Loss 0.12932. Accuracy 0.961.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0989151
Train loss (w/o reg) on all data: 0.0689397
Test loss (w/o reg) on all data: 0.25886
Train acc on all data:  0.987188784143
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 4.08775e-06
Norm of the params: 24.4848
              Random: fixed  15 labels. Loss 0.25886. Accuracy 0.932.
### Flips: 206, rs: 34, checks: 618
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0459866
Train loss (w/o reg) on all data: 0.0302157
Test loss (w/o reg) on all data: 0.168645
Train acc on all data:  0.994923857868
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.30292e-06
Norm of the params: 17.76
     Influence (LOO): fixed 172 labels. Loss 0.16864. Accuracy 0.973.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0348267
Train loss (w/o reg) on all data: 0.0166492
Test loss (w/o reg) on all data: 0.174945
Train acc on all data:  0.999516557892
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 5.45136e-06
Norm of the params: 19.067
                Loss: fixed 161 labels. Loss 0.17495. Accuracy 0.967.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0944415
Train loss (w/o reg) on all data: 0.065336
Test loss (w/o reg) on all data: 0.249208
Train acc on all data:  0.987188784143
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 6.41695e-06
Norm of the params: 24.1269
              Random: fixed  26 labels. Loss 0.24921. Accuracy 0.941.
### Flips: 206, rs: 34, checks: 824
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0418009
Train loss (w/o reg) on all data: 0.0270821
Test loss (w/o reg) on all data: 0.156793
Train acc on all data:  0.995890742084
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 8.58778e-07
Norm of the params: 17.1574
     Influence (LOO): fixed 185 labels. Loss 0.15679. Accuracy 0.978.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0334831
Train loss (w/o reg) on all data: 0.0160138
Test loss (w/o reg) on all data: 0.18966
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.15843e-06
Norm of the params: 18.6918
                Loss: fixed 169 labels. Loss 0.18966. Accuracy 0.973.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0912124
Train loss (w/o reg) on all data: 0.0628579
Test loss (w/o reg) on all data: 0.235552
Train acc on all data:  0.987913947305
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 6.80358e-06
Norm of the params: 23.8136
              Random: fixed  35 labels. Loss 0.23555. Accuracy 0.944.
### Flips: 206, rs: 34, checks: 1030
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0401572
Train loss (w/o reg) on all data: 0.025486
Test loss (w/o reg) on all data: 0.159679
Train acc on all data:  0.996132463138
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.29555e-07
Norm of the params: 17.1296
     Influence (LOO): fixed 190 labels. Loss 0.15968. Accuracy 0.977.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0324256
Train loss (w/o reg) on all data: 0.0155086
Test loss (w/o reg) on all data: 0.134931
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.26167e-06
Norm of the params: 18.394
                Loss: fixed 175 labels. Loss 0.13493. Accuracy 0.976.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0869667
Train loss (w/o reg) on all data: 0.0592116
Test loss (w/o reg) on all data: 0.231435
Train acc on all data:  0.99008943679
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 5.08551e-06
Norm of the params: 23.5606
              Random: fixed  49 labels. Loss 0.23144. Accuracy 0.945.
### Flips: 206, rs: 34, checks: 1236
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038704
Train loss (w/o reg) on all data: 0.0239488
Test loss (w/o reg) on all data: 0.163079
Train acc on all data:  0.996615905245
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 8.49822e-07
Norm of the params: 17.1786
     Influence (LOO): fixed 193 labels. Loss 0.16308. Accuracy 0.977.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0308536
Train loss (w/o reg) on all data: 0.0147796
Test loss (w/o reg) on all data: 0.124212
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.58386e-06
Norm of the params: 17.9298
                Loss: fixed 183 labels. Loss 0.12421. Accuracy 0.974.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0853503
Train loss (w/o reg) on all data: 0.0577759
Test loss (w/o reg) on all data: 0.244475
Train acc on all data:  0.990572878898
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 4.49764e-06
Norm of the params: 23.4838
              Random: fixed  56 labels. Loss 0.24448. Accuracy 0.948.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10168
Train loss (w/o reg) on all data: 0.0691893
Test loss (w/o reg) on all data: 0.436614
Train acc on all data:  0.98573845782
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 1.20266e-05
Norm of the params: 25.4914
Flipped loss: 0.43661. Accuracy: 0.923
### Flips: 206, rs: 35, checks: 206
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0535066
Train loss (w/o reg) on all data: 0.0347182
Test loss (w/o reg) on all data: 0.156726
Train acc on all data:  0.993956973652
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 5.30639e-06
Norm of the params: 19.3848
     Influence (LOO): fixed 137 labels. Loss 0.15673. Accuracy 0.970.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0507062
Train loss (w/o reg) on all data: 0.0250949
Test loss (w/o reg) on all data: 0.253644
Train acc on all data:  0.999033115784
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.00809e-05
Norm of the params: 22.6324
                Loss: fixed 103 labels. Loss 0.25364. Accuracy 0.945.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098462
Train loss (w/o reg) on all data: 0.066402
Test loss (w/o reg) on all data: 0.431023
Train acc on all data:  0.987430505197
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.0783e-05
Norm of the params: 25.3219
              Random: fixed   9 labels. Loss 0.43102. Accuracy 0.931.
### Flips: 206, rs: 35, checks: 412
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0468536
Train loss (w/o reg) on all data: 0.0302847
Test loss (w/o reg) on all data: 0.0946115
Train acc on all data:  0.99444041576
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 1.69567e-06
Norm of the params: 18.2037
     Influence (LOO): fixed 168 labels. Loss 0.09461. Accuracy 0.979.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0412624
Train loss (w/o reg) on all data: 0.0198218
Test loss (w/o reg) on all data: 0.172385
Train acc on all data:  0.999274836838
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 1.43544e-05
Norm of the params: 20.7078
                Loss: fixed 144 labels. Loss 0.17239. Accuracy 0.959.
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0960718
Train loss (w/o reg) on all data: 0.0651148
Test loss (w/o reg) on all data: 0.413525
Train acc on all data:  0.986947063089
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 2.01102e-05
Norm of the params: 24.8825
              Random: fixed  21 labels. Loss 0.41353. Accuracy 0.934.
### Flips: 206, rs: 35, checks: 618
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043683
Train loss (w/o reg) on all data: 0.0279163
Test loss (w/o reg) on all data: 0.12009
Train acc on all data:  0.995407299976
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 6.31023e-07
Norm of the params: 17.7577
     Influence (LOO): fixed 180 labels. Loss 0.12009. Accuracy 0.978.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036095
Train loss (w/o reg) on all data: 0.017274
Test loss (w/o reg) on all data: 0.116662
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 6.60544e-06
Norm of the params: 19.4016
                Loss: fixed 165 labels. Loss 0.11666. Accuracy 0.971.
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0943234
Train loss (w/o reg) on all data: 0.063834
Test loss (w/o reg) on all data: 0.392866
Train acc on all data:  0.987672226251
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 3.18729e-05
Norm of the params: 24.6939
              Random: fixed  27 labels. Loss 0.39287. Accuracy 0.934.
### Flips: 206, rs: 35, checks: 824
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0419403
Train loss (w/o reg) on all data: 0.0265732
Test loss (w/o reg) on all data: 0.108008
Train acc on all data:  0.996374184191
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.49989e-07
Norm of the params: 17.5312
     Influence (LOO): fixed 185 labels. Loss 0.10801. Accuracy 0.977.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0335221
Train loss (w/o reg) on all data: 0.0159589
Test loss (w/o reg) on all data: 0.10366
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.20404e-05
Norm of the params: 18.742
                Loss: fixed 179 labels. Loss 0.10366. Accuracy 0.976.
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0919774
Train loss (w/o reg) on all data: 0.0619767
Test loss (w/o reg) on all data: 0.390673
Train acc on all data:  0.987672226251
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 3.05466e-05
Norm of the params: 24.4952
              Random: fixed  36 labels. Loss 0.39067. Accuracy 0.938.
### Flips: 206, rs: 35, checks: 1030
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0407118
Train loss (w/o reg) on all data: 0.025384
Test loss (w/o reg) on all data: 0.102174
Train acc on all data:  0.996374184191
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.27073e-06
Norm of the params: 17.5088
     Influence (LOO): fixed 188 labels. Loss 0.10217. Accuracy 0.977.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0327918
Train loss (w/o reg) on all data: 0.0156264
Test loss (w/o reg) on all data: 0.0946425
Train acc on all data:  0.999516557892
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 8.36953e-06
Norm of the params: 18.5285
                Loss: fixed 186 labels. Loss 0.09464. Accuracy 0.978.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0905621
Train loss (w/o reg) on all data: 0.0608299
Test loss (w/o reg) on all data: 0.397818
Train acc on all data:  0.98888083152
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 9.36403e-06
Norm of the params: 24.3853
              Random: fixed  46 labels. Loss 0.39782. Accuracy 0.936.
### Flips: 206, rs: 35, checks: 1236
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0378672
Train loss (w/o reg) on all data: 0.0228895
Test loss (w/o reg) on all data: 0.139223
Train acc on all data:  0.996615905245
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.9865e-07
Norm of the params: 17.3076
     Influence (LOO): fixed 192 labels. Loss 0.13922. Accuracy 0.974.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0313253
Train loss (w/o reg) on all data: 0.0149807
Test loss (w/o reg) on all data: 0.0926106
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 2.71873e-06
Norm of the params: 18.0802
                Loss: fixed 191 labels. Loss 0.09261. Accuracy 0.975.
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0843861
Train loss (w/o reg) on all data: 0.0555935
Test loss (w/o reg) on all data: 0.406743
Train acc on all data:  0.989122552574
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 9.22768e-06
Norm of the params: 23.9969
              Random: fixed  60 labels. Loss 0.40674. Accuracy 0.942.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103712
Train loss (w/o reg) on all data: 0.0710575
Test loss (w/o reg) on all data: 0.228326
Train acc on all data:  0.98573845782
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.01695e-05
Norm of the params: 25.5556
Flipped loss: 0.22833. Accuracy: 0.929
### Flips: 206, rs: 36, checks: 206
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050935
Train loss (w/o reg) on all data: 0.0332377
Test loss (w/o reg) on all data: 0.108016
Train acc on all data:  0.993956973652
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 4.92289e-06
Norm of the params: 18.8135
     Influence (LOO): fixed 141 labels. Loss 0.10802. Accuracy 0.966.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0497682
Train loss (w/o reg) on all data: 0.0244886
Test loss (w/o reg) on all data: 0.164771
Train acc on all data:  0.999033115784
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 1.70115e-05
Norm of the params: 22.4854
                Loss: fixed 100 labels. Loss 0.16477. Accuracy 0.954.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0969593
Train loss (w/o reg) on all data: 0.0655077
Test loss (w/o reg) on all data: 0.252725
Train acc on all data:  0.986705342035
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 8.85947e-06
Norm of the params: 25.0805
              Random: fixed  14 labels. Loss 0.25272. Accuracy 0.934.
### Flips: 206, rs: 36, checks: 412
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0427104
Train loss (w/o reg) on all data: 0.0276652
Test loss (w/o reg) on all data: 0.097271
Train acc on all data:  0.995407299976
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 6.31419e-07
Norm of the params: 17.3465
     Influence (LOO): fixed 175 labels. Loss 0.09727. Accuracy 0.974.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0393898
Train loss (w/o reg) on all data: 0.0188763
Test loss (w/o reg) on all data: 0.170261
Train acc on all data:  0.999274836838
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 7.98944e-06
Norm of the params: 20.2551
                Loss: fixed 138 labels. Loss 0.17026. Accuracy 0.970.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0959944
Train loss (w/o reg) on all data: 0.0648801
Test loss (w/o reg) on all data: 0.237381
Train acc on all data:  0.986947063089
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.55984e-05
Norm of the params: 24.9457
              Random: fixed  21 labels. Loss 0.23738. Accuracy 0.935.
### Flips: 206, rs: 36, checks: 618
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040667
Train loss (w/o reg) on all data: 0.0258559
Test loss (w/o reg) on all data: 0.0954324
Train acc on all data:  0.995890742084
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 6.0801e-07
Norm of the params: 17.2111
     Influence (LOO): fixed 183 labels. Loss 0.09543. Accuracy 0.974.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0337254
Train loss (w/o reg) on all data: 0.016133
Test loss (w/o reg) on all data: 0.140418
Train acc on all data:  0.999516557892
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 5.28568e-06
Norm of the params: 18.7576
                Loss: fixed 160 labels. Loss 0.14042. Accuracy 0.971.
Using normal model
LBFGS training took [435] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0922737
Train loss (w/o reg) on all data: 0.0620311
Test loss (w/o reg) on all data: 0.216047
Train acc on all data:  0.987188784143
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 1.48984e-05
Norm of the params: 24.5938
              Random: fixed  31 labels. Loss 0.21605. Accuracy 0.938.
### Flips: 206, rs: 36, checks: 824
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0402008
Train loss (w/o reg) on all data: 0.0254833
Test loss (w/o reg) on all data: 0.0919699
Train acc on all data:  0.996132463138
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.15686e-06
Norm of the params: 17.1567
     Influence (LOO): fixed 186 labels. Loss 0.09197. Accuracy 0.977.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0318525
Train loss (w/o reg) on all data: 0.0152645
Test loss (w/o reg) on all data: 0.135785
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 4.84571e-06
Norm of the params: 18.2143
                Loss: fixed 173 labels. Loss 0.13578. Accuracy 0.973.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0895004
Train loss (w/o reg) on all data: 0.0597933
Test loss (w/o reg) on all data: 0.20962
Train acc on all data:  0.988155668359
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 2.39349e-05
Norm of the params: 24.375
              Random: fixed  39 labels. Loss 0.20962. Accuracy 0.939.
### Flips: 206, rs: 36, checks: 1030
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0392316
Train loss (w/o reg) on all data: 0.0245093
Test loss (w/o reg) on all data: 0.111329
Train acc on all data:  0.996374184191
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 6.77911e-07
Norm of the params: 17.1594
     Influence (LOO): fixed 190 labels. Loss 0.11133. Accuracy 0.975.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0310376
Train loss (w/o reg) on all data: 0.014921
Test loss (w/o reg) on all data: 0.152497
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 7.94227e-07
Norm of the params: 17.9536
                Loss: fixed 176 labels. Loss 0.15250. Accuracy 0.975.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0871559
Train loss (w/o reg) on all data: 0.0577712
Test loss (w/o reg) on all data: 0.20499
Train acc on all data:  0.989122552574
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.27533e-05
Norm of the params: 24.2424
              Random: fixed  47 labels. Loss 0.20499. Accuracy 0.941.
### Flips: 206, rs: 36, checks: 1236
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0365059
Train loss (w/o reg) on all data: 0.0218634
Test loss (w/o reg) on all data: 0.119417
Train acc on all data:  0.997099347353
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 9.30443e-07
Norm of the params: 17.1129
     Influence (LOO): fixed 194 labels. Loss 0.11942. Accuracy 0.976.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0297024
Train loss (w/o reg) on all data: 0.0143051
Test loss (w/o reg) on all data: 0.1819
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 5.19193e-07
Norm of the params: 17.5484
                Loss: fixed 190 labels. Loss 0.18190. Accuracy 0.974.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0849663
Train loss (w/o reg) on all data: 0.0560278
Test loss (w/o reg) on all data: 0.207417
Train acc on all data:  0.989122552574
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.86975e-05
Norm of the params: 24.0577
              Random: fixed  56 labels. Loss 0.20742. Accuracy 0.941.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103879
Train loss (w/o reg) on all data: 0.0721108
Test loss (w/o reg) on all data: 0.296656
Train acc on all data:  0.984046410442
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 9.37379e-06
Norm of the params: 25.2063
Flipped loss: 0.29666. Accuracy: 0.925
### Flips: 206, rs: 37, checks: 206
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0485968
Train loss (w/o reg) on all data: 0.0314189
Test loss (w/o reg) on all data: 0.16736
Train acc on all data:  0.994923857868
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 7.41767e-07
Norm of the params: 18.5353
     Influence (LOO): fixed 141 labels. Loss 0.16736. Accuracy 0.971.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0461969
Train loss (w/o reg) on all data: 0.0228105
Test loss (w/o reg) on all data: 0.146214
Train acc on all data:  0.99879139473
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 1.53706e-06
Norm of the params: 21.627
                Loss: fixed 104 labels. Loss 0.14621. Accuracy 0.959.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101844
Train loss (w/o reg) on all data: 0.070698
Test loss (w/o reg) on all data: 0.329764
Train acc on all data:  0.983804689388
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 3.56783e-06
Norm of the params: 24.9584
              Random: fixed   8 labels. Loss 0.32976. Accuracy 0.927.
### Flips: 206, rs: 37, checks: 412
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0456817
Train loss (w/o reg) on all data: 0.0300142
Test loss (w/o reg) on all data: 0.163965
Train acc on all data:  0.99444041576
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.09893e-06
Norm of the params: 17.7017
     Influence (LOO): fixed 167 labels. Loss 0.16397. Accuracy 0.974.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0375634
Train loss (w/o reg) on all data: 0.0181508
Test loss (w/o reg) on all data: 0.125277
Train acc on all data:  0.999033115784
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.75317e-06
Norm of the params: 19.7041
                Loss: fixed 138 labels. Loss 0.12528. Accuracy 0.963.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101054
Train loss (w/o reg) on all data: 0.0702561
Test loss (w/o reg) on all data: 0.327793
Train acc on all data:  0.984046410442
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 1.20042e-05
Norm of the params: 24.8187
              Random: fixed  18 labels. Loss 0.32779. Accuracy 0.925.
### Flips: 206, rs: 37, checks: 618
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0437451
Train loss (w/o reg) on all data: 0.0282654
Test loss (w/o reg) on all data: 0.161441
Train acc on all data:  0.995165578922
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 8.5232e-07
Norm of the params: 17.5953
     Influence (LOO): fixed 175 labels. Loss 0.16144. Accuracy 0.972.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0346286
Train loss (w/o reg) on all data: 0.0166166
Test loss (w/o reg) on all data: 0.105451
Train acc on all data:  0.999274836838
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 1.11296e-06
Norm of the params: 18.98
                Loss: fixed 156 labels. Loss 0.10545. Accuracy 0.970.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0975868
Train loss (w/o reg) on all data: 0.0671282
Test loss (w/o reg) on all data: 0.324877
Train acc on all data:  0.985255015712
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 4.29456e-06
Norm of the params: 24.6814
              Random: fixed  27 labels. Loss 0.32488. Accuracy 0.929.
### Flips: 206, rs: 37, checks: 824
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0423526
Train loss (w/o reg) on all data: 0.0274094
Test loss (w/o reg) on all data: 0.106741
Train acc on all data:  0.994923857868
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.50956e-06
Norm of the params: 17.2877
     Influence (LOO): fixed 183 labels. Loss 0.10674. Accuracy 0.974.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0317657
Train loss (w/o reg) on all data: 0.015147
Test loss (w/o reg) on all data: 0.0865662
Train acc on all data:  0.999516557892
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 6.60316e-07
Norm of the params: 18.2311
                Loss: fixed 176 labels. Loss 0.08657. Accuracy 0.977.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0913491
Train loss (w/o reg) on all data: 0.0622406
Test loss (w/o reg) on all data: 0.289742
Train acc on all data:  0.986947063089
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 3.17265e-06
Norm of the params: 24.1282
              Random: fixed  43 labels. Loss 0.28974. Accuracy 0.944.
### Flips: 206, rs: 37, checks: 1030
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0419798
Train loss (w/o reg) on all data: 0.0271796
Test loss (w/o reg) on all data: 0.106357
Train acc on all data:  0.995165578922
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.2164e-06
Norm of the params: 17.2048
     Influence (LOO): fixed 186 labels. Loss 0.10636. Accuracy 0.975.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0311849
Train loss (w/o reg) on all data: 0.0148569
Test loss (w/o reg) on all data: 0.0826665
Train acc on all data:  0.999516557892
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 7.13454e-07
Norm of the params: 18.071
                Loss: fixed 182 labels. Loss 0.08267. Accuracy 0.978.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088027
Train loss (w/o reg) on all data: 0.0596476
Test loss (w/o reg) on all data: 0.298862
Train acc on all data:  0.988155668359
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 6.60575e-06
Norm of the params: 23.8241
              Random: fixed  54 labels. Loss 0.29886. Accuracy 0.941.
### Flips: 206, rs: 37, checks: 1236
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0392006
Train loss (w/o reg) on all data: 0.0245394
Test loss (w/o reg) on all data: 0.125794
Train acc on all data:  0.996132463138
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 8.70861e-07
Norm of the params: 17.1238
     Influence (LOO): fixed 191 labels. Loss 0.12579. Accuracy 0.975.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029802
Train loss (w/o reg) on all data: 0.0142002
Test loss (w/o reg) on all data: 0.0853399
Train acc on all data:  0.999274836838
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.81013e-07
Norm of the params: 17.6646
                Loss: fixed 188 labels. Loss 0.08534. Accuracy 0.977.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0839018
Train loss (w/o reg) on all data: 0.0564699
Test loss (w/o reg) on all data: 0.274439
Train acc on all data:  0.988397389413
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.18995e-06
Norm of the params: 23.423
              Random: fixed  62 labels. Loss 0.27444. Accuracy 0.944.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106174
Train loss (w/o reg) on all data: 0.0739675
Test loss (w/o reg) on all data: 0.374283
Train acc on all data:  0.984046410442
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 8.6964e-06
Norm of the params: 25.3796
Flipped loss: 0.37428. Accuracy: 0.929
### Flips: 206, rs: 38, checks: 206
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0539666
Train loss (w/o reg) on all data: 0.036307
Test loss (w/o reg) on all data: 0.123548
Train acc on all data:  0.993956973652
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 4.67763e-06
Norm of the params: 18.7934
     Influence (LOO): fixed 140 labels. Loss 0.12355. Accuracy 0.968.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0526716
Train loss (w/o reg) on all data: 0.0261005
Test loss (w/o reg) on all data: 0.325296
Train acc on all data:  0.999516557892
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 5.6847e-06
Norm of the params: 23.0526
                Loss: fixed 102 labels. Loss 0.32530. Accuracy 0.950.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10451
Train loss (w/o reg) on all data: 0.0727045
Test loss (w/o reg) on all data: 0.340522
Train acc on all data:  0.984771573604
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.56042e-05
Norm of the params: 25.2211
              Random: fixed  13 labels. Loss 0.34052. Accuracy 0.931.
### Flips: 206, rs: 38, checks: 412
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0480868
Train loss (w/o reg) on all data: 0.0324474
Test loss (w/o reg) on all data: 0.123113
Train acc on all data:  0.994682136814
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 3.06735e-06
Norm of the params: 17.6858
     Influence (LOO): fixed 169 labels. Loss 0.12311. Accuracy 0.972.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041311
Train loss (w/o reg) on all data: 0.0200455
Test loss (w/o reg) on all data: 0.17716
Train acc on all data:  0.999516557892
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 1.52942e-06
Norm of the params: 20.623
                Loss: fixed 140 labels. Loss 0.17716. Accuracy 0.957.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101991
Train loss (w/o reg) on all data: 0.0709796
Test loss (w/o reg) on all data: 0.330582
Train acc on all data:  0.98573845782
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.40334e-05
Norm of the params: 24.9043
              Random: fixed  25 labels. Loss 0.33058. Accuracy 0.929.
### Flips: 206, rs: 38, checks: 618
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0463683
Train loss (w/o reg) on all data: 0.0309559
Test loss (w/o reg) on all data: 0.124054
Train acc on all data:  0.994682136814
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.90204e-06
Norm of the params: 17.557
     Influence (LOO): fixed 175 labels. Loss 0.12405. Accuracy 0.973.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036817
Train loss (w/o reg) on all data: 0.0176889
Test loss (w/o reg) on all data: 0.12222
Train acc on all data:  0.999516557892
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.54173e-06
Norm of the params: 19.5592
                Loss: fixed 163 labels. Loss 0.12222. Accuracy 0.968.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0953457
Train loss (w/o reg) on all data: 0.0653879
Test loss (w/o reg) on all data: 0.267636
Train acc on all data:  0.986705342035
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 7.58091e-06
Norm of the params: 24.4777
              Random: fixed  39 labels. Loss 0.26764. Accuracy 0.933.
### Flips: 206, rs: 38, checks: 824
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044412
Train loss (w/o reg) on all data: 0.0295137
Test loss (w/o reg) on all data: 0.103969
Train acc on all data:  0.995165578922
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.77637e-07
Norm of the params: 17.2617
     Influence (LOO): fixed 183 labels. Loss 0.10397. Accuracy 0.977.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033207
Train loss (w/o reg) on all data: 0.0158698
Test loss (w/o reg) on all data: 0.112126
Train acc on all data:  0.999516557892
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 9.50855e-07
Norm of the params: 18.6211
                Loss: fixed 177 labels. Loss 0.11213. Accuracy 0.970.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0913966
Train loss (w/o reg) on all data: 0.0620013
Test loss (w/o reg) on all data: 0.263702
Train acc on all data:  0.988155668359
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 3.5785e-06
Norm of the params: 24.2467
              Random: fixed  54 labels. Loss 0.26370. Accuracy 0.934.
### Flips: 206, rs: 38, checks: 1030
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0419968
Train loss (w/o reg) on all data: 0.0270692
Test loss (w/o reg) on all data: 0.10469
Train acc on all data:  0.995890742084
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.89793e-06
Norm of the params: 17.2787
     Influence (LOO): fixed 189 labels. Loss 0.10469. Accuracy 0.976.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0309189
Train loss (w/o reg) on all data: 0.0148174
Test loss (w/o reg) on all data: 0.131906
Train acc on all data:  0.999516557892
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 5.8948e-06
Norm of the params: 17.9451
                Loss: fixed 185 labels. Loss 0.13191. Accuracy 0.973.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0866477
Train loss (w/o reg) on all data: 0.0578647
Test loss (w/o reg) on all data: 0.217954
Train acc on all data:  0.989122552574
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 8.10954e-06
Norm of the params: 23.9929
              Random: fixed  64 labels. Loss 0.21795. Accuracy 0.939.
### Flips: 206, rs: 38, checks: 1236
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0402455
Train loss (w/o reg) on all data: 0.0252696
Test loss (w/o reg) on all data: 0.104024
Train acc on all data:  0.996374184191
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 8.15524e-07
Norm of the params: 17.3066
     Influence (LOO): fixed 191 labels. Loss 0.10402. Accuracy 0.976.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0302074
Train loss (w/o reg) on all data: 0.0144855
Test loss (w/o reg) on all data: 0.141328
Train acc on all data:  0.999516557892
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 5.43375e-07
Norm of the params: 17.7324
                Loss: fixed 191 labels. Loss 0.14133. Accuracy 0.977.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0845453
Train loss (w/o reg) on all data: 0.0562429
Test loss (w/o reg) on all data: 0.231125
Train acc on all data:  0.990572878898
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 1.23785e-05
Norm of the params: 23.7918
              Random: fixed  72 labels. Loss 0.23113. Accuracy 0.937.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10402
Train loss (w/o reg) on all data: 0.0725461
Test loss (w/o reg) on all data: 0.2722
Train acc on all data:  0.984046410442
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.06012e-05
Norm of the params: 25.0895
Flipped loss: 0.27220. Accuracy: 0.929
### Flips: 206, rs: 39, checks: 206
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0574752
Train loss (w/o reg) on all data: 0.0391757
Test loss (w/o reg) on all data: 0.152451
Train acc on all data:  0.992506647329
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 5.87223e-06
Norm of the params: 19.1308
     Influence (LOO): fixed 135 labels. Loss 0.15245. Accuracy 0.967.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049695
Train loss (w/o reg) on all data: 0.0246166
Test loss (w/o reg) on all data: 0.207965
Train acc on all data:  0.999274836838
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 7.93203e-06
Norm of the params: 22.3957
                Loss: fixed  99 labels. Loss 0.20797. Accuracy 0.953.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102562
Train loss (w/o reg) on all data: 0.0717532
Test loss (w/o reg) on all data: 0.252185
Train acc on all data:  0.984288131496
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 3.28603e-05
Norm of the params: 24.8231
              Random: fixed  10 labels. Loss 0.25218. Accuracy 0.930.
### Flips: 206, rs: 39, checks: 412
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0508847
Train loss (w/o reg) on all data: 0.0344229
Test loss (w/o reg) on all data: 0.13102
Train acc on all data:  0.993715252599
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.63655e-06
Norm of the params: 18.1448
     Influence (LOO): fixed 168 labels. Loss 0.13102. Accuracy 0.976.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0383944
Train loss (w/o reg) on all data: 0.0183848
Test loss (w/o reg) on all data: 0.235596
Train acc on all data:  0.999516557892
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 5.19526e-06
Norm of the params: 20.0048
                Loss: fixed 141 labels. Loss 0.23560. Accuracy 0.961.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0985683
Train loss (w/o reg) on all data: 0.0678039
Test loss (w/o reg) on all data: 0.2627
Train acc on all data:  0.985496736766
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 6.89786e-06
Norm of the params: 24.805
              Random: fixed  18 labels. Loss 0.26270. Accuracy 0.932.
### Flips: 206, rs: 39, checks: 618
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047284
Train loss (w/o reg) on all data: 0.0318111
Test loss (w/o reg) on all data: 0.131722
Train acc on all data:  0.99444041576
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.79461e-06
Norm of the params: 17.5915
     Influence (LOO): fixed 180 labels. Loss 0.13172. Accuracy 0.975.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0364121
Train loss (w/o reg) on all data: 0.0174637
Test loss (w/o reg) on all data: 0.228581
Train acc on all data:  0.999516557892
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.43049e-06
Norm of the params: 19.4671
                Loss: fixed 157 labels. Loss 0.22858. Accuracy 0.966.
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0966621
Train loss (w/o reg) on all data: 0.0661721
Test loss (w/o reg) on all data: 0.252279
Train acc on all data:  0.985980178874
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.7364e-06
Norm of the params: 24.6941
              Random: fixed  24 labels. Loss 0.25228. Accuracy 0.935.
### Flips: 206, rs: 39, checks: 824
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0442089
Train loss (w/o reg) on all data: 0.0292759
Test loss (w/o reg) on all data: 0.143196
Train acc on all data:  0.995165578922
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 4.76732e-07
Norm of the params: 17.2818
     Influence (LOO): fixed 185 labels. Loss 0.14320. Accuracy 0.973.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0345346
Train loss (w/o reg) on all data: 0.0165541
Test loss (w/o reg) on all data: 0.178212
Train acc on all data:  0.999516557892
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.02137e-06
Norm of the params: 18.9634
                Loss: fixed 170 labels. Loss 0.17821. Accuracy 0.974.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09202
Train loss (w/o reg) on all data: 0.0623554
Test loss (w/o reg) on all data: 0.256896
Train acc on all data:  0.987430505197
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 1.81703e-05
Norm of the params: 24.3576
              Random: fixed  36 labels. Loss 0.25690. Accuracy 0.938.
### Flips: 206, rs: 39, checks: 1030
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0415653
Train loss (w/o reg) on all data: 0.0266259
Test loss (w/o reg) on all data: 0.133738
Train acc on all data:  0.995890742084
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 5.29172e-07
Norm of the params: 17.2855
     Influence (LOO): fixed 190 labels. Loss 0.13374. Accuracy 0.976.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0321926
Train loss (w/o reg) on all data: 0.0153494
Test loss (w/o reg) on all data: 0.177637
Train acc on all data:  0.999516557892
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.22889e-05
Norm of the params: 18.3539
                Loss: fixed 181 labels. Loss 0.17764. Accuracy 0.976.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0893073
Train loss (w/o reg) on all data: 0.0604164
Test loss (w/o reg) on all data: 0.258806
Train acc on all data:  0.988155668359
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.22524e-05
Norm of the params: 24.0378
              Random: fixed  47 labels. Loss 0.25881. Accuracy 0.936.
### Flips: 206, rs: 39, checks: 1236
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0403091
Train loss (w/o reg) on all data: 0.0253634
Test loss (w/o reg) on all data: 0.123609
Train acc on all data:  0.996132463138
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.86892e-07
Norm of the params: 17.2891
     Influence (LOO): fixed 192 labels. Loss 0.12361. Accuracy 0.977.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030869
Train loss (w/o reg) on all data: 0.0147325
Test loss (w/o reg) on all data: 0.142634
Train acc on all data:  0.999516557892
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 5.0185e-06
Norm of the params: 17.9647
                Loss: fixed 191 labels. Loss 0.14263. Accuracy 0.975.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083044
Train loss (w/o reg) on all data: 0.0553179
Test loss (w/o reg) on all data: 0.240067
Train acc on all data:  0.989605994682
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 5.12174e-06
Norm of the params: 23.5483
              Random: fixed  61 labels. Loss 0.24007. Accuracy 0.944.
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158719
Train loss (w/o reg) on all data: 0.117925
Test loss (w/o reg) on all data: 0.442341
Train acc on all data:  0.963500120861
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 1.08276e-05
Norm of the params: 28.5635
Flipped loss: 0.44234. Accuracy: 0.886
### Flips: 412, rs: 0, checks: 206
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0956959
Train loss (w/o reg) on all data: 0.0680195
Test loss (w/o reg) on all data: 0.242121
Train acc on all data:  0.984288131496
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 8.80253e-06
Norm of the params: 23.5271
     Influence (LOO): fixed 164 labels. Loss 0.24212. Accuracy 0.929.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0808763
Train loss (w/o reg) on all data: 0.0437306
Test loss (w/o reg) on all data: 0.369337
Train acc on all data:  0.998066231569
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 6.78417e-06
Norm of the params: 27.2565
                Loss: fixed 145 labels. Loss 0.36934. Accuracy 0.906.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154123
Train loss (w/o reg) on all data: 0.114214
Test loss (w/o reg) on all data: 0.398431
Train acc on all data:  0.965192168238
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 1.34794e-05
Norm of the params: 28.2519
              Random: fixed  22 labels. Loss 0.39843. Accuracy 0.892.
### Flips: 412, rs: 0, checks: 412
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0805452
Train loss (w/o reg) on all data: 0.0583271
Test loss (w/o reg) on all data: 0.159229
Train acc on all data:  0.98573845782
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 4.16118e-06
Norm of the params: 21.0799
     Influence (LOO): fixed 255 labels. Loss 0.15923. Accuracy 0.956.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0623652
Train loss (w/o reg) on all data: 0.0312917
Test loss (w/o reg) on all data: 0.255756
Train acc on all data:  0.999274836838
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 8.76964e-06
Norm of the params: 24.9293
                Loss: fixed 212 labels. Loss 0.25576. Accuracy 0.933.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147964
Train loss (w/o reg) on all data: 0.108634
Test loss (w/o reg) on all data: 0.363116
Train acc on all data:  0.968817984046
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 8.6617e-06
Norm of the params: 28.0463
              Random: fixed  40 labels. Loss 0.36312. Accuracy 0.896.
### Flips: 412, rs: 0, checks: 618
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0709431
Train loss (w/o reg) on all data: 0.0511507
Test loss (w/o reg) on all data: 0.182265
Train acc on all data:  0.987913947305
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 6.58592e-06
Norm of the params: 19.896
     Influence (LOO): fixed 298 labels. Loss 0.18226. Accuracy 0.957.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0540251
Train loss (w/o reg) on all data: 0.0267897
Test loss (w/o reg) on all data: 0.205677
Train acc on all data:  0.999274836838
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.44866e-05
Norm of the params: 23.339
                Loss: fixed 249 labels. Loss 0.20568. Accuracy 0.936.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143411
Train loss (w/o reg) on all data: 0.104856
Test loss (w/o reg) on all data: 0.322996
Train acc on all data:  0.97026831037
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 2.25333e-05
Norm of the params: 27.7686
              Random: fixed  59 labels. Loss 0.32300. Accuracy 0.902.
### Flips: 412, rs: 0, checks: 824
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0648671
Train loss (w/o reg) on all data: 0.0465064
Test loss (w/o reg) on all data: 0.123307
Train acc on all data:  0.989847715736
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.46858e-06
Norm of the params: 19.1628
     Influence (LOO): fixed 327 labels. Loss 0.12331. Accuracy 0.968.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0464358
Train loss (w/o reg) on all data: 0.0225767
Test loss (w/o reg) on all data: 0.165913
Train acc on all data:  0.999516557892
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 5.81768e-06
Norm of the params: 21.8445
                Loss: fixed 280 labels. Loss 0.16591. Accuracy 0.946.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138518
Train loss (w/o reg) on all data: 0.100547
Test loss (w/o reg) on all data: 0.340617
Train acc on all data:  0.971718636693
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 9.65268e-06
Norm of the params: 27.5576
              Random: fixed  76 labels. Loss 0.34062. Accuracy 0.903.
### Flips: 412, rs: 0, checks: 1030
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0619732
Train loss (w/o reg) on all data: 0.0440446
Test loss (w/o reg) on all data: 0.134324
Train acc on all data:  0.990814599952
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 4.63408e-06
Norm of the params: 18.936
     Influence (LOO): fixed 339 labels. Loss 0.13432. Accuracy 0.966.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0440409
Train loss (w/o reg) on all data: 0.0213682
Test loss (w/o reg) on all data: 0.155502
Train acc on all data:  0.999516557892
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 2.32405e-06
Norm of the params: 21.2944
                Loss: fixed 301 labels. Loss 0.15550. Accuracy 0.952.
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131649
Train loss (w/o reg) on all data: 0.0944271
Test loss (w/o reg) on all data: 0.328219
Train acc on all data:  0.973168963017
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 1.47815e-05
Norm of the params: 27.2843
              Random: fixed 100 labels. Loss 0.32822. Accuracy 0.904.
### Flips: 412, rs: 0, checks: 1236
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0586341
Train loss (w/o reg) on all data: 0.0420185
Test loss (w/o reg) on all data: 0.124721
Train acc on all data:  0.990572878898
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.14412e-06
Norm of the params: 18.2294
     Influence (LOO): fixed 358 labels. Loss 0.12472. Accuracy 0.973.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0398884
Train loss (w/o reg) on all data: 0.0193417
Test loss (w/o reg) on all data: 0.147103
Train acc on all data:  0.999274836838
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 5.24728e-06
Norm of the params: 20.2715
                Loss: fixed 323 labels. Loss 0.14710. Accuracy 0.961.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127704
Train loss (w/o reg) on all data: 0.0914067
Test loss (w/o reg) on all data: 0.34206
Train acc on all data:  0.977278220933
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.9329e-05
Norm of the params: 26.9434
              Random: fixed 118 labels. Loss 0.34206. Accuracy 0.920.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159675
Train loss (w/o reg) on all data: 0.119143
Test loss (w/o reg) on all data: 0.470518
Train acc on all data:  0.9690597051
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 1.506e-05
Norm of the params: 28.4717
Flipped loss: 0.47052. Accuracy: 0.875
### Flips: 412, rs: 1, checks: 206
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0976665
Train loss (w/o reg) on all data: 0.0705755
Test loss (w/o reg) on all data: 0.283743
Train acc on all data:  0.981145757796
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 8.4763e-06
Norm of the params: 23.277
     Influence (LOO): fixed 170 labels. Loss 0.28374. Accuracy 0.929.
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0810639
Train loss (w/o reg) on all data: 0.0440927
Test loss (w/o reg) on all data: 0.346351
Train acc on all data:  0.997824510515
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.10458e-05
Norm of the params: 27.1923
                Loss: fixed 153 labels. Loss 0.34635. Accuracy 0.899.
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155492
Train loss (w/o reg) on all data: 0.114909
Test loss (w/o reg) on all data: 0.487951
Train acc on all data:  0.970751752478
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 8.27582e-05
Norm of the params: 28.4897
              Random: fixed  17 labels. Loss 0.48795. Accuracy 0.872.
### Flips: 412, rs: 1, checks: 412
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0812134
Train loss (w/o reg) on all data: 0.0597603
Test loss (w/o reg) on all data: 0.214229
Train acc on all data:  0.98452985255
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 9.61434e-06
Norm of the params: 20.7138
     Influence (LOO): fixed 255 labels. Loss 0.21423. Accuracy 0.950.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0605421
Train loss (w/o reg) on all data: 0.030413
Test loss (w/o reg) on all data: 0.323484
Train acc on all data:  0.999033115784
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 1.22545e-05
Norm of the params: 24.5475
                Loss: fixed 224 labels. Loss 0.32348. Accuracy 0.908.
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149854
Train loss (w/o reg) on all data: 0.110124
Test loss (w/o reg) on all data: 0.475201
Train acc on all data:  0.973168963017
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 5.81124e-05
Norm of the params: 28.1887
              Random: fixed  36 labels. Loss 0.47520. Accuracy 0.884.
### Flips: 412, rs: 1, checks: 618
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0734027
Train loss (w/o reg) on all data: 0.0548797
Test loss (w/o reg) on all data: 0.23956
Train acc on all data:  0.98573845782
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.2936e-06
Norm of the params: 19.2473
     Influence (LOO): fixed 301 labels. Loss 0.23956. Accuracy 0.968.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0512891
Train loss (w/o reg) on all data: 0.0251243
Test loss (w/o reg) on all data: 0.267728
Train acc on all data:  0.999274836838
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 1.6283e-05
Norm of the params: 22.8756
                Loss: fixed 259 labels. Loss 0.26773. Accuracy 0.928.
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146892
Train loss (w/o reg) on all data: 0.107637
Test loss (w/o reg) on all data: 0.470388
Train acc on all data:  0.973168963017
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 4.63242e-05
Norm of the params: 28.0199
              Random: fixed  50 labels. Loss 0.47039. Accuracy 0.889.
### Flips: 412, rs: 1, checks: 824
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0676101
Train loss (w/o reg) on all data: 0.0507988
Test loss (w/o reg) on all data: 0.20177
Train acc on all data:  0.986947063089
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.61678e-06
Norm of the params: 18.3364
     Influence (LOO): fixed 328 labels. Loss 0.20177. Accuracy 0.972.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0478816
Train loss (w/o reg) on all data: 0.0232336
Test loss (w/o reg) on all data: 0.234939
Train acc on all data:  0.999274836838
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 3.89521e-06
Norm of the params: 22.2027
                Loss: fixed 277 labels. Loss 0.23494. Accuracy 0.939.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143617
Train loss (w/o reg) on all data: 0.104916
Test loss (w/o reg) on all data: 0.475049
Train acc on all data:  0.973894126178
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.92231e-05
Norm of the params: 27.8211
              Random: fixed  68 labels. Loss 0.47505. Accuracy 0.897.
### Flips: 412, rs: 1, checks: 1030
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0641161
Train loss (w/o reg) on all data: 0.048089
Test loss (w/o reg) on all data: 0.192448
Train acc on all data:  0.987913947305
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.14812e-06
Norm of the params: 17.9037
     Influence (LOO): fixed 339 labels. Loss 0.19245. Accuracy 0.975.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0441635
Train loss (w/o reg) on all data: 0.0212688
Test loss (w/o reg) on all data: 0.22712
Train acc on all data:  0.999516557892
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 1.3789e-05
Norm of the params: 21.3984
                Loss: fixed 300 labels. Loss 0.22712. Accuracy 0.950.
Using normal model
LBFGS training took [371] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137529
Train loss (w/o reg) on all data: 0.0998409
Test loss (w/o reg) on all data: 0.490184
Train acc on all data:  0.975586173556
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 1.27017e-05
Norm of the params: 27.4549
              Random: fixed  87 labels. Loss 0.49018. Accuracy 0.893.
### Flips: 412, rs: 1, checks: 1236
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0622636
Train loss (w/o reg) on all data: 0.0467269
Test loss (w/o reg) on all data: 0.177641
Train acc on all data:  0.988397389413
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.10257e-06
Norm of the params: 17.6276
     Influence (LOO): fixed 351 labels. Loss 0.17764. Accuracy 0.972.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039796
Train loss (w/o reg) on all data: 0.018957
Test loss (w/o reg) on all data: 0.184199
Train acc on all data:  0.999516557892
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.65764e-05
Norm of the params: 20.4152
                Loss: fixed 320 labels. Loss 0.18420. Accuracy 0.961.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132963
Train loss (w/o reg) on all data: 0.0959912
Test loss (w/o reg) on all data: 0.457008
Train acc on all data:  0.976794778825
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 1.64503e-05
Norm of the params: 27.1927
              Random: fixed 108 labels. Loss 0.45701. Accuracy 0.900.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163852
Train loss (w/o reg) on all data: 0.12264
Test loss (w/o reg) on all data: 0.585259
Train acc on all data:  0.967609378777
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 5.08648e-05
Norm of the params: 28.7093
Flipped loss: 0.58526. Accuracy: 0.877
### Flips: 412, rs: 2, checks: 206
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101693
Train loss (w/o reg) on all data: 0.074394
Test loss (w/o reg) on all data: 0.493175
Train acc on all data:  0.981387478849
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 6.37178e-06
Norm of the params: 23.3664
     Influence (LOO): fixed 166 labels. Loss 0.49317. Accuracy 0.940.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0824908
Train loss (w/o reg) on all data: 0.0441638
Test loss (w/o reg) on all data: 0.529585
Train acc on all data:  0.998066231569
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.06156e-05
Norm of the params: 27.6865
                Loss: fixed 154 labels. Loss 0.52958. Accuracy 0.899.
Using normal model
LBFGS training took [510] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160452
Train loss (w/o reg) on all data: 0.119882
Test loss (w/o reg) on all data: 0.605782
Train acc on all data:  0.967609378777
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 3.96577e-05
Norm of the params: 28.485
              Random: fixed  15 labels. Loss 0.60578. Accuracy 0.883.
### Flips: 412, rs: 2, checks: 412
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0858244
Train loss (w/o reg) on all data: 0.0641397
Test loss (w/o reg) on all data: 0.33569
Train acc on all data:  0.98452985255
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 6.9476e-06
Norm of the params: 20.8253
     Influence (LOO): fixed 253 labels. Loss 0.33569. Accuracy 0.954.
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0640327
Train loss (w/o reg) on all data: 0.0319754
Test loss (w/o reg) on all data: 0.479129
Train acc on all data:  0.999274836838
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.9374e-05
Norm of the params: 25.3208
                Loss: fixed 219 labels. Loss 0.47913. Accuracy 0.916.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156067
Train loss (w/o reg) on all data: 0.115941
Test loss (w/o reg) on all data: 0.573359
Train acc on all data:  0.969301426154
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 0.000131319
Norm of the params: 28.3288
              Random: fixed  30 labels. Loss 0.57336. Accuracy 0.885.
### Flips: 412, rs: 2, checks: 618
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0793355
Train loss (w/o reg) on all data: 0.0597597
Test loss (w/o reg) on all data: 0.306275
Train acc on all data:  0.985013294658
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 6.77054e-06
Norm of the params: 19.7868
     Influence (LOO): fixed 295 labels. Loss 0.30628. Accuracy 0.956.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0546013
Train loss (w/o reg) on all data: 0.0266642
Test loss (w/o reg) on all data: 0.41753
Train acc on all data:  0.999274836838
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 5.74331e-06
Norm of the params: 23.6377
                Loss: fixed 256 labels. Loss 0.41753. Accuracy 0.930.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15108
Train loss (w/o reg) on all data: 0.111669
Test loss (w/o reg) on all data: 0.474499
Train acc on all data:  0.971235194585
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 4.44167e-05
Norm of the params: 28.0754
              Random: fixed  45 labels. Loss 0.47450. Accuracy 0.897.
### Flips: 412, rs: 2, checks: 824
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0743037
Train loss (w/o reg) on all data: 0.0560056
Test loss (w/o reg) on all data: 0.245592
Train acc on all data:  0.985496736766
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.34723e-06
Norm of the params: 19.1301
     Influence (LOO): fixed 314 labels. Loss 0.24559. Accuracy 0.966.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049305
Train loss (w/o reg) on all data: 0.02382
Test loss (w/o reg) on all data: 0.387699
Train acc on all data:  0.999274836838
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 7.8159e-06
Norm of the params: 22.5765
                Loss: fixed 281 labels. Loss 0.38770. Accuracy 0.941.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146242
Train loss (w/o reg) on all data: 0.10793
Test loss (w/o reg) on all data: 0.332138
Train acc on all data:  0.972202078801
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.53551e-05
Norm of the params: 27.6813
              Random: fixed  66 labels. Loss 0.33214. Accuracy 0.900.
### Flips: 412, rs: 2, checks: 1030
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0685735
Train loss (w/o reg) on all data: 0.0516118
Test loss (w/o reg) on all data: 0.252952
Train acc on all data:  0.986463620981
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 3.69821e-06
Norm of the params: 18.4183
     Influence (LOO): fixed 336 labels. Loss 0.25295. Accuracy 0.970.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0438167
Train loss (w/o reg) on all data: 0.0209814
Test loss (w/o reg) on all data: 0.338012
Train acc on all data:  0.999516557892
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 3.55301e-06
Norm of the params: 21.3707
                Loss: fixed 309 labels. Loss 0.33801. Accuracy 0.957.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143506
Train loss (w/o reg) on all data: 0.106317
Test loss (w/o reg) on all data: 0.33229
Train acc on all data:  0.971960357747
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 6.92482e-05
Norm of the params: 27.2724
              Random: fixed  85 labels. Loss 0.33229. Accuracy 0.904.
### Flips: 412, rs: 2, checks: 1236
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0627868
Train loss (w/o reg) on all data: 0.0468284
Test loss (w/o reg) on all data: 0.199294
Train acc on all data:  0.987913947305
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.05754e-06
Norm of the params: 17.8652
     Influence (LOO): fixed 346 labels. Loss 0.19929. Accuracy 0.968.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0426261
Train loss (w/o reg) on all data: 0.0204281
Test loss (w/o reg) on all data: 0.342969
Train acc on all data:  0.999516557892
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 3.23364e-06
Norm of the params: 21.0704
                Loss: fixed 318 labels. Loss 0.34297. Accuracy 0.955.
Using normal model
LBFGS training took [508] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133943
Train loss (w/o reg) on all data: 0.0984378
Test loss (w/o reg) on all data: 0.308942
Train acc on all data:  0.974135847232
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 9.99901e-05
Norm of the params: 26.6477
              Random: fixed 117 labels. Loss 0.30894. Accuracy 0.915.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160113
Train loss (w/o reg) on all data: 0.118046
Test loss (w/o reg) on all data: 0.520123
Train acc on all data:  0.971235194585
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 1.00738e-05
Norm of the params: 29.0058
Flipped loss: 0.52012. Accuracy: 0.871
### Flips: 412, rs: 3, checks: 206
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0917882
Train loss (w/o reg) on all data: 0.0634811
Test loss (w/o reg) on all data: 0.376857
Train acc on all data:  0.986705342035
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 4.43656e-06
Norm of the params: 23.7937
     Influence (LOO): fixed 169 labels. Loss 0.37686. Accuracy 0.915.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0813862
Train loss (w/o reg) on all data: 0.0433672
Test loss (w/o reg) on all data: 0.41541
Train acc on all data:  0.999033115784
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 3.73607e-06
Norm of the params: 27.575
                Loss: fixed 137 labels. Loss 0.41541. Accuracy 0.900.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153894
Train loss (w/o reg) on all data: 0.112818
Test loss (w/o reg) on all data: 0.478909
Train acc on all data:  0.973168963017
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 2.72142e-05
Norm of the params: 28.6622
              Random: fixed  19 labels. Loss 0.47891. Accuracy 0.878.
### Flips: 412, rs: 3, checks: 412
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0780433
Train loss (w/o reg) on all data: 0.054529
Test loss (w/o reg) on all data: 0.198646
Train acc on all data:  0.988639110467
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 3.31786e-06
Norm of the params: 21.6861
     Influence (LOO): fixed 252 labels. Loss 0.19865. Accuracy 0.939.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0640637
Train loss (w/o reg) on all data: 0.0324255
Test loss (w/o reg) on all data: 0.389893
Train acc on all data:  0.999274836838
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 1.12598e-05
Norm of the params: 25.1548
                Loss: fixed 206 labels. Loss 0.38989. Accuracy 0.912.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148978
Train loss (w/o reg) on all data: 0.108632
Test loss (w/o reg) on all data: 0.483678
Train acc on all data:  0.974377568286
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 1.17141e-05
Norm of the params: 28.4064
              Random: fixed  42 labels. Loss 0.48368. Accuracy 0.880.
### Flips: 412, rs: 3, checks: 618
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0724034
Train loss (w/o reg) on all data: 0.0517902
Test loss (w/o reg) on all data: 0.161406
Train acc on all data:  0.987913947305
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 1.10242e-06
Norm of the params: 20.3043
     Influence (LOO): fixed 297 labels. Loss 0.16141. Accuracy 0.957.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0566605
Train loss (w/o reg) on all data: 0.0280182
Test loss (w/o reg) on all data: 0.336818
Train acc on all data:  0.999274836838
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 1.07345e-05
Norm of the params: 23.9342
                Loss: fixed 240 labels. Loss 0.33682. Accuracy 0.929.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143295
Train loss (w/o reg) on all data: 0.103704
Test loss (w/o reg) on all data: 0.458541
Train acc on all data:  0.976069615664
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.11137e-05
Norm of the params: 28.1396
              Random: fixed  62 labels. Loss 0.45854. Accuracy 0.882.
### Flips: 412, rs: 3, checks: 824
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0682233
Train loss (w/o reg) on all data: 0.0489006
Test loss (w/o reg) on all data: 0.155965
Train acc on all data:  0.989605994682
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.07201e-06
Norm of the params: 19.6584
     Influence (LOO): fixed 323 labels. Loss 0.15597. Accuracy 0.963.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0525883
Train loss (w/o reg) on all data: 0.0258395
Test loss (w/o reg) on all data: 0.28144
Train acc on all data:  0.999274836838
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 4.0078e-06
Norm of the params: 23.1295
                Loss: fixed 266 labels. Loss 0.28144. Accuracy 0.927.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138725
Train loss (w/o reg) on all data: 0.0996862
Test loss (w/o reg) on all data: 0.485367
Train acc on all data:  0.977519941987
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 6.1604e-06
Norm of the params: 27.9425
              Random: fixed  80 labels. Loss 0.48537. Accuracy 0.885.
### Flips: 412, rs: 3, checks: 1030
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0619218
Train loss (w/o reg) on all data: 0.0447929
Test loss (w/o reg) on all data: 0.141099
Train acc on all data:  0.989364273628
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 9.88703e-07
Norm of the params: 18.5089
     Influence (LOO): fixed 347 labels. Loss 0.14110. Accuracy 0.973.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0452121
Train loss (w/o reg) on all data: 0.0218606
Test loss (w/o reg) on all data: 0.251293
Train acc on all data:  0.999274836838
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 1.90877e-06
Norm of the params: 21.6109
                Loss: fixed 300 labels. Loss 0.25129. Accuracy 0.940.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132955
Train loss (w/o reg) on all data: 0.0956595
Test loss (w/o reg) on all data: 0.488958
Train acc on all data:  0.97897026831
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 1.08226e-05
Norm of the params: 27.3113
              Random: fixed 109 labels. Loss 0.48896. Accuracy 0.895.
### Flips: 412, rs: 3, checks: 1236
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05989
Train loss (w/o reg) on all data: 0.0434924
Test loss (w/o reg) on all data: 0.118868
Train acc on all data:  0.989605994682
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 1.80694e-06
Norm of the params: 18.1095
     Influence (LOO): fixed 358 labels. Loss 0.11887. Accuracy 0.976.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0406189
Train loss (w/o reg) on all data: 0.0194851
Test loss (w/o reg) on all data: 0.237213
Train acc on all data:  0.999516557892
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 2.03852e-06
Norm of the params: 20.5591
                Loss: fixed 329 labels. Loss 0.23721. Accuracy 0.957.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12552
Train loss (w/o reg) on all data: 0.0893881
Test loss (w/o reg) on all data: 0.449769
Train acc on all data:  0.981145757796
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 1.15733e-05
Norm of the params: 26.882
              Random: fixed 131 labels. Loss 0.44977. Accuracy 0.895.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158548
Train loss (w/o reg) on all data: 0.117245
Test loss (w/o reg) on all data: 0.420716
Train acc on all data:  0.9690597051
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 2.05999e-05
Norm of the params: 28.7415
Flipped loss: 0.42072. Accuracy: 0.897
### Flips: 412, rs: 4, checks: 206
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0967388
Train loss (w/o reg) on all data: 0.0688697
Test loss (w/o reg) on all data: 0.273757
Train acc on all data:  0.983562968335
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 4.66056e-06
Norm of the params: 23.6089
     Influence (LOO): fixed 169 labels. Loss 0.27376. Accuracy 0.930.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0844799
Train loss (w/o reg) on all data: 0.045583
Test loss (w/o reg) on all data: 0.384184
Train acc on all data:  0.99879139473
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 6.37845e-06
Norm of the params: 27.8915
                Loss: fixed 143 labels. Loss 0.38418. Accuracy 0.901.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150402
Train loss (w/o reg) on all data: 0.109855
Test loss (w/o reg) on all data: 0.424198
Train acc on all data:  0.971235194585
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 2.63305e-05
Norm of the params: 28.4767
              Random: fixed  26 labels. Loss 0.42420. Accuracy 0.898.
### Flips: 412, rs: 4, checks: 412
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0824013
Train loss (w/o reg) on all data: 0.0596024
Test loss (w/o reg) on all data: 0.165652
Train acc on all data:  0.98573845782
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 5.34388e-06
Norm of the params: 21.3537
     Influence (LOO): fixed 258 labels. Loss 0.16565. Accuracy 0.958.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0661255
Train loss (w/o reg) on all data: 0.0336841
Test loss (w/o reg) on all data: 0.314051
Train acc on all data:  0.998549673677
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 7.1073e-06
Norm of the params: 25.4721
                Loss: fixed 205 labels. Loss 0.31405. Accuracy 0.922.
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145994
Train loss (w/o reg) on all data: 0.105826
Test loss (w/o reg) on all data: 0.404182
Train acc on all data:  0.972685520909
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 8.31533e-06
Norm of the params: 28.3434
              Random: fixed  42 labels. Loss 0.40418. Accuracy 0.894.
### Flips: 412, rs: 4, checks: 618
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074889
Train loss (w/o reg) on all data: 0.054592
Test loss (w/o reg) on all data: 0.123533
Train acc on all data:  0.986221899927
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.0988e-05
Norm of the params: 20.148
     Influence (LOO): fixed 295 labels. Loss 0.12353. Accuracy 0.967.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0573488
Train loss (w/o reg) on all data: 0.0287636
Test loss (w/o reg) on all data: 0.268248
Train acc on all data:  0.998549673677
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.08044e-05
Norm of the params: 23.9104
                Loss: fixed 245 labels. Loss 0.26825. Accuracy 0.936.
Using normal model
LBFGS training took [358] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141952
Train loss (w/o reg) on all data: 0.102387
Test loss (w/o reg) on all data: 0.390165
Train acc on all data:  0.974377568286
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.55071e-05
Norm of the params: 28.1304
              Random: fixed  60 labels. Loss 0.39016. Accuracy 0.899.
### Flips: 412, rs: 4, checks: 824
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0701613
Train loss (w/o reg) on all data: 0.0515243
Test loss (w/o reg) on all data: 0.115736
Train acc on all data:  0.987188784143
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 9.90569e-07
Norm of the params: 19.3064
     Influence (LOO): fixed 318 labels. Loss 0.11574. Accuracy 0.970.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0505397
Train loss (w/o reg) on all data: 0.0249429
Test loss (w/o reg) on all data: 0.218503
Train acc on all data:  0.999033115784
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 9.63673e-06
Norm of the params: 22.626
                Loss: fixed 275 labels. Loss 0.21850. Accuracy 0.941.
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138268
Train loss (w/o reg) on all data: 0.0997735
Test loss (w/o reg) on all data: 0.334962
Train acc on all data:  0.974377568286
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 8.58337e-06
Norm of the params: 27.747
              Random: fixed  82 labels. Loss 0.33496. Accuracy 0.900.
### Flips: 412, rs: 4, checks: 1030
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0664222
Train loss (w/o reg) on all data: 0.0490431
Test loss (w/o reg) on all data: 0.10051
Train acc on all data:  0.988155668359
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 2.16e-06
Norm of the params: 18.6436
     Influence (LOO): fixed 335 labels. Loss 0.10051. Accuracy 0.976.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0468856
Train loss (w/o reg) on all data: 0.0229464
Test loss (w/o reg) on all data: 0.199902
Train acc on all data:  0.999033115784
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 1.50791e-05
Norm of the params: 21.8812
                Loss: fixed 300 labels. Loss 0.19990. Accuracy 0.956.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13454
Train loss (w/o reg) on all data: 0.0969391
Test loss (w/o reg) on all data: 0.359589
Train acc on all data:  0.974861010394
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 2.2286e-05
Norm of the params: 27.4231
              Random: fixed 103 labels. Loss 0.35959. Accuracy 0.901.
### Flips: 412, rs: 4, checks: 1236
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062843
Train loss (w/o reg) on all data: 0.0462843
Test loss (w/o reg) on all data: 0.115614
Train acc on all data:  0.989364273628
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 8.9855e-07
Norm of the params: 18.1982
     Influence (LOO): fixed 350 labels. Loss 0.11561. Accuracy 0.974.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430125
Train loss (w/o reg) on all data: 0.0209386
Test loss (w/o reg) on all data: 0.213034
Train acc on all data:  0.999274836838
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 8.41385e-06
Norm of the params: 21.0113
                Loss: fixed 323 labels. Loss 0.21303. Accuracy 0.959.
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130848
Train loss (w/o reg) on all data: 0.0940885
Test loss (w/o reg) on all data: 0.344088
Train acc on all data:  0.976069615664
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 4.17604e-05
Norm of the params: 27.1142
              Random: fixed 122 labels. Loss 0.34409. Accuracy 0.904.
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161354
Train loss (w/o reg) on all data: 0.120489
Test loss (w/o reg) on all data: 0.377403
Train acc on all data:  0.968334541939
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 7.03637e-05
Norm of the params: 28.5886
Flipped loss: 0.37740. Accuracy: 0.891
### Flips: 412, rs: 5, checks: 206
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091107
Train loss (w/o reg) on all data: 0.0656605
Test loss (w/o reg) on all data: 0.234943
Train acc on all data:  0.984046410442
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 7.55157e-06
Norm of the params: 22.5595
     Influence (LOO): fixed 180 labels. Loss 0.23494. Accuracy 0.940.
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0809189
Train loss (w/o reg) on all data: 0.0438443
Test loss (w/o reg) on all data: 0.291076
Train acc on all data:  0.998549673677
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 2.39955e-05
Norm of the params: 27.2304
                Loss: fixed 150 labels. Loss 0.29108. Accuracy 0.924.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158117
Train loss (w/o reg) on all data: 0.117999
Test loss (w/o reg) on all data: 0.371384
Train acc on all data:  0.968817984046
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.09811e-05
Norm of the params: 28.3257
              Random: fixed  20 labels. Loss 0.37138. Accuracy 0.899.
### Flips: 412, rs: 5, checks: 412
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077236
Train loss (w/o reg) on all data: 0.0563439
Test loss (w/o reg) on all data: 0.159878
Train acc on all data:  0.985496736766
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 2.76479e-06
Norm of the params: 20.4412
     Influence (LOO): fixed 261 labels. Loss 0.15988. Accuracy 0.956.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0621532
Train loss (w/o reg) on all data: 0.0314568
Test loss (w/o reg) on all data: 0.235652
Train acc on all data:  0.999033115784
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 6.59959e-06
Norm of the params: 24.7776
                Loss: fixed 213 labels. Loss 0.23565. Accuracy 0.931.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153977
Train loss (w/o reg) on all data: 0.114361
Test loss (w/o reg) on all data: 0.339215
Train acc on all data:  0.969301426154
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 9.12075e-05
Norm of the params: 28.1485
              Random: fixed  39 labels. Loss 0.33922. Accuracy 0.909.
### Flips: 412, rs: 5, checks: 618
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0711382
Train loss (w/o reg) on all data: 0.0523871
Test loss (w/o reg) on all data: 0.145003
Train acc on all data:  0.986221899927
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 6.85648e-06
Norm of the params: 19.3655
     Influence (LOO): fixed 298 labels. Loss 0.14500. Accuracy 0.963.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0502162
Train loss (w/o reg) on all data: 0.0246095
Test loss (w/o reg) on all data: 0.208315
Train acc on all data:  0.999274836838
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 6.15566e-06
Norm of the params: 22.6304
                Loss: fixed 256 labels. Loss 0.20832. Accuracy 0.938.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151707
Train loss (w/o reg) on all data: 0.112651
Test loss (w/o reg) on all data: 0.327018
Train acc on all data:  0.970751752478
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 6.90258e-06
Norm of the params: 27.9485
              Random: fixed  54 labels. Loss 0.32702. Accuracy 0.906.
### Flips: 412, rs: 5, checks: 824
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0666801
Train loss (w/o reg) on all data: 0.0491875
Test loss (w/o reg) on all data: 0.110797
Train acc on all data:  0.987188784143
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 2.04736e-06
Norm of the params: 18.7043
     Influence (LOO): fixed 320 labels. Loss 0.11080. Accuracy 0.969.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0438885
Train loss (w/o reg) on all data: 0.0212292
Test loss (w/o reg) on all data: 0.165585
Train acc on all data:  0.999274836838
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 3.33832e-06
Norm of the params: 21.2882
                Loss: fixed 286 labels. Loss 0.16558. Accuracy 0.953.
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147991
Train loss (w/o reg) on all data: 0.10972
Test loss (w/o reg) on all data: 0.300639
Train acc on all data:  0.971718636693
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 3.53726e-05
Norm of the params: 27.6663
              Random: fixed  71 labels. Loss 0.30064. Accuracy 0.909.
### Flips: 412, rs: 5, checks: 1030
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0638978
Train loss (w/o reg) on all data: 0.0475511
Test loss (w/o reg) on all data: 0.112499
Train acc on all data:  0.987188784143
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.54974e-06
Norm of the params: 18.0813
     Influence (LOO): fixed 342 labels. Loss 0.11250. Accuracy 0.971.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0416446
Train loss (w/o reg) on all data: 0.0200942
Test loss (w/o reg) on all data: 0.159977
Train acc on all data:  0.999516557892
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 4.95402e-06
Norm of the params: 20.7607
                Loss: fixed 305 labels. Loss 0.15998. Accuracy 0.956.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139487
Train loss (w/o reg) on all data: 0.102453
Test loss (w/o reg) on all data: 0.280895
Train acc on all data:  0.972927241963
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 4.64526e-05
Norm of the params: 27.2155
              Random: fixed  95 labels. Loss 0.28089. Accuracy 0.915.
### Flips: 412, rs: 5, checks: 1236
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0615378
Train loss (w/o reg) on all data: 0.0454881
Test loss (w/o reg) on all data: 0.111973
Train acc on all data:  0.988397389413
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.04793e-06
Norm of the params: 17.9163
     Influence (LOO): fixed 352 labels. Loss 0.11197. Accuracy 0.974.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0390672
Train loss (w/o reg) on all data: 0.0187973
Test loss (w/o reg) on all data: 0.13285
Train acc on all data:  0.999516557892
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 5.02945e-06
Norm of the params: 20.1345
                Loss: fixed 324 labels. Loss 0.13285. Accuracy 0.962.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13116
Train loss (w/o reg) on all data: 0.0953452
Test loss (w/o reg) on all data: 0.280818
Train acc on all data:  0.97582789461
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 1.40555e-05
Norm of the params: 26.7637
              Random: fixed 113 labels. Loss 0.28082. Accuracy 0.919.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161469
Train loss (w/o reg) on all data: 0.120608
Test loss (w/o reg) on all data: 0.55051
Train acc on all data:  0.967851099831
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 2.6289e-05
Norm of the params: 28.587
Flipped loss: 0.55051. Accuracy: 0.883
### Flips: 412, rs: 6, checks: 206
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0949039
Train loss (w/o reg) on all data: 0.0676424
Test loss (w/o reg) on all data: 0.240403
Train acc on all data:  0.984288131496
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.20006e-05
Norm of the params: 23.3501
     Influence (LOO): fixed 170 labels. Loss 0.24040. Accuracy 0.935.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0830679
Train loss (w/o reg) on all data: 0.0452149
Test loss (w/o reg) on all data: 0.398241
Train acc on all data:  0.998066231569
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 5.34012e-06
Norm of the params: 27.5147
                Loss: fixed 148 labels. Loss 0.39824. Accuracy 0.900.
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156157
Train loss (w/o reg) on all data: 0.115819
Test loss (w/o reg) on all data: 0.468184
Train acc on all data:  0.970026589316
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 1.06541e-05
Norm of the params: 28.4037
              Random: fixed  20 labels. Loss 0.46818. Accuracy 0.883.
### Flips: 412, rs: 6, checks: 412
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0792883
Train loss (w/o reg) on all data: 0.0576785
Test loss (w/o reg) on all data: 0.297807
Train acc on all data:  0.985013294658
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 9.15939e-06
Norm of the params: 20.7893
     Influence (LOO): fixed 259 labels. Loss 0.29781. Accuracy 0.957.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0613589
Train loss (w/o reg) on all data: 0.0308777
Test loss (w/o reg) on all data: 0.298361
Train acc on all data:  0.999274836838
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 5.93428e-06
Norm of the params: 24.6906
                Loss: fixed 220 labels. Loss 0.29836. Accuracy 0.926.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152492
Train loss (w/o reg) on all data: 0.112346
Test loss (w/o reg) on all data: 0.45498
Train acc on all data:  0.970993473532
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 4.2013e-05
Norm of the params: 28.336
              Random: fixed  34 labels. Loss 0.45498. Accuracy 0.894.
### Flips: 412, rs: 6, checks: 618
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072076
Train loss (w/o reg) on all data: 0.0524866
Test loss (w/o reg) on all data: 0.280198
Train acc on all data:  0.986705342035
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 2.5116e-06
Norm of the params: 19.7936
     Influence (LOO): fixed 295 labels. Loss 0.28020. Accuracy 0.959.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0499005
Train loss (w/o reg) on all data: 0.024195
Test loss (w/o reg) on all data: 0.249782
Train acc on all data:  0.999516557892
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 1.31868e-05
Norm of the params: 22.674
                Loss: fixed 270 labels. Loss 0.24978. Accuracy 0.937.
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146956
Train loss (w/o reg) on all data: 0.106997
Test loss (w/o reg) on all data: 0.455241
Train acc on all data:  0.972927241963
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 1.7653e-05
Norm of the params: 28.2697
              Random: fixed  55 labels. Loss 0.45524. Accuracy 0.891.
### Flips: 412, rs: 6, checks: 824
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0687401
Train loss (w/o reg) on all data: 0.0503541
Test loss (w/o reg) on all data: 0.244892
Train acc on all data:  0.987430505197
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 4.65013e-06
Norm of the params: 19.176
     Influence (LOO): fixed 317 labels. Loss 0.24489. Accuracy 0.962.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0426772
Train loss (w/o reg) on all data: 0.02045
Test loss (w/o reg) on all data: 0.260396
Train acc on all data:  0.999516557892
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 1.21628e-05
Norm of the params: 21.0842
                Loss: fixed 299 labels. Loss 0.26040. Accuracy 0.948.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138573
Train loss (w/o reg) on all data: 0.0995373
Test loss (w/o reg) on all data: 0.448309
Train acc on all data:  0.976069615664
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 2.60188e-05
Norm of the params: 27.9413
              Random: fixed  80 labels. Loss 0.44831. Accuracy 0.898.
### Flips: 412, rs: 6, checks: 1030
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0665132
Train loss (w/o reg) on all data: 0.0490392
Test loss (w/o reg) on all data: 0.242016
Train acc on all data:  0.987913947305
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 4.36031e-06
Norm of the params: 18.6944
     Influence (LOO): fixed 336 labels. Loss 0.24202. Accuracy 0.965.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0416302
Train loss (w/o reg) on all data: 0.019966
Test loss (w/o reg) on all data: 0.245781
Train acc on all data:  0.999516557892
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 4.7233e-06
Norm of the params: 20.8155
                Loss: fixed 310 labels. Loss 0.24578. Accuracy 0.955.
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135598
Train loss (w/o reg) on all data: 0.0972468
Test loss (w/o reg) on all data: 0.41614
Train acc on all data:  0.975586173556
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.30867e-05
Norm of the params: 27.6953
              Random: fixed  96 labels. Loss 0.41614. Accuracy 0.903.
### Flips: 412, rs: 6, checks: 1236
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063515
Train loss (w/o reg) on all data: 0.0468829
Test loss (w/o reg) on all data: 0.259546
Train acc on all data:  0.988639110467
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.14153e-06
Norm of the params: 18.2385
     Influence (LOO): fixed 350 labels. Loss 0.25955. Accuracy 0.969.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0397818
Train loss (w/o reg) on all data: 0.0190308
Test loss (w/o reg) on all data: 0.208515
Train acc on all data:  0.999516557892
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 1.13606e-05
Norm of the params: 20.372
                Loss: fixed 325 labels. Loss 0.20851. Accuracy 0.957.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130479
Train loss (w/o reg) on all data: 0.0929406
Test loss (w/o reg) on all data: 0.405183
Train acc on all data:  0.977761663041
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 2.44413e-05
Norm of the params: 27.4
              Random: fixed 117 labels. Loss 0.40518. Accuracy 0.901.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164651
Train loss (w/o reg) on all data: 0.122525
Test loss (w/o reg) on all data: 0.601114
Train acc on all data:  0.968092820885
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 2.23023e-05
Norm of the params: 29.0262
Flipped loss: 0.60111. Accuracy: 0.892
### Flips: 412, rs: 7, checks: 206
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.099127
Train loss (w/o reg) on all data: 0.0709008
Test loss (w/o reg) on all data: 0.308646
Train acc on all data:  0.983562968335
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.00296e-05
Norm of the params: 23.7597
     Influence (LOO): fixed 168 labels. Loss 0.30865. Accuracy 0.929.
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0827251
Train loss (w/o reg) on all data: 0.0446706
Test loss (w/o reg) on all data: 0.500937
Train acc on all data:  0.999274836838
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 4.92951e-05
Norm of the params: 27.5878
                Loss: fixed 144 labels. Loss 0.50094. Accuracy 0.903.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157629
Train loss (w/o reg) on all data: 0.116281
Test loss (w/o reg) on all data: 0.590695
Train acc on all data:  0.969301426154
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 5.475e-05
Norm of the params: 28.7569
              Random: fixed  24 labels. Loss 0.59069. Accuracy 0.892.
### Flips: 412, rs: 7, checks: 412
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0880793
Train loss (w/o reg) on all data: 0.0645891
Test loss (w/o reg) on all data: 0.339534
Train acc on all data:  0.984288131496
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 4.31379e-06
Norm of the params: 21.6749
     Influence (LOO): fixed 246 labels. Loss 0.33953. Accuracy 0.951.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0654506
Train loss (w/o reg) on all data: 0.0334475
Test loss (w/o reg) on all data: 0.494465
Train acc on all data:  0.999516557892
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 6.70298e-06
Norm of the params: 25.2995
                Loss: fixed 201 labels. Loss 0.49447. Accuracy 0.924.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153062
Train loss (w/o reg) on all data: 0.112265
Test loss (w/o reg) on all data: 0.608168
Train acc on all data:  0.970993473532
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 1.30513e-05
Norm of the params: 28.5646
              Random: fixed  39 labels. Loss 0.60817. Accuracy 0.898.
### Flips: 412, rs: 7, checks: 618
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0806262
Train loss (w/o reg) on all data: 0.0592921
Test loss (w/o reg) on all data: 0.286836
Train acc on all data:  0.986221899927
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 1.81891e-06
Norm of the params: 20.6563
     Influence (LOO): fixed 287 labels. Loss 0.28684. Accuracy 0.959.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0549042
Train loss (w/o reg) on all data: 0.027141
Test loss (w/o reg) on all data: 0.368098
Train acc on all data:  0.999516557892
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 9.62506e-06
Norm of the params: 23.564
                Loss: fixed 250 labels. Loss 0.36810. Accuracy 0.938.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149582
Train loss (w/o reg) on all data: 0.108924
Test loss (w/o reg) on all data: 0.631224
Train acc on all data:  0.972443799855
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 2.85597e-05
Norm of the params: 28.5158
              Random: fixed  51 labels. Loss 0.63122. Accuracy 0.899.
### Flips: 412, rs: 7, checks: 824
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0692077
Train loss (w/o reg) on all data: 0.0498616
Test loss (w/o reg) on all data: 0.283773
Train acc on all data:  0.988639110467
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 8.03259e-06
Norm of the params: 19.6703
     Influence (LOO): fixed 320 labels. Loss 0.28377. Accuracy 0.968.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0508101
Train loss (w/o reg) on all data: 0.0249048
Test loss (w/o reg) on all data: 0.290209
Train acc on all data:  0.999516557892
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 6.87604e-06
Norm of the params: 22.7619
                Loss: fixed 277 labels. Loss 0.29021. Accuracy 0.943.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144436
Train loss (w/o reg) on all data: 0.104513
Test loss (w/o reg) on all data: 0.646786
Train acc on all data:  0.974377568286
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 1.03693e-05
Norm of the params: 28.2571
              Random: fixed  70 labels. Loss 0.64679. Accuracy 0.904.
### Flips: 412, rs: 7, checks: 1030
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0663016
Train loss (w/o reg) on all data: 0.0478012
Test loss (w/o reg) on all data: 0.209331
Train acc on all data:  0.989122552574
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 1.65996e-06
Norm of the params: 19.2356
     Influence (LOO): fixed 332 labels. Loss 0.20933. Accuracy 0.964.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0452565
Train loss (w/o reg) on all data: 0.021848
Test loss (w/o reg) on all data: 0.240888
Train acc on all data:  0.999516557892
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 2.18717e-05
Norm of the params: 21.6372
                Loss: fixed 300 labels. Loss 0.24089. Accuracy 0.943.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140316
Train loss (w/o reg) on all data: 0.101168
Test loss (w/o reg) on all data: 0.649196
Train acc on all data:  0.97461928934
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.38973e-05
Norm of the params: 27.9813
              Random: fixed  89 labels. Loss 0.64920. Accuracy 0.907.
### Flips: 412, rs: 7, checks: 1236
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0631514
Train loss (w/o reg) on all data: 0.0457883
Test loss (w/o reg) on all data: 0.173831
Train acc on all data:  0.989605994682
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.81359e-06
Norm of the params: 18.635
     Influence (LOO): fixed 345 labels. Loss 0.17383. Accuracy 0.967.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0411877
Train loss (w/o reg) on all data: 0.0197175
Test loss (w/o reg) on all data: 0.181229
Train acc on all data:  0.999516557892
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 6.27435e-06
Norm of the params: 20.7221
                Loss: fixed 320 labels. Loss 0.18123. Accuracy 0.957.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134015
Train loss (w/o reg) on all data: 0.0960192
Test loss (w/o reg) on all data: 0.603835
Train acc on all data:  0.976311336717
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 6.56072e-05
Norm of the params: 27.5666
              Random: fixed 114 labels. Loss 0.60383. Accuracy 0.907.
Using normal model
LBFGS training took [666] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159004
Train loss (w/o reg) on all data: 0.118235
Test loss (w/o reg) on all data: 0.413993
Train acc on all data:  0.968334541939
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 1.20895e-05
Norm of the params: 28.5552
Flipped loss: 0.41399. Accuracy: 0.870
### Flips: 412, rs: 8, checks: 206
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0900571
Train loss (w/o reg) on all data: 0.0633365
Test loss (w/o reg) on all data: 0.221258
Train acc on all data:  0.986947063089
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.54042e-05
Norm of the params: 23.1174
     Influence (LOO): fixed 173 labels. Loss 0.22126. Accuracy 0.924.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0778447
Train loss (w/o reg) on all data: 0.041943
Test loss (w/o reg) on all data: 0.342466
Train acc on all data:  0.999274836838
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 3.66903e-05
Norm of the params: 26.7962
                Loss: fixed 149 labels. Loss 0.34247. Accuracy 0.901.
Using normal model
LBFGS training took [544] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152136
Train loss (w/o reg) on all data: 0.112527
Test loss (w/o reg) on all data: 0.400183
Train acc on all data:  0.970026589316
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 6.8353e-05
Norm of the params: 28.1457
              Random: fixed  27 labels. Loss 0.40018. Accuracy 0.883.
### Flips: 412, rs: 8, checks: 412
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0809352
Train loss (w/o reg) on all data: 0.0582522
Test loss (w/o reg) on all data: 0.182707
Train acc on all data:  0.987430505197
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 5.50082e-06
Norm of the params: 21.2993
     Influence (LOO): fixed 250 labels. Loss 0.18271. Accuracy 0.952.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0603046
Train loss (w/o reg) on all data: 0.0305982
Test loss (w/o reg) on all data: 0.261753
Train acc on all data:  0.999274836838
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 1.38028e-05
Norm of the params: 24.3747
                Loss: fixed 214 labels. Loss 0.26175. Accuracy 0.921.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146184
Train loss (w/o reg) on all data: 0.107369
Test loss (w/o reg) on all data: 0.358535
Train acc on all data:  0.971476915639
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 6.77692e-05
Norm of the params: 27.8621
              Random: fixed  59 labels. Loss 0.35854. Accuracy 0.896.
### Flips: 412, rs: 8, checks: 618
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0703227
Train loss (w/o reg) on all data: 0.050563
Test loss (w/o reg) on all data: 0.177841
Train acc on all data:  0.988639110467
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 3.24454e-05
Norm of the params: 19.8795
     Influence (LOO): fixed 295 labels. Loss 0.17784. Accuracy 0.962.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0531169
Train loss (w/o reg) on all data: 0.0262475
Test loss (w/o reg) on all data: 0.237924
Train acc on all data:  0.999758278946
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 3.52088e-06
Norm of the params: 23.1816
                Loss: fixed 255 labels. Loss 0.23792. Accuracy 0.938.
Using normal model
LBFGS training took [596] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140258
Train loss (w/o reg) on all data: 0.102217
Test loss (w/o reg) on all data: 0.333882
Train acc on all data:  0.972685520909
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 0.00012733
Norm of the params: 27.5829
              Random: fixed  85 labels. Loss 0.33388. Accuracy 0.900.
### Flips: 412, rs: 8, checks: 824
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0674434
Train loss (w/o reg) on all data: 0.048609
Test loss (w/o reg) on all data: 0.181937
Train acc on all data:  0.98888083152
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 4.06436e-06
Norm of the params: 19.4085
     Influence (LOO): fixed 317 labels. Loss 0.18194. Accuracy 0.965.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0468895
Train loss (w/o reg) on all data: 0.0229387
Test loss (w/o reg) on all data: 0.229044
Train acc on all data:  0.999758278946
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 3.79398e-06
Norm of the params: 21.8864
                Loss: fixed 284 labels. Loss 0.22904. Accuracy 0.937.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135091
Train loss (w/o reg) on all data: 0.0981585
Test loss (w/o reg) on all data: 0.354366
Train acc on all data:  0.973894126178
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 7.25945e-05
Norm of the params: 27.1779
              Random: fixed 105 labels. Loss 0.35437. Accuracy 0.902.
### Flips: 412, rs: 8, checks: 1030
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0648852
Train loss (w/o reg) on all data: 0.0472081
Test loss (w/o reg) on all data: 0.13005
Train acc on all data:  0.989605994682
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 3.80827e-06
Norm of the params: 18.8027
     Influence (LOO): fixed 337 labels. Loss 0.13005. Accuracy 0.966.
Using normal model
LBFGS training took [355] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430633
Train loss (w/o reg) on all data: 0.020768
Test loss (w/o reg) on all data: 0.200424
Train acc on all data:  0.999274836838
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 6.5521e-06
Norm of the params: 21.1165
                Loss: fixed 305 labels. Loss 0.20042. Accuracy 0.948.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131537
Train loss (w/o reg) on all data: 0.0953019
Test loss (w/o reg) on all data: 0.347077
Train acc on all data:  0.97461928934
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 6.03206e-05
Norm of the params: 26.9205
              Random: fixed 121 labels. Loss 0.34708. Accuracy 0.901.
### Flips: 412, rs: 8, checks: 1236
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0612474
Train loss (w/o reg) on all data: 0.044709
Test loss (w/o reg) on all data: 0.139255
Train acc on all data:  0.99008943679
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.16363e-06
Norm of the params: 18.1871
     Influence (LOO): fixed 352 labels. Loss 0.13926. Accuracy 0.972.
Using normal model
LBFGS training took [365] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0408087
Train loss (w/o reg) on all data: 0.0195784
Test loss (w/o reg) on all data: 0.175927
Train acc on all data:  0.999516557892
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.99023e-05
Norm of the params: 20.606
                Loss: fixed 320 labels. Loss 0.17593. Accuracy 0.955.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125689
Train loss (w/o reg) on all data: 0.090323
Test loss (w/o reg) on all data: 0.326533
Train acc on all data:  0.976553057771
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.74671e-05
Norm of the params: 26.5955
              Random: fixed 141 labels. Loss 0.32653. Accuracy 0.910.
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.165724
Train loss (w/o reg) on all data: 0.123511
Test loss (w/o reg) on all data: 0.545854
Train acc on all data:  0.965675610346
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 6.19447e-05
Norm of the params: 29.0562
Flipped loss: 0.54585. Accuracy: 0.890
### Flips: 412, rs: 9, checks: 206
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0993057
Train loss (w/o reg) on all data: 0.0717953
Test loss (w/o reg) on all data: 0.364207
Train acc on all data:  0.981387478849
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 2.13682e-05
Norm of the params: 23.4566
     Influence (LOO): fixed 168 labels. Loss 0.36421. Accuracy 0.929.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0828976
Train loss (w/o reg) on all data: 0.0444039
Test loss (w/o reg) on all data: 0.608077
Train acc on all data:  0.999274836838
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 1.41686e-05
Norm of the params: 27.7466
                Loss: fixed 152 labels. Loss 0.60808. Accuracy 0.902.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161812
Train loss (w/o reg) on all data: 0.120207
Test loss (w/o reg) on all data: 0.558794
Train acc on all data:  0.966642494561
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 4.88586e-05
Norm of the params: 28.8461
              Random: fixed  17 labels. Loss 0.55879. Accuracy 0.897.
### Flips: 412, rs: 9, checks: 412
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082803
Train loss (w/o reg) on all data: 0.0613757
Test loss (w/o reg) on all data: 0.22556
Train acc on all data:  0.983321247281
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 6.51248e-06
Norm of the params: 20.7013
     Influence (LOO): fixed 260 labels. Loss 0.22556. Accuracy 0.955.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0620017
Train loss (w/o reg) on all data: 0.0310471
Test loss (w/o reg) on all data: 0.521742
Train acc on all data:  0.999758278946
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 5.62507e-06
Norm of the params: 24.8816
                Loss: fixed 220 labels. Loss 0.52174. Accuracy 0.928.
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156661
Train loss (w/o reg) on all data: 0.115908
Test loss (w/o reg) on all data: 0.566353
Train acc on all data:  0.967125936669
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 4.54358e-05
Norm of the params: 28.5493
              Random: fixed  38 labels. Loss 0.56635. Accuracy 0.908.
### Flips: 412, rs: 9, checks: 618
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0753828
Train loss (w/o reg) on all data: 0.0566297
Test loss (w/o reg) on all data: 0.328113
Train acc on all data:  0.983562968335
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 6.31437e-06
Norm of the params: 19.3665
     Influence (LOO): fixed 300 labels. Loss 0.32811. Accuracy 0.962.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0529188
Train loss (w/o reg) on all data: 0.0258828
Test loss (w/o reg) on all data: 0.418303
Train acc on all data:  0.999516557892
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 4.80537e-06
Norm of the params: 23.2534
                Loss: fixed 257 labels. Loss 0.41830. Accuracy 0.938.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150691
Train loss (w/o reg) on all data: 0.111006
Test loss (w/o reg) on all data: 0.523404
Train acc on all data:  0.969543147208
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 3.40905e-05
Norm of the params: 28.1726
              Random: fixed  60 labels. Loss 0.52340. Accuracy 0.906.
### Flips: 412, rs: 9, checks: 824
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0699771
Train loss (w/o reg) on all data: 0.0528316
Test loss (w/o reg) on all data: 0.411562
Train acc on all data:  0.98452985255
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 9.78614e-07
Norm of the params: 18.5178
     Influence (LOO): fixed 319 labels. Loss 0.41156. Accuracy 0.965.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0468746
Train loss (w/o reg) on all data: 0.0226018
Test loss (w/o reg) on all data: 0.316973
Train acc on all data:  0.999516557892
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 6.02129e-06
Norm of the params: 22.0331
                Loss: fixed 292 labels. Loss 0.31697. Accuracy 0.947.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145346
Train loss (w/o reg) on all data: 0.106355
Test loss (w/o reg) on all data: 0.522081
Train acc on all data:  0.971476915639
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 3.71662e-05
Norm of the params: 27.9251
              Random: fixed  77 labels. Loss 0.52208. Accuracy 0.916.
### Flips: 412, rs: 9, checks: 1030
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0638678
Train loss (w/o reg) on all data: 0.0481654
Test loss (w/o reg) on all data: 0.174679
Train acc on all data:  0.986221899927
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 4.71357e-06
Norm of the params: 17.7214
     Influence (LOO): fixed 343 labels. Loss 0.17468. Accuracy 0.970.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0435699
Train loss (w/o reg) on all data: 0.020926
Test loss (w/o reg) on all data: 0.232111
Train acc on all data:  0.999516557892
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 5.01659e-06
Norm of the params: 21.2809
                Loss: fixed 308 labels. Loss 0.23211. Accuracy 0.956.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139401
Train loss (w/o reg) on all data: 0.101438
Test loss (w/o reg) on all data: 0.540334
Train acc on all data:  0.974135847232
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 1.7913e-05
Norm of the params: 27.5546
              Random: fixed  95 labels. Loss 0.54033. Accuracy 0.918.
### Flips: 412, rs: 9, checks: 1236
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0613404
Train loss (w/o reg) on all data: 0.0458643
Test loss (w/o reg) on all data: 0.163085
Train acc on all data:  0.987430505197
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.06356e-06
Norm of the params: 17.5932
     Influence (LOO): fixed 354 labels. Loss 0.16309. Accuracy 0.971.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0391862
Train loss (w/o reg) on all data: 0.0188248
Test loss (w/o reg) on all data: 0.252077
Train acc on all data:  0.999516557892
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 5.95686e-06
Norm of the params: 20.1799
                Loss: fixed 332 labels. Loss 0.25208. Accuracy 0.960.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133588
Train loss (w/o reg) on all data: 0.0963049
Test loss (w/o reg) on all data: 0.538653
Train acc on all data:  0.975586173556
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 2.48269e-05
Norm of the params: 27.3067
              Random: fixed 112 labels. Loss 0.53865. Accuracy 0.914.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158636
Train loss (w/o reg) on all data: 0.118844
Test loss (w/o reg) on all data: 0.405005
Train acc on all data:  0.965433889292
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 5.1069e-05
Norm of the params: 28.2105
Flipped loss: 0.40500. Accuracy: 0.892
### Flips: 412, rs: 10, checks: 206
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0971397
Train loss (w/o reg) on all data: 0.069568
Test loss (w/o reg) on all data: 0.372358
Train acc on all data:  0.983079526227
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 3.59982e-06
Norm of the params: 23.4827
     Influence (LOO): fixed 171 labels. Loss 0.37236. Accuracy 0.931.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0782369
Train loss (w/o reg) on all data: 0.0415726
Test loss (w/o reg) on all data: 0.40892
Train acc on all data:  0.998549673677
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 5.1531e-06
Norm of the params: 27.0792
                Loss: fixed 155 labels. Loss 0.40892. Accuracy 0.912.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15337
Train loss (w/o reg) on all data: 0.114119
Test loss (w/o reg) on all data: 0.402836
Train acc on all data:  0.967367657723
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 7.57077e-06
Norm of the params: 28.0181
              Random: fixed  21 labels. Loss 0.40284. Accuracy 0.890.
### Flips: 412, rs: 10, checks: 412
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0810312
Train loss (w/o reg) on all data: 0.0580018
Test loss (w/o reg) on all data: 0.21061
Train acc on all data:  0.986221899927
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 2.71019e-06
Norm of the params: 21.4613
     Influence (LOO): fixed 248 labels. Loss 0.21061. Accuracy 0.956.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0620777
Train loss (w/o reg) on all data: 0.0312349
Test loss (w/o reg) on all data: 0.443868
Train acc on all data:  0.999274836838
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 9.61613e-06
Norm of the params: 24.8366
                Loss: fixed 215 labels. Loss 0.44387. Accuracy 0.926.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146996
Train loss (w/o reg) on all data: 0.108114
Test loss (w/o reg) on all data: 0.396773
Train acc on all data:  0.968817984046
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 1.02758e-05
Norm of the params: 27.8864
              Random: fixed  44 labels. Loss 0.39677. Accuracy 0.895.
### Flips: 412, rs: 10, checks: 618
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071376
Train loss (w/o reg) on all data: 0.0517901
Test loss (w/o reg) on all data: 0.132092
Train acc on all data:  0.987188784143
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 1.89299e-06
Norm of the params: 19.7919
     Influence (LOO): fixed 293 labels. Loss 0.13209. Accuracy 0.964.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0539034
Train loss (w/o reg) on all data: 0.0264761
Test loss (w/o reg) on all data: 0.418625
Train acc on all data:  0.999758278946
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 2.30084e-06
Norm of the params: 23.421
                Loss: fixed 252 labels. Loss 0.41863. Accuracy 0.940.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141213
Train loss (w/o reg) on all data: 0.103017
Test loss (w/o reg) on all data: 0.350781
Train acc on all data:  0.971235194585
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 8.89552e-06
Norm of the params: 27.6392
              Random: fixed  64 labels. Loss 0.35078. Accuracy 0.900.
### Flips: 412, rs: 10, checks: 824
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0661905
Train loss (w/o reg) on all data: 0.0486783
Test loss (w/o reg) on all data: 0.100237
Train acc on all data:  0.987672226251
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 9.17363e-07
Norm of the params: 18.7148
     Influence (LOO): fixed 319 labels. Loss 0.10024. Accuracy 0.974.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0478014
Train loss (w/o reg) on all data: 0.0232443
Test loss (w/o reg) on all data: 0.375819
Train acc on all data:  0.999516557892
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 3.3617e-06
Norm of the params: 22.1617
                Loss: fixed 291 labels. Loss 0.37582. Accuracy 0.952.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136053
Train loss (w/o reg) on all data: 0.0987634
Test loss (w/o reg) on all data: 0.348428
Train acc on all data:  0.972685520909
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 3.79915e-05
Norm of the params: 27.3092
              Random: fixed  85 labels. Loss 0.34843. Accuracy 0.903.
### Flips: 412, rs: 10, checks: 1030
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0624578
Train loss (w/o reg) on all data: 0.0460255
Test loss (w/o reg) on all data: 0.140109
Train acc on all data:  0.988397389413
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.05592e-06
Norm of the params: 18.1286
     Influence (LOO): fixed 342 labels. Loss 0.14011. Accuracy 0.972.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0450861
Train loss (w/o reg) on all data: 0.021794
Test loss (w/o reg) on all data: 0.384757
Train acc on all data:  0.999516557892
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 1.84146e-06
Norm of the params: 21.5834
                Loss: fixed 306 labels. Loss 0.38476. Accuracy 0.957.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130756
Train loss (w/o reg) on all data: 0.0939814
Test loss (w/o reg) on all data: 0.333857
Train acc on all data:  0.973410684071
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 2.36157e-05
Norm of the params: 27.12
              Random: fixed 106 labels. Loss 0.33386. Accuracy 0.901.
### Flips: 412, rs: 10, checks: 1236
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0587627
Train loss (w/o reg) on all data: 0.0430254
Test loss (w/o reg) on all data: 0.104128
Train acc on all data:  0.989122552574
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 9.89347e-07
Norm of the params: 17.7411
     Influence (LOO): fixed 353 labels. Loss 0.10413. Accuracy 0.970.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0398545
Train loss (w/o reg) on all data: 0.0190724
Test loss (w/o reg) on all data: 0.245801
Train acc on all data:  0.999516557892
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 2.23591e-06
Norm of the params: 20.3873
                Loss: fixed 326 labels. Loss 0.24580. Accuracy 0.969.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125894
Train loss (w/o reg) on all data: 0.0898517
Test loss (w/o reg) on all data: 0.309156
Train acc on all data:  0.976794778825
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 2.51156e-05
Norm of the params: 26.8487
              Random: fixed 126 labels. Loss 0.30916. Accuracy 0.917.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159561
Train loss (w/o reg) on all data: 0.119472
Test loss (w/o reg) on all data: 0.329559
Train acc on all data:  0.966400773507
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.27041e-05
Norm of the params: 28.3159
Flipped loss: 0.32956. Accuracy: 0.905
### Flips: 412, rs: 11, checks: 206
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0942247
Train loss (w/o reg) on all data: 0.06786
Test loss (w/o reg) on all data: 0.193475
Train acc on all data:  0.984046410442
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 9.98701e-06
Norm of the params: 22.9629
     Influence (LOO): fixed 167 labels. Loss 0.19348. Accuracy 0.944.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0797166
Train loss (w/o reg) on all data: 0.0431459
Test loss (w/o reg) on all data: 0.319892
Train acc on all data:  0.998307952623
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 1.79841e-05
Norm of the params: 27.0447
                Loss: fixed 146 labels. Loss 0.31989. Accuracy 0.922.
Using normal model
LBFGS training took [484] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153594
Train loss (w/o reg) on all data: 0.114518
Test loss (w/o reg) on all data: 0.309866
Train acc on all data:  0.968334541939
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 3.45289e-05
Norm of the params: 27.9558
              Random: fixed  22 labels. Loss 0.30987. Accuracy 0.912.
### Flips: 412, rs: 11, checks: 412
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0803366
Train loss (w/o reg) on all data: 0.0587235
Test loss (w/o reg) on all data: 0.176533
Train acc on all data:  0.985496736766
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 4.99022e-06
Norm of the params: 20.7909
     Influence (LOO): fixed 258 labels. Loss 0.17653. Accuracy 0.962.
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0614848
Train loss (w/o reg) on all data: 0.0311036
Test loss (w/o reg) on all data: 0.255231
Train acc on all data:  0.99879139473
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 9.45619e-06
Norm of the params: 24.65
                Loss: fixed 206 labels. Loss 0.25523. Accuracy 0.929.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150606
Train loss (w/o reg) on all data: 0.112192
Test loss (w/o reg) on all data: 0.301617
Train acc on all data:  0.969543147208
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 7.58611e-05
Norm of the params: 27.7179
              Random: fixed  39 labels. Loss 0.30162. Accuracy 0.914.
### Flips: 412, rs: 11, checks: 618
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0725176
Train loss (w/o reg) on all data: 0.053672
Test loss (w/o reg) on all data: 0.113678
Train acc on all data:  0.986221899927
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 6.32471e-06
Norm of the params: 19.4142
     Influence (LOO): fixed 300 labels. Loss 0.11368. Accuracy 0.972.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0526125
Train loss (w/o reg) on all data: 0.0258512
Test loss (w/o reg) on all data: 0.242138
Train acc on all data:  0.999274836838
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 2.41862e-05
Norm of the params: 23.1349
                Loss: fixed 249 labels. Loss 0.24214. Accuracy 0.943.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145237
Train loss (w/o reg) on all data: 0.10747
Test loss (w/o reg) on all data: 0.300327
Train acc on all data:  0.971235194585
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 4.81381e-05
Norm of the params: 27.4836
              Random: fixed  64 labels. Loss 0.30033. Accuracy 0.921.
### Flips: 412, rs: 11, checks: 824
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067373
Train loss (w/o reg) on all data: 0.0503189
Test loss (w/o reg) on all data: 0.152014
Train acc on all data:  0.986947063089
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.02223e-06
Norm of the params: 18.4684
     Influence (LOO): fixed 328 labels. Loss 0.15201. Accuracy 0.972.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0485577
Train loss (w/o reg) on all data: 0.0236977
Test loss (w/o reg) on all data: 0.235928
Train acc on all data:  0.999274836838
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 2.77958e-06
Norm of the params: 22.2979
                Loss: fixed 273 labels. Loss 0.23593. Accuracy 0.955.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139626
Train loss (w/o reg) on all data: 0.102689
Test loss (w/o reg) on all data: 0.296082
Train acc on all data:  0.973652405124
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 6.2548e-05
Norm of the params: 27.1797
              Random: fixed  82 labels. Loss 0.29608. Accuracy 0.922.
### Flips: 412, rs: 11, checks: 1030
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0631527
Train loss (w/o reg) on all data: 0.0464451
Test loss (w/o reg) on all data: 0.120094
Train acc on all data:  0.987672226251
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.29467e-06
Norm of the params: 18.2798
     Influence (LOO): fixed 345 labels. Loss 0.12009. Accuracy 0.973.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0435117
Train loss (w/o reg) on all data: 0.0208843
Test loss (w/o reg) on all data: 0.291167
Train acc on all data:  0.999274836838
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 3.14252e-06
Norm of the params: 21.2732
                Loss: fixed 302 labels. Loss 0.29117. Accuracy 0.960.
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129839
Train loss (w/o reg) on all data: 0.0936211
Test loss (w/o reg) on all data: 0.295716
Train acc on all data:  0.976553057771
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 2.27834e-05
Norm of the params: 26.914
              Random: fixed 114 labels. Loss 0.29572. Accuracy 0.918.
### Flips: 412, rs: 11, checks: 1236
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0613333
Train loss (w/o reg) on all data: 0.0448838
Test loss (w/o reg) on all data: 0.122417
Train acc on all data:  0.98888083152
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.48215e-06
Norm of the params: 18.1381
     Influence (LOO): fixed 350 labels. Loss 0.12242. Accuracy 0.972.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0404423
Train loss (w/o reg) on all data: 0.0193852
Test loss (w/o reg) on all data: 0.278822
Train acc on all data:  0.999274836838
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 2.76399e-06
Norm of the params: 20.5218
                Loss: fixed 322 labels. Loss 0.27882. Accuracy 0.953.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125276
Train loss (w/o reg) on all data: 0.0898262
Test loss (w/o reg) on all data: 0.276082
Train acc on all data:  0.978728547256
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 2.88689e-05
Norm of the params: 26.627
              Random: fixed 132 labels. Loss 0.27608. Accuracy 0.926.
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158253
Train loss (w/o reg) on all data: 0.117879
Test loss (w/o reg) on all data: 0.409101
Train acc on all data:  0.965433889292
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 5.06463e-05
Norm of the params: 28.416
Flipped loss: 0.40910. Accuracy: 0.896
### Flips: 412, rs: 12, checks: 206
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0944188
Train loss (w/o reg) on all data: 0.0691621
Test loss (w/o reg) on all data: 0.221108
Train acc on all data:  0.981387478849
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 1.75057e-05
Norm of the params: 22.4752
     Influence (LOO): fixed 174 labels. Loss 0.22111. Accuracy 0.948.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0781321
Train loss (w/o reg) on all data: 0.0421582
Test loss (w/o reg) on all data: 0.304092
Train acc on all data:  0.998066231569
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.95651e-05
Norm of the params: 26.8231
                Loss: fixed 153 labels. Loss 0.30409. Accuracy 0.920.
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154162
Train loss (w/o reg) on all data: 0.114792
Test loss (w/o reg) on all data: 0.399433
Train acc on all data:  0.966642494561
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.0341e-05
Norm of the params: 28.0606
              Random: fixed  23 labels. Loss 0.39943. Accuracy 0.897.
### Flips: 412, rs: 12, checks: 412
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0822987
Train loss (w/o reg) on all data: 0.0608526
Test loss (w/o reg) on all data: 0.176189
Train acc on all data:  0.983804689388
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 6.7128e-06
Norm of the params: 20.7104
     Influence (LOO): fixed 248 labels. Loss 0.17619. Accuracy 0.957.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0569495
Train loss (w/o reg) on all data: 0.0283823
Test loss (w/o reg) on all data: 0.212677
Train acc on all data:  0.999033115784
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 3.30197e-05
Norm of the params: 23.9028
                Loss: fixed 221 labels. Loss 0.21268. Accuracy 0.941.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148503
Train loss (w/o reg) on all data: 0.1101
Test loss (w/o reg) on all data: 0.381853
Train acc on all data:  0.968334541939
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.05666e-05
Norm of the params: 27.7138
              Random: fixed  43 labels. Loss 0.38185. Accuracy 0.899.
### Flips: 412, rs: 12, checks: 618
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0742193
Train loss (w/o reg) on all data: 0.0553619
Test loss (w/o reg) on all data: 0.157192
Train acc on all data:  0.985255015712
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 5.57688e-06
Norm of the params: 19.4203
     Influence (LOO): fixed 292 labels. Loss 0.15719. Accuracy 0.967.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0489035
Train loss (w/o reg) on all data: 0.0237541
Test loss (w/o reg) on all data: 0.190501
Train acc on all data:  0.999516557892
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 4.9405e-06
Norm of the params: 22.4274
                Loss: fixed 261 labels. Loss 0.19050. Accuracy 0.949.
Using normal model
LBFGS training took [404] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142317
Train loss (w/o reg) on all data: 0.104989
Test loss (w/o reg) on all data: 0.348628
Train acc on all data:  0.970751752478
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 1.60827e-05
Norm of the params: 27.3235
              Random: fixed  66 labels. Loss 0.34863. Accuracy 0.901.
### Flips: 412, rs: 12, checks: 824
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0696693
Train loss (w/o reg) on all data: 0.052394
Test loss (w/o reg) on all data: 0.12921
Train acc on all data:  0.985255015712
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 3.1105e-06
Norm of the params: 18.5878
     Influence (LOO): fixed 318 labels. Loss 0.12921. Accuracy 0.975.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0445244
Train loss (w/o reg) on all data: 0.02141
Test loss (w/o reg) on all data: 0.166265
Train acc on all data:  0.999516557892
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 2.60071e-06
Norm of the params: 21.5009
                Loss: fixed 285 labels. Loss 0.16626. Accuracy 0.957.
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137591
Train loss (w/o reg) on all data: 0.100815
Test loss (w/o reg) on all data: 0.33859
Train acc on all data:  0.971718636693
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.1609e-05
Norm of the params: 27.1203
              Random: fixed  84 labels. Loss 0.33859. Accuracy 0.906.
### Flips: 412, rs: 12, checks: 1030
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0658412
Train loss (w/o reg) on all data: 0.0498358
Test loss (w/o reg) on all data: 0.101913
Train acc on all data:  0.986463620981
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.35751e-06
Norm of the params: 17.8915
     Influence (LOO): fixed 331 labels. Loss 0.10191. Accuracy 0.974.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0422936
Train loss (w/o reg) on all data: 0.0202458
Test loss (w/o reg) on all data: 0.159051
Train acc on all data:  0.999516557892
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 8.6847e-06
Norm of the params: 20.9989
                Loss: fixed 300 labels. Loss 0.15905. Accuracy 0.958.
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131504
Train loss (w/o reg) on all data: 0.0958058
Test loss (w/o reg) on all data: 0.320375
Train acc on all data:  0.973894126178
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.90752e-05
Norm of the params: 26.7203
              Random: fixed 110 labels. Loss 0.32038. Accuracy 0.905.
### Flips: 412, rs: 12, checks: 1236
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0602496
Train loss (w/o reg) on all data: 0.0446357
Test loss (w/o reg) on all data: 0.0884986
Train acc on all data:  0.988397389413
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 5.7999e-06
Norm of the params: 17.6714
     Influence (LOO): fixed 347 labels. Loss 0.08850. Accuracy 0.979.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0395247
Train loss (w/o reg) on all data: 0.0188379
Test loss (w/o reg) on all data: 0.13362
Train acc on all data:  0.999516557892
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.49497e-05
Norm of the params: 20.3405
                Loss: fixed 318 labels. Loss 0.13362. Accuracy 0.965.
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124288
Train loss (w/o reg) on all data: 0.0898609
Test loss (w/o reg) on all data: 0.289948
Train acc on all data:  0.976553057771
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.07241e-05
Norm of the params: 26.2401
              Random: fixed 130 labels. Loss 0.28995. Accuracy 0.910.
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162325
Train loss (w/o reg) on all data: 0.12162
Test loss (w/o reg) on all data: 0.651408
Train acc on all data:  0.963741841914
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 1.94732e-05
Norm of the params: 28.5325
Flipped loss: 0.65141. Accuracy: 0.880
### Flips: 412, rs: 13, checks: 206
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0939898
Train loss (w/o reg) on all data: 0.0667398
Test loss (w/o reg) on all data: 0.295419
Train acc on all data:  0.984288131496
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 1.41305e-05
Norm of the params: 23.3452
     Influence (LOO): fixed 173 labels. Loss 0.29542. Accuracy 0.944.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082004
Train loss (w/o reg) on all data: 0.0446149
Test loss (w/o reg) on all data: 0.444115
Train acc on all data:  0.997341068407
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 1.783e-05
Norm of the params: 27.3456
                Loss: fixed 145 labels. Loss 0.44411. Accuracy 0.915.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156766
Train loss (w/o reg) on all data: 0.11689
Test loss (w/o reg) on all data: 0.616416
Train acc on all data:  0.966159052453
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 1.8729e-05
Norm of the params: 28.2402
              Random: fixed  19 labels. Loss 0.61642. Accuracy 0.888.
### Flips: 412, rs: 13, checks: 412
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0768806
Train loss (w/o reg) on all data: 0.0552597
Test loss (w/o reg) on all data: 0.242784
Train acc on all data:  0.987430505197
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.06543e-06
Norm of the params: 20.7946
     Influence (LOO): fixed 257 labels. Loss 0.24278. Accuracy 0.963.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06098
Train loss (w/o reg) on all data: 0.0305823
Test loss (w/o reg) on all data: 0.418917
Train acc on all data:  0.999274836838
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.67901e-05
Norm of the params: 24.6567
                Loss: fixed 217 labels. Loss 0.41892. Accuracy 0.933.
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151409
Train loss (w/o reg) on all data: 0.112298
Test loss (w/o reg) on all data: 0.619535
Train acc on all data:  0.968092820885
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.34401e-05
Norm of the params: 27.9683
              Random: fixed  39 labels. Loss 0.61953. Accuracy 0.894.
### Flips: 412, rs: 13, checks: 618
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070837
Train loss (w/o reg) on all data: 0.0516796
Test loss (w/o reg) on all data: 0.25635
Train acc on all data:  0.987913947305
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 6.6715e-06
Norm of the params: 19.5742
     Influence (LOO): fixed 304 labels. Loss 0.25635. Accuracy 0.965.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0527459
Train loss (w/o reg) on all data: 0.0258676
Test loss (w/o reg) on all data: 0.388781
Train acc on all data:  0.999274836838
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 9.61223e-06
Norm of the params: 23.1854
                Loss: fixed 251 labels. Loss 0.38878. Accuracy 0.938.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144336
Train loss (w/o reg) on all data: 0.106379
Test loss (w/o reg) on all data: 0.585037
Train acc on all data:  0.970993473532
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 2.22405e-05
Norm of the params: 27.5522
              Random: fixed  62 labels. Loss 0.58504. Accuracy 0.894.
### Flips: 412, rs: 13, checks: 824
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0677683
Train loss (w/o reg) on all data: 0.0495982
Test loss (w/o reg) on all data: 0.218297
Train acc on all data:  0.988397389413
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 1.51488e-06
Norm of the params: 19.0631
     Influence (LOO): fixed 323 labels. Loss 0.21830. Accuracy 0.964.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0490882
Train loss (w/o reg) on all data: 0.0240442
Test loss (w/o reg) on all data: 0.300207
Train acc on all data:  0.99879139473
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 5.00138e-06
Norm of the params: 22.3804
                Loss: fixed 279 labels. Loss 0.30021. Accuracy 0.954.
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139039
Train loss (w/o reg) on all data: 0.102213
Test loss (w/o reg) on all data: 0.554746
Train acc on all data:  0.972202078801
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.66207e-05
Norm of the params: 27.1387
              Random: fixed  88 labels. Loss 0.55475. Accuracy 0.907.
### Flips: 412, rs: 13, checks: 1030
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0650849
Train loss (w/o reg) on all data: 0.0480048
Test loss (w/o reg) on all data: 0.136855
Train acc on all data:  0.988155668359
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 2.28216e-06
Norm of the params: 18.4825
     Influence (LOO): fixed 337 labels. Loss 0.13686. Accuracy 0.966.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0457152
Train loss (w/o reg) on all data: 0.0223427
Test loss (w/o reg) on all data: 0.198059
Train acc on all data:  0.99879139473
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 4.03754e-06
Norm of the params: 21.6206
                Loss: fixed 297 labels. Loss 0.19806. Accuracy 0.959.
Using normal model
LBFGS training took [365] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132303
Train loss (w/o reg) on all data: 0.0967995
Test loss (w/o reg) on all data: 0.498295
Train acc on all data:  0.974135847232
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 2.13999e-05
Norm of the params: 26.6473
              Random: fixed 110 labels. Loss 0.49830. Accuracy 0.910.
### Flips: 412, rs: 13, checks: 1236
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0621922
Train loss (w/o reg) on all data: 0.0459878
Test loss (w/o reg) on all data: 0.146973
Train acc on all data:  0.98888083152
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 1.05027e-06
Norm of the params: 18.0024
     Influence (LOO): fixed 348 labels. Loss 0.14697. Accuracy 0.970.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0416593
Train loss (w/o reg) on all data: 0.0201736
Test loss (w/o reg) on all data: 0.198149
Train acc on all data:  0.999274836838
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 3.16017e-06
Norm of the params: 20.7295
                Loss: fixed 321 labels. Loss 0.19815. Accuracy 0.966.
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125197
Train loss (w/o reg) on all data: 0.0909594
Test loss (w/o reg) on all data: 0.470412
Train acc on all data:  0.975586173556
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 1.19857e-05
Norm of the params: 26.1678
              Random: fixed 133 labels. Loss 0.47041. Accuracy 0.909.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1583
Train loss (w/o reg) on all data: 0.115563
Test loss (w/o reg) on all data: 0.500079
Train acc on all data:  0.9690597051
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 1.18243e-05
Norm of the params: 29.236
Flipped loss: 0.50008. Accuracy: 0.873
### Flips: 412, rs: 14, checks: 206
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0890523
Train loss (w/o reg) on all data: 0.0620026
Test loss (w/o reg) on all data: 0.247818
Train acc on all data:  0.986705342035
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 4.43894e-06
Norm of the params: 23.2593
     Influence (LOO): fixed 180 labels. Loss 0.24782. Accuracy 0.932.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0780285
Train loss (w/o reg) on all data: 0.0416831
Test loss (w/o reg) on all data: 0.393628
Train acc on all data:  0.997341068407
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 1.49588e-05
Norm of the params: 26.9612
                Loss: fixed 146 labels. Loss 0.39363. Accuracy 0.904.
Using normal model
LBFGS training took [404] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153633
Train loss (w/o reg) on all data: 0.111846
Test loss (w/o reg) on all data: 0.509017
Train acc on all data:  0.970510031424
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 9.12422e-06
Norm of the params: 28.9094
              Random: fixed  22 labels. Loss 0.50902. Accuracy 0.874.
### Flips: 412, rs: 14, checks: 412
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0753798
Train loss (w/o reg) on all data: 0.0536452
Test loss (w/o reg) on all data: 0.354745
Train acc on all data:  0.987430505197
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 4.69215e-06
Norm of the params: 20.8493
     Influence (LOO): fixed 264 labels. Loss 0.35475. Accuracy 0.948.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0557735
Train loss (w/o reg) on all data: 0.0277191
Test loss (w/o reg) on all data: 0.314778
Train acc on all data:  0.998549673677
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 1.05495e-05
Norm of the params: 23.6873
                Loss: fixed 218 labels. Loss 0.31478. Accuracy 0.922.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148408
Train loss (w/o reg) on all data: 0.107416
Test loss (w/o reg) on all data: 0.530377
Train acc on all data:  0.973894126178
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 2.36719e-05
Norm of the params: 28.633
              Random: fixed  43 labels. Loss 0.53038. Accuracy 0.886.
### Flips: 412, rs: 14, checks: 618
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0678065
Train loss (w/o reg) on all data: 0.0490054
Test loss (w/o reg) on all data: 0.229235
Train acc on all data:  0.987913947305
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 2.64651e-06
Norm of the params: 19.3912
     Influence (LOO): fixed 306 labels. Loss 0.22924. Accuracy 0.957.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0478649
Train loss (w/o reg) on all data: 0.0234202
Test loss (w/o reg) on all data: 0.238383
Train acc on all data:  0.998549673677
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 4.49204e-06
Norm of the params: 22.1109
                Loss: fixed 259 labels. Loss 0.23838. Accuracy 0.934.
Using normal model
LBFGS training took [376] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142858
Train loss (w/o reg) on all data: 0.10272
Test loss (w/o reg) on all data: 0.495709
Train acc on all data:  0.975102731448
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 4.40752e-05
Norm of the params: 28.3331
              Random: fixed  67 labels. Loss 0.49571. Accuracy 0.899.
### Flips: 412, rs: 14, checks: 824
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0626311
Train loss (w/o reg) on all data: 0.0451359
Test loss (w/o reg) on all data: 0.257761
Train acc on all data:  0.989364273628
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.88538e-06
Norm of the params: 18.7057
     Influence (LOO): fixed 333 labels. Loss 0.25776. Accuracy 0.971.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0423682
Train loss (w/o reg) on all data: 0.0205032
Test loss (w/o reg) on all data: 0.265186
Train acc on all data:  0.999033115784
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 9.32033e-06
Norm of the params: 20.9117
                Loss: fixed 288 labels. Loss 0.26519. Accuracy 0.940.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138158
Train loss (w/o reg) on all data: 0.0990224
Test loss (w/o reg) on all data: 0.487613
Train acc on all data:  0.976553057771
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 2.5893e-05
Norm of the params: 27.9771
              Random: fixed  85 labels. Loss 0.48761. Accuracy 0.900.
### Flips: 412, rs: 14, checks: 1030
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0588884
Train loss (w/o reg) on all data: 0.0423667
Test loss (w/o reg) on all data: 0.184279
Train acc on all data:  0.989847715736
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 8.44649e-07
Norm of the params: 18.1779
     Influence (LOO): fixed 348 labels. Loss 0.18428. Accuracy 0.972.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0399412
Train loss (w/o reg) on all data: 0.0191704
Test loss (w/o reg) on all data: 0.238864
Train acc on all data:  0.999033115784
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 4.49982e-06
Norm of the params: 20.3818
                Loss: fixed 306 labels. Loss 0.23886. Accuracy 0.948.
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134238
Train loss (w/o reg) on all data: 0.0962197
Test loss (w/o reg) on all data: 0.486245
Train acc on all data:  0.976311336717
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 8.96511e-06
Norm of the params: 27.5747
              Random: fixed 105 labels. Loss 0.48624. Accuracy 0.908.
### Flips: 412, rs: 14, checks: 1236
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0560309
Train loss (w/o reg) on all data: 0.0402476
Test loss (w/o reg) on all data: 0.141801
Train acc on all data:  0.99008943679
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 5.84245e-07
Norm of the params: 17.767
     Influence (LOO): fixed 360 labels. Loss 0.14180. Accuracy 0.972.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0383667
Train loss (w/o reg) on all data: 0.0184478
Test loss (w/o reg) on all data: 0.211532
Train acc on all data:  0.999033115784
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 1.61257e-05
Norm of the params: 19.9594
                Loss: fixed 321 labels. Loss 0.21153. Accuracy 0.957.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126214
Train loss (w/o reg) on all data: 0.0894752
Test loss (w/o reg) on all data: 0.504019
Train acc on all data:  0.978486826203
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 8.81507e-06
Norm of the params: 27.1068
              Random: fixed 131 labels. Loss 0.50402. Accuracy 0.915.
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16037
Train loss (w/o reg) on all data: 0.119413
Test loss (w/o reg) on all data: 0.423629
Train acc on all data:  0.9690597051
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 7.72292e-06
Norm of the params: 28.6208
Flipped loss: 0.42363. Accuracy: 0.873
### Flips: 412, rs: 15, checks: 206
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0928501
Train loss (w/o reg) on all data: 0.0657957
Test loss (w/o reg) on all data: 0.248628
Train acc on all data:  0.984046410442
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 2.84394e-06
Norm of the params: 23.2613
     Influence (LOO): fixed 171 labels. Loss 0.24863. Accuracy 0.936.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0816136
Train loss (w/o reg) on all data: 0.0442904
Test loss (w/o reg) on all data: 0.395394
Train acc on all data:  0.997099347353
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 9.32009e-06
Norm of the params: 27.3215
                Loss: fixed 147 labels. Loss 0.39539. Accuracy 0.900.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156726
Train loss (w/o reg) on all data: 0.116559
Test loss (w/o reg) on all data: 0.465941
Train acc on all data:  0.970026589316
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 1.65884e-05
Norm of the params: 28.3436
              Random: fixed  19 labels. Loss 0.46594. Accuracy 0.883.
### Flips: 412, rs: 15, checks: 412
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0756327
Train loss (w/o reg) on all data: 0.0533768
Test loss (w/o reg) on all data: 0.197689
Train acc on all data:  0.987430505197
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 2.44101e-06
Norm of the params: 21.0978
     Influence (LOO): fixed 264 labels. Loss 0.19769. Accuracy 0.948.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0619196
Train loss (w/o reg) on all data: 0.0312798
Test loss (w/o reg) on all data: 0.31158
Train acc on all data:  0.998549673677
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.41532e-05
Norm of the params: 24.7547
                Loss: fixed 213 labels. Loss 0.31158. Accuracy 0.920.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152452
Train loss (w/o reg) on all data: 0.112847
Test loss (w/o reg) on all data: 0.460496
Train acc on all data:  0.971476915639
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 3.86898e-05
Norm of the params: 28.1445
              Random: fixed  40 labels. Loss 0.46050. Accuracy 0.890.
### Flips: 412, rs: 15, checks: 618
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0661911
Train loss (w/o reg) on all data: 0.0463798
Test loss (w/o reg) on all data: 0.157753
Train acc on all data:  0.988397389413
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 1.09268e-06
Norm of the params: 19.9054
     Influence (LOO): fixed 313 labels. Loss 0.15775. Accuracy 0.964.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0521089
Train loss (w/o reg) on all data: 0.025759
Test loss (w/o reg) on all data: 0.288677
Train acc on all data:  0.998549673677
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.56379e-05
Norm of the params: 22.9564
                Loss: fixed 254 labels. Loss 0.28868. Accuracy 0.931.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146823
Train loss (w/o reg) on all data: 0.108209
Test loss (w/o reg) on all data: 0.413663
Train acc on all data:  0.973652405124
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 8.39296e-06
Norm of the params: 27.7901
              Random: fixed  59 labels. Loss 0.41366. Accuracy 0.901.
### Flips: 412, rs: 15, checks: 824
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0602333
Train loss (w/o reg) on all data: 0.042065
Test loss (w/o reg) on all data: 0.225569
Train acc on all data:  0.990814599952
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.99184e-06
Norm of the params: 19.0621
     Influence (LOO): fixed 337 labels. Loss 0.22557. Accuracy 0.963.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0451424
Train loss (w/o reg) on all data: 0.022041
Test loss (w/o reg) on all data: 0.247197
Train acc on all data:  0.999033115784
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 3.30122e-06
Norm of the params: 21.4948
                Loss: fixed 291 labels. Loss 0.24720. Accuracy 0.943.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139082
Train loss (w/o reg) on all data: 0.101052
Test loss (w/o reg) on all data: 0.421561
Train acc on all data:  0.976553057771
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 4.62709e-05
Norm of the params: 27.5787
              Random: fixed  79 labels. Loss 0.42156. Accuracy 0.899.
### Flips: 412, rs: 15, checks: 1030
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0566698
Train loss (w/o reg) on all data: 0.0388808
Test loss (w/o reg) on all data: 0.215622
Train acc on all data:  0.991781484167
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 5.77623e-06
Norm of the params: 18.8621
     Influence (LOO): fixed 352 labels. Loss 0.21562. Accuracy 0.969.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0415445
Train loss (w/o reg) on all data: 0.0201786
Test loss (w/o reg) on all data: 0.298357
Train acc on all data:  0.99879139473
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 1.35808e-05
Norm of the params: 20.6717
                Loss: fixed 313 labels. Loss 0.29836. Accuracy 0.958.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135013
Train loss (w/o reg) on all data: 0.0972074
Test loss (w/o reg) on all data: 0.415793
Train acc on all data:  0.978245105149
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.64223e-05
Norm of the params: 27.4976
              Random: fixed  97 labels. Loss 0.41579. Accuracy 0.899.
### Flips: 412, rs: 15, checks: 1236
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0552188
Train loss (w/o reg) on all data: 0.0381095
Test loss (w/o reg) on all data: 0.157982
Train acc on all data:  0.992023205221
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.26497e-06
Norm of the params: 18.4983
     Influence (LOO): fixed 359 labels. Loss 0.15798. Accuracy 0.971.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0398903
Train loss (w/o reg) on all data: 0.0194035
Test loss (w/o reg) on all data: 0.30336
Train acc on all data:  0.999274836838
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 2.44132e-06
Norm of the params: 20.2419
                Loss: fixed 325 labels. Loss 0.30336. Accuracy 0.959.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130747
Train loss (w/o reg) on all data: 0.0940079
Test loss (w/o reg) on all data: 0.280033
Train acc on all data:  0.979211989364
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 8.17205e-06
Norm of the params: 27.107
              Random: fixed 119 labels. Loss 0.28003. Accuracy 0.912.
Using normal model
LBFGS training took [375] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158431
Train loss (w/o reg) on all data: 0.117197
Test loss (w/o reg) on all data: 0.54222
Train acc on all data:  0.967125936669
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.64673e-05
Norm of the params: 28.7173
Flipped loss: 0.54222. Accuracy: 0.876
### Flips: 412, rs: 16, checks: 206
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0961848
Train loss (w/o reg) on all data: 0.0686302
Test loss (w/o reg) on all data: 0.360724
Train acc on all data:  0.982354363065
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.06289e-05
Norm of the params: 23.4753
     Influence (LOO): fixed 175 labels. Loss 0.36072. Accuracy 0.931.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0810963
Train loss (w/o reg) on all data: 0.0439368
Test loss (w/o reg) on all data: 0.383872
Train acc on all data:  0.998066231569
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 2.04922e-05
Norm of the params: 27.2615
                Loss: fixed 141 labels. Loss 0.38387. Accuracy 0.910.
Using normal model
LBFGS training took [351] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156384
Train loss (w/o reg) on all data: 0.115587
Test loss (w/o reg) on all data: 0.528574
Train acc on all data:  0.968092820885
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 1.17461e-05
Norm of the params: 28.5648
              Random: fixed  12 labels. Loss 0.52857. Accuracy 0.880.
### Flips: 412, rs: 16, checks: 412
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0807572
Train loss (w/o reg) on all data: 0.0585919
Test loss (w/o reg) on all data: 0.392595
Train acc on all data:  0.984771573604
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 2.08614e-06
Norm of the params: 21.0549
     Influence (LOO): fixed 251 labels. Loss 0.39260. Accuracy 0.945.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0631378
Train loss (w/o reg) on all data: 0.0321948
Test loss (w/o reg) on all data: 0.388015
Train acc on all data:  0.998549673677
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 3.9506e-06
Norm of the params: 24.8769
                Loss: fixed 210 labels. Loss 0.38801. Accuracy 0.922.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151267
Train loss (w/o reg) on all data: 0.111205
Test loss (w/o reg) on all data: 0.487704
Train acc on all data:  0.970026589316
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.31046e-05
Norm of the params: 28.3061
              Random: fixed  34 labels. Loss 0.48770. Accuracy 0.876.
### Flips: 412, rs: 16, checks: 618
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0727862
Train loss (w/o reg) on all data: 0.0529204
Test loss (w/o reg) on all data: 0.310701
Train acc on all data:  0.986705342035
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 6.22999e-06
Norm of the params: 19.9328
     Influence (LOO): fixed 297 labels. Loss 0.31070. Accuracy 0.957.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0539167
Train loss (w/o reg) on all data: 0.0266872
Test loss (w/o reg) on all data: 0.348837
Train acc on all data:  0.999758278946
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 2.95607e-05
Norm of the params: 23.3364
                Loss: fixed 255 labels. Loss 0.34884. Accuracy 0.943.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145532
Train loss (w/o reg) on all data: 0.10592
Test loss (w/o reg) on all data: 0.476933
Train acc on all data:  0.971476915639
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 5.72091e-06
Norm of the params: 28.1466
              Random: fixed  55 labels. Loss 0.47693. Accuracy 0.887.
### Flips: 412, rs: 16, checks: 824
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0661482
Train loss (w/o reg) on all data: 0.0479793
Test loss (w/o reg) on all data: 0.277687
Train acc on all data:  0.987672226251
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 1.91974e-06
Norm of the params: 19.0625
     Influence (LOO): fixed 320 labels. Loss 0.27769. Accuracy 0.962.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0481529
Train loss (w/o reg) on all data: 0.0235475
Test loss (w/o reg) on all data: 0.33286
Train acc on all data:  0.999274836838
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.24254e-05
Norm of the params: 22.1835
                Loss: fixed 284 labels. Loss 0.33286. Accuracy 0.935.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137922
Train loss (w/o reg) on all data: 0.0997502
Test loss (w/o reg) on all data: 0.422008
Train acc on all data:  0.974861010394
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 7.26657e-06
Norm of the params: 27.6304
              Random: fixed  82 labels. Loss 0.42201. Accuracy 0.900.
### Flips: 412, rs: 16, checks: 1030
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0624554
Train loss (w/o reg) on all data: 0.0450809
Test loss (w/o reg) on all data: 0.287003
Train acc on all data:  0.988639110467
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 3.13669e-06
Norm of the params: 18.6411
     Influence (LOO): fixed 333 labels. Loss 0.28700. Accuracy 0.965.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0436277
Train loss (w/o reg) on all data: 0.0211484
Test loss (w/o reg) on all data: 0.324983
Train acc on all data:  0.999274836838
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 3.22017e-06
Norm of the params: 21.2035
                Loss: fixed 307 labels. Loss 0.32498. Accuracy 0.949.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133761
Train loss (w/o reg) on all data: 0.0961175
Test loss (w/o reg) on all data: 0.398173
Train acc on all data:  0.976553057771
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.27467e-05
Norm of the params: 27.4384
              Random: fixed 100 labels. Loss 0.39817. Accuracy 0.913.
### Flips: 412, rs: 16, checks: 1236
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0584269
Train loss (w/o reg) on all data: 0.0419154
Test loss (w/o reg) on all data: 0.267647
Train acc on all data:  0.989847715736
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.52621e-06
Norm of the params: 18.1722
     Influence (LOO): fixed 350 labels. Loss 0.26765. Accuracy 0.967.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0407332
Train loss (w/o reg) on all data: 0.019655
Test loss (w/o reg) on all data: 0.26619
Train acc on all data:  0.999516557892
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.3571e-05
Norm of the params: 20.532
                Loss: fixed 331 labels. Loss 0.26619. Accuracy 0.952.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129732
Train loss (w/o reg) on all data: 0.0928046
Test loss (w/o reg) on all data: 0.371803
Train acc on all data:  0.977761663041
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 6.16307e-06
Norm of the params: 27.1762
              Random: fixed 120 labels. Loss 0.37180. Accuracy 0.914.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157708
Train loss (w/o reg) on all data: 0.116741
Test loss (w/o reg) on all data: 0.329534
Train acc on all data:  0.969543147208
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 2.67711e-05
Norm of the params: 28.6239
Flipped loss: 0.32953. Accuracy: 0.893
### Flips: 412, rs: 17, checks: 206
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0929377
Train loss (w/o reg) on all data: 0.0663092
Test loss (w/o reg) on all data: 0.261808
Train acc on all data:  0.983562968335
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 1.10548e-05
Norm of the params: 23.0775
     Influence (LOO): fixed 166 labels. Loss 0.26181. Accuracy 0.937.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0798346
Train loss (w/o reg) on all data: 0.0425668
Test loss (w/o reg) on all data: 0.298674
Train acc on all data:  0.998066231569
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 3.2608e-05
Norm of the params: 27.3012
                Loss: fixed 143 labels. Loss 0.29867. Accuracy 0.908.
Using normal model
LBFGS training took [448] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152304
Train loss (w/o reg) on all data: 0.111992
Test loss (w/o reg) on all data: 0.321316
Train acc on all data:  0.970993473532
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 2.33199e-05
Norm of the params: 28.3946
              Random: fixed  20 labels. Loss 0.32132. Accuracy 0.897.
### Flips: 412, rs: 17, checks: 412
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076791
Train loss (w/o reg) on all data: 0.0546073
Test loss (w/o reg) on all data: 0.23472
Train acc on all data:  0.986705342035
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.64018e-05
Norm of the params: 21.0636
     Influence (LOO): fixed 246 labels. Loss 0.23472. Accuracy 0.952.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0601723
Train loss (w/o reg) on all data: 0.029898
Test loss (w/o reg) on all data: 0.336466
Train acc on all data:  0.999274836838
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 2.64377e-05
Norm of the params: 24.6066
                Loss: fixed 208 labels. Loss 0.33647. Accuracy 0.930.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148025
Train loss (w/o reg) on all data: 0.107971
Test loss (w/o reg) on all data: 0.309973
Train acc on all data:  0.973410684071
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.18508e-05
Norm of the params: 28.3035
              Random: fixed  43 labels. Loss 0.30997. Accuracy 0.906.
### Flips: 412, rs: 17, checks: 618
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0716388
Train loss (w/o reg) on all data: 0.0515198
Test loss (w/o reg) on all data: 0.219926
Train acc on all data:  0.986947063089
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 1.61589e-05
Norm of the params: 20.0594
     Influence (LOO): fixed 287 labels. Loss 0.21993. Accuracy 0.964.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0512495
Train loss (w/o reg) on all data: 0.0248473
Test loss (w/o reg) on all data: 0.30756
Train acc on all data:  0.999274836838
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 3.80791e-06
Norm of the params: 22.9792
                Loss: fixed 243 labels. Loss 0.30756. Accuracy 0.932.
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144629
Train loss (w/o reg) on all data: 0.105469
Test loss (w/o reg) on all data: 0.311729
Train acc on all data:  0.97461928934
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.91989e-05
Norm of the params: 27.9859
              Random: fixed  61 labels. Loss 0.31173. Accuracy 0.903.
### Flips: 412, rs: 17, checks: 824
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066754
Train loss (w/o reg) on all data: 0.0486823
Test loss (w/o reg) on all data: 0.218642
Train acc on all data:  0.987672226251
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 9.4354e-07
Norm of the params: 19.0114
     Influence (LOO): fixed 315 labels. Loss 0.21864. Accuracy 0.975.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0463512
Train loss (w/o reg) on all data: 0.02213
Test loss (w/o reg) on all data: 0.327063
Train acc on all data:  0.999274836838
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 5.27533e-06
Norm of the params: 22.0097
                Loss: fixed 266 labels. Loss 0.32706. Accuracy 0.937.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136698
Train loss (w/o reg) on all data: 0.0985011
Test loss (w/o reg) on all data: 0.318898
Train acc on all data:  0.977519941987
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 1.76075e-05
Norm of the params: 27.6393
              Random: fixed  84 labels. Loss 0.31890. Accuracy 0.914.
### Flips: 412, rs: 17, checks: 1030
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0623384
Train loss (w/o reg) on all data: 0.0451209
Test loss (w/o reg) on all data: 0.179037
Train acc on all data:  0.988397389413
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 9.92617e-07
Norm of the params: 18.5567
     Influence (LOO): fixed 333 labels. Loss 0.17904. Accuracy 0.978.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0414926
Train loss (w/o reg) on all data: 0.0197871
Test loss (w/o reg) on all data: 0.277553
Train acc on all data:  0.999274836838
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 3.10982e-06
Norm of the params: 20.8353
                Loss: fixed 294 labels. Loss 0.27755. Accuracy 0.952.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13031
Train loss (w/o reg) on all data: 0.0935396
Test loss (w/o reg) on all data: 0.354491
Train acc on all data:  0.977278220933
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 9.54022e-06
Norm of the params: 27.1186
              Random: fixed 107 labels. Loss 0.35449. Accuracy 0.914.
### Flips: 412, rs: 17, checks: 1236
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0601514
Train loss (w/o reg) on all data: 0.0436148
Test loss (w/o reg) on all data: 0.145643
Train acc on all data:  0.98888083152
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 8.10021e-06
Norm of the params: 18.1861
     Influence (LOO): fixed 346 labels. Loss 0.14564. Accuracy 0.979.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0401163
Train loss (w/o reg) on all data: 0.019112
Test loss (w/o reg) on all data: 0.251253
Train acc on all data:  0.999274836838
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 5.75328e-06
Norm of the params: 20.496
                Loss: fixed 307 labels. Loss 0.25125. Accuracy 0.957.
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127115
Train loss (w/o reg) on all data: 0.0913171
Test loss (w/o reg) on all data: 0.315436
Train acc on all data:  0.978245105149
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 3.62769e-05
Norm of the params: 26.7575
              Random: fixed 124 labels. Loss 0.31544. Accuracy 0.920.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162775
Train loss (w/o reg) on all data: 0.121419
Test loss (w/o reg) on all data: 0.535871
Train acc on all data:  0.9659173314
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.41463e-05
Norm of the params: 28.7598
Flipped loss: 0.53587. Accuracy: 0.889
### Flips: 412, rs: 18, checks: 206
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0960113
Train loss (w/o reg) on all data: 0.0687741
Test loss (w/o reg) on all data: 0.350691
Train acc on all data:  0.983079526227
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 3.16842e-06
Norm of the params: 23.3398
     Influence (LOO): fixed 168 labels. Loss 0.35069. Accuracy 0.930.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0786591
Train loss (w/o reg) on all data: 0.0423841
Test loss (w/o reg) on all data: 0.467854
Train acc on all data:  0.997582789461
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 5.02398e-05
Norm of the params: 26.9351
                Loss: fixed 153 labels. Loss 0.46785. Accuracy 0.912.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159481
Train loss (w/o reg) on all data: 0.118681
Test loss (w/o reg) on all data: 0.544182
Train acc on all data:  0.967125936669
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 1.3916e-05
Norm of the params: 28.5657
              Random: fixed  18 labels. Loss 0.54418. Accuracy 0.896.
### Flips: 412, rs: 18, checks: 412
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0826657
Train loss (w/o reg) on all data: 0.0600604
Test loss (w/o reg) on all data: 0.219156
Train acc on all data:  0.984771573604
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 8.38982e-06
Norm of the params: 21.2628
     Influence (LOO): fixed 242 labels. Loss 0.21916. Accuracy 0.950.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0600033
Train loss (w/o reg) on all data: 0.0301819
Test loss (w/o reg) on all data: 0.518149
Train acc on all data:  0.99879139473
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 2.91706e-06
Norm of the params: 24.4218
                Loss: fixed 223 labels. Loss 0.51815. Accuracy 0.926.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153504
Train loss (w/o reg) on all data: 0.113501
Test loss (w/o reg) on all data: 0.489896
Train acc on all data:  0.968334541939
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 2.74022e-05
Norm of the params: 28.2851
              Random: fixed  35 labels. Loss 0.48990. Accuracy 0.900.
### Flips: 412, rs: 18, checks: 618
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0739138
Train loss (w/o reg) on all data: 0.0542838
Test loss (w/o reg) on all data: 0.167277
Train acc on all data:  0.986221899927
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 2.61278e-06
Norm of the params: 19.8142
     Influence (LOO): fixed 292 labels. Loss 0.16728. Accuracy 0.963.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0501463
Train loss (w/o reg) on all data: 0.0245797
Test loss (w/o reg) on all data: 0.543573
Train acc on all data:  0.999033115784
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 4.98115e-06
Norm of the params: 22.6127
                Loss: fixed 260 labels. Loss 0.54357. Accuracy 0.940.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149267
Train loss (w/o reg) on all data: 0.110312
Test loss (w/o reg) on all data: 0.448663
Train acc on all data:  0.969784868262
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 8.46506e-06
Norm of the params: 27.9124
              Random: fixed  61 labels. Loss 0.44866. Accuracy 0.899.
### Flips: 412, rs: 18, checks: 824
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0695476
Train loss (w/o reg) on all data: 0.051668
Test loss (w/o reg) on all data: 0.153133
Train acc on all data:  0.986221899927
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 3.20757e-06
Norm of the params: 18.9101
     Influence (LOO): fixed 324 labels. Loss 0.15313. Accuracy 0.969.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0450949
Train loss (w/o reg) on all data: 0.021838
Test loss (w/o reg) on all data: 0.421917
Train acc on all data:  0.999516557892
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 4.19084e-06
Norm of the params: 21.5671
                Loss: fixed 286 labels. Loss 0.42192. Accuracy 0.941.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145811
Train loss (w/o reg) on all data: 0.107414
Test loss (w/o reg) on all data: 0.418359
Train acc on all data:  0.971235194585
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 1.63717e-05
Norm of the params: 27.7118
              Random: fixed  76 labels. Loss 0.41836. Accuracy 0.900.
### Flips: 412, rs: 18, checks: 1030
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0642208
Train loss (w/o reg) on all data: 0.0475273
Test loss (w/o reg) on all data: 0.130231
Train acc on all data:  0.987672226251
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 8.27368e-07
Norm of the params: 18.2721
     Influence (LOO): fixed 342 labels. Loss 0.13023. Accuracy 0.969.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043167
Train loss (w/o reg) on all data: 0.0209436
Test loss (w/o reg) on all data: 0.427015
Train acc on all data:  0.999516557892
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 5.06589e-06
Norm of the params: 21.0824
                Loss: fixed 310 labels. Loss 0.42701. Accuracy 0.945.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139269
Train loss (w/o reg) on all data: 0.101933
Test loss (w/o reg) on all data: 0.421403
Train acc on all data:  0.973410684071
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 6.70104e-06
Norm of the params: 27.3264
              Random: fixed  97 labels. Loss 0.42140. Accuracy 0.905.
### Flips: 412, rs: 18, checks: 1236
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061359
Train loss (w/o reg) on all data: 0.0452862
Test loss (w/o reg) on all data: 0.138269
Train acc on all data:  0.98888083152
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 3.24567e-06
Norm of the params: 17.9292
     Influence (LOO): fixed 353 labels. Loss 0.13827. Accuracy 0.966.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038712
Train loss (w/o reg) on all data: 0.0185998
Test loss (w/o reg) on all data: 0.380001
Train acc on all data:  0.999516557892
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 7.27816e-06
Norm of the params: 20.056
                Loss: fixed 334 labels. Loss 0.38000. Accuracy 0.958.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134125
Train loss (w/o reg) on all data: 0.0975322
Test loss (w/o reg) on all data: 0.400816
Train acc on all data:  0.974861010394
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 2.20559e-05
Norm of the params: 27.053
              Random: fixed 118 labels. Loss 0.40082. Accuracy 0.922.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160965
Train loss (w/o reg) on all data: 0.120109
Test loss (w/o reg) on all data: 0.391901
Train acc on all data:  0.969301426154
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 7.26403e-06
Norm of the params: 28.5853
Flipped loss: 0.39190. Accuracy: 0.900
### Flips: 412, rs: 19, checks: 206
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096972
Train loss (w/o reg) on all data: 0.070086
Test loss (w/o reg) on all data: 0.322316
Train acc on all data:  0.984046410442
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 5.09281e-06
Norm of the params: 23.1888
     Influence (LOO): fixed 166 labels. Loss 0.32232. Accuracy 0.937.
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0789498
Train loss (w/o reg) on all data: 0.0420954
Test loss (w/o reg) on all data: 0.365492
Train acc on all data:  0.99879139473
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 8.11759e-06
Norm of the params: 27.1493
                Loss: fixed 145 labels. Loss 0.36549. Accuracy 0.910.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155717
Train loss (w/o reg) on all data: 0.115464
Test loss (w/o reg) on all data: 0.371131
Train acc on all data:  0.971235194585
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 3.89975e-05
Norm of the params: 28.3735
              Random: fixed  19 labels. Loss 0.37113. Accuracy 0.908.
### Flips: 412, rs: 19, checks: 412
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0824265
Train loss (w/o reg) on all data: 0.0611802
Test loss (w/o reg) on all data: 0.18719
Train acc on all data:  0.984771573604
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 2.72887e-06
Norm of the params: 20.6137
     Influence (LOO): fixed 253 labels. Loss 0.18719. Accuracy 0.959.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0605251
Train loss (w/o reg) on all data: 0.0301321
Test loss (w/o reg) on all data: 0.313786
Train acc on all data:  0.999274836838
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 2.69633e-05
Norm of the params: 24.6548
                Loss: fixed 213 labels. Loss 0.31379. Accuracy 0.930.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15338
Train loss (w/o reg) on all data: 0.113557
Test loss (w/o reg) on all data: 0.351859
Train acc on all data:  0.972443799855
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 3.96614e-05
Norm of the params: 28.2214
              Random: fixed  36 labels. Loss 0.35186. Accuracy 0.911.
### Flips: 412, rs: 19, checks: 618
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0734347
Train loss (w/o reg) on all data: 0.0544785
Test loss (w/o reg) on all data: 0.166435
Train acc on all data:  0.986705342035
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 1.05921e-06
Norm of the params: 19.4711
     Influence (LOO): fixed 294 labels. Loss 0.16644. Accuracy 0.964.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0534672
Train loss (w/o reg) on all data: 0.0263424
Test loss (w/o reg) on all data: 0.253956
Train acc on all data:  0.999274836838
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 4.71727e-06
Norm of the params: 23.2915
                Loss: fixed 246 labels. Loss 0.25396. Accuracy 0.941.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146857
Train loss (w/o reg) on all data: 0.107925
Test loss (w/o reg) on all data: 0.340786
Train acc on all data:  0.974377568286
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 1.50576e-05
Norm of the params: 27.9042
              Random: fixed  57 labels. Loss 0.34079. Accuracy 0.909.
### Flips: 412, rs: 19, checks: 824
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0671913
Train loss (w/o reg) on all data: 0.0501226
Test loss (w/o reg) on all data: 0.165822
Train acc on all data:  0.987188784143
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.18114e-06
Norm of the params: 18.4763
     Influence (LOO): fixed 321 labels. Loss 0.16582. Accuracy 0.971.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0472246
Train loss (w/o reg) on all data: 0.0228176
Test loss (w/o reg) on all data: 0.222769
Train acc on all data:  0.999516557892
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 9.59078e-06
Norm of the params: 22.0939
                Loss: fixed 278 labels. Loss 0.22277. Accuracy 0.952.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141432
Train loss (w/o reg) on all data: 0.103377
Test loss (w/o reg) on all data: 0.320742
Train acc on all data:  0.974377568286
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 8.75468e-06
Norm of the params: 27.5883
              Random: fixed  78 labels. Loss 0.32074. Accuracy 0.905.
### Flips: 412, rs: 19, checks: 1030
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0628734
Train loss (w/o reg) on all data: 0.0464851
Test loss (w/o reg) on all data: 0.137994
Train acc on all data:  0.988397389413
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 8.54896e-07
Norm of the params: 18.1043
     Influence (LOO): fixed 338 labels. Loss 0.13799. Accuracy 0.972.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044918
Train loss (w/o reg) on all data: 0.0214877
Test loss (w/o reg) on all data: 0.229119
Train acc on all data:  0.999758278946
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 4.23957e-06
Norm of the params: 21.6473
                Loss: fixed 294 labels. Loss 0.22912. Accuracy 0.947.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137425
Train loss (w/o reg) on all data: 0.0997319
Test loss (w/o reg) on all data: 0.326879
Train acc on all data:  0.976069615664
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.29285e-05
Norm of the params: 27.4565
              Random: fixed  97 labels. Loss 0.32688. Accuracy 0.910.
### Flips: 412, rs: 19, checks: 1236
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0601211
Train loss (w/o reg) on all data: 0.0440297
Test loss (w/o reg) on all data: 0.161699
Train acc on all data:  0.989364273628
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.25884e-06
Norm of the params: 17.9396
     Influence (LOO): fixed 353 labels. Loss 0.16170. Accuracy 0.973.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0393599
Train loss (w/o reg) on all data: 0.0189073
Test loss (w/o reg) on all data: 0.148767
Train acc on all data:  0.999033115784
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 2.21934e-06
Norm of the params: 20.225
                Loss: fixed 325 labels. Loss 0.14877. Accuracy 0.966.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13211
Train loss (w/o reg) on all data: 0.0954012
Test loss (w/o reg) on all data: 0.311284
Train acc on all data:  0.977278220933
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 2.04699e-05
Norm of the params: 27.0957
              Random: fixed 118 labels. Loss 0.31128. Accuracy 0.915.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154266
Train loss (w/o reg) on all data: 0.113307
Test loss (w/o reg) on all data: 0.413013
Train acc on all data:  0.969784868262
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 3.48888e-05
Norm of the params: 28.6214
Flipped loss: 0.41301. Accuracy: 0.885
### Flips: 412, rs: 20, checks: 206
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0978792
Train loss (w/o reg) on all data: 0.0712208
Test loss (w/o reg) on all data: 0.240175
Train acc on all data:  0.981870920957
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 3.35133e-06
Norm of the params: 23.0904
     Influence (LOO): fixed 169 labels. Loss 0.24018. Accuracy 0.937.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0800573
Train loss (w/o reg) on all data: 0.042829
Test loss (w/o reg) on all data: 0.345586
Train acc on all data:  0.998066231569
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 4.81404e-06
Norm of the params: 27.2867
                Loss: fixed 146 labels. Loss 0.34559. Accuracy 0.909.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149802
Train loss (w/o reg) on all data: 0.109603
Test loss (w/o reg) on all data: 0.38634
Train acc on all data:  0.971235194585
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 1.55167e-05
Norm of the params: 28.3544
              Random: fixed  19 labels. Loss 0.38634. Accuracy 0.881.
### Flips: 412, rs: 20, checks: 412
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083269
Train loss (w/o reg) on all data: 0.0614182
Test loss (w/o reg) on all data: 0.168768
Train acc on all data:  0.985255015712
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 4.9739e-06
Norm of the params: 20.9049
     Influence (LOO): fixed 259 labels. Loss 0.16877. Accuracy 0.955.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0595085
Train loss (w/o reg) on all data: 0.0298259
Test loss (w/o reg) on all data: 0.226811
Train acc on all data:  0.998549673677
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 1.43377e-05
Norm of the params: 24.365
                Loss: fixed 215 labels. Loss 0.22681. Accuracy 0.940.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144138
Train loss (w/o reg) on all data: 0.104579
Test loss (w/o reg) on all data: 0.403562
Train acc on all data:  0.97461928934
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 4.04316e-06
Norm of the params: 28.1279
              Random: fixed  42 labels. Loss 0.40356. Accuracy 0.887.
### Flips: 412, rs: 20, checks: 618
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074036
Train loss (w/o reg) on all data: 0.0552
Test loss (w/o reg) on all data: 0.147705
Train acc on all data:  0.986705342035
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 1.54702e-06
Norm of the params: 19.4093
     Influence (LOO): fixed 305 labels. Loss 0.14771. Accuracy 0.958.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0515482
Train loss (w/o reg) on all data: 0.0254929
Test loss (w/o reg) on all data: 0.204695
Train acc on all data:  0.999274836838
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 2.99568e-06
Norm of the params: 22.8278
                Loss: fixed 256 labels. Loss 0.20470. Accuracy 0.952.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140748
Train loss (w/o reg) on all data: 0.101573
Test loss (w/o reg) on all data: 0.413227
Train acc on all data:  0.976069615664
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 1.89841e-05
Norm of the params: 27.9912
              Random: fixed  56 labels. Loss 0.41323. Accuracy 0.896.
### Flips: 412, rs: 20, checks: 824
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0687854
Train loss (w/o reg) on all data: 0.0514325
Test loss (w/o reg) on all data: 0.150463
Train acc on all data:  0.987672226251
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.4028e-06
Norm of the params: 18.6295
     Influence (LOO): fixed 329 labels. Loss 0.15046. Accuracy 0.965.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0480544
Train loss (w/o reg) on all data: 0.0234524
Test loss (w/o reg) on all data: 0.191718
Train acc on all data:  0.999274836838
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.92307e-06
Norm of the params: 22.182
                Loss: fixed 278 labels. Loss 0.19172. Accuracy 0.955.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136112
Train loss (w/o reg) on all data: 0.0977257
Test loss (w/o reg) on all data: 0.419047
Train acc on all data:  0.977278220933
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.89418e-05
Norm of the params: 27.7077
              Random: fixed  75 labels. Loss 0.41905. Accuracy 0.897.
### Flips: 412, rs: 20, checks: 1030
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0649211
Train loss (w/o reg) on all data: 0.0487939
Test loss (w/o reg) on all data: 0.154547
Train acc on all data:  0.987913947305
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 1.4315e-06
Norm of the params: 17.9595
     Influence (LOO): fixed 342 labels. Loss 0.15455. Accuracy 0.968.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0433709
Train loss (w/o reg) on all data: 0.0210544
Test loss (w/o reg) on all data: 0.173538
Train acc on all data:  0.999274836838
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.92918e-06
Norm of the params: 21.1265
                Loss: fixed 307 labels. Loss 0.17354. Accuracy 0.955.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130829
Train loss (w/o reg) on all data: 0.0930407
Test loss (w/o reg) on all data: 0.40995
Train acc on all data:  0.978728547256
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 5.82817e-06
Norm of the params: 27.4914
              Random: fixed  94 labels. Loss 0.40995. Accuracy 0.901.
### Flips: 412, rs: 20, checks: 1236
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0603434
Train loss (w/o reg) on all data: 0.0448817
Test loss (w/o reg) on all data: 0.139436
Train acc on all data:  0.989364273628
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 7.85818e-07
Norm of the params: 17.5851
     Influence (LOO): fixed 359 labels. Loss 0.13944. Accuracy 0.972.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0391619
Train loss (w/o reg) on all data: 0.0188607
Test loss (w/o reg) on all data: 0.159154
Train acc on all data:  0.999516557892
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 2.28894e-06
Norm of the params: 20.15
                Loss: fixed 331 labels. Loss 0.15915. Accuracy 0.960.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127201
Train loss (w/o reg) on all data: 0.0902658
Test loss (w/o reg) on all data: 0.405027
Train acc on all data:  0.978728547256
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.25356e-05
Norm of the params: 27.179
              Random: fixed 113 labels. Loss 0.40503. Accuracy 0.910.
Using normal model
LBFGS training took [564] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152522
Train loss (w/o reg) on all data: 0.111229
Test loss (w/o reg) on all data: 0.373906
Train acc on all data:  0.971718636693
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 3.47676e-05
Norm of the params: 28.7377
Flipped loss: 0.37391. Accuracy: 0.888
### Flips: 412, rs: 21, checks: 206
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0919558
Train loss (w/o reg) on all data: 0.0644274
Test loss (w/o reg) on all data: 0.209193
Train acc on all data:  0.98573845782
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 1.04756e-05
Norm of the params: 23.4642
     Influence (LOO): fixed 167 labels. Loss 0.20919. Accuracy 0.932.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0784295
Train loss (w/o reg) on all data: 0.0416184
Test loss (w/o reg) on all data: 0.2983
Train acc on all data:  0.99879139473
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 2.51239e-05
Norm of the params: 27.1334
                Loss: fixed 140 labels. Loss 0.29830. Accuracy 0.911.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145914
Train loss (w/o reg) on all data: 0.105199
Test loss (w/o reg) on all data: 0.370848
Train acc on all data:  0.974135847232
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 1.10241e-05
Norm of the params: 28.5356
              Random: fixed  24 labels. Loss 0.37085. Accuracy 0.888.
### Flips: 412, rs: 21, checks: 412
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0788366
Train loss (w/o reg) on all data: 0.0562033
Test loss (w/o reg) on all data: 0.161276
Train acc on all data:  0.986705342035
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 4.88131e-06
Norm of the params: 21.276
     Influence (LOO): fixed 252 labels. Loss 0.16128. Accuracy 0.948.
Using normal model
LBFGS training took [348] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0610453
Train loss (w/o reg) on all data: 0.0306891
Test loss (w/o reg) on all data: 0.233977
Train acc on all data:  0.999516557892
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 6.69097e-06
Norm of the params: 24.6399
                Loss: fixed 202 labels. Loss 0.23398. Accuracy 0.924.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139791
Train loss (w/o reg) on all data: 0.100327
Test loss (w/o reg) on all data: 0.356019
Train acc on all data:  0.975344452502
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 3.46868e-05
Norm of the params: 28.094
              Random: fixed  49 labels. Loss 0.35602. Accuracy 0.893.
### Flips: 412, rs: 21, checks: 618
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0702485
Train loss (w/o reg) on all data: 0.0505956
Test loss (w/o reg) on all data: 0.133837
Train acc on all data:  0.988155668359
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.55767e-06
Norm of the params: 19.8257
     Influence (LOO): fixed 300 labels. Loss 0.13384. Accuracy 0.963.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051069
Train loss (w/o reg) on all data: 0.0250344
Test loss (w/o reg) on all data: 0.186261
Train acc on all data:  0.999516557892
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 1.60224e-05
Norm of the params: 22.8187
                Loss: fixed 256 labels. Loss 0.18626. Accuracy 0.947.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136093
Train loss (w/o reg) on all data: 0.0974856
Test loss (w/o reg) on all data: 0.348128
Train acc on all data:  0.976311336717
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.44229e-05
Norm of the params: 27.7877
              Random: fixed  65 labels. Loss 0.34813. Accuracy 0.894.
### Flips: 412, rs: 21, checks: 824
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0650669
Train loss (w/o reg) on all data: 0.0472674
Test loss (w/o reg) on all data: 0.12795
Train acc on all data:  0.989122552574
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.95534e-06
Norm of the params: 18.8677
     Influence (LOO): fixed 326 labels. Loss 0.12795. Accuracy 0.965.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0481723
Train loss (w/o reg) on all data: 0.0234508
Test loss (w/o reg) on all data: 0.180541
Train acc on all data:  0.999274836838
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 4.69606e-06
Norm of the params: 22.2358
                Loss: fixed 276 labels. Loss 0.18054. Accuracy 0.947.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131017
Train loss (w/o reg) on all data: 0.0935804
Test loss (w/o reg) on all data: 0.309389
Train acc on all data:  0.978003384095
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 3.73839e-05
Norm of the params: 27.3629
              Random: fixed  91 labels. Loss 0.30939. Accuracy 0.896.
### Flips: 412, rs: 21, checks: 1030
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0598519
Train loss (w/o reg) on all data: 0.043254
Test loss (w/o reg) on all data: 0.129089
Train acc on all data:  0.990331157844
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 9.53606e-07
Norm of the params: 18.2197
     Influence (LOO): fixed 345 labels. Loss 0.12909. Accuracy 0.973.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0430389
Train loss (w/o reg) on all data: 0.0207591
Test loss (w/o reg) on all data: 0.170432
Train acc on all data:  0.999516557892
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 5.00312e-06
Norm of the params: 21.1092
                Loss: fixed 300 labels. Loss 0.17043. Accuracy 0.954.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124037
Train loss (w/o reg) on all data: 0.0876137
Test loss (w/o reg) on all data: 0.327087
Train acc on all data:  0.980420594634
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 4.325e-05
Norm of the params: 26.9901
              Random: fixed 114 labels. Loss 0.32709. Accuracy 0.900.
### Flips: 412, rs: 21, checks: 1236
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0569548
Train loss (w/o reg) on all data: 0.0409528
Test loss (w/o reg) on all data: 0.147748
Train acc on all data:  0.990814599952
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 9.47205e-07
Norm of the params: 17.8897
     Influence (LOO): fixed 353 labels. Loss 0.14775. Accuracy 0.973.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0405148
Train loss (w/o reg) on all data: 0.0195224
Test loss (w/o reg) on all data: 0.163939
Train acc on all data:  0.999516557892
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 7.55491e-06
Norm of the params: 20.4902
                Loss: fixed 323 labels. Loss 0.16394. Accuracy 0.955.
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119144
Train loss (w/o reg) on all data: 0.0836066
Test loss (w/o reg) on all data: 0.304769
Train acc on all data:  0.980904036742
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 3.29686e-05
Norm of the params: 26.6598
              Random: fixed 139 labels. Loss 0.30477. Accuracy 0.899.
Using normal model
LBFGS training took [365] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15948
Train loss (w/o reg) on all data: 0.118614
Test loss (w/o reg) on all data: 0.429655
Train acc on all data:  0.967851099831
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.09096e-05
Norm of the params: 28.5888
Flipped loss: 0.42965. Accuracy: 0.897
### Flips: 412, rs: 22, checks: 206
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095449
Train loss (w/o reg) on all data: 0.0682423
Test loss (w/o reg) on all data: 0.199835
Train acc on all data:  0.983562968335
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.27421e-05
Norm of the params: 23.3267
     Influence (LOO): fixed 168 labels. Loss 0.19983. Accuracy 0.941.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0860548
Train loss (w/o reg) on all data: 0.0470983
Test loss (w/o reg) on all data: 0.325534
Train acc on all data:  0.997824510515
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 5.74055e-06
Norm of the params: 27.9129
                Loss: fixed 150 labels. Loss 0.32553. Accuracy 0.913.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151629
Train loss (w/o reg) on all data: 0.111833
Test loss (w/o reg) on all data: 0.371526
Train acc on all data:  0.9690597051
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 1.97897e-05
Norm of the params: 28.212
              Random: fixed  29 labels. Loss 0.37153. Accuracy 0.898.
### Flips: 412, rs: 22, checks: 412
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0825949
Train loss (w/o reg) on all data: 0.0597413
Test loss (w/o reg) on all data: 0.149015
Train acc on all data:  0.984771573604
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 3.07627e-06
Norm of the params: 21.3793
     Influence (LOO): fixed 249 labels. Loss 0.14901. Accuracy 0.960.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0607624
Train loss (w/o reg) on all data: 0.0302799
Test loss (w/o reg) on all data: 0.200743
Train acc on all data:  0.999516557892
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 3.5542e-06
Norm of the params: 24.6911
                Loss: fixed 230 labels. Loss 0.20074. Accuracy 0.948.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147414
Train loss (w/o reg) on all data: 0.108566
Test loss (w/o reg) on all data: 0.361918
Train acc on all data:  0.970510031424
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 3.33776e-05
Norm of the params: 27.874
              Random: fixed  45 labels. Loss 0.36192. Accuracy 0.902.
### Flips: 412, rs: 22, checks: 618
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0728148
Train loss (w/o reg) on all data: 0.0526069
Test loss (w/o reg) on all data: 0.123783
Train acc on all data:  0.986947063089
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 3.43755e-06
Norm of the params: 20.1037
     Influence (LOO): fixed 300 labels. Loss 0.12378. Accuracy 0.968.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0509393
Train loss (w/o reg) on all data: 0.0249066
Test loss (w/o reg) on all data: 0.170843
Train acc on all data:  0.999274836838
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 1.40982e-05
Norm of the params: 22.8179
                Loss: fixed 272 labels. Loss 0.17084. Accuracy 0.953.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140874
Train loss (w/o reg) on all data: 0.103021
Test loss (w/o reg) on all data: 0.344621
Train acc on all data:  0.972927241963
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 2.46503e-05
Norm of the params: 27.5145
              Random: fixed  71 labels. Loss 0.34462. Accuracy 0.914.
### Flips: 412, rs: 22, checks: 824
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067936
Train loss (w/o reg) on all data: 0.0495215
Test loss (w/o reg) on all data: 0.109267
Train acc on all data:  0.987188784143
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 4.17198e-06
Norm of the params: 19.1909
     Influence (LOO): fixed 322 labels. Loss 0.10927. Accuracy 0.968.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0436629
Train loss (w/o reg) on all data: 0.0209311
Test loss (w/o reg) on all data: 0.141665
Train acc on all data:  0.999516557892
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 2.19528e-06
Norm of the params: 21.3222
                Loss: fixed 301 labels. Loss 0.14166. Accuracy 0.958.
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134556
Train loss (w/o reg) on all data: 0.0978255
Test loss (w/o reg) on all data: 0.34117
Train acc on all data:  0.974135847232
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 6.04016e-06
Norm of the params: 27.1036
              Random: fixed  95 labels. Loss 0.34117. Accuracy 0.923.
### Flips: 412, rs: 22, checks: 1030
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0642912
Train loss (w/o reg) on all data: 0.0469553
Test loss (w/o reg) on all data: 0.105819
Train acc on all data:  0.987672226251
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 7.55798e-07
Norm of the params: 18.6204
     Influence (LOO): fixed 336 labels. Loss 0.10582. Accuracy 0.970.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0393739
Train loss (w/o reg) on all data: 0.0188891
Test loss (w/o reg) on all data: 0.128496
Train acc on all data:  0.999274836838
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 4.61789e-06
Norm of the params: 20.241
                Loss: fixed 324 labels. Loss 0.12850. Accuracy 0.965.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128001
Train loss (w/o reg) on all data: 0.0922063
Test loss (w/o reg) on all data: 0.307703
Train acc on all data:  0.976069615664
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 7.35258e-06
Norm of the params: 26.7562
              Random: fixed 113 labels. Loss 0.30770. Accuracy 0.924.
### Flips: 412, rs: 22, checks: 1236
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0598639
Train loss (w/o reg) on all data: 0.0435469
Test loss (w/o reg) on all data: 0.0980132
Train acc on all data:  0.989122552574
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.48106e-06
Norm of the params: 18.0649
     Influence (LOO): fixed 350 labels. Loss 0.09801. Accuracy 0.974.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0370979
Train loss (w/o reg) on all data: 0.0178348
Test loss (w/o reg) on all data: 0.127569
Train acc on all data:  0.999274836838
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 7.45826e-06
Norm of the params: 19.6281
                Loss: fixed 341 labels. Loss 0.12757. Accuracy 0.969.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120026
Train loss (w/o reg) on all data: 0.085709
Test loss (w/o reg) on all data: 0.2761
Train acc on all data:  0.978486826203
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 5.84969e-06
Norm of the params: 26.1983
              Random: fixed 145 labels. Loss 0.27610. Accuracy 0.928.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156412
Train loss (w/o reg) on all data: 0.114921
Test loss (w/o reg) on all data: 0.437828
Train acc on all data:  0.971476915639
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 2.34455e-05
Norm of the params: 28.8067
Flipped loss: 0.43783. Accuracy: 0.883
### Flips: 412, rs: 23, checks: 206
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0922076
Train loss (w/o reg) on all data: 0.0647902
Test loss (w/o reg) on all data: 0.214664
Train acc on all data:  0.985013294658
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 9.92212e-06
Norm of the params: 23.4169
     Influence (LOO): fixed 169 labels. Loss 0.21466. Accuracy 0.945.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0778032
Train loss (w/o reg) on all data: 0.0412654
Test loss (w/o reg) on all data: 0.365428
Train acc on all data:  0.99879139473
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 1.75398e-05
Norm of the params: 27.0325
                Loss: fixed 152 labels. Loss 0.36543. Accuracy 0.915.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153092
Train loss (w/o reg) on all data: 0.112436
Test loss (w/o reg) on all data: 0.42296
Train acc on all data:  0.972202078801
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 1.6891e-05
Norm of the params: 28.515
              Random: fixed  20 labels. Loss 0.42296. Accuracy 0.880.
### Flips: 412, rs: 23, checks: 412
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0787611
Train loss (w/o reg) on all data: 0.0564086
Test loss (w/o reg) on all data: 0.183444
Train acc on all data:  0.987188784143
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 6.92952e-06
Norm of the params: 21.1436
     Influence (LOO): fixed 251 labels. Loss 0.18344. Accuracy 0.954.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0633692
Train loss (w/o reg) on all data: 0.031771
Test loss (w/o reg) on all data: 0.321061
Train acc on all data:  0.999516557892
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.13331e-05
Norm of the params: 25.1389
                Loss: fixed 207 labels. Loss 0.32106. Accuracy 0.936.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14764
Train loss (w/o reg) on all data: 0.107891
Test loss (w/o reg) on all data: 0.395614
Train acc on all data:  0.974135847232
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 1.22836e-05
Norm of the params: 28.1952
              Random: fixed  41 labels. Loss 0.39561. Accuracy 0.887.
### Flips: 412, rs: 23, checks: 618
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0720136
Train loss (w/o reg) on all data: 0.0517706
Test loss (w/o reg) on all data: 0.159404
Train acc on all data:  0.988397389413
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.90044e-06
Norm of the params: 20.1212
     Influence (LOO): fixed 292 labels. Loss 0.15940. Accuracy 0.963.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0552848
Train loss (w/o reg) on all data: 0.0272496
Test loss (w/o reg) on all data: 0.271866
Train acc on all data:  0.999516557892
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 1.47855e-05
Norm of the params: 23.6792
                Loss: fixed 241 labels. Loss 0.27187. Accuracy 0.940.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143508
Train loss (w/o reg) on all data: 0.104627
Test loss (w/o reg) on all data: 0.39858
Train acc on all data:  0.975102731448
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 8.23384e-06
Norm of the params: 27.8861
              Random: fixed  62 labels. Loss 0.39858. Accuracy 0.896.
### Flips: 412, rs: 23, checks: 824
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0671472
Train loss (w/o reg) on all data: 0.04839
Test loss (w/o reg) on all data: 0.16091
Train acc on all data:  0.988639110467
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.4202e-06
Norm of the params: 19.3686
     Influence (LOO): fixed 315 labels. Loss 0.16091. Accuracy 0.966.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0436133
Train loss (w/o reg) on all data: 0.0210308
Test loss (w/o reg) on all data: 0.218005
Train acc on all data:  0.999516557892
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 2.09355e-06
Norm of the params: 21.252
                Loss: fixed 293 labels. Loss 0.21800. Accuracy 0.959.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135981
Train loss (w/o reg) on all data: 0.0979945
Test loss (w/o reg) on all data: 0.370838
Train acc on all data:  0.977519941987
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.19312e-05
Norm of the params: 27.5634
              Random: fixed  84 labels. Loss 0.37084. Accuracy 0.903.
### Flips: 412, rs: 23, checks: 1030
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0621852
Train loss (w/o reg) on all data: 0.0443625
Test loss (w/o reg) on all data: 0.130986
Train acc on all data:  0.990331157844
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 3.18176e-06
Norm of the params: 18.88
     Influence (LOO): fixed 336 labels. Loss 0.13099. Accuracy 0.970.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0406128
Train loss (w/o reg) on all data: 0.0195156
Test loss (w/o reg) on all data: 0.151728
Train acc on all data:  0.999516557892
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.37574e-05
Norm of the params: 20.5413
                Loss: fixed 309 labels. Loss 0.15173. Accuracy 0.961.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12857
Train loss (w/o reg) on all data: 0.091374
Test loss (w/o reg) on all data: 0.367846
Train acc on all data:  0.979695431472
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 1.01337e-05
Norm of the params: 27.2751
              Random: fixed 107 labels. Loss 0.36785. Accuracy 0.902.
### Flips: 412, rs: 23, checks: 1236
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0610725
Train loss (w/o reg) on all data: 0.0435699
Test loss (w/o reg) on all data: 0.116858
Train acc on all data:  0.990331157844
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 1.00936e-05
Norm of the params: 18.7096
     Influence (LOO): fixed 347 labels. Loss 0.11686. Accuracy 0.973.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0384853
Train loss (w/o reg) on all data: 0.0184626
Test loss (w/o reg) on all data: 0.160728
Train acc on all data:  0.999516557892
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 8.22281e-06
Norm of the params: 20.0113
                Loss: fixed 323 labels. Loss 0.16073. Accuracy 0.968.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123778
Train loss (w/o reg) on all data: 0.0876109
Test loss (w/o reg) on all data: 0.356413
Train acc on all data:  0.981145757796
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 6.69512e-05
Norm of the params: 26.895
              Random: fixed 128 labels. Loss 0.35641. Accuracy 0.899.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160452
Train loss (w/o reg) on all data: 0.120365
Test loss (w/o reg) on all data: 0.601789
Train acc on all data:  0.96470872613
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.04631e-05
Norm of the params: 28.315
Flipped loss: 0.60179. Accuracy: 0.889
### Flips: 412, rs: 24, checks: 206
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0940612
Train loss (w/o reg) on all data: 0.0681386
Test loss (w/o reg) on all data: 0.402777
Train acc on all data:  0.981870920957
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 1.50287e-05
Norm of the params: 22.7695
     Influence (LOO): fixed 174 labels. Loss 0.40278. Accuracy 0.953.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0797512
Train loss (w/o reg) on all data: 0.0438898
Test loss (w/o reg) on all data: 0.51926
Train acc on all data:  0.999033115784
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 3.87739e-05
Norm of the params: 26.7811
                Loss: fixed 149 labels. Loss 0.51926. Accuracy 0.909.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155996
Train loss (w/o reg) on all data: 0.116649
Test loss (w/o reg) on all data: 0.558506
Train acc on all data:  0.968817984046
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 2.37935e-05
Norm of the params: 28.0523
              Random: fixed  20 labels. Loss 0.55851. Accuracy 0.900.
### Flips: 412, rs: 24, checks: 412
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0811766
Train loss (w/o reg) on all data: 0.0593855
Test loss (w/o reg) on all data: 0.22956
Train acc on all data:  0.984288131496
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 7.40892e-06
Norm of the params: 20.8763
     Influence (LOO): fixed 257 labels. Loss 0.22956. Accuracy 0.964.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0581072
Train loss (w/o reg) on all data: 0.0295309
Test loss (w/o reg) on all data: 0.402515
Train acc on all data:  0.999274836838
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.78353e-05
Norm of the params: 23.9066
                Loss: fixed 223 labels. Loss 0.40252. Accuracy 0.941.
Using normal model
LBFGS training took [360] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151369
Train loss (w/o reg) on all data: 0.112851
Test loss (w/o reg) on all data: 0.548651
Train acc on all data:  0.970751752478
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 2.22171e-05
Norm of the params: 27.7554
              Random: fixed  36 labels. Loss 0.54865. Accuracy 0.907.
### Flips: 412, rs: 24, checks: 618
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0710447
Train loss (w/o reg) on all data: 0.0520234
Test loss (w/o reg) on all data: 0.256029
Train acc on all data:  0.985980178874
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 5.48586e-06
Norm of the params: 19.5045
     Influence (LOO): fixed 302 labels. Loss 0.25603. Accuracy 0.969.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0509794
Train loss (w/o reg) on all data: 0.0253033
Test loss (w/o reg) on all data: 0.350857
Train acc on all data:  0.999516557892
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 3.54776e-06
Norm of the params: 22.661
                Loss: fixed 255 labels. Loss 0.35086. Accuracy 0.947.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148347
Train loss (w/o reg) on all data: 0.110155
Test loss (w/o reg) on all data: 0.5488
Train acc on all data:  0.970751752478
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.43367e-05
Norm of the params: 27.6377
              Random: fixed  46 labels. Loss 0.54880. Accuracy 0.907.
### Flips: 412, rs: 24, checks: 824
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0679551
Train loss (w/o reg) on all data: 0.0497435
Test loss (w/o reg) on all data: 0.265939
Train acc on all data:  0.986705342035
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 3.10055e-06
Norm of the params: 19.0849
     Influence (LOO): fixed 321 labels. Loss 0.26594. Accuracy 0.971.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0457653
Train loss (w/o reg) on all data: 0.0225069
Test loss (w/o reg) on all data: 0.246165
Train acc on all data:  0.999516557892
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 1.42015e-05
Norm of the params: 21.5678
                Loss: fixed 287 labels. Loss 0.24617. Accuracy 0.954.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141765
Train loss (w/o reg) on all data: 0.105169
Test loss (w/o reg) on all data: 0.517747
Train acc on all data:  0.972443799855
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 1.95001e-05
Norm of the params: 27.0542
              Random: fixed  71 labels. Loss 0.51775. Accuracy 0.914.
### Flips: 412, rs: 24, checks: 1030
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0628537
Train loss (w/o reg) on all data: 0.046014
Test loss (w/o reg) on all data: 0.17943
Train acc on all data:  0.987913947305
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.35107e-06
Norm of the params: 18.3519
     Influence (LOO): fixed 337 labels. Loss 0.17943. Accuracy 0.969.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0433986
Train loss (w/o reg) on all data: 0.0212544
Test loss (w/o reg) on all data: 0.217175
Train acc on all data:  0.999516557892
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 2.42585e-05
Norm of the params: 21.0448
                Loss: fixed 304 labels. Loss 0.21717. Accuracy 0.953.
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136291
Train loss (w/o reg) on all data: 0.10047
Test loss (w/o reg) on all data: 0.480006
Train acc on all data:  0.973410684071
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.29918e-05
Norm of the params: 26.7659
              Random: fixed  88 labels. Loss 0.48001. Accuracy 0.910.
### Flips: 412, rs: 24, checks: 1236
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0602969
Train loss (w/o reg) on all data: 0.0442542
Test loss (w/o reg) on all data: 0.149213
Train acc on all data:  0.98888083152
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 3.45009e-06
Norm of the params: 17.9124
     Influence (LOO): fixed 351 labels. Loss 0.14921. Accuracy 0.974.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0387658
Train loss (w/o reg) on all data: 0.0187331
Test loss (w/o reg) on all data: 0.213213
Train acc on all data:  0.999516557892
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 3.48024e-06
Norm of the params: 20.0164
                Loss: fixed 331 labels. Loss 0.21321. Accuracy 0.958.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128648
Train loss (w/o reg) on all data: 0.0942841
Test loss (w/o reg) on all data: 0.472408
Train acc on all data:  0.976311336717
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 3.8421e-05
Norm of the params: 26.2161
              Random: fixed 111 labels. Loss 0.47241. Accuracy 0.920.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160653
Train loss (w/o reg) on all data: 0.117464
Test loss (w/o reg) on all data: 0.543721
Train acc on all data:  0.968576262993
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 1.51591e-05
Norm of the params: 29.39
Flipped loss: 0.54372. Accuracy: 0.878
### Flips: 412, rs: 25, checks: 206
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0942932
Train loss (w/o reg) on all data: 0.0656165
Test loss (w/o reg) on all data: 0.353314
Train acc on all data:  0.985980178874
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 7.6854e-06
Norm of the params: 23.9486
     Influence (LOO): fixed 173 labels. Loss 0.35331. Accuracy 0.933.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086128
Train loss (w/o reg) on all data: 0.0474654
Test loss (w/o reg) on all data: 0.438818
Train acc on all data:  0.997824510515
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 9.3442e-06
Norm of the params: 27.8074
                Loss: fixed 138 labels. Loss 0.43882. Accuracy 0.910.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157794
Train loss (w/o reg) on all data: 0.115163
Test loss (w/o reg) on all data: 0.531599
Train acc on all data:  0.968817984046
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 1.5712e-05
Norm of the params: 29.1995
              Random: fixed  17 labels. Loss 0.53160. Accuracy 0.886.
### Flips: 412, rs: 25, checks: 412
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0803828
Train loss (w/o reg) on all data: 0.0568497
Test loss (w/o reg) on all data: 0.260464
Train acc on all data:  0.986705342035
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 1.08543e-05
Norm of the params: 21.6947
     Influence (LOO): fixed 252 labels. Loss 0.26046. Accuracy 0.954.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067406
Train loss (w/o reg) on all data: 0.0347879
Test loss (w/o reg) on all data: 0.367092
Train acc on all data:  0.999274836838
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 4.00838e-06
Norm of the params: 25.5414
                Loss: fixed 199 labels. Loss 0.36709. Accuracy 0.928.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152425
Train loss (w/o reg) on all data: 0.110155
Test loss (w/o reg) on all data: 0.519471
Train acc on all data:  0.969543147208
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 4.78398e-05
Norm of the params: 29.0756
              Random: fixed  39 labels. Loss 0.51947. Accuracy 0.890.
### Flips: 412, rs: 25, checks: 618
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0724661
Train loss (w/o reg) on all data: 0.0518786
Test loss (w/o reg) on all data: 0.295741
Train acc on all data:  0.986947063089
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.53454e-05
Norm of the params: 20.2916
     Influence (LOO): fixed 297 labels. Loss 0.29574. Accuracy 0.965.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052348
Train loss (w/o reg) on all data: 0.0259878
Test loss (w/o reg) on all data: 0.248264
Train acc on all data:  0.999274836838
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 3.9356e-06
Norm of the params: 22.9609
                Loss: fixed 261 labels. Loss 0.24826. Accuracy 0.958.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145127
Train loss (w/o reg) on all data: 0.104366
Test loss (w/o reg) on all data: 0.541546
Train acc on all data:  0.97461928934
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 1.43711e-05
Norm of the params: 28.552
              Random: fixed  64 labels. Loss 0.54155. Accuracy 0.901.
### Flips: 412, rs: 25, checks: 824
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0682671
Train loss (w/o reg) on all data: 0.0494807
Test loss (w/o reg) on all data: 0.297086
Train acc on all data:  0.988155668359
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.34211e-06
Norm of the params: 19.3837
     Influence (LOO): fixed 314 labels. Loss 0.29709. Accuracy 0.966.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0458623
Train loss (w/o reg) on all data: 0.0224534
Test loss (w/o reg) on all data: 0.195491
Train acc on all data:  0.999033115784
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 6.52737e-06
Norm of the params: 21.6374
                Loss: fixed 290 labels. Loss 0.19549. Accuracy 0.956.
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139817
Train loss (w/o reg) on all data: 0.0995406
Test loss (w/o reg) on all data: 0.52377
Train acc on all data:  0.975102731448
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 2.023e-05
Norm of the params: 28.3817
              Random: fixed  83 labels. Loss 0.52377. Accuracy 0.903.
### Flips: 412, rs: 25, checks: 1030
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0649444
Train loss (w/o reg) on all data: 0.0470249
Test loss (w/o reg) on all data: 0.259409
Train acc on all data:  0.988397389413
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 2.35512e-06
Norm of the params: 18.9312
     Influence (LOO): fixed 337 labels. Loss 0.25941. Accuracy 0.973.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0407869
Train loss (w/o reg) on all data: 0.0197553
Test loss (w/o reg) on all data: 0.168849
Train acc on all data:  0.999274836838
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 3.18633e-06
Norm of the params: 20.5093
                Loss: fixed 313 labels. Loss 0.16885. Accuracy 0.960.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133661
Train loss (w/o reg) on all data: 0.0946099
Test loss (w/o reg) on all data: 0.458564
Train acc on all data:  0.977278220933
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 9.14461e-06
Norm of the params: 27.947
              Random: fixed 107 labels. Loss 0.45856. Accuracy 0.902.
### Flips: 412, rs: 25, checks: 1236
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0633126
Train loss (w/o reg) on all data: 0.0458583
Test loss (w/o reg) on all data: 0.232263
Train acc on all data:  0.989122552574
Test acc on all data:   0.973913043478
Norm of the mean of gradients: 1.84517e-06
Norm of the params: 18.6839
     Influence (LOO): fixed 347 labels. Loss 0.23226. Accuracy 0.974.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0374553
Train loss (w/o reg) on all data: 0.0180271
Test loss (w/o reg) on all data: 0.167017
Train acc on all data:  0.999274836838
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 2.70066e-06
Norm of the params: 19.7121
                Loss: fixed 334 labels. Loss 0.16702. Accuracy 0.960.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130036
Train loss (w/o reg) on all data: 0.0919809
Test loss (w/o reg) on all data: 0.459765
Train acc on all data:  0.978728547256
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 7.09178e-06
Norm of the params: 27.588
              Random: fixed 124 labels. Loss 0.45977. Accuracy 0.901.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147811
Train loss (w/o reg) on all data: 0.10596
Test loss (w/o reg) on all data: 0.472787
Train acc on all data:  0.974135847232
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 2.62813e-05
Norm of the params: 28.9313
Flipped loss: 0.47279. Accuracy: 0.888
### Flips: 412, rs: 26, checks: 206
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0860432
Train loss (w/o reg) on all data: 0.0589819
Test loss (w/o reg) on all data: 0.336529
Train acc on all data:  0.986463620981
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 1.96619e-05
Norm of the params: 23.2643
     Influence (LOO): fixed 170 labels. Loss 0.33653. Accuracy 0.939.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0771308
Train loss (w/o reg) on all data: 0.0405072
Test loss (w/o reg) on all data: 0.440394
Train acc on all data:  0.999274836838
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 7.63092e-06
Norm of the params: 27.0642
                Loss: fixed 138 labels. Loss 0.44039. Accuracy 0.905.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140121
Train loss (w/o reg) on all data: 0.0992383
Test loss (w/o reg) on all data: 0.483432
Train acc on all data:  0.97582789461
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 7.4624e-05
Norm of the params: 28.5948
              Random: fixed  27 labels. Loss 0.48343. Accuracy 0.890.
### Flips: 412, rs: 26, checks: 412
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0730406
Train loss (w/o reg) on all data: 0.0514427
Test loss (w/o reg) on all data: 0.283255
Train acc on all data:  0.986947063089
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 4.57982e-06
Norm of the params: 20.7836
     Influence (LOO): fixed 263 labels. Loss 0.28326. Accuracy 0.961.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0607609
Train loss (w/o reg) on all data: 0.030371
Test loss (w/o reg) on all data: 0.346709
Train acc on all data:  0.999274836838
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 1.26513e-05
Norm of the params: 24.6535
                Loss: fixed 196 labels. Loss 0.34671. Accuracy 0.927.
Using normal model
LBFGS training took [434] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135081
Train loss (w/o reg) on all data: 0.0951657
Test loss (w/o reg) on all data: 0.440317
Train acc on all data:  0.976794778825
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 2.41574e-05
Norm of the params: 28.2543
              Random: fixed  45 labels. Loss 0.44032. Accuracy 0.893.
### Flips: 412, rs: 26, checks: 618
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0660541
Train loss (w/o reg) on all data: 0.0460694
Test loss (w/o reg) on all data: 0.303605
Train acc on all data:  0.989364273628
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 6.39282e-06
Norm of the params: 19.9923
     Influence (LOO): fixed 302 labels. Loss 0.30361. Accuracy 0.964.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0510599
Train loss (w/o reg) on all data: 0.0249146
Test loss (w/o reg) on all data: 0.411523
Train acc on all data:  0.999516557892
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 6.53436e-06
Norm of the params: 22.8671
                Loss: fixed 240 labels. Loss 0.41152. Accuracy 0.940.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129035
Train loss (w/o reg) on all data: 0.0906726
Test loss (w/o reg) on all data: 0.4478
Train acc on all data:  0.978245105149
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.35895e-05
Norm of the params: 27.6992
              Random: fixed  66 labels. Loss 0.44780. Accuracy 0.907.
### Flips: 412, rs: 26, checks: 824
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0618101
Train loss (w/o reg) on all data: 0.0434544
Test loss (w/o reg) on all data: 0.274543
Train acc on all data:  0.99008943679
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 1.47072e-05
Norm of the params: 19.1602
     Influence (LOO): fixed 327 labels. Loss 0.27454. Accuracy 0.970.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0462941
Train loss (w/o reg) on all data: 0.0225517
Test loss (w/o reg) on all data: 0.399104
Train acc on all data:  0.999516557892
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 2.65612e-06
Norm of the params: 21.791
                Loss: fixed 266 labels. Loss 0.39910. Accuracy 0.938.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124141
Train loss (w/o reg) on all data: 0.0869338
Test loss (w/o reg) on all data: 0.409295
Train acc on all data:  0.979453710418
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 9.9785e-06
Norm of the params: 27.2789
              Random: fixed  87 labels. Loss 0.40929. Accuracy 0.918.
### Flips: 412, rs: 26, checks: 1030
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0586457
Train loss (w/o reg) on all data: 0.0412647
Test loss (w/o reg) on all data: 0.170353
Train acc on all data:  0.990814599952
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 1.536e-05
Norm of the params: 18.6446
     Influence (LOO): fixed 345 labels. Loss 0.17035. Accuracy 0.970.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0437895
Train loss (w/o reg) on all data: 0.0212781
Test loss (w/o reg) on all data: 0.323732
Train acc on all data:  0.999516557892
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 9.7226e-06
Norm of the params: 21.2186
                Loss: fixed 288 labels. Loss 0.32373. Accuracy 0.945.
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120142
Train loss (w/o reg) on all data: 0.0838088
Test loss (w/o reg) on all data: 0.395309
Train acc on all data:  0.979937152526
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 9.00855e-06
Norm of the params: 26.9566
              Random: fixed 104 labels. Loss 0.39531. Accuracy 0.925.
### Flips: 412, rs: 26, checks: 1236
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0571596
Train loss (w/o reg) on all data: 0.0400286
Test loss (w/o reg) on all data: 0.133414
Train acc on all data:  0.991298042059
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 4.0343e-06
Norm of the params: 18.51
     Influence (LOO): fixed 354 labels. Loss 0.13341. Accuracy 0.969.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0408571
Train loss (w/o reg) on all data: 0.0196937
Test loss (w/o reg) on all data: 0.26844
Train acc on all data:  0.999516557892
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 1.38756e-05
Norm of the params: 20.5735
                Loss: fixed 313 labels. Loss 0.26844. Accuracy 0.949.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117032
Train loss (w/o reg) on all data: 0.0815278
Test loss (w/o reg) on all data: 0.337711
Train acc on all data:  0.980904036742
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 8.70015e-06
Norm of the params: 26.6474
              Random: fixed 124 labels. Loss 0.33771. Accuracy 0.923.
Using normal model
LBFGS training took [370] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163654
Train loss (w/o reg) on all data: 0.121992
Test loss (w/o reg) on all data: 0.399759
Train acc on all data:  0.963983562968
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 3.82391e-05
Norm of the params: 28.8657
Flipped loss: 0.39976. Accuracy: 0.882
### Flips: 412, rs: 27, checks: 206
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098016
Train loss (w/o reg) on all data: 0.0710523
Test loss (w/o reg) on all data: 0.239233
Train acc on all data:  0.982354363065
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 4.19156e-06
Norm of the params: 23.2223
     Influence (LOO): fixed 168 labels. Loss 0.23923. Accuracy 0.935.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0776326
Train loss (w/o reg) on all data: 0.041881
Test loss (w/o reg) on all data: 0.306214
Train acc on all data:  0.99879139473
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 5.74281e-06
Norm of the params: 26.7401
                Loss: fixed 156 labels. Loss 0.30621. Accuracy 0.916.
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159394
Train loss (w/o reg) on all data: 0.118487
Test loss (w/o reg) on all data: 0.390847
Train acc on all data:  0.965433889292
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.19758e-05
Norm of the params: 28.6032
              Random: fixed  17 labels. Loss 0.39085. Accuracy 0.889.
### Flips: 412, rs: 27, checks: 412
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0851704
Train loss (w/o reg) on all data: 0.0628309
Test loss (w/o reg) on all data: 0.177353
Train acc on all data:  0.983804689388
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 3.7933e-06
Norm of the params: 21.1374
     Influence (LOO): fixed 248 labels. Loss 0.17735. Accuracy 0.957.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0581867
Train loss (w/o reg) on all data: 0.0289408
Test loss (w/o reg) on all data: 0.214929
Train acc on all data:  0.999516557892
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 5.20081e-06
Norm of the params: 24.1851
                Loss: fixed 228 labels. Loss 0.21493. Accuracy 0.933.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15565
Train loss (w/o reg) on all data: 0.115203
Test loss (w/o reg) on all data: 0.391285
Train acc on all data:  0.967125936669
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 7.26606e-05
Norm of the params: 28.4417
              Random: fixed  36 labels. Loss 0.39128. Accuracy 0.885.
### Flips: 412, rs: 27, checks: 618
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0752264
Train loss (w/o reg) on all data: 0.0555105
Test loss (w/o reg) on all data: 0.142813
Train acc on all data:  0.986463620981
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.72533e-05
Norm of the params: 19.8574
     Influence (LOO): fixed 293 labels. Loss 0.14281. Accuracy 0.969.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0501732
Train loss (w/o reg) on all data: 0.0244371
Test loss (w/o reg) on all data: 0.16803
Train acc on all data:  0.999274836838
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 3.21145e-06
Norm of the params: 22.6875
                Loss: fixed 270 labels. Loss 0.16803. Accuracy 0.948.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149869
Train loss (w/o reg) on all data: 0.110294
Test loss (w/o reg) on all data: 0.392562
Train acc on all data:  0.972443799855
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 3.27346e-05
Norm of the params: 28.1336
              Random: fixed  55 labels. Loss 0.39256. Accuracy 0.900.
### Flips: 412, rs: 27, checks: 824
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071007
Train loss (w/o reg) on all data: 0.0525731
Test loss (w/o reg) on all data: 0.129125
Train acc on all data:  0.986705342035
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.34159e-06
Norm of the params: 19.201
     Influence (LOO): fixed 315 labels. Loss 0.12912. Accuracy 0.971.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0462665
Train loss (w/o reg) on all data: 0.0223895
Test loss (w/o reg) on all data: 0.164458
Train acc on all data:  0.999516557892
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 6.43152e-06
Norm of the params: 21.8527
                Loss: fixed 292 labels. Loss 0.16446. Accuracy 0.954.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147298
Train loss (w/o reg) on all data: 0.108438
Test loss (w/o reg) on all data: 0.394454
Train acc on all data:  0.971960357747
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 1.02719e-05
Norm of the params: 27.8783
              Random: fixed  70 labels. Loss 0.39445. Accuracy 0.900.
### Flips: 412, rs: 27, checks: 1030
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066363
Train loss (w/o reg) on all data: 0.0487969
Test loss (w/o reg) on all data: 0.0976721
Train acc on all data:  0.987913947305
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 4.10826e-06
Norm of the params: 18.7436
     Influence (LOO): fixed 331 labels. Loss 0.09767. Accuracy 0.977.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0428201
Train loss (w/o reg) on all data: 0.0205946
Test loss (w/o reg) on all data: 0.185226
Train acc on all data:  0.999274836838
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 2.72693e-06
Norm of the params: 21.0834
                Loss: fixed 310 labels. Loss 0.18523. Accuracy 0.957.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141264
Train loss (w/o reg) on all data: 0.103214
Test loss (w/o reg) on all data: 0.381113
Train acc on all data:  0.973652405124
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 2.98634e-05
Norm of the params: 27.5862
              Random: fixed  91 labels. Loss 0.38111. Accuracy 0.896.
### Flips: 412, rs: 27, checks: 1236
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0634389
Train loss (w/o reg) on all data: 0.0466692
Test loss (w/o reg) on all data: 0.101315
Train acc on all data:  0.988639110467
Test acc on all data:   0.977777777778
Norm of the mean of gradients: 2.62425e-06
Norm of the params: 18.3138
     Influence (LOO): fixed 343 labels. Loss 0.10132. Accuracy 0.978.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0393244
Train loss (w/o reg) on all data: 0.0187576
Test loss (w/o reg) on all data: 0.168052
Train acc on all data:  0.999274836838
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 1.11463e-06
Norm of the params: 20.2814
                Loss: fixed 334 labels. Loss 0.16805. Accuracy 0.958.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136665
Train loss (w/o reg) on all data: 0.0993839
Test loss (w/o reg) on all data: 0.37585
Train acc on all data:  0.974861010394
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 7.68467e-06
Norm of the params: 27.3062
              Random: fixed 108 labels. Loss 0.37585. Accuracy 0.893.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156456
Train loss (w/o reg) on all data: 0.116226
Test loss (w/o reg) on all data: 0.486693
Train acc on all data:  0.965675610346
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 4.96551e-05
Norm of the params: 28.3654
Flipped loss: 0.48669. Accuracy: 0.887
### Flips: 412, rs: 28, checks: 206
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0914609
Train loss (w/o reg) on all data: 0.0647158
Test loss (w/o reg) on all data: 0.382081
Train acc on all data:  0.985496736766
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.30167e-05
Norm of the params: 23.128
     Influence (LOO): fixed 168 labels. Loss 0.38208. Accuracy 0.924.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0769713
Train loss (w/o reg) on all data: 0.0407708
Test loss (w/o reg) on all data: 0.393114
Train acc on all data:  0.99879139473
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 8.3452e-06
Norm of the params: 26.9075
                Loss: fixed 147 labels. Loss 0.39311. Accuracy 0.919.
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152222
Train loss (w/o reg) on all data: 0.112957
Test loss (w/o reg) on all data: 0.520745
Train acc on all data:  0.966159052453
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 1.51855e-05
Norm of the params: 28.0232
              Random: fixed  25 labels. Loss 0.52075. Accuracy 0.883.
### Flips: 412, rs: 28, checks: 412
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0786246
Train loss (w/o reg) on all data: 0.0570598
Test loss (w/o reg) on all data: 0.285932
Train acc on all data:  0.986221899927
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 1.38524e-06
Norm of the params: 20.7677
     Influence (LOO): fixed 254 labels. Loss 0.28593. Accuracy 0.950.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0610806
Train loss (w/o reg) on all data: 0.0305357
Test loss (w/o reg) on all data: 0.293851
Train acc on all data:  0.999274836838
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 2.56463e-05
Norm of the params: 24.7164
                Loss: fixed 202 labels. Loss 0.29385. Accuracy 0.928.
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146208
Train loss (w/o reg) on all data: 0.10795
Test loss (w/o reg) on all data: 0.52872
Train acc on all data:  0.970993473532
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 7.66708e-06
Norm of the params: 27.6615
              Random: fixed  53 labels. Loss 0.52872. Accuracy 0.900.
### Flips: 412, rs: 28, checks: 618
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0728272
Train loss (w/o reg) on all data: 0.0532947
Test loss (w/o reg) on all data: 0.220781
Train acc on all data:  0.986947063089
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 4.65957e-06
Norm of the params: 19.7649
     Influence (LOO): fixed 293 labels. Loss 0.22078. Accuracy 0.957.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0532536
Train loss (w/o reg) on all data: 0.0261331
Test loss (w/o reg) on all data: 0.242243
Train acc on all data:  0.999516557892
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 3.89482e-06
Norm of the params: 23.2897
                Loss: fixed 238 labels. Loss 0.24224. Accuracy 0.942.
Using normal model
LBFGS training took [355] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140221
Train loss (w/o reg) on all data: 0.102817
Test loss (w/o reg) on all data: 0.414971
Train acc on all data:  0.973168963017
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 3.45207e-05
Norm of the params: 27.3509
              Random: fixed  80 labels. Loss 0.41497. Accuracy 0.901.
### Flips: 412, rs: 28, checks: 824
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0675311
Train loss (w/o reg) on all data: 0.0494499
Test loss (w/o reg) on all data: 0.17233
Train acc on all data:  0.988397389413
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 8.18691e-06
Norm of the params: 19.0164
     Influence (LOO): fixed 319 labels. Loss 0.17233. Accuracy 0.961.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0485956
Train loss (w/o reg) on all data: 0.0236658
Test loss (w/o reg) on all data: 0.230722
Train acc on all data:  0.999516557892
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 4.75405e-06
Norm of the params: 22.3293
                Loss: fixed 263 labels. Loss 0.23072. Accuracy 0.945.
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135007
Train loss (w/o reg) on all data: 0.0982693
Test loss (w/o reg) on all data: 0.420225
Train acc on all data:  0.97582789461
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 3.00606e-05
Norm of the params: 27.1064
              Random: fixed 104 labels. Loss 0.42023. Accuracy 0.907.
### Flips: 412, rs: 28, checks: 1030
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0655927
Train loss (w/o reg) on all data: 0.0481025
Test loss (w/o reg) on all data: 0.152623
Train acc on all data:  0.988397389413
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.91435e-06
Norm of the params: 18.7031
     Influence (LOO): fixed 333 labels. Loss 0.15262. Accuracy 0.966.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0437117
Train loss (w/o reg) on all data: 0.0211096
Test loss (w/o reg) on all data: 0.219865
Train acc on all data:  0.999516557892
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 5.81635e-06
Norm of the params: 21.2612
                Loss: fixed 294 labels. Loss 0.21987. Accuracy 0.954.
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13015
Train loss (w/o reg) on all data: 0.0946878
Test loss (w/o reg) on all data: 0.379596
Train acc on all data:  0.97582789461
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 1.79794e-05
Norm of the params: 26.6316
              Random: fixed 120 labels. Loss 0.37960. Accuracy 0.917.
### Flips: 412, rs: 28, checks: 1236
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0635293
Train loss (w/o reg) on all data: 0.0467311
Test loss (w/o reg) on all data: 0.156398
Train acc on all data:  0.98888083152
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.56054e-05
Norm of the params: 18.3293
     Influence (LOO): fixed 344 labels. Loss 0.15640. Accuracy 0.969.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0403922
Train loss (w/o reg) on all data: 0.0195548
Test loss (w/o reg) on all data: 0.229563
Train acc on all data:  0.999516557892
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 1.71295e-05
Norm of the params: 20.4144
                Loss: fixed 321 labels. Loss 0.22956. Accuracy 0.956.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123683
Train loss (w/o reg) on all data: 0.0896086
Test loss (w/o reg) on all data: 0.335997
Train acc on all data:  0.978486826203
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 2.17869e-05
Norm of the params: 26.1053
              Random: fixed 140 labels. Loss 0.33600. Accuracy 0.923.
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157658
Train loss (w/o reg) on all data: 0.115553
Test loss (w/o reg) on all data: 0.515569
Train acc on all data:  0.969543147208
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 9.16158e-06
Norm of the params: 29.019
Flipped loss: 0.51557. Accuracy: 0.890
### Flips: 412, rs: 29, checks: 206
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0925927
Train loss (w/o reg) on all data: 0.0652246
Test loss (w/o reg) on all data: 0.425573
Train acc on all data:  0.985255015712
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.77077e-05
Norm of the params: 23.3958
     Influence (LOO): fixed 166 labels. Loss 0.42557. Accuracy 0.930.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0791321
Train loss (w/o reg) on all data: 0.0423855
Test loss (w/o reg) on all data: 0.570484
Train acc on all data:  0.999033115784
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 2.95643e-05
Norm of the params: 27.1096
                Loss: fixed 143 labels. Loss 0.57048. Accuracy 0.907.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15171
Train loss (w/o reg) on all data: 0.110535
Test loss (w/o reg) on all data: 0.509763
Train acc on all data:  0.971235194585
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 2.21396e-05
Norm of the params: 28.6966
              Random: fixed  17 labels. Loss 0.50976. Accuracy 0.892.
### Flips: 412, rs: 29, checks: 412
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0776161
Train loss (w/o reg) on all data: 0.0551749
Test loss (w/o reg) on all data: 0.312053
Train acc on all data:  0.986705342035
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 1.05816e-05
Norm of the params: 21.1855
     Influence (LOO): fixed 251 labels. Loss 0.31205. Accuracy 0.949.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0602448
Train loss (w/o reg) on all data: 0.0303868
Test loss (w/o reg) on all data: 0.358063
Train acc on all data:  0.999274836838
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 1.00937e-05
Norm of the params: 24.4368
                Loss: fixed 210 labels. Loss 0.35806. Accuracy 0.927.
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14524
Train loss (w/o reg) on all data: 0.105535
Test loss (w/o reg) on all data: 0.505884
Train acc on all data:  0.973168963017
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 2.42309e-05
Norm of the params: 28.1799
              Random: fixed  40 labels. Loss 0.50588. Accuracy 0.895.
### Flips: 412, rs: 29, checks: 618
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069206
Train loss (w/o reg) on all data: 0.0498651
Test loss (w/o reg) on all data: 0.326655
Train acc on all data:  0.988397389413
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 1.2343e-05
Norm of the params: 19.6677
     Influence (LOO): fixed 295 labels. Loss 0.32665. Accuracy 0.957.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0505098
Train loss (w/o reg) on all data: 0.0247458
Test loss (w/o reg) on all data: 0.2728
Train acc on all data:  0.999516557892
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 4.37826e-06
Norm of the params: 22.6998
                Loss: fixed 256 labels. Loss 0.27280. Accuracy 0.936.
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139431
Train loss (w/o reg) on all data: 0.100502
Test loss (w/o reg) on all data: 0.505092
Train acc on all data:  0.974861010394
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 1.74401e-05
Norm of the params: 27.9032
              Random: fixed  65 labels. Loss 0.50509. Accuracy 0.901.
### Flips: 412, rs: 29, checks: 824
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0657748
Train loss (w/o reg) on all data: 0.0476005
Test loss (w/o reg) on all data: 0.208646
Train acc on all data:  0.98888083152
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 6.21511e-06
Norm of the params: 19.0653
     Influence (LOO): fixed 321 labels. Loss 0.20865. Accuracy 0.964.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0446826
Train loss (w/o reg) on all data: 0.0214601
Test loss (w/o reg) on all data: 0.237568
Train acc on all data:  0.999516557892
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.88549e-06
Norm of the params: 21.5511
                Loss: fixed 283 labels. Loss 0.23757. Accuracy 0.944.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133452
Train loss (w/o reg) on all data: 0.0962292
Test loss (w/o reg) on all data: 0.428661
Train acc on all data:  0.97582789461
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 2.86168e-05
Norm of the params: 27.2848
              Random: fixed  90 labels. Loss 0.42866. Accuracy 0.906.
### Flips: 412, rs: 29, checks: 1030
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0625412
Train loss (w/o reg) on all data: 0.0457988
Test loss (w/o reg) on all data: 0.200889
Train acc on all data:  0.988639110467
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 3.7866e-06
Norm of the params: 18.2989
     Influence (LOO): fixed 337 labels. Loss 0.20089. Accuracy 0.967.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043068
Train loss (w/o reg) on all data: 0.020707
Test loss (w/o reg) on all data: 0.239357
Train acc on all data:  0.999274836838
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.00832e-05
Norm of the params: 21.1476
                Loss: fixed 299 labels. Loss 0.23936. Accuracy 0.944.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129224
Train loss (w/o reg) on all data: 0.0928052
Test loss (w/o reg) on all data: 0.380273
Train acc on all data:  0.976069615664
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 3.04434e-05
Norm of the params: 26.9885
              Random: fixed 106 labels. Loss 0.38027. Accuracy 0.910.
### Flips: 412, rs: 29, checks: 1236
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0617135
Train loss (w/o reg) on all data: 0.045318
Test loss (w/o reg) on all data: 0.200715
Train acc on all data:  0.988397389413
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 4.17548e-06
Norm of the params: 18.1083
     Influence (LOO): fixed 346 labels. Loss 0.20072. Accuracy 0.970.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0387359
Train loss (w/o reg) on all data: 0.0186341
Test loss (w/o reg) on all data: 0.231784
Train acc on all data:  0.999516557892
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 2.16608e-06
Norm of the params: 20.0508
                Loss: fixed 326 labels. Loss 0.23178. Accuracy 0.956.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124523
Train loss (w/o reg) on all data: 0.0886716
Test loss (w/o reg) on all data: 0.342318
Train acc on all data:  0.977761663041
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.08955e-05
Norm of the params: 26.7774
              Random: fixed 128 labels. Loss 0.34232. Accuracy 0.920.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160804
Train loss (w/o reg) on all data: 0.119703
Test loss (w/o reg) on all data: 0.376567
Train acc on all data:  0.966400773507
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 3.08647e-05
Norm of the params: 28.6711
Flipped loss: 0.37657. Accuracy: 0.899
### Flips: 412, rs: 30, checks: 206
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0970416
Train loss (w/o reg) on all data: 0.0706328
Test loss (w/o reg) on all data: 0.217401
Train acc on all data:  0.983079526227
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.02567e-05
Norm of the params: 22.9821
     Influence (LOO): fixed 177 labels. Loss 0.21740. Accuracy 0.930.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0808777
Train loss (w/o reg) on all data: 0.0434815
Test loss (w/o reg) on all data: 0.32804
Train acc on all data:  0.998549673677
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 3.55445e-05
Norm of the params: 27.3482
                Loss: fixed 148 labels. Loss 0.32804. Accuracy 0.902.
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15791
Train loss (w/o reg) on all data: 0.117134
Test loss (w/o reg) on all data: 0.370385
Train acc on all data:  0.968576262993
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 5.13618e-05
Norm of the params: 28.5574
              Random: fixed  16 labels. Loss 0.37039. Accuracy 0.903.
### Flips: 412, rs: 30, checks: 412
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0832044
Train loss (w/o reg) on all data: 0.061178
Test loss (w/o reg) on all data: 0.164419
Train acc on all data:  0.983562968335
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 1.99472e-06
Norm of the params: 20.9888
     Influence (LOO): fixed 254 labels. Loss 0.16442. Accuracy 0.953.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0626556
Train loss (w/o reg) on all data: 0.0317382
Test loss (w/o reg) on all data: 0.244229
Train acc on all data:  0.99879139473
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 5.02997e-06
Norm of the params: 24.8666
                Loss: fixed 217 labels. Loss 0.24423. Accuracy 0.923.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153826
Train loss (w/o reg) on all data: 0.113842
Test loss (w/o reg) on all data: 0.39404
Train acc on all data:  0.969543147208
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 2.57415e-05
Norm of the params: 28.2786
              Random: fixed  32 labels. Loss 0.39404. Accuracy 0.900.
### Flips: 412, rs: 30, checks: 618
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0755281
Train loss (w/o reg) on all data: 0.0563018
Test loss (w/o reg) on all data: 0.132884
Train acc on all data:  0.985255015712
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 1.40292e-06
Norm of the params: 19.6094
     Influence (LOO): fixed 291 labels. Loss 0.13288. Accuracy 0.968.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0489012
Train loss (w/o reg) on all data: 0.0237491
Test loss (w/o reg) on all data: 0.203183
Train acc on all data:  0.999516557892
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 3.82662e-06
Norm of the params: 22.4286
                Loss: fixed 276 labels. Loss 0.20318. Accuracy 0.941.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15031
Train loss (w/o reg) on all data: 0.110926
Test loss (w/o reg) on all data: 0.37534
Train acc on all data:  0.970510031424
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.10299e-05
Norm of the params: 28.0656
              Random: fixed  43 labels. Loss 0.37534. Accuracy 0.906.
### Flips: 412, rs: 30, checks: 824
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0704607
Train loss (w/o reg) on all data: 0.0526632
Test loss (w/o reg) on all data: 0.14203
Train acc on all data:  0.985980178874
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.95775e-06
Norm of the params: 18.8666
     Influence (LOO): fixed 320 labels. Loss 0.14203. Accuracy 0.970.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0455637
Train loss (w/o reg) on all data: 0.0219565
Test loss (w/o reg) on all data: 0.179554
Train acc on all data:  0.999516557892
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 4.50497e-06
Norm of the params: 21.7289
                Loss: fixed 293 labels. Loss 0.17955. Accuracy 0.950.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14479
Train loss (w/o reg) on all data: 0.106418
Test loss (w/o reg) on all data: 0.341035
Train acc on all data:  0.972202078801
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 1.31665e-05
Norm of the params: 27.7028
              Random: fixed  65 labels. Loss 0.34104. Accuracy 0.909.
### Flips: 412, rs: 30, checks: 1030
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064094
Train loss (w/o reg) on all data: 0.0476254
Test loss (w/o reg) on all data: 0.171352
Train acc on all data:  0.987430505197
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 1.56714e-06
Norm of the params: 18.1486
     Influence (LOO): fixed 340 labels. Loss 0.17135. Accuracy 0.972.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0426269
Train loss (w/o reg) on all data: 0.0204633
Test loss (w/o reg) on all data: 0.175376
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 6.08488e-06
Norm of the params: 21.0541
                Loss: fixed 314 labels. Loss 0.17538. Accuracy 0.951.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139353
Train loss (w/o reg) on all data: 0.101718
Test loss (w/o reg) on all data: 0.345969
Train acc on all data:  0.97461928934
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.06713e-05
Norm of the params: 27.4355
              Random: fixed  88 labels. Loss 0.34597. Accuracy 0.916.
### Flips: 412, rs: 30, checks: 1236
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0619606
Train loss (w/o reg) on all data: 0.0455857
Test loss (w/o reg) on all data: 0.18691
Train acc on all data:  0.988155668359
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 3.10715e-06
Norm of the params: 18.0969
     Influence (LOO): fixed 351 labels. Loss 0.18691. Accuracy 0.971.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0392683
Train loss (w/o reg) on all data: 0.0188136
Test loss (w/o reg) on all data: 0.127572
Train acc on all data:  0.999274836838
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 2.82826e-06
Norm of the params: 20.226
                Loss: fixed 335 labels. Loss 0.12757. Accuracy 0.964.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132726
Train loss (w/o reg) on all data: 0.0956886
Test loss (w/o reg) on all data: 0.345766
Train acc on all data:  0.976794778825
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 1.1328e-05
Norm of the params: 27.2169
              Random: fixed 111 labels. Loss 0.34577. Accuracy 0.922.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161048
Train loss (w/o reg) on all data: 0.121541
Test loss (w/o reg) on all data: 0.343717
Train acc on all data:  0.967367657723
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 2.66877e-05
Norm of the params: 28.1096
Flipped loss: 0.34372. Accuracy: 0.890
### Flips: 412, rs: 31, checks: 206
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0992218
Train loss (w/o reg) on all data: 0.072243
Test loss (w/o reg) on all data: 0.310283
Train acc on all data:  0.981870920957
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 1.16846e-05
Norm of the params: 23.2288
     Influence (LOO): fixed 164 labels. Loss 0.31028. Accuracy 0.942.
Using normal model
LBFGS training took [390] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0765614
Train loss (w/o reg) on all data: 0.0413718
Test loss (w/o reg) on all data: 0.324917
Train acc on all data:  0.998066231569
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 2.64956e-05
Norm of the params: 26.5291
                Loss: fixed 160 labels. Loss 0.32492. Accuracy 0.918.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154793
Train loss (w/o reg) on all data: 0.116006
Test loss (w/o reg) on all data: 0.319033
Train acc on all data:  0.968576262993
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 2.56909e-05
Norm of the params: 27.8521
              Random: fixed  28 labels. Loss 0.31903. Accuracy 0.908.
### Flips: 412, rs: 31, checks: 412
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087191
Train loss (w/o reg) on all data: 0.0643015
Test loss (w/o reg) on all data: 0.175268
Train acc on all data:  0.982354363065
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 9.40066e-06
Norm of the params: 21.396
     Influence (LOO): fixed 244 labels. Loss 0.17527. Accuracy 0.961.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0615735
Train loss (w/o reg) on all data: 0.0309764
Test loss (w/o reg) on all data: 0.384377
Train acc on all data:  0.999516557892
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 2.99673e-05
Norm of the params: 24.7374
                Loss: fixed 211 labels. Loss 0.38438. Accuracy 0.933.
Using normal model
LBFGS training took [401] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149328
Train loss (w/o reg) on all data: 0.111852
Test loss (w/o reg) on all data: 0.295242
Train acc on all data:  0.97026831037
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.0691e-05
Norm of the params: 27.3776
              Random: fixed  49 labels. Loss 0.29524. Accuracy 0.907.
### Flips: 412, rs: 31, checks: 618
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0797793
Train loss (w/o reg) on all data: 0.0597402
Test loss (w/o reg) on all data: 0.145941
Train acc on all data:  0.984046410442
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.95675e-06
Norm of the params: 20.0195
     Influence (LOO): fixed 285 labels. Loss 0.14594. Accuracy 0.968.
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0533098
Train loss (w/o reg) on all data: 0.0261316
Test loss (w/o reg) on all data: 0.354578
Train acc on all data:  0.999516557892
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 2.44644e-05
Norm of the params: 23.3145
                Loss: fixed 249 labels. Loss 0.35458. Accuracy 0.930.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145093
Train loss (w/o reg) on all data: 0.108044
Test loss (w/o reg) on all data: 0.278862
Train acc on all data:  0.973652405124
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.92061e-05
Norm of the params: 27.2211
              Random: fixed  66 labels. Loss 0.27886. Accuracy 0.913.
### Flips: 412, rs: 31, checks: 824
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0739357
Train loss (w/o reg) on all data: 0.0553846
Test loss (w/o reg) on all data: 0.177093
Train acc on all data:  0.985013294658
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 2.34982e-06
Norm of the params: 19.2619
     Influence (LOO): fixed 311 labels. Loss 0.17709. Accuracy 0.972.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0484664
Train loss (w/o reg) on all data: 0.0236578
Test loss (w/o reg) on all data: 0.341347
Train acc on all data:  0.999516557892
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 2.85259e-05
Norm of the params: 22.2749
                Loss: fixed 276 labels. Loss 0.34135. Accuracy 0.947.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142247
Train loss (w/o reg) on all data: 0.106235
Test loss (w/o reg) on all data: 0.260337
Train acc on all data:  0.973652405124
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 3.06285e-05
Norm of the params: 26.8375
              Random: fixed  82 labels. Loss 0.26034. Accuracy 0.910.
### Flips: 412, rs: 31, checks: 1030
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0678164
Train loss (w/o reg) on all data: 0.0498972
Test loss (w/o reg) on all data: 0.165265
Train acc on all data:  0.987430505197
Test acc on all data:   0.978743961353
Norm of the mean of gradients: 1.79326e-05
Norm of the params: 18.931
     Influence (LOO): fixed 329 labels. Loss 0.16527. Accuracy 0.979.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0449217
Train loss (w/o reg) on all data: 0.0218496
Test loss (w/o reg) on all data: 0.332212
Train acc on all data:  0.999516557892
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 2.45395e-05
Norm of the params: 21.4812
                Loss: fixed 296 labels. Loss 0.33221. Accuracy 0.951.
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13477
Train loss (w/o reg) on all data: 0.0995897
Test loss (w/o reg) on all data: 0.277133
Train acc on all data:  0.97582789461
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 1.91461e-05
Norm of the params: 26.5257
              Random: fixed 106 labels. Loss 0.27713. Accuracy 0.911.
### Flips: 412, rs: 31, checks: 1236
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0654772
Train loss (w/o reg) on all data: 0.0478904
Test loss (w/o reg) on all data: 0.170229
Train acc on all data:  0.988155668359
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 8.90348e-06
Norm of the params: 18.7546
     Influence (LOO): fixed 339 labels. Loss 0.17023. Accuracy 0.977.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0408365
Train loss (w/o reg) on all data: 0.0196775
Test loss (w/o reg) on all data: 0.345133
Train acc on all data:  0.999516557892
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 7.81234e-06
Norm of the params: 20.5713
                Loss: fixed 318 labels. Loss 0.34513. Accuracy 0.960.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130794
Train loss (w/o reg) on all data: 0.0967123
Test loss (w/o reg) on all data: 0.249983
Train acc on all data:  0.976311336717
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.21533e-05
Norm of the params: 26.108
              Random: fixed 125 labels. Loss 0.24998. Accuracy 0.920.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160886
Train loss (w/o reg) on all data: 0.119732
Test loss (w/o reg) on all data: 0.461825
Train acc on all data:  0.966642494561
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 2.89683e-05
Norm of the params: 28.6896
Flipped loss: 0.46182. Accuracy: 0.871
### Flips: 412, rs: 32, checks: 206
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095534
Train loss (w/o reg) on all data: 0.0674031
Test loss (w/o reg) on all data: 0.286011
Train acc on all data:  0.983321247281
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.48339e-05
Norm of the params: 23.7196
     Influence (LOO): fixed 171 labels. Loss 0.28601. Accuracy 0.916.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077321
Train loss (w/o reg) on all data: 0.041122
Test loss (w/o reg) on all data: 0.37957
Train acc on all data:  0.998549673677
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 8.37224e-06
Norm of the params: 26.9069
                Loss: fixed 156 labels. Loss 0.37957. Accuracy 0.902.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157713
Train loss (w/o reg) on all data: 0.117457
Test loss (w/o reg) on all data: 0.434067
Train acc on all data:  0.967851099831
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.45949e-05
Norm of the params: 28.3749
              Random: fixed  20 labels. Loss 0.43407. Accuracy 0.876.
### Flips: 412, rs: 32, checks: 412
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0818917
Train loss (w/o reg) on all data: 0.058209
Test loss (w/o reg) on all data: 0.182619
Train acc on all data:  0.985980178874
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 5.46711e-06
Norm of the params: 21.7636
     Influence (LOO): fixed 257 labels. Loss 0.18262. Accuracy 0.947.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0608777
Train loss (w/o reg) on all data: 0.0306577
Test loss (w/o reg) on all data: 0.357082
Train acc on all data:  0.999274836838
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 4.0203e-06
Norm of the params: 24.5846
                Loss: fixed 218 labels. Loss 0.35708. Accuracy 0.921.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153407
Train loss (w/o reg) on all data: 0.113798
Test loss (w/o reg) on all data: 0.406097
Train acc on all data:  0.9690597051
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 3.73481e-05
Norm of the params: 28.1457
              Random: fixed  39 labels. Loss 0.40610. Accuracy 0.889.
### Flips: 412, rs: 32, checks: 618
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0748347
Train loss (w/o reg) on all data: 0.0537721
Test loss (w/o reg) on all data: 0.163508
Train acc on all data:  0.987188784143
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 2.52289e-06
Norm of the params: 20.5244
     Influence (LOO): fixed 297 labels. Loss 0.16351. Accuracy 0.959.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0549623
Train loss (w/o reg) on all data: 0.0272948
Test loss (w/o reg) on all data: 0.343746
Train acc on all data:  0.999516557892
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 6.60097e-06
Norm of the params: 23.5234
                Loss: fixed 249 labels. Loss 0.34375. Accuracy 0.933.
Using normal model
LBFGS training took [376] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147897
Train loss (w/o reg) on all data: 0.109298
Test loss (w/o reg) on all data: 0.395429
Train acc on all data:  0.971235194585
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 1.29357e-05
Norm of the params: 27.7845
              Random: fixed  60 labels. Loss 0.39543. Accuracy 0.891.
### Flips: 412, rs: 32, checks: 824
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0689097
Train loss (w/o reg) on all data: 0.0495516
Test loss (w/o reg) on all data: 0.125495
Train acc on all data:  0.988639110467
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 5.18401e-06
Norm of the params: 19.6764
     Influence (LOO): fixed 321 labels. Loss 0.12549. Accuracy 0.964.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0486297
Train loss (w/o reg) on all data: 0.0237514
Test loss (w/o reg) on all data: 0.270589
Train acc on all data:  0.999274836838
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 3.71105e-06
Norm of the params: 22.3062
                Loss: fixed 278 labels. Loss 0.27059. Accuracy 0.928.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142256
Train loss (w/o reg) on all data: 0.104332
Test loss (w/o reg) on all data: 0.355677
Train acc on all data:  0.973410684071
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 3.26305e-05
Norm of the params: 27.5408
              Random: fixed  84 labels. Loss 0.35568. Accuracy 0.901.
### Flips: 412, rs: 32, checks: 1030
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0659942
Train loss (w/o reg) on all data: 0.0478861
Test loss (w/o reg) on all data: 0.144514
Train acc on all data:  0.987913947305
Test acc on all data:   0.970048309179
Norm of the mean of gradients: 2.31836e-06
Norm of the params: 19.0305
     Influence (LOO): fixed 332 labels. Loss 0.14451. Accuracy 0.970.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0458826
Train loss (w/o reg) on all data: 0.0222323
Test loss (w/o reg) on all data: 0.260023
Train acc on all data:  0.999274836838
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 4.179e-06
Norm of the params: 21.7487
                Loss: fixed 294 labels. Loss 0.26002. Accuracy 0.941.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136349
Train loss (w/o reg) on all data: 0.0989565
Test loss (w/o reg) on all data: 0.349744
Train acc on all data:  0.97582789461
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 8.05557e-06
Norm of the params: 27.3467
              Random: fixed 102 labels. Loss 0.34974. Accuracy 0.904.
### Flips: 412, rs: 32, checks: 1236
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0602399
Train loss (w/o reg) on all data: 0.0440355
Test loss (w/o reg) on all data: 0.125909
Train acc on all data:  0.98888083152
Test acc on all data:   0.972946859903
Norm of the mean of gradients: 8.73344e-07
Norm of the params: 18.0024
     Influence (LOO): fixed 350 labels. Loss 0.12591. Accuracy 0.973.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0418201
Train loss (w/o reg) on all data: 0.0201269
Test loss (w/o reg) on all data: 0.271758
Train acc on all data:  0.999516557892
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 1.05419e-05
Norm of the params: 20.8294
                Loss: fixed 321 labels. Loss 0.27176. Accuracy 0.959.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131933
Train loss (w/o reg) on all data: 0.0957332
Test loss (w/o reg) on all data: 0.327826
Train acc on all data:  0.977036499879
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 1.82771e-05
Norm of the params: 26.9074
              Random: fixed 124 labels. Loss 0.32783. Accuracy 0.915.
Using normal model
LBFGS training took [557] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155338
Train loss (w/o reg) on all data: 0.115365
Test loss (w/o reg) on all data: 0.438589
Train acc on all data:  0.968817984046
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.49301e-05
Norm of the params: 28.2748
Flipped loss: 0.43859. Accuracy: 0.900
### Flips: 412, rs: 33, checks: 206
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0906906
Train loss (w/o reg) on all data: 0.0647222
Test loss (w/o reg) on all data: 0.223209
Train acc on all data:  0.984046410442
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 2.73853e-05
Norm of the params: 22.7897
     Influence (LOO): fixed 173 labels. Loss 0.22321. Accuracy 0.937.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0736918
Train loss (w/o reg) on all data: 0.038964
Test loss (w/o reg) on all data: 0.322632
Train acc on all data:  0.998066231569
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 7.04901e-06
Norm of the params: 26.3544
                Loss: fixed 150 labels. Loss 0.32263. Accuracy 0.914.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151897
Train loss (w/o reg) on all data: 0.112525
Test loss (w/o reg) on all data: 0.446292
Train acc on all data:  0.970751752478
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 3.85671e-05
Norm of the params: 28.0616
              Random: fixed  18 labels. Loss 0.44629. Accuracy 0.895.
### Flips: 412, rs: 33, checks: 412
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0765438
Train loss (w/o reg) on all data: 0.055276
Test loss (w/o reg) on all data: 0.198986
Train acc on all data:  0.986221899927
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 5.15673e-06
Norm of the params: 20.6242
     Influence (LOO): fixed 261 labels. Loss 0.19899. Accuracy 0.954.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0567299
Train loss (w/o reg) on all data: 0.0281317
Test loss (w/o reg) on all data: 0.295718
Train acc on all data:  0.99879139473
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 3.98064e-06
Norm of the params: 23.9158
                Loss: fixed 220 labels. Loss 0.29572. Accuracy 0.928.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146064
Train loss (w/o reg) on all data: 0.1069
Test loss (w/o reg) on all data: 0.470529
Train acc on all data:  0.972685520909
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.89569e-05
Norm of the params: 27.9871
              Random: fixed  36 labels. Loss 0.47053. Accuracy 0.900.
### Flips: 412, rs: 33, checks: 618
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0685432
Train loss (w/o reg) on all data: 0.049958
Test loss (w/o reg) on all data: 0.136764
Train acc on all data:  0.986947063089
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 1.23219e-06
Norm of the params: 19.2796
     Influence (LOO): fixed 303 labels. Loss 0.13676. Accuracy 0.966.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0523299
Train loss (w/o reg) on all data: 0.0255693
Test loss (w/o reg) on all data: 0.270553
Train acc on all data:  0.99879139473
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 2.72263e-06
Norm of the params: 23.1346
                Loss: fixed 250 labels. Loss 0.27055. Accuracy 0.933.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140807
Train loss (w/o reg) on all data: 0.102945
Test loss (w/o reg) on all data: 0.482672
Train acc on all data:  0.973410684071
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 3.46217e-05
Norm of the params: 27.5179
              Random: fixed  62 labels. Loss 0.48267. Accuracy 0.907.
### Flips: 412, rs: 33, checks: 824
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0648753
Train loss (w/o reg) on all data: 0.047103
Test loss (w/o reg) on all data: 0.121357
Train acc on all data:  0.987913947305
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.22372e-06
Norm of the params: 18.8533
     Influence (LOO): fixed 326 labels. Loss 0.12136. Accuracy 0.968.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0452575
Train loss (w/o reg) on all data: 0.0218511
Test loss (w/o reg) on all data: 0.261204
Train acc on all data:  0.999033115784
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 1.4564e-05
Norm of the params: 21.6363
                Loss: fixed 285 labels. Loss 0.26120. Accuracy 0.957.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137346
Train loss (w/o reg) on all data: 0.10035
Test loss (w/o reg) on all data: 0.459139
Train acc on all data:  0.974135847232
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 3.86975e-05
Norm of the params: 27.2015
              Random: fixed  77 labels. Loss 0.45914. Accuracy 0.909.
### Flips: 412, rs: 33, checks: 1030
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0609152
Train loss (w/o reg) on all data: 0.0443298
Test loss (w/o reg) on all data: 0.184312
Train acc on all data:  0.98888083152
Test acc on all data:   0.974879227053
Norm of the mean of gradients: 1.73779e-06
Norm of the params: 18.2128
     Influence (LOO): fixed 340 labels. Loss 0.18431. Accuracy 0.975.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0423228
Train loss (w/o reg) on all data: 0.0204453
Test loss (w/o reg) on all data: 0.237487
Train acc on all data:  0.999274836838
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 2.49195e-06
Norm of the params: 20.9177
                Loss: fixed 305 labels. Loss 0.23749. Accuracy 0.961.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134717
Train loss (w/o reg) on all data: 0.0982238
Test loss (w/o reg) on all data: 0.448382
Train acc on all data:  0.975344452502
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 6.48084e-05
Norm of the params: 27.016
              Random: fixed  94 labels. Loss 0.44838. Accuracy 0.913.
### Flips: 412, rs: 33, checks: 1236
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0598494
Train loss (w/o reg) on all data: 0.0436429
Test loss (w/o reg) on all data: 0.160956
Train acc on all data:  0.98888083152
Test acc on all data:   0.976811594203
Norm of the mean of gradients: 1.36026e-06
Norm of the params: 18.0036
     Influence (LOO): fixed 353 labels. Loss 0.16096. Accuracy 0.977.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0397234
Train loss (w/o reg) on all data: 0.0190813
Test loss (w/o reg) on all data: 0.255983
Train acc on all data:  0.999274836838
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 4.50684e-06
Norm of the params: 20.3185
                Loss: fixed 324 labels. Loss 0.25598. Accuracy 0.966.
Using normal model
LBFGS training took [491] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129067
Train loss (w/o reg) on all data: 0.093787
Test loss (w/o reg) on all data: 0.433316
Train acc on all data:  0.977036499879
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 4.54463e-05
Norm of the params: 26.5632
              Random: fixed 112 labels. Loss 0.43332. Accuracy 0.920.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169154
Train loss (w/o reg) on all data: 0.12672
Test loss (w/o reg) on all data: 0.409923
Train acc on all data:  0.967609378777
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 8.90428e-05
Norm of the params: 29.1321
Flipped loss: 0.40992. Accuracy: 0.887
### Flips: 412, rs: 34, checks: 206
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0915512
Train loss (w/o reg) on all data: 0.0639441
Test loss (w/o reg) on all data: 0.204573
Train acc on all data:  0.986705342035
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.10063e-05
Norm of the params: 23.4977
     Influence (LOO): fixed 184 labels. Loss 0.20457. Accuracy 0.934.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0843633
Train loss (w/o reg) on all data: 0.0454743
Test loss (w/o reg) on all data: 0.322487
Train acc on all data:  0.998549673677
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 3.27944e-05
Norm of the params: 27.8887
                Loss: fixed 149 labels. Loss 0.32249. Accuracy 0.904.
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164759
Train loss (w/o reg) on all data: 0.123123
Test loss (w/o reg) on all data: 0.388687
Train acc on all data:  0.968092820885
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 5.09145e-05
Norm of the params: 28.857
              Random: fixed  19 labels. Loss 0.38869. Accuracy 0.893.
### Flips: 412, rs: 34, checks: 412
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0786915
Train loss (w/o reg) on all data: 0.0562133
Test loss (w/o reg) on all data: 0.159709
Train acc on all data:  0.988155668359
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 9.35144e-06
Norm of the params: 21.203
     Influence (LOO): fixed 267 labels. Loss 0.15971. Accuracy 0.957.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0631348
Train loss (w/o reg) on all data: 0.0319786
Test loss (w/o reg) on all data: 0.22572
Train acc on all data:  0.999033115784
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 8.26798e-06
Norm of the params: 24.9624
                Loss: fixed 220 labels. Loss 0.22572. Accuracy 0.927.
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159908
Train loss (w/o reg) on all data: 0.119142
Test loss (w/o reg) on all data: 0.345074
Train acc on all data:  0.97026831037
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 4.14878e-05
Norm of the params: 28.5537
              Random: fixed  39 labels. Loss 0.34507. Accuracy 0.893.
### Flips: 412, rs: 34, checks: 618
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0704819
Train loss (w/o reg) on all data: 0.051069
Test loss (w/o reg) on all data: 0.223062
Train acc on all data:  0.98888083152
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 5.02849e-06
Norm of the params: 19.7042
     Influence (LOO): fixed 312 labels. Loss 0.22306. Accuracy 0.964.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0539581
Train loss (w/o reg) on all data: 0.0267599
Test loss (w/o reg) on all data: 0.190956
Train acc on all data:  0.999274836838
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 7.90323e-06
Norm of the params: 23.323
                Loss: fixed 252 labels. Loss 0.19096. Accuracy 0.948.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154125
Train loss (w/o reg) on all data: 0.113392
Test loss (w/o reg) on all data: 0.357921
Train acc on all data:  0.973652405124
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 4.36009e-05
Norm of the params: 28.5422
              Random: fixed  53 labels. Loss 0.35792. Accuracy 0.900.
### Flips: 412, rs: 34, checks: 824
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066563
Train loss (w/o reg) on all data: 0.0485983
Test loss (w/o reg) on all data: 0.177861
Train acc on all data:  0.989364273628
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 4.48352e-06
Norm of the params: 18.9551
     Influence (LOO): fixed 332 labels. Loss 0.17786. Accuracy 0.971.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049912
Train loss (w/o reg) on all data: 0.0244917
Test loss (w/o reg) on all data: 0.190271
Train acc on all data:  0.999033115784
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 6.96803e-06
Norm of the params: 22.5479
                Loss: fixed 275 labels. Loss 0.19027. Accuracy 0.954.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150762
Train loss (w/o reg) on all data: 0.110252
Test loss (w/o reg) on all data: 0.337586
Train acc on all data:  0.97461928934
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 3.58303e-05
Norm of the params: 28.4643
              Random: fixed  69 labels. Loss 0.33759. Accuracy 0.904.
### Flips: 412, rs: 34, checks: 1030
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062805
Train loss (w/o reg) on all data: 0.04579
Test loss (w/o reg) on all data: 0.194171
Train acc on all data:  0.989847715736
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 1.75692e-06
Norm of the params: 18.4472
     Influence (LOO): fixed 345 labels. Loss 0.19417. Accuracy 0.969.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0457074
Train loss (w/o reg) on all data: 0.0222563
Test loss (w/o reg) on all data: 0.155691
Train acc on all data:  0.999274836838
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 1.52223e-05
Norm of the params: 21.6569
                Loss: fixed 296 labels. Loss 0.15569. Accuracy 0.960.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143718
Train loss (w/o reg) on all data: 0.104416
Test loss (w/o reg) on all data: 0.326102
Train acc on all data:  0.976069615664
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 1.40603e-05
Norm of the params: 28.0363
              Random: fixed  91 labels. Loss 0.32610. Accuracy 0.909.
### Flips: 412, rs: 34, checks: 1236
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0592483
Train loss (w/o reg) on all data: 0.0432319
Test loss (w/o reg) on all data: 0.158153
Train acc on all data:  0.990814599952
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 6.41475e-06
Norm of the params: 17.8977
     Influence (LOO): fixed 361 labels. Loss 0.15815. Accuracy 0.971.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0417719
Train loss (w/o reg) on all data: 0.0202505
Test loss (w/o reg) on all data: 0.168642
Train acc on all data:  0.999274836838
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 2.02346e-05
Norm of the params: 20.7467
                Loss: fixed 318 labels. Loss 0.16864. Accuracy 0.965.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13853
Train loss (w/o reg) on all data: 0.0998292
Test loss (w/o reg) on all data: 0.318809
Train acc on all data:  0.978003384095
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 1.49624e-05
Norm of the params: 27.8211
              Random: fixed 106 labels. Loss 0.31881. Accuracy 0.911.
Using normal model
LBFGS training took [477] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162708
Train loss (w/o reg) on all data: 0.12122
Test loss (w/o reg) on all data: 0.399522
Train acc on all data:  0.969543147208
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 2.17597e-05
Norm of the params: 28.8055
Flipped loss: 0.39952. Accuracy: 0.902
### Flips: 412, rs: 35, checks: 206
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0937568
Train loss (w/o reg) on all data: 0.06675
Test loss (w/o reg) on all data: 0.284968
Train acc on all data:  0.983562968335
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 2.13324e-05
Norm of the params: 23.2408
     Influence (LOO): fixed 180 labels. Loss 0.28497. Accuracy 0.941.
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0808557
Train loss (w/o reg) on all data: 0.0432387
Test loss (w/o reg) on all data: 0.353545
Train acc on all data:  0.998549673677
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 1.95326e-05
Norm of the params: 27.4288
                Loss: fixed 154 labels. Loss 0.35354. Accuracy 0.914.
Using normal model
LBFGS training took [411] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157258
Train loss (w/o reg) on all data: 0.116865
Test loss (w/o reg) on all data: 0.361973
Train acc on all data:  0.970026589316
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 3.58829e-05
Norm of the params: 28.4229
              Random: fixed  24 labels. Loss 0.36197. Accuracy 0.914.
### Flips: 412, rs: 35, checks: 412
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0822189
Train loss (w/o reg) on all data: 0.0599316
Test loss (w/o reg) on all data: 0.180915
Train acc on all data:  0.985255015712
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 4.14292e-06
Norm of the params: 21.1127
     Influence (LOO): fixed 252 labels. Loss 0.18092. Accuracy 0.947.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0618603
Train loss (w/o reg) on all data: 0.0311854
Test loss (w/o reg) on all data: 0.258787
Train acc on all data:  0.999033115784
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 1.13641e-05
Norm of the params: 24.7689
                Loss: fixed 216 labels. Loss 0.25879. Accuracy 0.929.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152102
Train loss (w/o reg) on all data: 0.112634
Test loss (w/o reg) on all data: 0.387462
Train acc on all data:  0.971960357747
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 2.73878e-05
Norm of the params: 28.0956
              Random: fixed  45 labels. Loss 0.38746. Accuracy 0.910.
### Flips: 412, rs: 35, checks: 618
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0760361
Train loss (w/o reg) on all data: 0.055864
Test loss (w/o reg) on all data: 0.186457
Train acc on all data:  0.98573845782
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 1.0694e-05
Norm of the params: 20.0859
     Influence (LOO): fixed 288 labels. Loss 0.18646. Accuracy 0.960.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0517332
Train loss (w/o reg) on all data: 0.025304
Test loss (w/o reg) on all data: 0.221131
Train acc on all data:  0.999033115784
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.67841e-05
Norm of the params: 22.991
                Loss: fixed 255 labels. Loss 0.22113. Accuracy 0.941.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147106
Train loss (w/o reg) on all data: 0.108396
Test loss (w/o reg) on all data: 0.362657
Train acc on all data:  0.972202078801
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.81828e-05
Norm of the params: 27.8244
              Random: fixed  66 labels. Loss 0.36266. Accuracy 0.907.
### Flips: 412, rs: 35, checks: 824
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0701509
Train loss (w/o reg) on all data: 0.0518502
Test loss (w/o reg) on all data: 0.124441
Train acc on all data:  0.987188784143
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 7.21022e-06
Norm of the params: 19.1315
     Influence (LOO): fixed 318 labels. Loss 0.12444. Accuracy 0.967.
Using normal model
LBFGS training took [358] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0449133
Train loss (w/o reg) on all data: 0.0216591
Test loss (w/o reg) on all data: 0.207817
Train acc on all data:  0.999033115784
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 2.53382e-05
Norm of the params: 21.5658
                Loss: fixed 291 labels. Loss 0.20782. Accuracy 0.954.
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142065
Train loss (w/o reg) on all data: 0.104282
Test loss (w/o reg) on all data: 0.350183
Train acc on all data:  0.97461928934
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.91694e-05
Norm of the params: 27.4893
              Random: fixed  85 labels. Loss 0.35018. Accuracy 0.907.
### Flips: 412, rs: 35, checks: 1030
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0672126
Train loss (w/o reg) on all data: 0.0501011
Test loss (w/o reg) on all data: 0.121437
Train acc on all data:  0.987672226251
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 6.15085e-06
Norm of the params: 18.4995
     Influence (LOO): fixed 331 labels. Loss 0.12144. Accuracy 0.971.
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0423547
Train loss (w/o reg) on all data: 0.0203912
Test loss (w/o reg) on all data: 0.168277
Train acc on all data:  0.999274836838
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 8.31196e-06
Norm of the params: 20.9587
                Loss: fixed 305 labels. Loss 0.16828. Accuracy 0.958.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135982
Train loss (w/o reg) on all data: 0.0990234
Test loss (w/o reg) on all data: 0.383671
Train acc on all data:  0.97582789461
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 3.7342e-05
Norm of the params: 27.1877
              Random: fixed 105 labels. Loss 0.38367. Accuracy 0.915.
### Flips: 412, rs: 35, checks: 1236
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0645672
Train loss (w/o reg) on all data: 0.0479481
Test loss (w/o reg) on all data: 0.12407
Train acc on all data:  0.988397389413
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.32991e-05
Norm of the params: 18.2313
     Influence (LOO): fixed 340 labels. Loss 0.12407. Accuracy 0.968.
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0388764
Train loss (w/o reg) on all data: 0.0185345
Test loss (w/o reg) on all data: 0.139723
Train acc on all data:  0.999033115784
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 5.49592e-06
Norm of the params: 20.1702
                Loss: fixed 331 labels. Loss 0.13972. Accuracy 0.957.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13157
Train loss (w/o reg) on all data: 0.0953941
Test loss (w/o reg) on all data: 0.386532
Train acc on all data:  0.976553057771
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 1.06644e-05
Norm of the params: 26.8984
              Random: fixed 125 labels. Loss 0.38653. Accuracy 0.912.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167703
Train loss (w/o reg) on all data: 0.126677
Test loss (w/o reg) on all data: 0.321965
Train acc on all data:  0.963741841914
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 6.72187e-05
Norm of the params: 28.6446
Flipped loss: 0.32197. Accuracy: 0.895
### Flips: 412, rs: 36, checks: 206
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0990134
Train loss (w/o reg) on all data: 0.0719335
Test loss (w/o reg) on all data: 0.196249
Train acc on all data:  0.980662315688
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 2.89547e-05
Norm of the params: 23.2723
     Influence (LOO): fixed 180 labels. Loss 0.19625. Accuracy 0.940.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0855473
Train loss (w/o reg) on all data: 0.046881
Test loss (w/o reg) on all data: 0.278232
Train acc on all data:  0.997582789461
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 5.48531e-05
Norm of the params: 27.8087
                Loss: fixed 160 labels. Loss 0.27823. Accuracy 0.923.
Using normal model
LBFGS training took [464] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162914
Train loss (w/o reg) on all data: 0.122412
Test loss (w/o reg) on all data: 0.31903
Train acc on all data:  0.9659173314
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 4.15866e-05
Norm of the params: 28.4613
              Random: fixed  18 labels. Loss 0.31903. Accuracy 0.894.
### Flips: 412, rs: 36, checks: 412
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0842124
Train loss (w/o reg) on all data: 0.0623802
Test loss (w/o reg) on all data: 0.147574
Train acc on all data:  0.982596084119
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 1.62292e-05
Norm of the params: 20.896
     Influence (LOO): fixed 256 labels. Loss 0.14757. Accuracy 0.958.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0637053
Train loss (w/o reg) on all data: 0.0321726
Test loss (w/o reg) on all data: 0.251278
Train acc on all data:  0.999516557892
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.069e-05
Norm of the params: 25.1128
                Loss: fixed 225 labels. Loss 0.25128. Accuracy 0.933.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154928
Train loss (w/o reg) on all data: 0.114957
Test loss (w/o reg) on all data: 0.296969
Train acc on all data:  0.970026589316
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 4.8574e-05
Norm of the params: 28.2739
              Random: fixed  49 labels. Loss 0.29697. Accuracy 0.902.
### Flips: 412, rs: 36, checks: 618
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0757731
Train loss (w/o reg) on all data: 0.0564755
Test loss (w/o reg) on all data: 0.144216
Train acc on all data:  0.98452985255
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 5.69714e-06
Norm of the params: 19.6457
     Influence (LOO): fixed 289 labels. Loss 0.14422. Accuracy 0.960.
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0540327
Train loss (w/o reg) on all data: 0.026624
Test loss (w/o reg) on all data: 0.197042
Train acc on all data:  0.999516557892
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 7.0688e-06
Norm of the params: 23.4131
                Loss: fixed 269 labels. Loss 0.19704. Accuracy 0.947.
Using normal model
LBFGS training took [404] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148518
Train loss (w/o reg) on all data: 0.109101
Test loss (w/o reg) on all data: 0.305825
Train acc on all data:  0.972202078801
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 3.65783e-05
Norm of the params: 28.0776
              Random: fixed  69 labels. Loss 0.30583. Accuracy 0.901.
### Flips: 412, rs: 36, checks: 824
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0692467
Train loss (w/o reg) on all data: 0.051472
Test loss (w/o reg) on all data: 0.137865
Train acc on all data:  0.986463620981
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 1.22684e-06
Norm of the params: 18.8546
     Influence (LOO): fixed 317 labels. Loss 0.13786. Accuracy 0.965.
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0440689
Train loss (w/o reg) on all data: 0.0211871
Test loss (w/o reg) on all data: 0.186375
Train acc on all data:  0.999516557892
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 5.70996e-06
Norm of the params: 21.3924
                Loss: fixed 302 labels. Loss 0.18637. Accuracy 0.958.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143851
Train loss (w/o reg) on all data: 0.104638
Test loss (w/o reg) on all data: 0.334491
Train acc on all data:  0.974135847232
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.63273e-05
Norm of the params: 28.0045
              Random: fixed  90 labels. Loss 0.33449. Accuracy 0.900.
### Flips: 412, rs: 36, checks: 1030
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0669985
Train loss (w/o reg) on all data: 0.0500197
Test loss (w/o reg) on all data: 0.133831
Train acc on all data:  0.987430505197
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.07116e-06
Norm of the params: 18.4276
     Influence (LOO): fixed 332 labels. Loss 0.13383. Accuracy 0.967.
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0404741
Train loss (w/o reg) on all data: 0.0193585
Test loss (w/o reg) on all data: 0.216343
Train acc on all data:  0.999516557892
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.00034e-05
Norm of the params: 20.5502
                Loss: fixed 324 labels. Loss 0.21634. Accuracy 0.963.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137964
Train loss (w/o reg) on all data: 0.0999395
Test loss (w/o reg) on all data: 0.325157
Train acc on all data:  0.975586173556
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 1.31382e-05
Norm of the params: 27.577
              Random: fixed 111 labels. Loss 0.32516. Accuracy 0.901.
### Flips: 412, rs: 36, checks: 1236
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0628498
Train loss (w/o reg) on all data: 0.0464527
Test loss (w/o reg) on all data: 0.142647
Train acc on all data:  0.988639110467
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 1.04465e-06
Norm of the params: 18.1092
     Influence (LOO): fixed 346 labels. Loss 0.14265. Accuracy 0.971.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0389697
Train loss (w/o reg) on all data: 0.0186245
Test loss (w/o reg) on all data: 0.200153
Train acc on all data:  0.999516557892
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 2.01551e-05
Norm of the params: 20.1719
                Loss: fixed 334 labels. Loss 0.20015. Accuracy 0.963.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13259
Train loss (w/o reg) on all data: 0.0948401
Test loss (w/o reg) on all data: 0.323173
Train acc on all data:  0.977036499879
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.60692e-05
Norm of the params: 27.4774
              Random: fixed 126 labels. Loss 0.32317. Accuracy 0.910.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156085
Train loss (w/o reg) on all data: 0.115308
Test loss (w/o reg) on all data: 0.40908
Train acc on all data:  0.971476915639
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.56903e-05
Norm of the params: 28.5577
Flipped loss: 0.40908. Accuracy: 0.876
### Flips: 412, rs: 37, checks: 206
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090295
Train loss (w/o reg) on all data: 0.0632689
Test loss (w/o reg) on all data: 0.245238
Train acc on all data:  0.985255015712
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 2.11273e-06
Norm of the params: 23.2491
     Influence (LOO): fixed 171 labels. Loss 0.24524. Accuracy 0.931.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0795991
Train loss (w/o reg) on all data: 0.0424417
Test loss (w/o reg) on all data: 0.351469
Train acc on all data:  0.999033115784
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.50664e-05
Norm of the params: 27.2608
                Loss: fixed 142 labels. Loss 0.35147. Accuracy 0.905.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151806
Train loss (w/o reg) on all data: 0.111704
Test loss (w/o reg) on all data: 0.401616
Train acc on all data:  0.972202078801
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 2.08174e-05
Norm of the params: 28.3205
              Random: fixed  23 labels. Loss 0.40162. Accuracy 0.886.
### Flips: 412, rs: 37, checks: 412
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0771481
Train loss (w/o reg) on all data: 0.0553114
Test loss (w/o reg) on all data: 0.184982
Train acc on all data:  0.985980178874
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 5.55629e-06
Norm of the params: 20.8982
     Influence (LOO): fixed 257 labels. Loss 0.18498. Accuracy 0.949.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0627702
Train loss (w/o reg) on all data: 0.031203
Test loss (w/o reg) on all data: 0.287189
Train acc on all data:  0.999516557892
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 2.75416e-06
Norm of the params: 25.1266
                Loss: fixed 209 labels. Loss 0.28719. Accuracy 0.918.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143258
Train loss (w/o reg) on all data: 0.104716
Test loss (w/o reg) on all data: 0.35367
Train acc on all data:  0.974861010394
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 7.90077e-06
Norm of the params: 27.7643
              Random: fixed  50 labels. Loss 0.35367. Accuracy 0.893.
### Flips: 412, rs: 37, checks: 618
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0697349
Train loss (w/o reg) on all data: 0.0502711
Test loss (w/o reg) on all data: 0.158567
Train acc on all data:  0.987913947305
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 1.36139e-06
Norm of the params: 19.7301
     Influence (LOO): fixed 298 labels. Loss 0.15857. Accuracy 0.960.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0516165
Train loss (w/o reg) on all data: 0.0250207
Test loss (w/o reg) on all data: 0.256256
Train acc on all data:  0.999516557892
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 2.89688e-06
Norm of the params: 23.0633
                Loss: fixed 260 labels. Loss 0.25626. Accuracy 0.938.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13696
Train loss (w/o reg) on all data: 0.0992875
Test loss (w/o reg) on all data: 0.329497
Train acc on all data:  0.977761663041
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 2.02554e-05
Norm of the params: 27.4489
              Random: fixed  69 labels. Loss 0.32950. Accuracy 0.901.
### Flips: 412, rs: 37, checks: 824
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0663339
Train loss (w/o reg) on all data: 0.048075
Test loss (w/o reg) on all data: 0.138415
Train acc on all data:  0.988639110467
Test acc on all data:   0.971014492754
Norm of the mean of gradients: 4.22281e-06
Norm of the params: 19.1096
     Influence (LOO): fixed 319 labels. Loss 0.13842. Accuracy 0.971.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0465316
Train loss (w/o reg) on all data: 0.0224925
Test loss (w/o reg) on all data: 0.238117
Train acc on all data:  0.999516557892
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 2.43896e-06
Norm of the params: 21.9267
                Loss: fixed 279 labels. Loss 0.23812. Accuracy 0.949.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132226
Train loss (w/o reg) on all data: 0.0952773
Test loss (w/o reg) on all data: 0.34116
Train acc on all data:  0.97897026831
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 4.69743e-05
Norm of the params: 27.1843
              Random: fixed  85 labels. Loss 0.34116. Accuracy 0.897.
### Flips: 412, rs: 37, checks: 1030
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0605596
Train loss (w/o reg) on all data: 0.0442577
Test loss (w/o reg) on all data: 0.123214
Train acc on all data:  0.989364273628
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 6.62751e-07
Norm of the params: 18.0565
     Influence (LOO): fixed 340 labels. Loss 0.12321. Accuracy 0.976.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0441256
Train loss (w/o reg) on all data: 0.0212943
Test loss (w/o reg) on all data: 0.221511
Train acc on all data:  0.999516557892
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 5.40238e-06
Norm of the params: 21.3688
                Loss: fixed 297 labels. Loss 0.22151. Accuracy 0.954.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126159
Train loss (w/o reg) on all data: 0.0903193
Test loss (w/o reg) on all data: 0.308951
Train acc on all data:  0.979695431472
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 5.76577e-06
Norm of the params: 26.773
              Random: fixed 105 labels. Loss 0.30895. Accuracy 0.907.
### Flips: 412, rs: 37, checks: 1236
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0579452
Train loss (w/o reg) on all data: 0.0420067
Test loss (w/o reg) on all data: 0.12707
Train acc on all data:  0.990331157844
Test acc on all data:   0.975845410628
Norm of the mean of gradients: 8.78216e-07
Norm of the params: 17.8541
     Influence (LOO): fixed 351 labels. Loss 0.12707. Accuracy 0.976.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0398895
Train loss (w/o reg) on all data: 0.0192184
Test loss (w/o reg) on all data: 0.180167
Train acc on all data:  0.999516557892
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.41436e-06
Norm of the params: 20.3327
                Loss: fixed 324 labels. Loss 0.18017. Accuracy 0.961.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12026
Train loss (w/o reg) on all data: 0.0852869
Test loss (w/o reg) on all data: 0.297326
Train acc on all data:  0.981387478849
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 8.28548e-06
Norm of the params: 26.4474
              Random: fixed 128 labels. Loss 0.29733. Accuracy 0.914.
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156871
Train loss (w/o reg) on all data: 0.116564
Test loss (w/o reg) on all data: 0.522148
Train acc on all data:  0.970026589316
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 3.34978e-05
Norm of the params: 28.3924
Flipped loss: 0.52215. Accuracy: 0.895
### Flips: 412, rs: 38, checks: 206
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883826
Train loss (w/o reg) on all data: 0.0614944
Test loss (w/o reg) on all data: 0.473106
Train acc on all data:  0.986705342035
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 6.44281e-06
Norm of the params: 23.1898
     Influence (LOO): fixed 175 labels. Loss 0.47311. Accuracy 0.934.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079756
Train loss (w/o reg) on all data: 0.0425174
Test loss (w/o reg) on all data: 0.532892
Train acc on all data:  0.99879139473
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 3.09169e-05
Norm of the params: 27.2905
                Loss: fixed 138 labels. Loss 0.53289. Accuracy 0.908.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153777
Train loss (w/o reg) on all data: 0.114072
Test loss (w/o reg) on all data: 0.526605
Train acc on all data:  0.971235194585
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 9.00993e-06
Norm of the params: 28.1797
              Random: fixed  20 labels. Loss 0.52661. Accuracy 0.897.
### Flips: 412, rs: 38, checks: 412
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0756686
Train loss (w/o reg) on all data: 0.0536929
Test loss (w/o reg) on all data: 0.312588
Train acc on all data:  0.987672226251
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 3.55817e-06
Norm of the params: 20.9646
     Influence (LOO): fixed 259 labels. Loss 0.31259. Accuracy 0.954.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0629562
Train loss (w/o reg) on all data: 0.0317832
Test loss (w/o reg) on all data: 0.478533
Train acc on all data:  0.99879139473
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 1.89775e-05
Norm of the params: 24.9692
                Loss: fixed 194 labels. Loss 0.47853. Accuracy 0.914.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148278
Train loss (w/o reg) on all data: 0.109677
Test loss (w/o reg) on all data: 0.571657
Train acc on all data:  0.971960357747
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 2.13771e-05
Norm of the params: 27.7853
              Random: fixed  41 labels. Loss 0.57166. Accuracy 0.894.
### Flips: 412, rs: 38, checks: 618
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0665117
Train loss (w/o reg) on all data: 0.0478793
Test loss (w/o reg) on all data: 0.196173
Train acc on all data:  0.989605994682
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 2.47955e-06
Norm of the params: 19.3041
     Influence (LOO): fixed 305 labels. Loss 0.19617. Accuracy 0.965.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0529412
Train loss (w/o reg) on all data: 0.0260214
Test loss (w/o reg) on all data: 0.397052
Train acc on all data:  0.999033115784
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 1.10365e-05
Norm of the params: 23.2034
                Loss: fixed 238 labels. Loss 0.39705. Accuracy 0.937.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14165
Train loss (w/o reg) on all data: 0.103815
Test loss (w/o reg) on all data: 0.568995
Train acc on all data:  0.974377568286
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 3.18832e-05
Norm of the params: 27.5079
              Random: fixed  64 labels. Loss 0.56900. Accuracy 0.897.
### Flips: 412, rs: 38, checks: 824
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062297
Train loss (w/o reg) on all data: 0.0449671
Test loss (w/o reg) on all data: 0.149145
Train acc on all data:  0.99008943679
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 9.67125e-07
Norm of the params: 18.6171
     Influence (LOO): fixed 326 labels. Loss 0.14915. Accuracy 0.968.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0490137
Train loss (w/o reg) on all data: 0.0239069
Test loss (w/o reg) on all data: 0.359016
Train acc on all data:  0.999274836838
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 4.78607e-06
Norm of the params: 22.4084
                Loss: fixed 263 labels. Loss 0.35902. Accuracy 0.944.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136002
Train loss (w/o reg) on all data: 0.0986725
Test loss (w/o reg) on all data: 0.574334
Train acc on all data:  0.978003384095
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 2.18426e-05
Norm of the params: 27.3237
              Random: fixed  87 labels. Loss 0.57433. Accuracy 0.900.
### Flips: 412, rs: 38, checks: 1030
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0581532
Train loss (w/o reg) on all data: 0.0421702
Test loss (w/o reg) on all data: 0.151652
Train acc on all data:  0.990572878898
Test acc on all data:   0.969082125604
Norm of the mean of gradients: 9.84283e-07
Norm of the params: 17.879
     Influence (LOO): fixed 340 labels. Loss 0.15165. Accuracy 0.969.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0462448
Train loss (w/o reg) on all data: 0.0223797
Test loss (w/o reg) on all data: 0.326599
Train acc on all data:  0.999274836838
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 5.48777e-06
Norm of the params: 21.8472
                Loss: fixed 285 labels. Loss 0.32660. Accuracy 0.947.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132315
Train loss (w/o reg) on all data: 0.0951878
Test loss (w/o reg) on all data: 0.546635
Train acc on all data:  0.978245105149
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 2.58323e-05
Norm of the params: 27.2498
              Random: fixed 102 labels. Loss 0.54664. Accuracy 0.900.
### Flips: 412, rs: 38, checks: 1236
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0539675
Train loss (w/o reg) on all data: 0.038639
Test loss (w/o reg) on all data: 0.1339
Train acc on all data:  0.991298042059
Test acc on all data:   0.971980676329
Norm of the mean of gradients: 3.77157e-06
Norm of the params: 17.5091
     Influence (LOO): fixed 355 labels. Loss 0.13390. Accuracy 0.972.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0398156
Train loss (w/o reg) on all data: 0.0190805
Test loss (w/o reg) on all data: 0.245817
Train acc on all data:  0.999516557892
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 4.42691e-06
Norm of the params: 20.3642
                Loss: fixed 314 labels. Loss 0.24582. Accuracy 0.957.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130414
Train loss (w/o reg) on all data: 0.0938148
Test loss (w/o reg) on all data: 0.534606
Train acc on all data:  0.978486826203
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 6.28454e-06
Norm of the params: 27.055
              Random: fixed 115 labels. Loss 0.53461. Accuracy 0.907.
Using normal model
LBFGS training took [477] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159291
Train loss (w/o reg) on all data: 0.116525
Test loss (w/o reg) on all data: 0.631628
Train acc on all data:  0.968817984046
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 3.34149e-05
Norm of the params: 29.2458
Flipped loss: 0.63163. Accuracy: 0.873
### Flips: 412, rs: 39, checks: 206
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0887831
Train loss (w/o reg) on all data: 0.0616558
Test loss (w/o reg) on all data: 0.475385
Train acc on all data:  0.986705342035
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.12374e-05
Norm of the params: 23.2926
     Influence (LOO): fixed 168 labels. Loss 0.47539. Accuracy 0.941.
Using normal model
LBFGS training took [365] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0821716
Train loss (w/o reg) on all data: 0.0445087
Test loss (w/o reg) on all data: 0.43593
Train acc on all data:  0.997824510515
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.52461e-05
Norm of the params: 27.4455
                Loss: fixed 145 labels. Loss 0.43593. Accuracy 0.913.
Using normal model
LBFGS training took [408] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154594
Train loss (w/o reg) on all data: 0.11268
Test loss (w/o reg) on all data: 0.689731
Train acc on all data:  0.970510031424
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 3.16351e-05
Norm of the params: 28.953
              Random: fixed  16 labels. Loss 0.68973. Accuracy 0.878.
### Flips: 412, rs: 39, checks: 412
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0741223
Train loss (w/o reg) on all data: 0.0521451
Test loss (w/o reg) on all data: 0.40412
Train acc on all data:  0.988639110467
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 1.99156e-06
Norm of the params: 20.9653
     Influence (LOO): fixed 255 labels. Loss 0.40412. Accuracy 0.957.
Using normal model
LBFGS training took [375] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0597806
Train loss (w/o reg) on all data: 0.0302519
Test loss (w/o reg) on all data: 0.36909
Train acc on all data:  0.998307952623
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 9.67109e-06
Norm of the params: 24.3017
                Loss: fixed 210 labels. Loss 0.36909. Accuracy 0.939.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15033
Train loss (w/o reg) on all data: 0.108985
Test loss (w/o reg) on all data: 0.672346
Train acc on all data:  0.971960357747
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 2.28032e-05
Norm of the params: 28.756
              Random: fixed  39 labels. Loss 0.67235. Accuracy 0.889.
### Flips: 412, rs: 39, checks: 618
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0673823
Train loss (w/o reg) on all data: 0.0482367
Test loss (w/o reg) on all data: 0.408344
Train acc on all data:  0.989364273628
Test acc on all data:   0.963285024155
Norm of the mean of gradients: 1.85578e-06
Norm of the params: 19.5681
     Influence (LOO): fixed 296 labels. Loss 0.40834. Accuracy 0.963.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0508093
Train loss (w/o reg) on all data: 0.0250601
Test loss (w/o reg) on all data: 0.314005
Train acc on all data:  0.999033115784
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 2.77694e-05
Norm of the params: 22.6933
                Loss: fixed 257 labels. Loss 0.31401. Accuracy 0.944.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142667
Train loss (w/o reg) on all data: 0.102806
Test loss (w/o reg) on all data: 0.691526
Train acc on all data:  0.973894126178
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.65479e-05
Norm of the params: 28.2353
              Random: fixed  70 labels. Loss 0.69153. Accuracy 0.897.
### Flips: 412, rs: 39, checks: 824
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0648971
Train loss (w/o reg) on all data: 0.0462517
Test loss (w/o reg) on all data: 0.327594
Train acc on all data:  0.989847715736
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 4.28718e-06
Norm of the params: 19.3108
     Influence (LOO): fixed 317 labels. Loss 0.32759. Accuracy 0.968.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0483687
Train loss (w/o reg) on all data: 0.0236589
Test loss (w/o reg) on all data: 0.282562
Train acc on all data:  0.999033115784
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 9.80282e-06
Norm of the params: 22.2305
                Loss: fixed 278 labels. Loss 0.28256. Accuracy 0.956.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136684
Train loss (w/o reg) on all data: 0.0983534
Test loss (w/o reg) on all data: 0.653677
Train acc on all data:  0.978245105149
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.81592e-05
Norm of the params: 27.6878
              Random: fixed  84 labels. Loss 0.65368. Accuracy 0.906.
### Flips: 412, rs: 39, checks: 1030
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0634108
Train loss (w/o reg) on all data: 0.0455633
Test loss (w/o reg) on all data: 0.252091
Train acc on all data:  0.989122552574
Test acc on all data:   0.979710144928
Norm of the mean of gradients: 4.08377e-06
Norm of the params: 18.8931
     Influence (LOO): fixed 328 labels. Loss 0.25209. Accuracy 0.980.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044437
Train loss (w/o reg) on all data: 0.0216694
Test loss (w/o reg) on all data: 0.244226
Train acc on all data:  0.999033115784
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 2.07292e-05
Norm of the params: 21.339
                Loss: fixed 298 labels. Loss 0.24423. Accuracy 0.959.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128474
Train loss (w/o reg) on all data: 0.0917131
Test loss (w/o reg) on all data: 0.655081
Train acc on all data:  0.98017887358
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 3.53959e-05
Norm of the params: 27.1149
              Random: fixed 107 labels. Loss 0.65508. Accuracy 0.926.
### Flips: 412, rs: 39, checks: 1236
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0601505
Train loss (w/o reg) on all data: 0.0435045
Test loss (w/o reg) on all data: 0.189344
Train acc on all data:  0.989605994682
Test acc on all data:   0.980676328502
Norm of the mean of gradients: 1.10945e-06
Norm of the params: 18.2461
     Influence (LOO): fixed 342 labels. Loss 0.18934. Accuracy 0.981.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0423204
Train loss (w/o reg) on all data: 0.020588
Test loss (w/o reg) on all data: 0.252827
Train acc on all data:  0.999274836838
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 6.60229e-06
Norm of the params: 20.8482
                Loss: fixed 312 labels. Loss 0.25283. Accuracy 0.966.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120412
Train loss (w/o reg) on all data: 0.0849908
Test loss (w/o reg) on all data: 0.613783
Train acc on all data:  0.982112642011
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 8.07396e-06
Norm of the params: 26.6163
              Random: fixed 127 labels. Loss 0.61378. Accuracy 0.939.
Using normal model
LBFGS training took [623] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204254
Train loss (w/o reg) on all data: 0.156723
Test loss (w/o reg) on all data: 0.619638
Train acc on all data:  0.955523326082
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 4.7181e-05
Norm of the params: 30.8321
Flipped loss: 0.61964. Accuracy: 0.832
### Flips: 618, rs: 0, checks: 206
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138334
Train loss (w/o reg) on all data: 0.101934
Test loss (w/o reg) on all data: 0.358311
Train acc on all data:  0.970993473532
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.53319e-05
Norm of the params: 26.9816
     Influence (LOO): fixed 173 labels. Loss 0.35831. Accuracy 0.889.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115155
Train loss (w/o reg) on all data: 0.0670682
Test loss (w/o reg) on all data: 0.556768
Train acc on all data:  0.995165578922
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 1.62936e-05
Norm of the params: 31.012
                Loss: fixed 171 labels. Loss 0.55677. Accuracy 0.848.
Using normal model
LBFGS training took [549] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196941
Train loss (w/o reg) on all data: 0.150739
Test loss (w/o reg) on all data: 0.581902
Train acc on all data:  0.954556441866
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 3.54217e-05
Norm of the params: 30.3983
              Random: fixed  40 labels. Loss 0.58190. Accuracy 0.836.
### Flips: 618, rs: 0, checks: 412
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115934
Train loss (w/o reg) on all data: 0.0858286
Test loss (w/o reg) on all data: 0.245784
Train acc on all data:  0.976794778825
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 2.26826e-05
Norm of the params: 24.538
     Influence (LOO): fixed 285 labels. Loss 0.24578. Accuracy 0.926.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0898712
Train loss (w/o reg) on all data: 0.0480141
Test loss (w/o reg) on all data: 0.467043
Train acc on all data:  0.999033115784
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 3.31722e-05
Norm of the params: 28.9334
                Loss: fixed 258 labels. Loss 0.46704. Accuracy 0.884.
Using normal model
LBFGS training took [562] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189937
Train loss (w/o reg) on all data: 0.144618
Test loss (w/o reg) on all data: 0.539509
Train acc on all data:  0.95600676819
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 0.000148571
Norm of the params: 30.1062
              Random: fixed  70 labels. Loss 0.53951. Accuracy 0.856.
### Flips: 618, rs: 0, checks: 618
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105796
Train loss (w/o reg) on all data: 0.079099
Test loss (w/o reg) on all data: 0.27962
Train acc on all data:  0.978245105149
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 2.77814e-05
Norm of the params: 23.1071
     Influence (LOO): fixed 359 labels. Loss 0.27962. Accuracy 0.936.
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0729769
Train loss (w/o reg) on all data: 0.0373789
Test loss (w/o reg) on all data: 0.34479
Train acc on all data:  0.99879139473
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 7.56417e-06
Norm of the params: 26.6826
                Loss: fixed 329 labels. Loss 0.34479. Accuracy 0.909.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186434
Train loss (w/o reg) on all data: 0.141922
Test loss (w/o reg) on all data: 0.53308
Train acc on all data:  0.956731931351
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 0.000182439
Norm of the params: 29.8371
              Random: fixed  92 labels. Loss 0.53308. Accuracy 0.858.
### Flips: 618, rs: 0, checks: 824
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0974955
Train loss (w/o reg) on all data: 0.0732587
Test loss (w/o reg) on all data: 0.240008
Train acc on all data:  0.97897026831
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 9.42751e-06
Norm of the params: 22.0167
     Influence (LOO): fixed 405 labels. Loss 0.24001. Accuracy 0.956.
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0616319
Train loss (w/o reg) on all data: 0.0306852
Test loss (w/o reg) on all data: 0.378886
Train acc on all data:  0.999516557892
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 1.86139e-05
Norm of the params: 24.8784
                Loss: fixed 380 labels. Loss 0.37889. Accuracy 0.925.
Using normal model
LBFGS training took [564] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179406
Train loss (w/o reg) on all data: 0.135994
Test loss (w/o reg) on all data: 0.566635
Train acc on all data:  0.961566352429
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 5.92817e-05
Norm of the params: 29.4659
              Random: fixed 127 labels. Loss 0.56664. Accuracy 0.871.
### Flips: 618, rs: 0, checks: 1030
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0899187
Train loss (w/o reg) on all data: 0.067542
Test loss (w/o reg) on all data: 0.296717
Train acc on all data:  0.981145757796
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 3.756e-06
Norm of the params: 21.155
     Influence (LOO): fixed 448 labels. Loss 0.29672. Accuracy 0.958.
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0576996
Train loss (w/o reg) on all data: 0.0283929
Test loss (w/o reg) on all data: 0.331087
Train acc on all data:  0.999516557892
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 3.2392e-05
Norm of the params: 24.2102
                Loss: fixed 407 labels. Loss 0.33109. Accuracy 0.929.
Using normal model
LBFGS training took [522] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175275
Train loss (w/o reg) on all data: 0.132598
Test loss (w/o reg) on all data: 0.550154
Train acc on all data:  0.963016678753
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 7.13498e-05
Norm of the params: 29.2156
              Random: fixed 152 labels. Loss 0.55015. Accuracy 0.883.
### Flips: 618, rs: 0, checks: 1236
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0855639
Train loss (w/o reg) on all data: 0.0641222
Test loss (w/o reg) on all data: 0.213782
Train acc on all data:  0.982354363065
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 7.41415e-06
Norm of the params: 20.7083
     Influence (LOO): fixed 476 labels. Loss 0.21378. Accuracy 0.958.
Using normal model
LBFGS training took [358] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0520794
Train loss (w/o reg) on all data: 0.0254022
Test loss (w/o reg) on all data: 0.291352
Train acc on all data:  0.999516557892
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 8.83122e-06
Norm of the params: 23.0986
                Loss: fixed 439 labels. Loss 0.29135. Accuracy 0.943.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.168163
Train loss (w/o reg) on all data: 0.126755
Test loss (w/o reg) on all data: 0.484066
Train acc on all data:  0.96470872613
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 2.01002e-05
Norm of the params: 28.7778
              Random: fixed 182 labels. Loss 0.48407. Accuracy 0.884.
Using normal model
LBFGS training took [595] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205574
Train loss (w/o reg) on all data: 0.157494
Test loss (w/o reg) on all data: 0.685957
Train acc on all data:  0.954072999758
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.011e-05
Norm of the params: 31.0095
Flipped loss: 0.68596. Accuracy: 0.862
### Flips: 618, rs: 1, checks: 206
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135871
Train loss (w/o reg) on all data: 0.0995824
Test loss (w/o reg) on all data: 0.575312
Train acc on all data:  0.974377568286
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 4.36188e-05
Norm of the params: 26.9403
     Influence (LOO): fixed 164 labels. Loss 0.57531. Accuracy 0.893.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110151
Train loss (w/o reg) on all data: 0.0639212
Test loss (w/o reg) on all data: 0.633305
Train acc on all data:  0.995890742084
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 3.08481e-05
Norm of the params: 30.4072
                Loss: fixed 170 labels. Loss 0.63330. Accuracy 0.879.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200044
Train loss (w/o reg) on all data: 0.152364
Test loss (w/o reg) on all data: 0.643606
Train acc on all data:  0.955765047136
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 5.022e-05
Norm of the params: 30.8803
              Random: fixed  29 labels. Loss 0.64361. Accuracy 0.875.
### Flips: 618, rs: 1, checks: 412
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116514
Train loss (w/o reg) on all data: 0.0861236
Test loss (w/o reg) on all data: 0.406516
Train acc on all data:  0.977519941987
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 5.40159e-06
Norm of the params: 24.6536
     Influence (LOO): fixed 283 labels. Loss 0.40652. Accuracy 0.927.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0847263
Train loss (w/o reg) on all data: 0.044901
Test loss (w/o reg) on all data: 0.519894
Train acc on all data:  0.999274836838
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 4.15618e-05
Norm of the params: 28.2224
                Loss: fixed 254 labels. Loss 0.51989. Accuracy 0.900.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192469
Train loss (w/o reg) on all data: 0.145793
Test loss (w/o reg) on all data: 0.593162
Train acc on all data:  0.961082910321
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 1.64045e-05
Norm of the params: 30.5533
              Random: fixed  66 labels. Loss 0.59316. Accuracy 0.887.
### Flips: 618, rs: 1, checks: 618
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105986
Train loss (w/o reg) on all data: 0.0789432
Test loss (w/o reg) on all data: 0.37994
Train acc on all data:  0.979211989364
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 8.99244e-06
Norm of the params: 23.2561
     Influence (LOO): fixed 356 labels. Loss 0.37994. Accuracy 0.938.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0745653
Train loss (w/o reg) on all data: 0.038553
Test loss (w/o reg) on all data: 0.414925
Train acc on all data:  0.999274836838
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.64488e-05
Norm of the params: 26.8374
                Loss: fixed 306 labels. Loss 0.41493. Accuracy 0.913.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180403
Train loss (w/o reg) on all data: 0.134852
Test loss (w/o reg) on all data: 0.616276
Train acc on all data:  0.96470872613
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 5.5681e-05
Norm of the params: 30.1833
              Random: fixed 103 labels. Loss 0.61628. Accuracy 0.892.
### Flips: 618, rs: 1, checks: 824
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0964349
Train loss (w/o reg) on all data: 0.0726363
Test loss (w/o reg) on all data: 0.329673
Train acc on all data:  0.980662315688
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 5.91236e-06
Norm of the params: 21.8168
     Influence (LOO): fixed 419 labels. Loss 0.32967. Accuracy 0.957.
Using normal model
LBFGS training took [355] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0653551
Train loss (w/o reg) on all data: 0.0330567
Test loss (w/o reg) on all data: 0.346587
Train acc on all data:  0.999758278946
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 1.70446e-05
Norm of the params: 25.4159
                Loss: fixed 344 labels. Loss 0.34659. Accuracy 0.917.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175097
Train loss (w/o reg) on all data: 0.130566
Test loss (w/o reg) on all data: 0.558386
Train acc on all data:  0.965433889292
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 8.50409e-05
Norm of the params: 29.8432
              Random: fixed 132 labels. Loss 0.55839. Accuracy 0.891.
### Flips: 618, rs: 1, checks: 1030
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0882048
Train loss (w/o reg) on all data: 0.0668777
Test loss (w/o reg) on all data: 0.409043
Train acc on all data:  0.982112642011
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 1.02504e-05
Norm of the params: 20.6529
     Influence (LOO): fixed 457 labels. Loss 0.40904. Accuracy 0.957.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0611552
Train loss (w/o reg) on all data: 0.0305315
Test loss (w/o reg) on all data: 0.382054
Train acc on all data:  0.999758278946
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 8.75232e-06
Norm of the params: 24.7482
                Loss: fixed 375 labels. Loss 0.38205. Accuracy 0.924.
Using normal model
LBFGS training took [512] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166027
Train loss (w/o reg) on all data: 0.122604
Test loss (w/o reg) on all data: 0.566931
Train acc on all data:  0.969543147208
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 9.11035e-05
Norm of the params: 29.4695
              Random: fixed 168 labels. Loss 0.56693. Accuracy 0.903.
### Flips: 618, rs: 1, checks: 1236
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0833468
Train loss (w/o reg) on all data: 0.0632863
Test loss (w/o reg) on all data: 0.339746
Train acc on all data:  0.982596084119
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 1.02669e-05
Norm of the params: 20.0302
     Influence (LOO): fixed 478 labels. Loss 0.33975. Accuracy 0.967.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0567998
Train loss (w/o reg) on all data: 0.0281158
Test loss (w/o reg) on all data: 0.337165
Train acc on all data:  0.999516557892
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 7.45582e-06
Norm of the params: 23.9516
                Loss: fixed 405 labels. Loss 0.33717. Accuracy 0.929.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158774
Train loss (w/o reg) on all data: 0.116545
Test loss (w/o reg) on all data: 0.541427
Train acc on all data:  0.972202078801
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 2.80614e-05
Norm of the params: 29.0619
              Random: fixed 192 labels. Loss 0.54143. Accuracy 0.904.
Using normal model
LBFGS training took [549] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203952
Train loss (w/o reg) on all data: 0.155905
Test loss (w/o reg) on all data: 0.582648
Train acc on all data:  0.948271694465
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 3.45618e-05
Norm of the params: 30.9991
Flipped loss: 0.58265. Accuracy: 0.859
### Flips: 618, rs: 2, checks: 206
Using normal model
LBFGS training took [415] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13888
Train loss (w/o reg) on all data: 0.102717
Test loss (w/o reg) on all data: 0.389892
Train acc on all data:  0.97026831037
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 3.82664e-05
Norm of the params: 26.8934
     Influence (LOO): fixed 167 labels. Loss 0.38989. Accuracy 0.902.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114347
Train loss (w/o reg) on all data: 0.0676116
Test loss (w/o reg) on all data: 0.49437
Train acc on all data:  0.993231810491
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 5.11218e-05
Norm of the params: 30.5729
                Loss: fixed 180 labels. Loss 0.49437. Accuracy 0.866.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197165
Train loss (w/o reg) on all data: 0.14969
Test loss (w/o reg) on all data: 0.570349
Train acc on all data:  0.952622673435
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 5.67204e-05
Norm of the params: 30.814
              Random: fixed  34 labels. Loss 0.57035. Accuracy 0.870.
### Flips: 618, rs: 2, checks: 412
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120659
Train loss (w/o reg) on all data: 0.0908557
Test loss (w/o reg) on all data: 0.328344
Train acc on all data:  0.972685520909
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 7.79982e-06
Norm of the params: 24.4146
     Influence (LOO): fixed 288 labels. Loss 0.32834. Accuracy 0.929.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888562
Train loss (w/o reg) on all data: 0.0476785
Test loss (w/o reg) on all data: 0.373434
Train acc on all data:  0.999274836838
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 3.88709e-05
Norm of the params: 28.6976
                Loss: fixed 257 labels. Loss 0.37343. Accuracy 0.893.
Using normal model
LBFGS training took [528] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191116
Train loss (w/o reg) on all data: 0.144675
Test loss (w/o reg) on all data: 0.577694
Train acc on all data:  0.954556441866
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 5.50512e-05
Norm of the params: 30.4765
              Random: fixed  65 labels. Loss 0.57769. Accuracy 0.858.
### Flips: 618, rs: 2, checks: 618
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1113
Train loss (w/o reg) on all data: 0.0848488
Test loss (w/o reg) on all data: 0.209317
Train acc on all data:  0.973894126178
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 9.56374e-06
Norm of the params: 23.0005
     Influence (LOO): fixed 352 labels. Loss 0.20932. Accuracy 0.946.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0747768
Train loss (w/o reg) on all data: 0.0383621
Test loss (w/o reg) on all data: 0.381957
Train acc on all data:  0.999516557892
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 2.94745e-05
Norm of the params: 26.9869
                Loss: fixed 316 labels. Loss 0.38196. Accuracy 0.901.
Using normal model
LBFGS training took [521] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184994
Train loss (w/o reg) on all data: 0.139618
Test loss (w/o reg) on all data: 0.520434
Train acc on all data:  0.957940536621
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 1.5771e-05
Norm of the params: 30.1248
              Random: fixed  96 labels. Loss 0.52043. Accuracy 0.870.
### Flips: 618, rs: 2, checks: 824
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104286
Train loss (w/o reg) on all data: 0.0793093
Test loss (w/o reg) on all data: 0.306051
Train acc on all data:  0.97582789461
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 6.3843e-06
Norm of the params: 22.3505
     Influence (LOO): fixed 397 labels. Loss 0.30605. Accuracy 0.956.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0659321
Train loss (w/o reg) on all data: 0.0331968
Test loss (w/o reg) on all data: 0.33626
Train acc on all data:  0.999516557892
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.11106e-05
Norm of the params: 25.5872
                Loss: fixed 366 labels. Loss 0.33626. Accuracy 0.913.
Using normal model
LBFGS training took [544] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178059
Train loss (w/o reg) on all data: 0.133566
Test loss (w/o reg) on all data: 0.483357
Train acc on all data:  0.96470872613
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 2.31987e-05
Norm of the params: 29.8304
              Random: fixed 130 labels. Loss 0.48336. Accuracy 0.885.
### Flips: 618, rs: 2, checks: 1030
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0961919
Train loss (w/o reg) on all data: 0.0734763
Test loss (w/o reg) on all data: 0.300143
Train acc on all data:  0.977519941987
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 2.18415e-05
Norm of the params: 21.3146
     Influence (LOO): fixed 432 labels. Loss 0.30014. Accuracy 0.954.
Using normal model
LBFGS training took [492] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0612024
Train loss (w/o reg) on all data: 0.0304809
Test loss (w/o reg) on all data: 0.285693
Train acc on all data:  0.999516557892
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 7.51185e-05
Norm of the params: 24.7877
                Loss: fixed 399 labels. Loss 0.28569. Accuracy 0.928.
Using normal model
LBFGS training took [535] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169717
Train loss (w/o reg) on all data: 0.126134
Test loss (w/o reg) on all data: 0.511907
Train acc on all data:  0.967367657723
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 6.60132e-05
Norm of the params: 29.5241
              Random: fixed 161 labels. Loss 0.51191. Accuracy 0.889.
### Flips: 618, rs: 2, checks: 1236
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0885521
Train loss (w/o reg) on all data: 0.0678869
Test loss (w/o reg) on all data: 0.262788
Train acc on all data:  0.978245105149
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 2.81528e-06
Norm of the params: 20.3299
     Influence (LOO): fixed 461 labels. Loss 0.26279. Accuracy 0.964.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0526885
Train loss (w/o reg) on all data: 0.0256951
Test loss (w/o reg) on all data: 0.288041
Train acc on all data:  0.999516557892
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 1.23665e-05
Norm of the params: 23.2351
                Loss: fixed 440 labels. Loss 0.28804. Accuracy 0.937.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16132
Train loss (w/o reg) on all data: 0.11934
Test loss (w/o reg) on all data: 0.553632
Train acc on all data:  0.968817984046
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 7.09956e-05
Norm of the params: 28.9757
              Random: fixed 194 labels. Loss 0.55363. Accuracy 0.893.
Using normal model
LBFGS training took [536] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210308
Train loss (w/o reg) on all data: 0.162596
Test loss (w/o reg) on all data: 0.594713
Train acc on all data:  0.949722020788
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 6.72439e-05
Norm of the params: 30.891
Flipped loss: 0.59471. Accuracy: 0.827
### Flips: 618, rs: 3, checks: 206
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145474
Train loss (w/o reg) on all data: 0.109167
Test loss (w/o reg) on all data: 0.530867
Train acc on all data:  0.970026589316
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 2.83285e-05
Norm of the params: 26.947
     Influence (LOO): fixed 166 labels. Loss 0.53087. Accuracy 0.891.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121161
Train loss (w/o reg) on all data: 0.0721869
Test loss (w/o reg) on all data: 0.607998
Train acc on all data:  0.989847715736
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.441e-05
Norm of the params: 31.2967
                Loss: fixed 174 labels. Loss 0.60800. Accuracy 0.843.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201194
Train loss (w/o reg) on all data: 0.154489
Test loss (w/o reg) on all data: 0.55402
Train acc on all data:  0.954556441866
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 6.09605e-05
Norm of the params: 30.5629
              Random: fixed  34 labels. Loss 0.55402. Accuracy 0.840.
### Flips: 618, rs: 3, checks: 412
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123587
Train loss (w/o reg) on all data: 0.094523
Test loss (w/o reg) on all data: 0.404862
Train acc on all data:  0.972927241963
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 7.24847e-06
Norm of the params: 24.1097
     Influence (LOO): fixed 288 labels. Loss 0.40486. Accuracy 0.925.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0937068
Train loss (w/o reg) on all data: 0.0507575
Test loss (w/o reg) on all data: 0.593454
Train acc on all data:  0.997824510515
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 2.68056e-05
Norm of the params: 29.3085
                Loss: fixed 261 labels. Loss 0.59345. Accuracy 0.878.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194755
Train loss (w/o reg) on all data: 0.148964
Test loss (w/o reg) on all data: 0.540636
Train acc on all data:  0.957457094513
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 5.78935e-05
Norm of the params: 30.2624
              Random: fixed  68 labels. Loss 0.54064. Accuracy 0.845.
### Flips: 618, rs: 3, checks: 618
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114231
Train loss (w/o reg) on all data: 0.0883975
Test loss (w/o reg) on all data: 0.352246
Train acc on all data:  0.97461928934
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 7.50568e-06
Norm of the params: 22.7303
     Influence (LOO): fixed 352 labels. Loss 0.35225. Accuracy 0.938.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0767229
Train loss (w/o reg) on all data: 0.0396477
Test loss (w/o reg) on all data: 0.459751
Train acc on all data:  0.999033115784
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 6.65436e-06
Norm of the params: 27.2306
                Loss: fixed 320 labels. Loss 0.45975. Accuracy 0.899.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186822
Train loss (w/o reg) on all data: 0.141679
Test loss (w/o reg) on all data: 0.555104
Train acc on all data:  0.960599468214
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 1.40688e-05
Norm of the params: 30.0475
              Random: fixed  99 labels. Loss 0.55510. Accuracy 0.850.
### Flips: 618, rs: 3, checks: 824
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104616
Train loss (w/o reg) on all data: 0.0811388
Test loss (w/o reg) on all data: 0.304957
Train acc on all data:  0.976311336717
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 4.54874e-06
Norm of the params: 21.6692
     Influence (LOO): fixed 399 labels. Loss 0.30496. Accuracy 0.945.
Using normal model
LBFGS training took [365] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0713854
Train loss (w/o reg) on all data: 0.0365431
Test loss (w/o reg) on all data: 0.379434
Train acc on all data:  0.999033115784
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 8.45385e-06
Norm of the params: 26.3978
                Loss: fixed 352 labels. Loss 0.37943. Accuracy 0.903.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179134
Train loss (w/o reg) on all data: 0.13472
Test loss (w/o reg) on all data: 0.469044
Train acc on all data:  0.962533236645
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 4.72522e-05
Norm of the params: 29.8039
              Random: fixed 130 labels. Loss 0.46904. Accuracy 0.860.
### Flips: 618, rs: 3, checks: 1030
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0987272
Train loss (w/o reg) on all data: 0.0769004
Test loss (w/o reg) on all data: 0.282425
Train acc on all data:  0.976311336717
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.88158e-06
Norm of the params: 20.8934
     Influence (LOO): fixed 437 labels. Loss 0.28242. Accuracy 0.952.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0636226
Train loss (w/o reg) on all data: 0.0317848
Test loss (w/o reg) on all data: 0.359165
Train acc on all data:  0.999274836838
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 2.00458e-05
Norm of the params: 25.234
                Loss: fixed 389 labels. Loss 0.35917. Accuracy 0.914.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.170551
Train loss (w/o reg) on all data: 0.127711
Test loss (w/o reg) on all data: 0.412832
Train acc on all data:  0.963016678753
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 2.04465e-05
Norm of the params: 29.2711
              Random: fixed 164 labels. Loss 0.41283. Accuracy 0.868.
### Flips: 618, rs: 3, checks: 1236
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0920041
Train loss (w/o reg) on all data: 0.0722951
Test loss (w/o reg) on all data: 0.214139
Train acc on all data:  0.977761663041
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 5.90769e-06
Norm of the params: 19.854
     Influence (LOO): fixed 467 labels. Loss 0.21414. Accuracy 0.959.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0560587
Train loss (w/o reg) on all data: 0.0277584
Test loss (w/o reg) on all data: 0.268847
Train acc on all data:  0.999274836838
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.11897e-05
Norm of the params: 23.7909
                Loss: fixed 426 labels. Loss 0.26885. Accuracy 0.929.
Using normal model
LBFGS training took [523] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163428
Train loss (w/o reg) on all data: 0.121833
Test loss (w/o reg) on all data: 0.402472
Train acc on all data:  0.968576262993
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.78966e-05
Norm of the params: 28.8425
              Random: fixed 196 labels. Loss 0.40247. Accuracy 0.876.
Using normal model
LBFGS training took [457] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204227
Train loss (w/o reg) on all data: 0.158243
Test loss (w/o reg) on all data: 0.574324
Train acc on all data:  0.949722020788
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 2.71687e-05
Norm of the params: 30.3265
Flipped loss: 0.57432. Accuracy: 0.849
### Flips: 618, rs: 4, checks: 206
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13666
Train loss (w/o reg) on all data: 0.101445
Test loss (w/o reg) on all data: 0.490986
Train acc on all data:  0.972202078801
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 2.65845e-05
Norm of the params: 26.5384
     Influence (LOO): fixed 172 labels. Loss 0.49099. Accuracy 0.893.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116492
Train loss (w/o reg) on all data: 0.0692759
Test loss (w/o reg) on all data: 0.635087
Train acc on all data:  0.993231810491
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 8.27046e-06
Norm of the params: 30.7297
                Loss: fixed 168 labels. Loss 0.63509. Accuracy 0.869.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197138
Train loss (w/o reg) on all data: 0.151979
Test loss (w/o reg) on all data: 0.567463
Train acc on all data:  0.952622673435
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.08181e-05
Norm of the params: 30.053
              Random: fixed  28 labels. Loss 0.56746. Accuracy 0.862.
### Flips: 618, rs: 4, checks: 412
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115459
Train loss (w/o reg) on all data: 0.0859569
Test loss (w/o reg) on all data: 0.483344
Train acc on all data:  0.977036499879
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 4.77092e-06
Norm of the params: 24.2907
     Influence (LOO): fixed 293 labels. Loss 0.48334. Accuracy 0.921.
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0891962
Train loss (w/o reg) on all data: 0.048071
Test loss (w/o reg) on all data: 0.607265
Train acc on all data:  0.998307952623
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 2.18184e-05
Norm of the params: 28.6793
                Loss: fixed 252 labels. Loss 0.60726. Accuracy 0.890.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190058
Train loss (w/o reg) on all data: 0.145971
Test loss (w/o reg) on all data: 0.583062
Train acc on all data:  0.955281605028
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 1.67827e-05
Norm of the params: 29.6942
              Random: fixed  61 labels. Loss 0.58306. Accuracy 0.866.
### Flips: 618, rs: 4, checks: 618
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106703
Train loss (w/o reg) on all data: 0.0804185
Test loss (w/o reg) on all data: 0.400735
Train acc on all data:  0.97897026831
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 3.97935e-06
Norm of the params: 22.9279
     Influence (LOO): fixed 360 labels. Loss 0.40074. Accuracy 0.942.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0739424
Train loss (w/o reg) on all data: 0.0380569
Test loss (w/o reg) on all data: 0.499725
Train acc on all data:  0.998066231569
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 3.1068e-05
Norm of the params: 26.7901
                Loss: fixed 317 labels. Loss 0.49972. Accuracy 0.901.
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187402
Train loss (w/o reg) on all data: 0.143888
Test loss (w/o reg) on all data: 0.576401
Train acc on all data:  0.955523326082
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 3.89625e-05
Norm of the params: 29.5003
              Random: fixed  75 labels. Loss 0.57640. Accuracy 0.871.
### Flips: 618, rs: 4, checks: 824
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0996851
Train loss (w/o reg) on all data: 0.0750508
Test loss (w/o reg) on all data: 0.396704
Train acc on all data:  0.979937152526
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 8.1353e-06
Norm of the params: 22.1965
     Influence (LOO): fixed 406 labels. Loss 0.39670. Accuracy 0.943.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064282
Train loss (w/o reg) on all data: 0.0323627
Test loss (w/o reg) on all data: 0.419043
Train acc on all data:  0.99879139473
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 1.29008e-05
Norm of the params: 25.2663
                Loss: fixed 357 labels. Loss 0.41904. Accuracy 0.914.
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183274
Train loss (w/o reg) on all data: 0.139986
Test loss (w/o reg) on all data: 0.586718
Train acc on all data:  0.956973652405
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 9.90555e-06
Norm of the params: 29.4237
              Random: fixed  94 labels. Loss 0.58672. Accuracy 0.864.
### Flips: 618, rs: 4, checks: 1030
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092199
Train loss (w/o reg) on all data: 0.0700647
Test loss (w/o reg) on all data: 0.377338
Train acc on all data:  0.980662315688
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 5.09795e-06
Norm of the params: 21.0401
     Influence (LOO): fixed 448 labels. Loss 0.37734. Accuracy 0.954.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0600853
Train loss (w/o reg) on all data: 0.0297957
Test loss (w/o reg) on all data: 0.371895
Train acc on all data:  0.999516557892
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 9.42372e-06
Norm of the params: 24.6129
                Loss: fixed 383 labels. Loss 0.37190. Accuracy 0.919.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178695
Train loss (w/o reg) on all data: 0.136321
Test loss (w/o reg) on all data: 0.579018
Train acc on all data:  0.957940536621
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 2.29288e-05
Norm of the params: 29.1115
              Random: fixed 115 labels. Loss 0.57902. Accuracy 0.864.
### Flips: 618, rs: 4, checks: 1236
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0843817
Train loss (w/o reg) on all data: 0.0648135
Test loss (w/o reg) on all data: 0.361844
Train acc on all data:  0.981870920957
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 7.57661e-06
Norm of the params: 19.7829
     Influence (LOO): fixed 482 labels. Loss 0.36184. Accuracy 0.962.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0558826
Train loss (w/o reg) on all data: 0.0275545
Test loss (w/o reg) on all data: 0.351525
Train acc on all data:  0.999516557892
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 2.697e-05
Norm of the params: 23.8026
                Loss: fixed 412 labels. Loss 0.35152. Accuracy 0.929.
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171764
Train loss (w/o reg) on all data: 0.130133
Test loss (w/o reg) on all data: 0.539131
Train acc on all data:  0.960841189268
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 3.00889e-05
Norm of the params: 28.8549
              Random: fixed 149 labels. Loss 0.53913. Accuracy 0.867.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2094
Train loss (w/o reg) on all data: 0.161094
Test loss (w/o reg) on all data: 0.585092
Train acc on all data:  0.949963741842
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 2.74359e-05
Norm of the params: 31.0824
Flipped loss: 0.58509. Accuracy: 0.831
### Flips: 618, rs: 5, checks: 206
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141995
Train loss (w/o reg) on all data: 0.105321
Test loss (w/o reg) on all data: 0.368164
Train acc on all data:  0.970026589316
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 2.36171e-05
Norm of the params: 27.0831
     Influence (LOO): fixed 168 labels. Loss 0.36816. Accuracy 0.890.
Using normal model
LBFGS training took [408] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12054
Train loss (w/o reg) on all data: 0.07291
Test loss (w/o reg) on all data: 0.646005
Train acc on all data:  0.989847715736
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 1.21309e-05
Norm of the params: 30.8643
                Loss: fixed 177 labels. Loss 0.64601. Accuracy 0.838.
Using normal model
LBFGS training took [370] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20443
Train loss (w/o reg) on all data: 0.157285
Test loss (w/o reg) on all data: 0.556212
Train acc on all data:  0.950930626058
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 4.44466e-05
Norm of the params: 30.7068
              Random: fixed  30 labels. Loss 0.55621. Accuracy 0.836.
### Flips: 618, rs: 5, checks: 412
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121206
Train loss (w/o reg) on all data: 0.0910504
Test loss (w/o reg) on all data: 0.265894
Train acc on all data:  0.972927241963
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 5.25477e-05
Norm of the params: 24.5582
     Influence (LOO): fixed 283 labels. Loss 0.26589. Accuracy 0.923.
Using normal model
LBFGS training took [411] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08903
Train loss (w/o reg) on all data: 0.0484161
Test loss (w/o reg) on all data: 0.550194
Train acc on all data:  0.997341068407
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.04014e-05
Norm of the params: 28.5005
                Loss: fixed 263 labels. Loss 0.55019. Accuracy 0.879.
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196041
Train loss (w/o reg) on all data: 0.150046
Test loss (w/o reg) on all data: 0.565942
Train acc on all data:  0.952864394489
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 3.08789e-05
Norm of the params: 30.3297
              Random: fixed  61 labels. Loss 0.56594. Accuracy 0.836.
### Flips: 618, rs: 5, checks: 618
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110753
Train loss (w/o reg) on all data: 0.0838
Test loss (w/o reg) on all data: 0.208154
Train acc on all data:  0.975344452502
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 6.13996e-06
Norm of the params: 23.2179
     Influence (LOO): fixed 358 labels. Loss 0.20815. Accuracy 0.933.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0730488
Train loss (w/o reg) on all data: 0.0376405
Test loss (w/o reg) on all data: 0.372627
Train acc on all data:  0.998549673677
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 4.80025e-06
Norm of the params: 26.6114
                Loss: fixed 331 labels. Loss 0.37263. Accuracy 0.907.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18865
Train loss (w/o reg) on all data: 0.143931
Test loss (w/o reg) on all data: 0.487936
Train acc on all data:  0.954556441866
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 3.6938e-05
Norm of the params: 29.9063
              Random: fixed  94 labels. Loss 0.48794. Accuracy 0.852.
### Flips: 618, rs: 5, checks: 824
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10172
Train loss (w/o reg) on all data: 0.077386
Test loss (w/o reg) on all data: 0.189783
Train acc on all data:  0.977519941987
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.66189e-05
Norm of the params: 22.0607
     Influence (LOO): fixed 405 labels. Loss 0.18978. Accuracy 0.945.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0653494
Train loss (w/o reg) on all data: 0.0330058
Test loss (w/o reg) on all data: 0.437653
Train acc on all data:  0.999033115784
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 1.27518e-05
Norm of the params: 25.4337
                Loss: fixed 366 labels. Loss 0.43765. Accuracy 0.911.
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18105
Train loss (w/o reg) on all data: 0.137711
Test loss (w/o reg) on all data: 0.436602
Train acc on all data:  0.956731931351
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 2.7137e-05
Norm of the params: 29.4414
              Random: fixed 127 labels. Loss 0.43660. Accuracy 0.866.
### Flips: 618, rs: 5, checks: 1030
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0933677
Train loss (w/o reg) on all data: 0.071042
Test loss (w/o reg) on all data: 0.200002
Train acc on all data:  0.979211989364
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 1.29521e-05
Norm of the params: 21.1308
     Influence (LOO): fixed 437 labels. Loss 0.20000. Accuracy 0.946.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0585783
Train loss (w/o reg) on all data: 0.028992
Test loss (w/o reg) on all data: 0.410564
Train acc on all data:  0.999274836838
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.80892e-05
Norm of the params: 24.3254
                Loss: fixed 404 labels. Loss 0.41056. Accuracy 0.924.
Using normal model
LBFGS training took [390] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172049
Train loss (w/o reg) on all data: 0.129999
Test loss (w/o reg) on all data: 0.399338
Train acc on all data:  0.962774957699
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 6.3076e-05
Norm of the params: 28.9998
              Random: fixed 168 labels. Loss 0.39934. Accuracy 0.873.
### Flips: 618, rs: 5, checks: 1236
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0890086
Train loss (w/o reg) on all data: 0.0678438
Test loss (w/o reg) on all data: 0.197586
Train acc on all data:  0.980420594634
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 4.11081e-06
Norm of the params: 20.5742
     Influence (LOO): fixed 459 labels. Loss 0.19759. Accuracy 0.949.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0540759
Train loss (w/o reg) on all data: 0.0265516
Test loss (w/o reg) on all data: 0.350935
Train acc on all data:  0.999274836838
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.23117e-05
Norm of the params: 23.4624
                Loss: fixed 428 labels. Loss 0.35093. Accuracy 0.934.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167629
Train loss (w/o reg) on all data: 0.126352
Test loss (w/o reg) on all data: 0.414
Train acc on all data:  0.963500120861
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 2.8095e-05
Norm of the params: 28.7325
              Random: fixed 192 labels. Loss 0.41400. Accuracy 0.874.
Using normal model
LBFGS training took [583] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211206
Train loss (w/o reg) on all data: 0.159937
Test loss (w/o reg) on all data: 0.682733
Train acc on all data:  0.953831278704
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 5.29735e-05
Norm of the params: 32.0216
Flipped loss: 0.68273. Accuracy: 0.835
### Flips: 618, rs: 6, checks: 206
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13999
Train loss (w/o reg) on all data: 0.10178
Test loss (w/o reg) on all data: 0.526242
Train acc on all data:  0.97582789461
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 1.71032e-05
Norm of the params: 27.6443
     Influence (LOO): fixed 173 labels. Loss 0.52624. Accuracy 0.891.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119518
Train loss (w/o reg) on all data: 0.0696592
Test loss (w/o reg) on all data: 0.534928
Train acc on all data:  0.995890742084
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 4.69533e-05
Norm of the params: 31.5782
                Loss: fixed 168 labels. Loss 0.53493. Accuracy 0.865.
Using normal model
LBFGS training took [512] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203317
Train loss (w/o reg) on all data: 0.15334
Test loss (w/o reg) on all data: 0.600063
Train acc on all data:  0.95600676819
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 2.47943e-05
Norm of the params: 31.6153
              Random: fixed  40 labels. Loss 0.60006. Accuracy 0.840.
### Flips: 618, rs: 6, checks: 412
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120566
Train loss (w/o reg) on all data: 0.0882948
Test loss (w/o reg) on all data: 0.397267
Train acc on all data:  0.977761663041
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 1.64362e-05
Norm of the params: 25.4051
     Influence (LOO): fixed 290 labels. Loss 0.39727. Accuracy 0.917.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0937551
Train loss (w/o reg) on all data: 0.0513732
Test loss (w/o reg) on all data: 0.58856
Train acc on all data:  0.998066231569
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 1.86402e-05
Norm of the params: 29.1142
                Loss: fixed 251 labels. Loss 0.58856. Accuracy 0.895.
Using normal model
LBFGS training took [532] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199155
Train loss (w/o reg) on all data: 0.149753
Test loss (w/o reg) on all data: 0.561723
Train acc on all data:  0.956973652405
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 2.0486e-05
Norm of the params: 31.4331
              Random: fixed  66 labels. Loss 0.56172. Accuracy 0.842.
### Flips: 618, rs: 6, checks: 618
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106755
Train loss (w/o reg) on all data: 0.0779112
Test loss (w/o reg) on all data: 0.308608
Train acc on all data:  0.979695431472
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 6.5274e-06
Norm of the params: 24.0183
     Influence (LOO): fixed 356 labels. Loss 0.30861. Accuracy 0.929.
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0744816
Train loss (w/o reg) on all data: 0.0383838
Test loss (w/o reg) on all data: 0.630166
Train acc on all data:  0.999033115784
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 8.71421e-06
Norm of the params: 26.8693
                Loss: fixed 317 labels. Loss 0.63017. Accuracy 0.900.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192346
Train loss (w/o reg) on all data: 0.143847
Test loss (w/o reg) on all data: 0.558891
Train acc on all data:  0.958907420836
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 7.4239e-05
Norm of the params: 31.1445
              Random: fixed  98 labels. Loss 0.55889. Accuracy 0.847.
### Flips: 618, rs: 6, checks: 824
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094726
Train loss (w/o reg) on all data: 0.0692866
Test loss (w/o reg) on all data: 0.216025
Train acc on all data:  0.982837805173
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 6.28214e-06
Norm of the params: 22.5563
     Influence (LOO): fixed 411 labels. Loss 0.21602. Accuracy 0.943.
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0684897
Train loss (w/o reg) on all data: 0.0347607
Test loss (w/o reg) on all data: 0.586914
Train acc on all data:  0.999033115784
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 9.8023e-06
Norm of the params: 25.9727
                Loss: fixed 357 labels. Loss 0.58691. Accuracy 0.908.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187237
Train loss (w/o reg) on all data: 0.139258
Test loss (w/o reg) on all data: 0.571173
Train acc on all data:  0.96035774716
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 4.37273e-05
Norm of the params: 30.977
              Random: fixed 122 labels. Loss 0.57117. Accuracy 0.862.
### Flips: 618, rs: 6, checks: 1030
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090269
Train loss (w/o reg) on all data: 0.0667197
Test loss (w/o reg) on all data: 0.214962
Train acc on all data:  0.982837805173
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 1.70765e-06
Norm of the params: 21.7022
     Influence (LOO): fixed 440 labels. Loss 0.21496. Accuracy 0.949.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0626712
Train loss (w/o reg) on all data: 0.0313028
Test loss (w/o reg) on all data: 0.593555
Train acc on all data:  0.999274836838
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 4.50762e-06
Norm of the params: 25.0473
                Loss: fixed 388 labels. Loss 0.59356. Accuracy 0.919.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179776
Train loss (w/o reg) on all data: 0.13353
Test loss (w/o reg) on all data: 0.530542
Train acc on all data:  0.962533236645
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 4.66584e-05
Norm of the params: 30.4125
              Random: fixed 155 labels. Loss 0.53054. Accuracy 0.873.
### Flips: 618, rs: 6, checks: 1236
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0853968
Train loss (w/o reg) on all data: 0.0633206
Test loss (w/o reg) on all data: 0.243899
Train acc on all data:  0.984046410442
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 1.53831e-06
Norm of the params: 21.0124
     Influence (LOO): fixed 473 labels. Loss 0.24390. Accuracy 0.947.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0572271
Train loss (w/o reg) on all data: 0.0283381
Test loss (w/o reg) on all data: 0.507384
Train acc on all data:  0.999516557892
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 8.04197e-06
Norm of the params: 24.037
                Loss: fixed 415 labels. Loss 0.50738. Accuracy 0.927.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17246
Train loss (w/o reg) on all data: 0.127199
Test loss (w/o reg) on all data: 0.508112
Train acc on all data:  0.963983562968
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 1.70114e-05
Norm of the params: 30.087
              Random: fixed 177 labels. Loss 0.50811. Accuracy 0.883.
Using normal model
LBFGS training took [510] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213516
Train loss (w/o reg) on all data: 0.166522
Test loss (w/o reg) on all data: 0.462909
Train acc on all data:  0.949480299734
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 5.22933e-05
Norm of the params: 30.6575
Flipped loss: 0.46291. Accuracy: 0.858
### Flips: 618, rs: 7, checks: 206
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141348
Train loss (w/o reg) on all data: 0.105978
Test loss (w/o reg) on all data: 0.41927
Train acc on all data:  0.970751752478
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 7.67491e-06
Norm of the params: 26.5971
     Influence (LOO): fixed 173 labels. Loss 0.41927. Accuracy 0.904.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112938
Train loss (w/o reg) on all data: 0.0663888
Test loss (w/o reg) on all data: 0.414916
Train acc on all data:  0.996374184191
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 3.4804e-05
Norm of the params: 30.5121
                Loss: fixed 185 labels. Loss 0.41492. Accuracy 0.867.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206585
Train loss (w/o reg) on all data: 0.160291
Test loss (w/o reg) on all data: 0.500422
Train acc on all data:  0.952380952381
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 3.11133e-05
Norm of the params: 30.4284
              Random: fixed  34 labels. Loss 0.50042. Accuracy 0.861.
### Flips: 618, rs: 7, checks: 412
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12051
Train loss (w/o reg) on all data: 0.0921654
Test loss (w/o reg) on all data: 0.321058
Train acc on all data:  0.973652405124
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.32065e-05
Norm of the params: 23.8096
     Influence (LOO): fixed 288 labels. Loss 0.32106. Accuracy 0.931.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0869456
Train loss (w/o reg) on all data: 0.0469314
Test loss (w/o reg) on all data: 0.427412
Train acc on all data:  0.999033115784
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 3.65649e-05
Norm of the params: 28.2893
                Loss: fixed 269 labels. Loss 0.42741. Accuracy 0.891.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200763
Train loss (w/o reg) on all data: 0.155217
Test loss (w/o reg) on all data: 0.510322
Train acc on all data:  0.954556441866
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 1.52925e-05
Norm of the params: 30.1815
              Random: fixed  59 labels. Loss 0.51032. Accuracy 0.871.
### Flips: 618, rs: 7, checks: 618
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109621
Train loss (w/o reg) on all data: 0.0845249
Test loss (w/o reg) on all data: 0.386194
Train acc on all data:  0.976311336717
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 4.51512e-06
Norm of the params: 22.4037
     Influence (LOO): fixed 360 labels. Loss 0.38619. Accuracy 0.944.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0747996
Train loss (w/o reg) on all data: 0.0389936
Test loss (w/o reg) on all data: 0.304267
Train acc on all data:  0.999033115784
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 2.69971e-05
Norm of the params: 26.7604
                Loss: fixed 324 labels. Loss 0.30427. Accuracy 0.908.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196945
Train loss (w/o reg) on all data: 0.152056
Test loss (w/o reg) on all data: 0.508215
Train acc on all data:  0.95600676819
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 1.45027e-05
Norm of the params: 29.9629
              Random: fixed  82 labels. Loss 0.50821. Accuracy 0.875.
### Flips: 618, rs: 7, checks: 824
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0974725
Train loss (w/o reg) on all data: 0.0758194
Test loss (w/o reg) on all data: 0.227543
Train acc on all data:  0.977278220933
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 3.61872e-06
Norm of the params: 20.8101
     Influence (LOO): fixed 416 labels. Loss 0.22754. Accuracy 0.952.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0676964
Train loss (w/o reg) on all data: 0.0347299
Test loss (w/o reg) on all data: 0.321002
Train acc on all data:  0.999033115784
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 3.72759e-06
Norm of the params: 25.6774
                Loss: fixed 356 labels. Loss 0.32100. Accuracy 0.918.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189515
Train loss (w/o reg) on all data: 0.145671
Test loss (w/o reg) on all data: 0.526913
Train acc on all data:  0.958182257675
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 2.44298e-05
Norm of the params: 29.6121
              Random: fixed 109 labels. Loss 0.52691. Accuracy 0.879.
### Flips: 618, rs: 7, checks: 1030
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0941657
Train loss (w/o reg) on all data: 0.0734036
Test loss (w/o reg) on all data: 0.190776
Train acc on all data:  0.978486826203
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 3.95274e-06
Norm of the params: 20.3775
     Influence (LOO): fixed 441 labels. Loss 0.19078. Accuracy 0.956.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0600259
Train loss (w/o reg) on all data: 0.0301927
Test loss (w/o reg) on all data: 0.302912
Train acc on all data:  0.999033115784
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 2.41271e-05
Norm of the params: 24.4267
                Loss: fixed 394 labels. Loss 0.30291. Accuracy 0.919.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182781
Train loss (w/o reg) on all data: 0.139865
Test loss (w/o reg) on all data: 0.472162
Train acc on all data:  0.961324631375
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 3.57595e-05
Norm of the params: 29.2974
              Random: fixed 136 labels. Loss 0.47216. Accuracy 0.898.
### Flips: 618, rs: 7, checks: 1236
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0878171
Train loss (w/o reg) on all data: 0.0682606
Test loss (w/o reg) on all data: 0.188898
Train acc on all data:  0.97897026831
Test acc on all data:   0.96231884058
Norm of the mean of gradients: 4.00489e-06
Norm of the params: 19.777
     Influence (LOO): fixed 472 labels. Loss 0.18890. Accuracy 0.962.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0526376
Train loss (w/o reg) on all data: 0.0260166
Test loss (w/o reg) on all data: 0.232906
Train acc on all data:  0.999516557892
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 5.52661e-06
Norm of the params: 23.0742
                Loss: fixed 434 labels. Loss 0.23291. Accuracy 0.934.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17556
Train loss (w/o reg) on all data: 0.133761
Test loss (w/o reg) on all data: 0.461123
Train acc on all data:  0.964225284022
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 1.56569e-05
Norm of the params: 28.9132
              Random: fixed 171 labels. Loss 0.46112. Accuracy 0.895.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208517
Train loss (w/o reg) on all data: 0.158717
Test loss (w/o reg) on all data: 0.671398
Train acc on all data:  0.951655789219
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 8.53415e-05
Norm of the params: 31.5597
Flipped loss: 0.67140. Accuracy: 0.843
### Flips: 618, rs: 8, checks: 206
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136419
Train loss (w/o reg) on all data: 0.100518
Test loss (w/o reg) on all data: 0.394277
Train acc on all data:  0.973168963017
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 7.5705e-06
Norm of the params: 26.7958
     Influence (LOO): fixed 173 labels. Loss 0.39428. Accuracy 0.888.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117056
Train loss (w/o reg) on all data: 0.069071
Test loss (w/o reg) on all data: 0.635504
Train acc on all data:  0.994682136814
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 2.14065e-05
Norm of the params: 30.979
                Loss: fixed 173 labels. Loss 0.63550. Accuracy 0.861.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203005
Train loss (w/o reg) on all data: 0.153981
Test loss (w/o reg) on all data: 0.654851
Train acc on all data:  0.953106115543
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 1.48089e-05
Norm of the params: 31.3125
              Random: fixed  34 labels. Loss 0.65485. Accuracy 0.857.
### Flips: 618, rs: 8, checks: 412
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117453
Train loss (w/o reg) on all data: 0.0872033
Test loss (w/o reg) on all data: 0.335941
Train acc on all data:  0.976553057771
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 4.7098e-06
Norm of the params: 24.5966
     Influence (LOO): fixed 279 labels. Loss 0.33594. Accuracy 0.919.
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0895493
Train loss (w/o reg) on all data: 0.0479181
Test loss (w/o reg) on all data: 0.466523
Train acc on all data:  0.999033115784
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 1.73979e-05
Norm of the params: 28.8552
                Loss: fixed 260 labels. Loss 0.46652. Accuracy 0.886.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195886
Train loss (w/o reg) on all data: 0.148227
Test loss (w/o reg) on all data: 0.700109
Train acc on all data:  0.955523326082
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 5.76478e-05
Norm of the params: 30.8734
              Random: fixed  68 labels. Loss 0.70011. Accuracy 0.852.
### Flips: 618, rs: 8, checks: 618
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107817
Train loss (w/o reg) on all data: 0.0803148
Test loss (w/o reg) on all data: 0.254491
Train acc on all data:  0.97897026831
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 2.99011e-06
Norm of the params: 23.4531
     Influence (LOO): fixed 347 labels. Loss 0.25449. Accuracy 0.928.
Using normal model
LBFGS training took [351] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0764723
Train loss (w/o reg) on all data: 0.039866
Test loss (w/o reg) on all data: 0.430138
Train acc on all data:  0.999033115784
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.12952e-05
Norm of the params: 27.0578
                Loss: fixed 315 labels. Loss 0.43014. Accuracy 0.897.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188669
Train loss (w/o reg) on all data: 0.142034
Test loss (w/o reg) on all data: 0.741312
Train acc on all data:  0.958907420836
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 2.52863e-05
Norm of the params: 30.5401
              Random: fixed  99 labels. Loss 0.74131. Accuracy 0.868.
### Flips: 618, rs: 8, checks: 824
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0985982
Train loss (w/o reg) on all data: 0.0737902
Test loss (w/o reg) on all data: 0.229581
Train acc on all data:  0.979211989364
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.60307e-05
Norm of the params: 22.2747
     Influence (LOO): fixed 392 labels. Loss 0.22958. Accuracy 0.945.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0657852
Train loss (w/o reg) on all data: 0.0331602
Test loss (w/o reg) on all data: 0.354261
Train acc on all data:  0.999516557892
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.24262e-05
Norm of the params: 25.5441
                Loss: fixed 353 labels. Loss 0.35426. Accuracy 0.913.
Using normal model
LBFGS training took [416] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181318
Train loss (w/o reg) on all data: 0.135384
Test loss (w/o reg) on all data: 0.673868
Train acc on all data:  0.962533236645
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 1.35025e-05
Norm of the params: 30.3098
              Random: fixed 132 labels. Loss 0.67387. Accuracy 0.868.
### Flips: 618, rs: 8, checks: 1030
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093057
Train loss (w/o reg) on all data: 0.0702844
Test loss (w/o reg) on all data: 0.197686
Train acc on all data:  0.981145757796
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.12563e-05
Norm of the params: 21.3413
     Influence (LOO): fixed 433 labels. Loss 0.19769. Accuracy 0.952.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0595039
Train loss (w/o reg) on all data: 0.0294524
Test loss (w/o reg) on all data: 0.362943
Train acc on all data:  0.999516557892
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 5.47637e-06
Norm of the params: 24.5159
                Loss: fixed 390 labels. Loss 0.36294. Accuracy 0.918.
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174938
Train loss (w/o reg) on all data: 0.130024
Test loss (w/o reg) on all data: 0.711794
Train acc on all data:  0.963741841914
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 2.12805e-05
Norm of the params: 29.9713
              Random: fixed 166 labels. Loss 0.71179. Accuracy 0.874.
### Flips: 618, rs: 8, checks: 1236
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0870198
Train loss (w/o reg) on all data: 0.0657607
Test loss (w/o reg) on all data: 0.171884
Train acc on all data:  0.981870920957
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 2.17386e-06
Norm of the params: 20.6199
     Influence (LOO): fixed 460 labels. Loss 0.17188. Accuracy 0.955.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0553531
Train loss (w/o reg) on all data: 0.0272936
Test loss (w/o reg) on all data: 0.409405
Train acc on all data:  0.999274836838
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 3.71846e-06
Norm of the params: 23.6895
                Loss: fixed 416 labels. Loss 0.40941. Accuracy 0.921.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167737
Train loss (w/o reg) on all data: 0.123933
Test loss (w/o reg) on all data: 0.705624
Train acc on all data:  0.966884215615
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 5.72523e-05
Norm of the params: 29.5986
              Random: fixed 193 labels. Loss 0.70562. Accuracy 0.877.
Using normal model
LBFGS training took [577] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204021
Train loss (w/o reg) on all data: 0.156652
Test loss (w/o reg) on all data: 0.582532
Train acc on all data:  0.956248489243
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 6.67875e-05
Norm of the params: 30.7796
Flipped loss: 0.58253. Accuracy: 0.854
### Flips: 618, rs: 9, checks: 206
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13583
Train loss (w/o reg) on all data: 0.0999912
Test loss (w/o reg) on all data: 0.476358
Train acc on all data:  0.973168963017
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 3.11674e-05
Norm of the params: 26.7726
     Influence (LOO): fixed 171 labels. Loss 0.47636. Accuracy 0.896.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11267
Train loss (w/o reg) on all data: 0.0661943
Test loss (w/o reg) on all data: 0.515841
Train acc on all data:  0.992748368383
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 3.14883e-05
Norm of the params: 30.4879
                Loss: fixed 170 labels. Loss 0.51584. Accuracy 0.872.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194765
Train loss (w/o reg) on all data: 0.147615
Test loss (w/o reg) on all data: 0.572172
Train acc on all data:  0.959632583998
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 1.8896e-05
Norm of the params: 30.7083
              Random: fixed  33 labels. Loss 0.57217. Accuracy 0.850.
### Flips: 618, rs: 9, checks: 412
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119194
Train loss (w/o reg) on all data: 0.0887362
Test loss (w/o reg) on all data: 0.327522
Train acc on all data:  0.974861010394
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 2.55284e-05
Norm of the params: 24.6812
     Influence (LOO): fixed 277 labels. Loss 0.32752. Accuracy 0.915.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0872046
Train loss (w/o reg) on all data: 0.0462994
Test loss (w/o reg) on all data: 0.440083
Train acc on all data:  0.998066231569
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 1.13845e-05
Norm of the params: 28.6025
                Loss: fixed 262 labels. Loss 0.44008. Accuracy 0.891.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190076
Train loss (w/o reg) on all data: 0.143689
Test loss (w/o reg) on all data: 0.566358
Train acc on all data:  0.961566352429
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 2.13982e-05
Norm of the params: 30.4588
              Random: fixed  59 labels. Loss 0.56636. Accuracy 0.854.
### Flips: 618, rs: 9, checks: 618
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107027
Train loss (w/o reg) on all data: 0.0809019
Test loss (w/o reg) on all data: 0.253935
Train acc on all data:  0.976311336717
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.07649e-05
Norm of the params: 22.8584
     Influence (LOO): fixed 351 labels. Loss 0.25393. Accuracy 0.931.
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0728775
Train loss (w/o reg) on all data: 0.0371205
Test loss (w/o reg) on all data: 0.355828
Train acc on all data:  0.998549673677
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 4.17827e-05
Norm of the params: 26.7421
                Loss: fixed 324 labels. Loss 0.35583. Accuracy 0.901.
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182731
Train loss (w/o reg) on all data: 0.137619
Test loss (w/o reg) on all data: 0.53501
Train acc on all data:  0.963258399807
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 4.47652e-05
Norm of the params: 30.0373
              Random: fixed  92 labels. Loss 0.53501. Accuracy 0.864.
### Flips: 618, rs: 9, checks: 824
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0980849
Train loss (w/o reg) on all data: 0.0740648
Test loss (w/o reg) on all data: 0.204251
Train acc on all data:  0.97897026831
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 3.1695e-06
Norm of the params: 21.9181
     Influence (LOO): fixed 402 labels. Loss 0.20425. Accuracy 0.943.
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065473
Train loss (w/o reg) on all data: 0.0327174
Test loss (w/o reg) on all data: 0.298846
Train acc on all data:  0.999033115784
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 1.92013e-05
Norm of the params: 25.5952
                Loss: fixed 359 labels. Loss 0.29885. Accuracy 0.917.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175133
Train loss (w/o reg) on all data: 0.131196
Test loss (w/o reg) on all data: 0.48813
Train acc on all data:  0.964950447184
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 9.84575e-05
Norm of the params: 29.6435
              Random: fixed 124 labels. Loss 0.48813. Accuracy 0.870.
### Flips: 618, rs: 9, checks: 1030
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0931593
Train loss (w/o reg) on all data: 0.0710485
Test loss (w/o reg) on all data: 0.219145
Train acc on all data:  0.979453710418
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 3.15676e-06
Norm of the params: 21.0289
     Influence (LOO): fixed 440 labels. Loss 0.21914. Accuracy 0.951.
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0615813
Train loss (w/o reg) on all data: 0.0306884
Test loss (w/o reg) on all data: 0.286162
Train acc on all data:  0.999033115784
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 1.56335e-05
Norm of the params: 24.8568
                Loss: fixed 384 labels. Loss 0.28616. Accuracy 0.926.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169158
Train loss (w/o reg) on all data: 0.12623
Test loss (w/o reg) on all data: 0.51477
Train acc on all data:  0.966400773507
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.09322e-05
Norm of the params: 29.3011
              Random: fixed 155 labels. Loss 0.51477. Accuracy 0.877.
### Flips: 618, rs: 9, checks: 1236
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0877349
Train loss (w/o reg) on all data: 0.0673406
Test loss (w/o reg) on all data: 0.241403
Train acc on all data:  0.980420594634
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 5.29321e-06
Norm of the params: 20.1962
     Influence (LOO): fixed 466 labels. Loss 0.24140. Accuracy 0.957.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0568845
Train loss (w/o reg) on all data: 0.0279458
Test loss (w/o reg) on all data: 0.336149
Train acc on all data:  0.999274836838
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 1.54277e-05
Norm of the params: 24.0577
                Loss: fixed 409 labels. Loss 0.33615. Accuracy 0.926.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164987
Train loss (w/o reg) on all data: 0.122847
Test loss (w/o reg) on all data: 0.515877
Train acc on all data:  0.9690597051
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.53045e-05
Norm of the params: 29.031
              Random: fixed 177 labels. Loss 0.51588. Accuracy 0.876.
Using normal model
LBFGS training took [401] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206048
Train loss (w/o reg) on all data: 0.157164
Test loss (w/o reg) on all data: 0.614057
Train acc on all data:  0.952380952381
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.10065e-05
Norm of the params: 31.268
Flipped loss: 0.61406. Accuracy: 0.843
### Flips: 618, rs: 10, checks: 206
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135304
Train loss (w/o reg) on all data: 0.0982858
Test loss (w/o reg) on all data: 0.538545
Train acc on all data:  0.973410684071
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.61768e-05
Norm of the params: 27.2096
     Influence (LOO): fixed 167 labels. Loss 0.53855. Accuracy 0.907.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115
Train loss (w/o reg) on all data: 0.0671865
Test loss (w/o reg) on all data: 0.67033
Train acc on all data:  0.99444041576
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.38887e-05
Norm of the params: 30.9237
                Loss: fixed 169 labels. Loss 0.67033. Accuracy 0.864.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197691
Train loss (w/o reg) on all data: 0.149806
Test loss (w/o reg) on all data: 0.565985
Train acc on all data:  0.956490210297
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 1.50109e-05
Norm of the params: 30.9467
              Random: fixed  44 labels. Loss 0.56599. Accuracy 0.852.
### Flips: 618, rs: 10, checks: 412
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117402
Train loss (w/o reg) on all data: 0.0860826
Test loss (w/o reg) on all data: 0.45954
Train acc on all data:  0.976069615664
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 9.86151e-06
Norm of the params: 25.0278
     Influence (LOO): fixed 283 labels. Loss 0.45954. Accuracy 0.928.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091112
Train loss (w/o reg) on all data: 0.0485056
Test loss (w/o reg) on all data: 0.645188
Train acc on all data:  0.99879139473
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.42657e-05
Norm of the params: 29.1912
                Loss: fixed 242 labels. Loss 0.64519. Accuracy 0.889.
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192477
Train loss (w/o reg) on all data: 0.145473
Test loss (w/o reg) on all data: 0.556451
Train acc on all data:  0.958182257675
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 5.24693e-05
Norm of the params: 30.6608
              Random: fixed  75 labels. Loss 0.55645. Accuracy 0.854.
### Flips: 618, rs: 10, checks: 618
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105511
Train loss (w/o reg) on all data: 0.0780431
Test loss (w/o reg) on all data: 0.373203
Train acc on all data:  0.977761663041
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 3.13164e-06
Norm of the params: 23.4385
     Influence (LOO): fixed 355 labels. Loss 0.37320. Accuracy 0.942.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076064
Train loss (w/o reg) on all data: 0.0391738
Test loss (w/o reg) on all data: 0.566542
Train acc on all data:  0.99879139473
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.32254e-05
Norm of the params: 27.1625
                Loss: fixed 304 labels. Loss 0.56654. Accuracy 0.903.
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188258
Train loss (w/o reg) on all data: 0.141451
Test loss (w/o reg) on all data: 0.573196
Train acc on all data:  0.959632583998
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.09812e-05
Norm of the params: 30.5965
              Random: fixed 102 labels. Loss 0.57320. Accuracy 0.864.
### Flips: 618, rs: 10, checks: 824
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0945298
Train loss (w/o reg) on all data: 0.07072
Test loss (w/o reg) on all data: 0.281508
Train acc on all data:  0.98017887358
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 2.23183e-05
Norm of the params: 21.8219
     Influence (LOO): fixed 419 labels. Loss 0.28151. Accuracy 0.958.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0649229
Train loss (w/o reg) on all data: 0.0323724
Test loss (w/o reg) on all data: 0.494982
Train acc on all data:  0.999033115784
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.48612e-05
Norm of the params: 25.5149
                Loss: fixed 358 labels. Loss 0.49498. Accuracy 0.924.
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184968
Train loss (w/o reg) on all data: 0.138839
Test loss (w/o reg) on all data: 0.568057
Train acc on all data:  0.963016678753
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 4.35485e-05
Norm of the params: 30.3743
              Random: fixed 120 labels. Loss 0.56806. Accuracy 0.870.
### Flips: 618, rs: 10, checks: 1030
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088199
Train loss (w/o reg) on all data: 0.0665193
Test loss (w/o reg) on all data: 0.254506
Train acc on all data:  0.981629199903
Test acc on all data:   0.967149758454
Norm of the mean of gradients: 5.99338e-06
Norm of the params: 20.8229
     Influence (LOO): fixed 460 labels. Loss 0.25451. Accuracy 0.967.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0595846
Train loss (w/o reg) on all data: 0.0293308
Test loss (w/o reg) on all data: 0.45537
Train acc on all data:  0.999274836838
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 7.60484e-06
Norm of the params: 24.5983
                Loss: fixed 388 labels. Loss 0.45537. Accuracy 0.928.
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178084
Train loss (w/o reg) on all data: 0.132886
Test loss (w/o reg) on all data: 0.542088
Train acc on all data:  0.962533236645
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 1.86654e-05
Norm of the params: 30.066
              Random: fixed 151 labels. Loss 0.54209. Accuracy 0.874.
### Flips: 618, rs: 10, checks: 1236
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0800654
Train loss (w/o reg) on all data: 0.0598957
Test loss (w/o reg) on all data: 0.184457
Train acc on all data:  0.983804689388
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 3.01306e-06
Norm of the params: 20.0847
     Influence (LOO): fixed 494 labels. Loss 0.18446. Accuracy 0.965.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0550293
Train loss (w/o reg) on all data: 0.0268157
Test loss (w/o reg) on all data: 0.502525
Train acc on all data:  0.999274836838
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 4.89037e-06
Norm of the params: 23.7544
                Loss: fixed 409 labels. Loss 0.50252. Accuracy 0.933.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166631
Train loss (w/o reg) on all data: 0.123023
Test loss (w/o reg) on all data: 0.498118
Train acc on all data:  0.968817984046
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 1.68658e-05
Norm of the params: 29.5324
              Random: fixed 193 labels. Loss 0.49812. Accuracy 0.888.
Using normal model
LBFGS training took [408] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204923
Train loss (w/o reg) on all data: 0.157357
Test loss (w/o reg) on all data: 0.57485
Train acc on all data:  0.951897510273
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 2.91848e-05
Norm of the params: 30.8438
Flipped loss: 0.57485. Accuracy: 0.842
### Flips: 618, rs: 11, checks: 206
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138011
Train loss (w/o reg) on all data: 0.102455
Test loss (w/o reg) on all data: 0.377495
Train acc on all data:  0.970993473532
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 1.84961e-05
Norm of the params: 26.6667
     Influence (LOO): fixed 166 labels. Loss 0.37749. Accuracy 0.895.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114184
Train loss (w/o reg) on all data: 0.0664701
Test loss (w/o reg) on all data: 0.574867
Train acc on all data:  0.993231810491
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 3.74272e-05
Norm of the params: 30.8913
                Loss: fixed 177 labels. Loss 0.57487. Accuracy 0.847.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195565
Train loss (w/o reg) on all data: 0.148657
Test loss (w/o reg) on all data: 0.575324
Train acc on all data:  0.955039883974
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 8.57753e-06
Norm of the params: 30.6293
              Random: fixed  41 labels. Loss 0.57532. Accuracy 0.845.
### Flips: 618, rs: 11, checks: 412
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1185
Train loss (w/o reg) on all data: 0.0891402
Test loss (w/o reg) on all data: 0.267009
Train acc on all data:  0.975586173556
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 1.57609e-05
Norm of the params: 24.232
     Influence (LOO): fixed 277 labels. Loss 0.26701. Accuracy 0.912.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0912696
Train loss (w/o reg) on all data: 0.049044
Test loss (w/o reg) on all data: 0.48699
Train acc on all data:  0.998307952623
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 5.14876e-06
Norm of the params: 29.0605
                Loss: fixed 250 labels. Loss 0.48699. Accuracy 0.871.
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191153
Train loss (w/o reg) on all data: 0.14422
Test loss (w/o reg) on all data: 0.533493
Train acc on all data:  0.957698815567
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.48298e-05
Norm of the params: 30.6376
              Random: fixed  63 labels. Loss 0.53349. Accuracy 0.860.
### Flips: 618, rs: 11, checks: 618
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109064
Train loss (w/o reg) on all data: 0.0829106
Test loss (w/o reg) on all data: 0.213004
Train acc on all data:  0.976553057771
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 3.59693e-06
Norm of the params: 22.8706
     Influence (LOO): fixed 349 labels. Loss 0.21300. Accuracy 0.938.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075087
Train loss (w/o reg) on all data: 0.0387708
Test loss (w/o reg) on all data: 0.420959
Train acc on all data:  0.99879139473
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 3.02573e-06
Norm of the params: 26.9504
                Loss: fixed 312 labels. Loss 0.42096. Accuracy 0.886.
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185882
Train loss (w/o reg) on all data: 0.139663
Test loss (w/o reg) on all data: 0.515421
Train acc on all data:  0.96035774716
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 6.48787e-05
Norm of the params: 30.4037
              Random: fixed  92 labels. Loss 0.51542. Accuracy 0.861.
### Flips: 618, rs: 11, checks: 824
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100843
Train loss (w/o reg) on all data: 0.0773411
Test loss (w/o reg) on all data: 0.186717
Train acc on all data:  0.977519941987
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 2.5025e-06
Norm of the params: 21.6805
     Influence (LOO): fixed 409 labels. Loss 0.18672. Accuracy 0.949.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0683745
Train loss (w/o reg) on all data: 0.0346884
Test loss (w/o reg) on all data: 0.341695
Train acc on all data:  0.999516557892
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 7.11003e-06
Norm of the params: 25.9562
                Loss: fixed 341 labels. Loss 0.34169. Accuracy 0.906.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182354
Train loss (w/o reg) on all data: 0.136964
Test loss (w/o reg) on all data: 0.488552
Train acc on all data:  0.961808073483
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 5.95948e-06
Norm of the params: 30.1297
              Random: fixed 116 labels. Loss 0.48855. Accuracy 0.870.
### Flips: 618, rs: 11, checks: 1030
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0945253
Train loss (w/o reg) on all data: 0.0726307
Test loss (w/o reg) on all data: 0.157508
Train acc on all data:  0.978728547256
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 8.55169e-06
Norm of the params: 20.9259
     Influence (LOO): fixed 442 labels. Loss 0.15751. Accuracy 0.956.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0624227
Train loss (w/o reg) on all data: 0.0312515
Test loss (w/o reg) on all data: 0.331854
Train acc on all data:  0.999033115784
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 9.65397e-06
Norm of the params: 24.9685
                Loss: fixed 374 labels. Loss 0.33185. Accuracy 0.911.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177199
Train loss (w/o reg) on all data: 0.132529
Test loss (w/o reg) on all data: 0.467052
Train acc on all data:  0.962774957699
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 3.41567e-05
Norm of the params: 29.8897
              Random: fixed 150 labels. Loss 0.46705. Accuracy 0.872.
### Flips: 618, rs: 11, checks: 1236
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0901017
Train loss (w/o reg) on all data: 0.06933
Test loss (w/o reg) on all data: 0.141561
Train acc on all data:  0.979211989364
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 1.20698e-05
Norm of the params: 20.3822
     Influence (LOO): fixed 464 labels. Loss 0.14156. Accuracy 0.960.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0554631
Train loss (w/o reg) on all data: 0.0276266
Test loss (w/o reg) on all data: 0.270845
Train acc on all data:  0.99879139473
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 3.51015e-06
Norm of the params: 23.5951
                Loss: fixed 418 labels. Loss 0.27084. Accuracy 0.924.
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16963
Train loss (w/o reg) on all data: 0.126291
Test loss (w/o reg) on all data: 0.457389
Train acc on all data:  0.965675610346
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 6.90685e-06
Norm of the params: 29.4413
              Random: fixed 181 labels. Loss 0.45739. Accuracy 0.877.
Using normal model
LBFGS training took [585] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197529
Train loss (w/o reg) on all data: 0.150074
Test loss (w/o reg) on all data: 0.790132
Train acc on all data:  0.95479816292
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 5.64523e-05
Norm of the params: 30.8072
Flipped loss: 0.79013. Accuracy: 0.822
### Flips: 618, rs: 12, checks: 206
Using normal model
LBFGS training took [348] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132093
Train loss (w/o reg) on all data: 0.0978904
Test loss (w/o reg) on all data: 0.450225
Train acc on all data:  0.97461928934
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.11743e-05
Norm of the params: 26.1545
     Influence (LOO): fixed 171 labels. Loss 0.45022. Accuracy 0.879.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108416
Train loss (w/o reg) on all data: 0.0628119
Test loss (w/o reg) on all data: 0.717986
Train acc on all data:  0.995890742084
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 2.34657e-05
Norm of the params: 30.2006
                Loss: fixed 162 labels. Loss 0.71799. Accuracy 0.843.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19159
Train loss (w/o reg) on all data: 0.145263
Test loss (w/o reg) on all data: 0.770686
Train acc on all data:  0.95600676819
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.90908e-05
Norm of the params: 30.439
              Random: fixed  38 labels. Loss 0.77069. Accuracy 0.824.
### Flips: 618, rs: 12, checks: 412
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116191
Train loss (w/o reg) on all data: 0.0873695
Test loss (w/o reg) on all data: 0.310841
Train acc on all data:  0.97582789461
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 1.46955e-05
Norm of the params: 24.0088
     Influence (LOO): fixed 288 labels. Loss 0.31084. Accuracy 0.912.
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0838431
Train loss (w/o reg) on all data: 0.043986
Test loss (w/o reg) on all data: 0.482743
Train acc on all data:  0.999274836838
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.52079e-05
Norm of the params: 28.2337
                Loss: fixed 243 labels. Loss 0.48274. Accuracy 0.877.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184383
Train loss (w/o reg) on all data: 0.139005
Test loss (w/o reg) on all data: 0.684443
Train acc on all data:  0.957215373459
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 1.65568e-05
Norm of the params: 30.1258
              Random: fixed  72 labels. Loss 0.68444. Accuracy 0.836.
### Flips: 618, rs: 12, checks: 618
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106822
Train loss (w/o reg) on all data: 0.08145
Test loss (w/o reg) on all data: 0.233757
Train acc on all data:  0.977519941987
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 5.97604e-06
Norm of the params: 22.5264
     Influence (LOO): fixed 360 labels. Loss 0.23376. Accuracy 0.936.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0724457
Train loss (w/o reg) on all data: 0.0370667
Test loss (w/o reg) on all data: 0.430656
Train acc on all data:  0.999033115784
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 2.10783e-05
Norm of the params: 26.6004
                Loss: fixed 302 labels. Loss 0.43066. Accuracy 0.884.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178645
Train loss (w/o reg) on all data: 0.133693
Test loss (w/o reg) on all data: 0.68073
Train acc on all data:  0.959874305052
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 2.14024e-05
Norm of the params: 29.9842
              Random: fixed 103 labels. Loss 0.68073. Accuracy 0.841.
### Flips: 618, rs: 12, checks: 824
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.099943
Train loss (w/o reg) on all data: 0.0764656
Test loss (w/o reg) on all data: 0.197621
Train acc on all data:  0.979211989364
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 4.94092e-06
Norm of the params: 21.669
     Influence (LOO): fixed 408 labels. Loss 0.19762. Accuracy 0.932.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066266
Train loss (w/o reg) on all data: 0.0333698
Test loss (w/o reg) on all data: 0.401386
Train acc on all data:  0.998549673677
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 2.08333e-05
Norm of the params: 25.65
                Loss: fixed 331 labels. Loss 0.40139. Accuracy 0.899.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171683
Train loss (w/o reg) on all data: 0.12847
Test loss (w/o reg) on all data: 0.518038
Train acc on all data:  0.962049794537
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 1.54307e-05
Norm of the params: 29.3981
              Random: fixed 138 labels. Loss 0.51804. Accuracy 0.841.
### Flips: 618, rs: 12, checks: 1030
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0923074
Train loss (w/o reg) on all data: 0.0703929
Test loss (w/o reg) on all data: 0.184876
Train acc on all data:  0.980904036742
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 5.46477e-06
Norm of the params: 20.9354
     Influence (LOO): fixed 446 labels. Loss 0.18488. Accuracy 0.949.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0619137
Train loss (w/o reg) on all data: 0.0308008
Test loss (w/o reg) on all data: 0.394491
Train acc on all data:  0.99879139473
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 5.21479e-06
Norm of the params: 24.9451
                Loss: fixed 359 labels. Loss 0.39449. Accuracy 0.904.
Using normal model
LBFGS training took [492] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161636
Train loss (w/o reg) on all data: 0.120272
Test loss (w/o reg) on all data: 0.500492
Train acc on all data:  0.965192168238
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 1.91976e-05
Norm of the params: 28.7623
              Random: fixed 173 labels. Loss 0.50049. Accuracy 0.854.
### Flips: 618, rs: 12, checks: 1236
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0854015
Train loss (w/o reg) on all data: 0.0647491
Test loss (w/o reg) on all data: 0.178933
Train acc on all data:  0.982837805173
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 1.02419e-05
Norm of the params: 20.3236
     Influence (LOO): fixed 471 labels. Loss 0.17893. Accuracy 0.957.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0581377
Train loss (w/o reg) on all data: 0.0287569
Test loss (w/o reg) on all data: 0.393676
Train acc on all data:  0.999274836838
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 6.61565e-06
Norm of the params: 24.2408
                Loss: fixed 392 labels. Loss 0.39368. Accuracy 0.914.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153939
Train loss (w/o reg) on all data: 0.114311
Test loss (w/o reg) on all data: 0.466939
Train acc on all data:  0.967367657723
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 3.06521e-05
Norm of the params: 28.1525
              Random: fixed 202 labels. Loss 0.46694. Accuracy 0.858.
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203289
Train loss (w/o reg) on all data: 0.154862
Test loss (w/o reg) on all data: 0.804128
Train acc on all data:  0.95358955765
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 3.30852e-05
Norm of the params: 31.1216
Flipped loss: 0.80413. Accuracy: 0.845
### Flips: 618, rs: 13, checks: 206
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133776
Train loss (w/o reg) on all data: 0.09823
Test loss (w/o reg) on all data: 0.726421
Train acc on all data:  0.97461928934
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 2.48946e-05
Norm of the params: 26.6631
     Influence (LOO): fixed 177 labels. Loss 0.72642. Accuracy 0.893.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114783
Train loss (w/o reg) on all data: 0.0673472
Test loss (w/o reg) on all data: 0.862715
Train acc on all data:  0.995165578922
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 8.73825e-06
Norm of the params: 30.8014
                Loss: fixed 167 labels. Loss 0.86271. Accuracy 0.873.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19901
Train loss (w/o reg) on all data: 0.151169
Test loss (w/o reg) on all data: 0.767238
Train acc on all data:  0.954556441866
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 2.52803e-05
Norm of the params: 30.9326
              Random: fixed  23 labels. Loss 0.76724. Accuracy 0.852.
### Flips: 618, rs: 13, checks: 412
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115296
Train loss (w/o reg) on all data: 0.0851261
Test loss (w/o reg) on all data: 0.694105
Train acc on all data:  0.977761663041
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 7.19903e-06
Norm of the params: 24.5643
     Influence (LOO): fixed 281 labels. Loss 0.69411. Accuracy 0.916.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0848983
Train loss (w/o reg) on all data: 0.0454018
Test loss (w/o reg) on all data: 0.667367
Train acc on all data:  0.998549673677
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 5.57439e-06
Norm of the params: 28.1057
                Loss: fixed 266 labels. Loss 0.66737. Accuracy 0.898.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192027
Train loss (w/o reg) on all data: 0.145188
Test loss (w/o reg) on all data: 0.800857
Train acc on all data:  0.956973652405
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 2.62904e-05
Norm of the params: 30.6068
              Random: fixed  52 labels. Loss 0.80086. Accuracy 0.855.
### Flips: 618, rs: 13, checks: 618
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102725
Train loss (w/o reg) on all data: 0.0764122
Test loss (w/o reg) on all data: 0.466532
Train acc on all data:  0.978486826203
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.2126e-06
Norm of the params: 22.9404
     Influence (LOO): fixed 357 labels. Loss 0.46653. Accuracy 0.935.
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0725763
Train loss (w/o reg) on all data: 0.0373291
Test loss (w/o reg) on all data: 0.67041
Train acc on all data:  0.99879139473
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 5.32995e-06
Norm of the params: 26.5508
                Loss: fixed 327 labels. Loss 0.67041. Accuracy 0.906.
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186215
Train loss (w/o reg) on all data: 0.14067
Test loss (w/o reg) on all data: 0.791458
Train acc on all data:  0.959390862944
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 1.70328e-05
Norm of the params: 30.1813
              Random: fixed  79 labels. Loss 0.79146. Accuracy 0.868.
### Flips: 618, rs: 13, checks: 824
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0971973
Train loss (w/o reg) on all data: 0.0731231
Test loss (w/o reg) on all data: 0.459879
Train acc on all data:  0.978728547256
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 1.75854e-05
Norm of the params: 21.9428
     Influence (LOO): fixed 406 labels. Loss 0.45988. Accuracy 0.943.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0653054
Train loss (w/o reg) on all data: 0.0330438
Test loss (w/o reg) on all data: 0.481357
Train acc on all data:  0.999033115784
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 2.17869e-05
Norm of the params: 25.4014
                Loss: fixed 367 labels. Loss 0.48136. Accuracy 0.915.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177866
Train loss (w/o reg) on all data: 0.132843
Test loss (w/o reg) on all data: 0.841168
Train acc on all data:  0.961808073483
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 1.06382e-05
Norm of the params: 30.0077
              Random: fixed 114 labels. Loss 0.84117. Accuracy 0.871.
### Flips: 618, rs: 13, checks: 1030
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0904222
Train loss (w/o reg) on all data: 0.0684409
Test loss (w/o reg) on all data: 0.431133
Train acc on all data:  0.98017887358
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 3.56971e-06
Norm of the params: 20.9672
     Influence (LOO): fixed 449 labels. Loss 0.43113. Accuracy 0.944.
Using normal model
LBFGS training took [360] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0627918
Train loss (w/o reg) on all data: 0.031671
Test loss (w/o reg) on all data: 0.42593
Train acc on all data:  0.999033115784
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 7.69433e-06
Norm of the params: 24.9482
                Loss: fixed 384 labels. Loss 0.42593. Accuracy 0.919.
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171149
Train loss (w/o reg) on all data: 0.127186
Test loss (w/o reg) on all data: 0.753401
Train acc on all data:  0.961566352429
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 1.52849e-05
Norm of the params: 29.6523
              Random: fixed 144 labels. Loss 0.75340. Accuracy 0.875.
### Flips: 618, rs: 13, checks: 1236
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0832662
Train loss (w/o reg) on all data: 0.063423
Test loss (w/o reg) on all data: 0.395347
Train acc on all data:  0.981387478849
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 7.39175e-06
Norm of the params: 19.9214
     Influence (LOO): fixed 487 labels. Loss 0.39535. Accuracy 0.959.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0567442
Train loss (w/o reg) on all data: 0.028211
Test loss (w/o reg) on all data: 0.359708
Train acc on all data:  0.99879139473
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.0071e-06
Norm of the params: 23.8886
                Loss: fixed 417 labels. Loss 0.35971. Accuracy 0.935.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163586
Train loss (w/o reg) on all data: 0.120822
Test loss (w/o reg) on all data: 0.741021
Train acc on all data:  0.964467005076
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 2.03566e-05
Norm of the params: 29.2452
              Random: fixed 181 labels. Loss 0.74102. Accuracy 0.878.
Using normal model
LBFGS training took [600] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205946
Train loss (w/o reg) on all data: 0.158602
Test loss (w/o reg) on all data: 0.673425
Train acc on all data:  0.951414068165
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 9.32342e-05
Norm of the params: 30.7713
Flipped loss: 0.67342. Accuracy: 0.834
### Flips: 618, rs: 14, checks: 206
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134912
Train loss (w/o reg) on all data: 0.0999986
Test loss (w/o reg) on all data: 0.520718
Train acc on all data:  0.972202078801
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 2.57012e-05
Norm of the params: 26.4248
     Influence (LOO): fixed 174 labels. Loss 0.52072. Accuracy 0.890.
Using normal model
LBFGS training took [529] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113527
Train loss (w/o reg) on all data: 0.0667478
Test loss (w/o reg) on all data: 0.643502
Train acc on all data:  0.991298042059
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 6.70186e-05
Norm of the params: 30.5874
                Loss: fixed 172 labels. Loss 0.64350. Accuracy 0.839.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195481
Train loss (w/o reg) on all data: 0.149627
Test loss (w/o reg) on all data: 0.658891
Train acc on all data:  0.956248489243
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 4.24127e-05
Norm of the params: 30.2834
              Random: fixed  42 labels. Loss 0.65889. Accuracy 0.834.
### Flips: 618, rs: 14, checks: 412
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117749
Train loss (w/o reg) on all data: 0.0884808
Test loss (w/o reg) on all data: 0.442218
Train acc on all data:  0.975102731448
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 8.22405e-06
Norm of the params: 24.1944
     Influence (LOO): fixed 278 labels. Loss 0.44222. Accuracy 0.930.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0868809
Train loss (w/o reg) on all data: 0.0461291
Test loss (w/o reg) on all data: 0.481051
Train acc on all data:  0.998549673677
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 1.32125e-05
Norm of the params: 28.5488
                Loss: fixed 266 labels. Loss 0.48105. Accuracy 0.859.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187934
Train loss (w/o reg) on all data: 0.143237
Test loss (w/o reg) on all data: 0.679781
Train acc on all data:  0.957698815567
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 6.53703e-05
Norm of the params: 29.8988
              Random: fixed  74 labels. Loss 0.67978. Accuracy 0.849.
### Flips: 618, rs: 14, checks: 618
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105833
Train loss (w/o reg) on all data: 0.0799257
Test loss (w/o reg) on all data: 0.410584
Train acc on all data:  0.976553057771
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 7.93611e-06
Norm of the params: 22.7626
     Influence (LOO): fixed 356 labels. Loss 0.41058. Accuracy 0.946.
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0759774
Train loss (w/o reg) on all data: 0.0391277
Test loss (w/o reg) on all data: 0.357457
Train acc on all data:  0.999274836838
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 3.30071e-05
Norm of the params: 27.1476
                Loss: fixed 318 labels. Loss 0.35746. Accuracy 0.895.
Using normal model
LBFGS training took [522] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182066
Train loss (w/o reg) on all data: 0.138222
Test loss (w/o reg) on all data: 0.678051
Train acc on all data:  0.959390862944
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 3.57979e-05
Norm of the params: 29.6124
              Random: fixed 100 labels. Loss 0.67805. Accuracy 0.851.
### Flips: 618, rs: 14, checks: 824
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0982494
Train loss (w/o reg) on all data: 0.0746754
Test loss (w/o reg) on all data: 0.420187
Train acc on all data:  0.977519941987
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 8.49466e-06
Norm of the params: 21.7136
     Influence (LOO): fixed 412 labels. Loss 0.42019. Accuracy 0.951.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0663776
Train loss (w/o reg) on all data: 0.0336186
Test loss (w/o reg) on all data: 0.314983
Train acc on all data:  0.999274836838
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 6.3519e-06
Norm of the params: 25.5965
                Loss: fixed 365 labels. Loss 0.31498. Accuracy 0.900.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174577
Train loss (w/o reg) on all data: 0.131882
Test loss (w/o reg) on all data: 0.670963
Train acc on all data:  0.961324631375
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.4746e-05
Norm of the params: 29.2213
              Random: fixed 128 labels. Loss 0.67096. Accuracy 0.862.
### Flips: 618, rs: 14, checks: 1030
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0930412
Train loss (w/o reg) on all data: 0.0707198
Test loss (w/o reg) on all data: 0.369685
Train acc on all data:  0.979211989364
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 4.95459e-06
Norm of the params: 21.1288
     Influence (LOO): fixed 442 labels. Loss 0.36969. Accuracy 0.955.
Using normal model
LBFGS training took [375] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0591675
Train loss (w/o reg) on all data: 0.0294549
Test loss (w/o reg) on all data: 0.280704
Train acc on all data:  0.999033115784
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 5.29874e-06
Norm of the params: 24.3773
                Loss: fixed 401 labels. Loss 0.28070. Accuracy 0.922.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166365
Train loss (w/o reg) on all data: 0.124719
Test loss (w/o reg) on all data: 0.675243
Train acc on all data:  0.964225284022
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 2.15244e-05
Norm of the params: 28.8603
              Random: fixed 158 labels. Loss 0.67524. Accuracy 0.866.
### Flips: 618, rs: 14, checks: 1236
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0853775
Train loss (w/o reg) on all data: 0.065234
Test loss (w/o reg) on all data: 0.371039
Train acc on all data:  0.979937152526
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.10602e-05
Norm of the params: 20.0716
     Influence (LOO): fixed 480 labels. Loss 0.37104. Accuracy 0.961.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053056
Train loss (w/o reg) on all data: 0.0261038
Test loss (w/o reg) on all data: 0.320727
Train acc on all data:  0.999274836838
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 6.41023e-06
Norm of the params: 23.2173
                Loss: fixed 437 labels. Loss 0.32073. Accuracy 0.928.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161146
Train loss (w/o reg) on all data: 0.119836
Test loss (w/o reg) on all data: 0.63369
Train acc on all data:  0.966400773507
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 3.86923e-05
Norm of the params: 28.7435
              Random: fixed 181 labels. Loss 0.63369. Accuracy 0.872.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205343
Train loss (w/o reg) on all data: 0.156802
Test loss (w/o reg) on all data: 0.533974
Train acc on all data:  0.949480299734
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 2.01673e-05
Norm of the params: 31.158
Flipped loss: 0.53397. Accuracy: 0.842
### Flips: 618, rs: 15, checks: 206
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138782
Train loss (w/o reg) on all data: 0.103122
Test loss (w/o reg) on all data: 0.331263
Train acc on all data:  0.971476915639
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 1.15214e-05
Norm of the params: 26.7056
     Influence (LOO): fixed 170 labels. Loss 0.33126. Accuracy 0.908.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116979
Train loss (w/o reg) on all data: 0.0692106
Test loss (w/o reg) on all data: 0.437472
Train acc on all data:  0.990814599952
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 3.14107e-05
Norm of the params: 30.9089
                Loss: fixed 172 labels. Loss 0.43747. Accuracy 0.865.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201343
Train loss (w/o reg) on all data: 0.153331
Test loss (w/o reg) on all data: 0.519601
Train acc on all data:  0.951655789219
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 6.44164e-05
Norm of the params: 30.9879
              Random: fixed  26 labels. Loss 0.51960. Accuracy 0.845.
### Flips: 618, rs: 15, checks: 412
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114649
Train loss (w/o reg) on all data: 0.0855753
Test loss (w/o reg) on all data: 0.247446
Train acc on all data:  0.97582789461
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 2.417e-05
Norm of the params: 24.1138
     Influence (LOO): fixed 294 labels. Loss 0.24745. Accuracy 0.929.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883509
Train loss (w/o reg) on all data: 0.0472526
Test loss (w/o reg) on all data: 0.393728
Train acc on all data:  0.998066231569
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 1.06574e-05
Norm of the params: 28.67
                Loss: fixed 263 labels. Loss 0.39373. Accuracy 0.891.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194746
Train loss (w/o reg) on all data: 0.147952
Test loss (w/o reg) on all data: 0.514052
Train acc on all data:  0.95479816292
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 1.70924e-05
Norm of the params: 30.5924
              Random: fixed  55 labels. Loss 0.51405. Accuracy 0.849.
### Flips: 618, rs: 15, checks: 618
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106917
Train loss (w/o reg) on all data: 0.0805886
Test loss (w/o reg) on all data: 0.239067
Train acc on all data:  0.977036499879
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 8.20657e-06
Norm of the params: 22.9472
     Influence (LOO): fixed 355 labels. Loss 0.23907. Accuracy 0.942.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0753337
Train loss (w/o reg) on all data: 0.0389628
Test loss (w/o reg) on all data: 0.410874
Train acc on all data:  0.998549673677
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 3.37783e-05
Norm of the params: 26.9707
                Loss: fixed 315 labels. Loss 0.41087. Accuracy 0.900.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186524
Train loss (w/o reg) on all data: 0.141023
Test loss (w/o reg) on all data: 0.517265
Train acc on all data:  0.956248489243
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 2.97319e-05
Norm of the params: 30.1668
              Random: fixed  93 labels. Loss 0.51727. Accuracy 0.861.
### Flips: 618, rs: 15, checks: 824
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0995804
Train loss (w/o reg) on all data: 0.0757092
Test loss (w/o reg) on all data: 0.20768
Train acc on all data:  0.978245105149
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.29528e-05
Norm of the params: 21.85
     Influence (LOO): fixed 405 labels. Loss 0.20768. Accuracy 0.952.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0683165
Train loss (w/o reg) on all data: 0.0348514
Test loss (w/o reg) on all data: 0.340883
Train acc on all data:  0.998307952623
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 3.67442e-05
Norm of the params: 25.8709
                Loss: fixed 347 labels. Loss 0.34088. Accuracy 0.911.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178109
Train loss (w/o reg) on all data: 0.133946
Test loss (w/o reg) on all data: 0.492264
Train acc on all data:  0.957457094513
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 5.24115e-05
Norm of the params: 29.7197
              Random: fixed 132 labels. Loss 0.49226. Accuracy 0.869.
### Flips: 618, rs: 15, checks: 1030
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0915034
Train loss (w/o reg) on all data: 0.0703548
Test loss (w/o reg) on all data: 0.182333
Train acc on all data:  0.980420594634
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 2.33999e-06
Norm of the params: 20.5663
     Influence (LOO): fixed 451 labels. Loss 0.18233. Accuracy 0.953.
Using normal model
LBFGS training took [390] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0611416
Train loss (w/o reg) on all data: 0.0303827
Test loss (w/o reg) on all data: 0.303597
Train acc on all data:  0.99879139473
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 1.35872e-05
Norm of the params: 24.8028
                Loss: fixed 384 labels. Loss 0.30360. Accuracy 0.918.
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17071
Train loss (w/o reg) on all data: 0.127474
Test loss (w/o reg) on all data: 0.458971
Train acc on all data:  0.961082910321
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 2.64152e-05
Norm of the params: 29.4061
              Random: fixed 159 labels. Loss 0.45897. Accuracy 0.877.
### Flips: 618, rs: 15, checks: 1236
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0825712
Train loss (w/o reg) on all data: 0.0630014
Test loss (w/o reg) on all data: 0.166286
Train acc on all data:  0.982354363065
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 3.59084e-06
Norm of the params: 19.7837
     Influence (LOO): fixed 483 labels. Loss 0.16629. Accuracy 0.958.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0561017
Train loss (w/o reg) on all data: 0.0277906
Test loss (w/o reg) on all data: 0.287867
Train acc on all data:  0.999274836838
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 7.50994e-06
Norm of the params: 23.7954
                Loss: fixed 418 labels. Loss 0.28787. Accuracy 0.935.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163412
Train loss (w/o reg) on all data: 0.121171
Test loss (w/o reg) on all data: 0.492307
Train acc on all data:  0.962774957699
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 4.51913e-05
Norm of the params: 29.0657
              Random: fixed 192 labels. Loss 0.49231. Accuracy 0.868.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2013
Train loss (w/o reg) on all data: 0.153324
Test loss (w/o reg) on all data: 0.541167
Train acc on all data:  0.951655789219
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.18103e-05
Norm of the params: 30.976
Flipped loss: 0.54117. Accuracy: 0.840
### Flips: 618, rs: 16, checks: 206
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136257
Train loss (w/o reg) on all data: 0.0998139
Test loss (w/o reg) on all data: 0.402888
Train acc on all data:  0.971960357747
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 6.54815e-05
Norm of the params: 26.9975
     Influence (LOO): fixed 170 labels. Loss 0.40289. Accuracy 0.891.
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113528
Train loss (w/o reg) on all data: 0.066879
Test loss (w/o reg) on all data: 0.589581
Train acc on all data:  0.994682136814
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 5.01248e-05
Norm of the params: 30.5448
                Loss: fixed 174 labels. Loss 0.58958. Accuracy 0.871.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196009
Train loss (w/o reg) on all data: 0.148533
Test loss (w/o reg) on all data: 0.549623
Train acc on all data:  0.955765047136
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 3.64677e-05
Norm of the params: 30.8144
              Random: fixed  25 labels. Loss 0.54962. Accuracy 0.850.
### Flips: 618, rs: 16, checks: 412
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117374
Train loss (w/o reg) on all data: 0.0865764
Test loss (w/o reg) on all data: 0.29106
Train acc on all data:  0.975586173556
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 1.15632e-05
Norm of the params: 24.8186
     Influence (LOO): fixed 272 labels. Loss 0.29106. Accuracy 0.909.
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0900855
Train loss (w/o reg) on all data: 0.0483368
Test loss (w/o reg) on all data: 0.520685
Train acc on all data:  0.99879139473
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 3.52847e-05
Norm of the params: 28.8959
                Loss: fixed 263 labels. Loss 0.52069. Accuracy 0.877.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191352
Train loss (w/o reg) on all data: 0.144861
Test loss (w/o reg) on all data: 0.517406
Train acc on all data:  0.955039883974
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 0.000100068
Norm of the params: 30.493
              Random: fixed  56 labels. Loss 0.51741. Accuracy 0.845.
### Flips: 618, rs: 16, checks: 618
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108411
Train loss (w/o reg) on all data: 0.0809993
Test loss (w/o reg) on all data: 0.243501
Train acc on all data:  0.97582789461
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 3.78453e-06
Norm of the params: 23.4142
     Influence (LOO): fixed 348 labels. Loss 0.24350. Accuracy 0.929.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0804034
Train loss (w/o reg) on all data: 0.0418508
Test loss (w/o reg) on all data: 0.466098
Train acc on all data:  0.999033115784
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 3.35491e-05
Norm of the params: 27.7678
                Loss: fixed 312 labels. Loss 0.46610. Accuracy 0.891.
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183305
Train loss (w/o reg) on all data: 0.137822
Test loss (w/o reg) on all data: 0.493712
Train acc on all data:  0.960599468214
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 2.82436e-05
Norm of the params: 30.1606
              Random: fixed  90 labels. Loss 0.49371. Accuracy 0.861.
### Flips: 618, rs: 16, checks: 824
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0973947
Train loss (w/o reg) on all data: 0.0731792
Test loss (w/o reg) on all data: 0.209394
Train acc on all data:  0.978486826203
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.82422e-06
Norm of the params: 22.007
     Influence (LOO): fixed 397 labels. Loss 0.20939. Accuracy 0.941.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0687997
Train loss (w/o reg) on all data: 0.0348116
Test loss (w/o reg) on all data: 0.466202
Train acc on all data:  0.999758278946
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 2.99241e-05
Norm of the params: 26.0722
                Loss: fixed 362 labels. Loss 0.46620. Accuracy 0.902.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176861
Train loss (w/o reg) on all data: 0.132583
Test loss (w/o reg) on all data: 0.477106
Train acc on all data:  0.963258399807
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.34352e-05
Norm of the params: 29.7583
              Random: fixed 116 labels. Loss 0.47711. Accuracy 0.865.
### Flips: 618, rs: 16, checks: 1030
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0922408
Train loss (w/o reg) on all data: 0.0698051
Test loss (w/o reg) on all data: 0.170689
Train acc on all data:  0.979211989364
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.60736e-06
Norm of the params: 21.1829
     Influence (LOO): fixed 435 labels. Loss 0.17069. Accuracy 0.955.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0615541
Train loss (w/o reg) on all data: 0.0307407
Test loss (w/o reg) on all data: 0.409445
Train acc on all data:  0.999274836838
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 9.90516e-06
Norm of the params: 24.8247
                Loss: fixed 401 labels. Loss 0.40944. Accuracy 0.916.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.170788
Train loss (w/o reg) on all data: 0.1277
Test loss (w/o reg) on all data: 0.48512
Train acc on all data:  0.964467005076
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.46445e-05
Norm of the params: 29.3558
              Random: fixed 143 labels. Loss 0.48512. Accuracy 0.865.
### Flips: 618, rs: 16, checks: 1236
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0825852
Train loss (w/o reg) on all data: 0.0619895
Test loss (w/o reg) on all data: 0.139558
Train acc on all data:  0.981145757796
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 8.09207e-06
Norm of the params: 20.2957
     Influence (LOO): fixed 474 labels. Loss 0.13956. Accuracy 0.961.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0534769
Train loss (w/o reg) on all data: 0.0261121
Test loss (w/o reg) on all data: 0.35647
Train acc on all data:  0.999033115784
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 6.45177e-06
Norm of the params: 23.3943
                Loss: fixed 436 labels. Loss 0.35647. Accuracy 0.927.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163578
Train loss (w/o reg) on all data: 0.121162
Test loss (w/o reg) on all data: 0.420118
Train acc on all data:  0.967125936669
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 4.22506e-05
Norm of the params: 29.126
              Random: fixed 175 labels. Loss 0.42012. Accuracy 0.881.
Using normal model
LBFGS training took [439] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198392
Train loss (w/o reg) on all data: 0.151349
Test loss (w/o reg) on all data: 0.683754
Train acc on all data:  0.952864394489
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 6.76898e-05
Norm of the params: 30.6735
Flipped loss: 0.68375. Accuracy: 0.835
### Flips: 618, rs: 17, checks: 206
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127926
Train loss (w/o reg) on all data: 0.0926519
Test loss (w/o reg) on all data: 0.658712
Train acc on all data:  0.973168963017
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.03251e-05
Norm of the params: 26.5609
     Influence (LOO): fixed 175 labels. Loss 0.65871. Accuracy 0.882.
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111339
Train loss (w/o reg) on all data: 0.0648945
Test loss (w/o reg) on all data: 0.720095
Train acc on all data:  0.99444041576
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 3.98803e-05
Norm of the params: 30.4778
                Loss: fixed 169 labels. Loss 0.72010. Accuracy 0.857.
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189294
Train loss (w/o reg) on all data: 0.142959
Test loss (w/o reg) on all data: 0.656667
Train acc on all data:  0.957940536621
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 1.61346e-05
Norm of the params: 30.4416
              Random: fixed  34 labels. Loss 0.65667. Accuracy 0.850.
### Flips: 618, rs: 17, checks: 412
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108217
Train loss (w/o reg) on all data: 0.0786281
Test loss (w/o reg) on all data: 0.607804
Train acc on all data:  0.978003384095
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 2.19243e-05
Norm of the params: 24.3264
     Influence (LOO): fixed 287 labels. Loss 0.60780. Accuracy 0.906.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0863582
Train loss (w/o reg) on all data: 0.0463525
Test loss (w/o reg) on all data: 0.675121
Train acc on all data:  0.997824510515
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 7.03899e-06
Norm of the params: 28.2863
                Loss: fixed 254 labels. Loss 0.67512. Accuracy 0.879.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182641
Train loss (w/o reg) on all data: 0.137298
Test loss (w/o reg) on all data: 0.612951
Train acc on all data:  0.957940536621
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 2.48128e-05
Norm of the params: 30.1141
              Random: fixed  68 labels. Loss 0.61295. Accuracy 0.853.
### Flips: 618, rs: 17, checks: 618
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0963629
Train loss (w/o reg) on all data: 0.0707368
Test loss (w/o reg) on all data: 0.609901
Train acc on all data:  0.981387478849
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 5.97891e-06
Norm of the params: 22.639
     Influence (LOO): fixed 368 labels. Loss 0.60990. Accuracy 0.932.
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0724382
Train loss (w/o reg) on all data: 0.0369247
Test loss (w/o reg) on all data: 0.60274
Train acc on all data:  0.998549673677
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.06448e-05
Norm of the params: 26.6509
                Loss: fixed 310 labels. Loss 0.60274. Accuracy 0.900.
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176431
Train loss (w/o reg) on all data: 0.131966
Test loss (w/o reg) on all data: 0.621218
Train acc on all data:  0.96035774716
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 4.42502e-05
Norm of the params: 29.8212
              Random: fixed  96 labels. Loss 0.62122. Accuracy 0.859.
### Flips: 618, rs: 17, checks: 824
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888186
Train loss (w/o reg) on all data: 0.0665174
Test loss (w/o reg) on all data: 0.562961
Train acc on all data:  0.981387478849
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 1.23943e-05
Norm of the params: 21.1193
     Influence (LOO): fixed 424 labels. Loss 0.56296. Accuracy 0.949.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0623715
Train loss (w/o reg) on all data: 0.0308022
Test loss (w/o reg) on all data: 0.486433
Train acc on all data:  0.999274836838
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 5.79844e-06
Norm of the params: 25.1274
                Loss: fixed 362 labels. Loss 0.48643. Accuracy 0.924.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169983
Train loss (w/o reg) on all data: 0.126315
Test loss (w/o reg) on all data: 0.587867
Train acc on all data:  0.962291515591
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 6.33357e-05
Norm of the params: 29.5525
              Random: fixed 124 labels. Loss 0.58787. Accuracy 0.868.
### Flips: 618, rs: 17, checks: 1030
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083361
Train loss (w/o reg) on all data: 0.0628651
Test loss (w/o reg) on all data: 0.475304
Train acc on all data:  0.981629199903
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 5.52698e-06
Norm of the params: 20.2464
     Influence (LOO): fixed 455 labels. Loss 0.47530. Accuracy 0.955.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0571949
Train loss (w/o reg) on all data: 0.0280026
Test loss (w/o reg) on all data: 0.421297
Train acc on all data:  0.999274836838
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 2.48323e-05
Norm of the params: 24.1629
                Loss: fixed 396 labels. Loss 0.42130. Accuracy 0.920.
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16138
Train loss (w/o reg) on all data: 0.119166
Test loss (w/o reg) on all data: 0.526412
Train acc on all data:  0.965433889292
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 4.07819e-05
Norm of the params: 29.0565
              Random: fixed 165 labels. Loss 0.52641. Accuracy 0.871.
### Flips: 618, rs: 17, checks: 1236
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0776265
Train loss (w/o reg) on all data: 0.0582039
Test loss (w/o reg) on all data: 0.416332
Train acc on all data:  0.983321247281
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 1.59227e-06
Norm of the params: 19.7092
     Influence (LOO): fixed 483 labels. Loss 0.41633. Accuracy 0.957.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0510023
Train loss (w/o reg) on all data: 0.0246283
Test loss (w/o reg) on all data: 0.376625
Train acc on all data:  0.999516557892
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 5.12952e-06
Norm of the params: 22.9669
                Loss: fixed 427 labels. Loss 0.37663. Accuracy 0.932.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154199
Train loss (w/o reg) on all data: 0.112998
Test loss (w/o reg) on all data: 0.525599
Train acc on all data:  0.968334541939
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 3.11947e-05
Norm of the params: 28.7056
              Random: fixed 195 labels. Loss 0.52560. Accuracy 0.886.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201928
Train loss (w/o reg) on all data: 0.154196
Test loss (w/o reg) on all data: 0.61819
Train acc on all data:  0.954072999758
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 2.6763e-05
Norm of the params: 30.8973
Flipped loss: 0.61819. Accuracy: 0.863
### Flips: 618, rs: 18, checks: 206
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136768
Train loss (w/o reg) on all data: 0.0994548
Test loss (w/o reg) on all data: 0.625941
Train acc on all data:  0.972685520909
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 3.08354e-05
Norm of the params: 27.3177
     Influence (LOO): fixed 166 labels. Loss 0.62594. Accuracy 0.895.
Using normal model
LBFGS training took [434] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117433
Train loss (w/o reg) on all data: 0.0683963
Test loss (w/o reg) on all data: 0.606109
Train acc on all data:  0.994682136814
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.04498e-05
Norm of the params: 31.3166
                Loss: fixed 167 labels. Loss 0.60611. Accuracy 0.864.
Using normal model
LBFGS training took [471] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19526
Train loss (w/o reg) on all data: 0.14795
Test loss (w/o reg) on all data: 0.646811
Train acc on all data:  0.957940536621
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 1.25213e-05
Norm of the params: 30.7605
              Random: fixed  30 labels. Loss 0.64681. Accuracy 0.872.
### Flips: 618, rs: 18, checks: 412
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114673
Train loss (w/o reg) on all data: 0.0836227
Test loss (w/o reg) on all data: 0.355393
Train acc on all data:  0.977278220933
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 2.02654e-05
Norm of the params: 24.9199
     Influence (LOO): fixed 279 labels. Loss 0.35539. Accuracy 0.921.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0884882
Train loss (w/o reg) on all data: 0.0463968
Test loss (w/o reg) on all data: 0.680537
Train acc on all data:  0.999033115784
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.12201e-05
Norm of the params: 29.0143
                Loss: fixed 256 labels. Loss 0.68054. Accuracy 0.877.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188599
Train loss (w/o reg) on all data: 0.142702
Test loss (w/o reg) on all data: 0.597577
Train acc on all data:  0.96035774716
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 3.66111e-05
Norm of the params: 30.2976
              Random: fixed  64 labels. Loss 0.59758. Accuracy 0.875.
### Flips: 618, rs: 18, checks: 618
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104203
Train loss (w/o reg) on all data: 0.0769603
Test loss (w/o reg) on all data: 0.25102
Train acc on all data:  0.977761663041
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 3.89933e-06
Norm of the params: 23.3422
     Influence (LOO): fixed 354 labels. Loss 0.25102. Accuracy 0.940.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0780829
Train loss (w/o reg) on all data: 0.0399584
Test loss (w/o reg) on all data: 0.647722
Train acc on all data:  0.999033115784
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 3.55569e-05
Norm of the params: 27.6133
                Loss: fixed 298 labels. Loss 0.64772. Accuracy 0.889.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180008
Train loss (w/o reg) on all data: 0.135347
Test loss (w/o reg) on all data: 0.559675
Train acc on all data:  0.962291515591
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.41146e-05
Norm of the params: 29.8868
              Random: fixed  99 labels. Loss 0.55968. Accuracy 0.889.
### Flips: 618, rs: 18, checks: 824
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0975446
Train loss (w/o reg) on all data: 0.07297
Test loss (w/o reg) on all data: 0.333422
Train acc on all data:  0.978003384095
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 6.05107e-06
Norm of the params: 22.1696
     Influence (LOO): fixed 402 labels. Loss 0.33342. Accuracy 0.946.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0697379
Train loss (w/o reg) on all data: 0.0353732
Test loss (w/o reg) on all data: 0.622639
Train acc on all data:  0.99879139473
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 2.12298e-05
Norm of the params: 26.2163
                Loss: fixed 342 labels. Loss 0.62264. Accuracy 0.899.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17302
Train loss (w/o reg) on all data: 0.128895
Test loss (w/o reg) on all data: 0.568739
Train acc on all data:  0.965192168238
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 1.12301e-05
Norm of the params: 29.7072
              Random: fixed 126 labels. Loss 0.56874. Accuracy 0.883.
### Flips: 618, rs: 18, checks: 1030
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0893824
Train loss (w/o reg) on all data: 0.067493
Test loss (w/o reg) on all data: 0.271491
Train acc on all data:  0.979937152526
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 2.10985e-06
Norm of the params: 20.9234
     Influence (LOO): fixed 446 labels. Loss 0.27149. Accuracy 0.954.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0626547
Train loss (w/o reg) on all data: 0.0314593
Test loss (w/o reg) on all data: 0.604058
Train acc on all data:  0.998549673677
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 1.954e-05
Norm of the params: 24.9782
                Loss: fixed 392 labels. Loss 0.60406. Accuracy 0.923.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166185
Train loss (w/o reg) on all data: 0.122987
Test loss (w/o reg) on all data: 0.56874
Train acc on all data:  0.967851099831
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 1.12527e-05
Norm of the params: 29.3929
              Random: fixed 159 labels. Loss 0.56874. Accuracy 0.890.
### Flips: 618, rs: 18, checks: 1236
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0826456
Train loss (w/o reg) on all data: 0.0621904
Test loss (w/o reg) on all data: 0.303878
Train acc on all data:  0.982596084119
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 3.25193e-06
Norm of the params: 20.2263
     Influence (LOO): fixed 479 labels. Loss 0.30388. Accuracy 0.953.
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0570775
Train loss (w/o reg) on all data: 0.0283041
Test loss (w/o reg) on all data: 0.538193
Train acc on all data:  0.999033115784
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.15064e-05
Norm of the params: 23.9889
                Loss: fixed 426 labels. Loss 0.53819. Accuracy 0.934.
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160647
Train loss (w/o reg) on all data: 0.118301
Test loss (w/o reg) on all data: 0.480024
Train acc on all data:  0.97026831037
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.70562e-05
Norm of the params: 29.1019
              Random: fixed 184 labels. Loss 0.48002. Accuracy 0.897.
Using normal model
LBFGS training took [610] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211453
Train loss (w/o reg) on all data: 0.163119
Test loss (w/o reg) on all data: 0.477548
Train acc on all data:  0.94923857868
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 0.000124773
Norm of the params: 31.0915
Flipped loss: 0.47755. Accuracy: 0.841
### Flips: 618, rs: 19, checks: 206
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144009
Train loss (w/o reg) on all data: 0.107193
Test loss (w/o reg) on all data: 0.425157
Train acc on all data:  0.969301426154
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 3.40893e-05
Norm of the params: 27.1352
     Influence (LOO): fixed 165 labels. Loss 0.42516. Accuracy 0.883.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121128
Train loss (w/o reg) on all data: 0.0729309
Test loss (w/o reg) on all data: 0.447926
Train acc on all data:  0.988639110467
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 8.01323e-06
Norm of the params: 31.0473
                Loss: fixed 168 labels. Loss 0.44793. Accuracy 0.848.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205133
Train loss (w/o reg) on all data: 0.157888
Test loss (w/o reg) on all data: 0.457147
Train acc on all data:  0.950688905004
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 7.96341e-05
Norm of the params: 30.7391
              Random: fixed  32 labels. Loss 0.45715. Accuracy 0.842.
### Flips: 618, rs: 19, checks: 412
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124454
Train loss (w/o reg) on all data: 0.0936569
Test loss (w/o reg) on all data: 0.392242
Train acc on all data:  0.973894126178
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 4.05628e-06
Norm of the params: 24.8184
     Influence (LOO): fixed 287 labels. Loss 0.39224. Accuracy 0.913.
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0912914
Train loss (w/o reg) on all data: 0.0494236
Test loss (w/o reg) on all data: 0.42371
Train acc on all data:  0.997582789461
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.70199e-05
Norm of the params: 28.9371
                Loss: fixed 267 labels. Loss 0.42371. Accuracy 0.889.
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19684
Train loss (w/o reg) on all data: 0.150627
Test loss (w/o reg) on all data: 0.442366
Train acc on all data:  0.954072999758
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 4.20236e-05
Norm of the params: 30.4016
              Random: fixed  59 labels. Loss 0.44237. Accuracy 0.860.
### Flips: 618, rs: 19, checks: 618
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112928
Train loss (w/o reg) on all data: 0.0854633
Test loss (w/o reg) on all data: 0.376714
Train acc on all data:  0.97461928934
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 3.04801e-06
Norm of the params: 23.437
     Influence (LOO): fixed 354 labels. Loss 0.37671. Accuracy 0.926.
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0834294
Train loss (w/o reg) on all data: 0.0442615
Test loss (w/o reg) on all data: 0.340368
Train acc on all data:  0.999033115784
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 9.87374e-06
Norm of the params: 27.9885
                Loss: fixed 310 labels. Loss 0.34037. Accuracy 0.898.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190432
Train loss (w/o reg) on all data: 0.144781
Test loss (w/o reg) on all data: 0.438229
Train acc on all data:  0.956248489243
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.27371e-05
Norm of the params: 30.2161
              Random: fixed  94 labels. Loss 0.43823. Accuracy 0.860.
### Flips: 618, rs: 19, checks: 824
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104547
Train loss (w/o reg) on all data: 0.0796624
Test loss (w/o reg) on all data: 0.381498
Train acc on all data:  0.976311336717
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.28788e-05
Norm of the params: 22.309
     Influence (LOO): fixed 399 labels. Loss 0.38150. Accuracy 0.936.
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0753903
Train loss (w/o reg) on all data: 0.0392314
Test loss (w/o reg) on all data: 0.322638
Train acc on all data:  0.999033115784
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.29244e-05
Norm of the params: 26.892
                Loss: fixed 345 labels. Loss 0.32264. Accuracy 0.907.
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182287
Train loss (w/o reg) on all data: 0.137458
Test loss (w/o reg) on all data: 0.432153
Train acc on all data:  0.95914914189
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 1.23955e-05
Norm of the params: 29.9427
              Random: fixed 129 labels. Loss 0.43215. Accuracy 0.866.
### Flips: 618, rs: 19, checks: 1030
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984042
Train loss (w/o reg) on all data: 0.0756118
Test loss (w/o reg) on all data: 0.373575
Train acc on all data:  0.976794778825
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 1.93781e-06
Norm of the params: 21.3506
     Influence (LOO): fixed 432 labels. Loss 0.37358. Accuracy 0.949.
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0682766
Train loss (w/o reg) on all data: 0.0348251
Test loss (w/o reg) on all data: 0.33184
Train acc on all data:  0.999274836838
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.95807e-05
Norm of the params: 25.8656
                Loss: fixed 383 labels. Loss 0.33184. Accuracy 0.916.
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177078
Train loss (w/o reg) on all data: 0.132997
Test loss (w/o reg) on all data: 0.412359
Train acc on all data:  0.958907420836
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 6.42561e-05
Norm of the params: 29.6921
              Random: fixed 155 labels. Loss 0.41236. Accuracy 0.869.
### Flips: 618, rs: 19, checks: 1236
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0921709
Train loss (w/o reg) on all data: 0.0707917
Test loss (w/o reg) on all data: 0.335324
Train acc on all data:  0.979211989364
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 2.98977e-06
Norm of the params: 20.6781
     Influence (LOO): fixed 459 labels. Loss 0.33532. Accuracy 0.952.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0615775
Train loss (w/o reg) on all data: 0.0312343
Test loss (w/o reg) on all data: 0.27774
Train acc on all data:  0.99879139473
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 2.20484e-05
Norm of the params: 24.6346
                Loss: fixed 424 labels. Loss 0.27774. Accuracy 0.920.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.170952
Train loss (w/o reg) on all data: 0.128137
Test loss (w/o reg) on all data: 0.38887
Train acc on all data:  0.960599468214
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 1.59491e-05
Norm of the params: 29.2627
              Random: fixed 181 labels. Loss 0.38887. Accuracy 0.881.
Using normal model
LBFGS training took [474] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206086
Train loss (w/o reg) on all data: 0.158927
Test loss (w/o reg) on all data: 0.690096
Train acc on all data:  0.952864394489
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 2.08581e-05
Norm of the params: 30.7112
Flipped loss: 0.69010. Accuracy: 0.847
### Flips: 618, rs: 20, checks: 206
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133515
Train loss (w/o reg) on all data: 0.0975671
Test loss (w/o reg) on all data: 0.556903
Train acc on all data:  0.97461928934
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 7.0853e-06
Norm of the params: 26.8133
     Influence (LOO): fixed 178 labels. Loss 0.55690. Accuracy 0.871.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111415
Train loss (w/o reg) on all data: 0.0650699
Test loss (w/o reg) on all data: 0.625597
Train acc on all data:  0.991056321006
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 7.1985e-06
Norm of the params: 30.4451
                Loss: fixed 167 labels. Loss 0.62560. Accuracy 0.846.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199508
Train loss (w/o reg) on all data: 0.153081
Test loss (w/o reg) on all data: 0.685911
Train acc on all data:  0.953831278704
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 1.39349e-05
Norm of the params: 30.472
              Random: fixed  25 labels. Loss 0.68591. Accuracy 0.853.
### Flips: 618, rs: 20, checks: 412
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114233
Train loss (w/o reg) on all data: 0.0839331
Test loss (w/o reg) on all data: 0.352663
Train acc on all data:  0.977278220933
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 5.14002e-06
Norm of the params: 24.6171
     Influence (LOO): fixed 292 labels. Loss 0.35266. Accuracy 0.912.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0853225
Train loss (w/o reg) on all data: 0.0456691
Test loss (w/o reg) on all data: 0.584689
Train acc on all data:  0.998307952623
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.77278e-05
Norm of the params: 28.1615
                Loss: fixed 253 labels. Loss 0.58469. Accuracy 0.876.
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190263
Train loss (w/o reg) on all data: 0.145003
Test loss (w/o reg) on all data: 0.713002
Train acc on all data:  0.956973652405
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 4.40893e-05
Norm of the params: 30.0867
              Random: fixed  58 labels. Loss 0.71300. Accuracy 0.854.
### Flips: 618, rs: 20, checks: 618
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104394
Train loss (w/o reg) on all data: 0.0774492
Test loss (w/o reg) on all data: 0.318377
Train acc on all data:  0.978003384095
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.08455e-06
Norm of the params: 23.214
     Influence (LOO): fixed 362 labels. Loss 0.31838. Accuracy 0.935.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0737374
Train loss (w/o reg) on all data: 0.0380121
Test loss (w/o reg) on all data: 0.53243
Train acc on all data:  0.999033115784
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 2.20509e-05
Norm of the params: 26.7302
                Loss: fixed 302 labels. Loss 0.53243. Accuracy 0.901.
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184774
Train loss (w/o reg) on all data: 0.140539
Test loss (w/o reg) on all data: 0.693717
Train acc on all data:  0.959874305052
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 3.74839e-05
Norm of the params: 29.7437
              Random: fixed  85 labels. Loss 0.69372. Accuracy 0.859.
### Flips: 618, rs: 20, checks: 824
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0956671
Train loss (w/o reg) on all data: 0.0716897
Test loss (w/o reg) on all data: 0.332144
Train acc on all data:  0.979453710418
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 5.06872e-06
Norm of the params: 21.8986
     Influence (LOO): fixed 416 labels. Loss 0.33214. Accuracy 0.950.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0681572
Train loss (w/o reg) on all data: 0.0345421
Test loss (w/o reg) on all data: 0.532783
Train acc on all data:  0.999033115784
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.92023e-05
Norm of the params: 25.9288
                Loss: fixed 334 labels. Loss 0.53278. Accuracy 0.903.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179118
Train loss (w/o reg) on all data: 0.135707
Test loss (w/o reg) on all data: 0.684682
Train acc on all data:  0.961082910321
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.89706e-05
Norm of the params: 29.4654
              Random: fixed 114 labels. Loss 0.68468. Accuracy 0.860.
### Flips: 618, rs: 20, checks: 1030
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089258
Train loss (w/o reg) on all data: 0.0676027
Test loss (w/o reg) on all data: 0.261855
Train acc on all data:  0.979937152526
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 9.82205e-06
Norm of the params: 20.8112
     Influence (LOO): fixed 456 labels. Loss 0.26186. Accuracy 0.951.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0618307
Train loss (w/o reg) on all data: 0.030791
Test loss (w/o reg) on all data: 0.397895
Train acc on all data:  0.999516557892
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 3.85019e-05
Norm of the params: 24.9157
                Loss: fixed 368 labels. Loss 0.39789. Accuracy 0.921.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172416
Train loss (w/o reg) on all data: 0.129653
Test loss (w/o reg) on all data: 0.652382
Train acc on all data:  0.963500120861
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 2.44063e-05
Norm of the params: 29.2449
              Random: fixed 151 labels. Loss 0.65238. Accuracy 0.871.
### Flips: 618, rs: 20, checks: 1236
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0833598
Train loss (w/o reg) on all data: 0.0632716
Test loss (w/o reg) on all data: 0.218152
Train acc on all data:  0.982112642011
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 3.93058e-06
Norm of the params: 20.0441
     Influence (LOO): fixed 488 labels. Loss 0.21815. Accuracy 0.964.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0562129
Train loss (w/o reg) on all data: 0.0277094
Test loss (w/o reg) on all data: 0.369251
Train acc on all data:  0.999516557892
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 8.02094e-06
Norm of the params: 23.8761
                Loss: fixed 395 labels. Loss 0.36925. Accuracy 0.922.
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164488
Train loss (w/o reg) on all data: 0.123045
Test loss (w/o reg) on all data: 0.594607
Train acc on all data:  0.965675610346
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 2.41509e-05
Norm of the params: 28.7899
              Random: fixed 186 labels. Loss 0.59461. Accuracy 0.871.
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207432
Train loss (w/o reg) on all data: 0.159432
Test loss (w/o reg) on all data: 0.59414
Train acc on all data:  0.950688905004
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 1.20875e-05
Norm of the params: 30.9841
Flipped loss: 0.59414. Accuracy: 0.850
### Flips: 618, rs: 21, checks: 206
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141775
Train loss (w/o reg) on all data: 0.105051
Test loss (w/o reg) on all data: 0.529573
Train acc on all data:  0.971476915639
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 3.02919e-05
Norm of the params: 27.1013
     Influence (LOO): fixed 166 labels. Loss 0.52957. Accuracy 0.895.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120982
Train loss (w/o reg) on all data: 0.0735347
Test loss (w/o reg) on all data: 0.535491
Train acc on all data:  0.989122552574
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.20502e-05
Norm of the params: 30.8051
                Loss: fixed 165 labels. Loss 0.53549. Accuracy 0.860.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198344
Train loss (w/o reg) on all data: 0.151379
Test loss (w/o reg) on all data: 0.498611
Train acc on all data:  0.95479816292
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 1.42745e-05
Norm of the params: 30.6481
              Random: fixed  40 labels. Loss 0.49861. Accuracy 0.869.
### Flips: 618, rs: 21, checks: 412
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11829
Train loss (w/o reg) on all data: 0.087722
Test loss (w/o reg) on all data: 0.326637
Train acc on all data:  0.975102731448
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 6.18642e-06
Norm of the params: 24.7258
     Influence (LOO): fixed 286 labels. Loss 0.32664. Accuracy 0.931.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0955471
Train loss (w/o reg) on all data: 0.0533038
Test loss (w/o reg) on all data: 0.577217
Train acc on all data:  0.993715252599
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 1.70002e-05
Norm of the params: 29.0666
                Loss: fixed 249 labels. Loss 0.57722. Accuracy 0.887.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193983
Train loss (w/o reg) on all data: 0.148045
Test loss (w/o reg) on all data: 0.463783
Train acc on all data:  0.955281605028
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 3.46884e-05
Norm of the params: 30.3112
              Random: fixed  69 labels. Loss 0.46378. Accuracy 0.858.
### Flips: 618, rs: 21, checks: 618
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107947
Train loss (w/o reg) on all data: 0.080354
Test loss (w/o reg) on all data: 0.236533
Train acc on all data:  0.977761663041
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 4.76303e-06
Norm of the params: 23.4918
     Influence (LOO): fixed 354 labels. Loss 0.23653. Accuracy 0.944.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0778846
Train loss (w/o reg) on all data: 0.0412505
Test loss (w/o reg) on all data: 0.482123
Train acc on all data:  0.99879139473
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 2.95623e-05
Norm of the params: 27.0681
                Loss: fixed 317 labels. Loss 0.48212. Accuracy 0.917.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185739
Train loss (w/o reg) on all data: 0.140898
Test loss (w/o reg) on all data: 0.47495
Train acc on all data:  0.958182257675
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 6.94784e-05
Norm of the params: 29.9469
              Random: fixed  96 labels. Loss 0.47495. Accuracy 0.861.
### Flips: 618, rs: 21, checks: 824
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0996761
Train loss (w/o reg) on all data: 0.07449
Test loss (w/o reg) on all data: 0.234885
Train acc on all data:  0.979211989364
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 5.05936e-06
Norm of the params: 22.4438
     Influence (LOO): fixed 408 labels. Loss 0.23488. Accuracy 0.955.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0681341
Train loss (w/o reg) on all data: 0.0355328
Test loss (w/o reg) on all data: 0.44883
Train acc on all data:  0.998307952623
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 1.30711e-05
Norm of the params: 25.5348
                Loss: fixed 365 labels. Loss 0.44883. Accuracy 0.919.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178674
Train loss (w/o reg) on all data: 0.13451
Test loss (w/o reg) on all data: 0.506932
Train acc on all data:  0.963258399807
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 0.00011129
Norm of the params: 29.72
              Random: fixed 124 labels. Loss 0.50693. Accuracy 0.871.
### Flips: 618, rs: 21, checks: 1030
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09084
Train loss (w/o reg) on all data: 0.0689187
Test loss (w/o reg) on all data: 0.173665
Train acc on all data:  0.979937152526
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 3.33258e-06
Norm of the params: 20.9386
     Influence (LOO): fixed 459 labels. Loss 0.17367. Accuracy 0.965.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0608162
Train loss (w/o reg) on all data: 0.0308996
Test loss (w/o reg) on all data: 0.380519
Train acc on all data:  0.999033115784
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 9.9034e-06
Norm of the params: 24.4608
                Loss: fixed 406 labels. Loss 0.38052. Accuracy 0.929.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17199
Train loss (w/o reg) on all data: 0.128655
Test loss (w/o reg) on all data: 0.511433
Train acc on all data:  0.9659173314
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 1.10866e-05
Norm of the params: 29.4397
              Random: fixed 151 labels. Loss 0.51143. Accuracy 0.872.
### Flips: 618, rs: 21, checks: 1236
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0844245
Train loss (w/o reg) on all data: 0.0643147
Test loss (w/o reg) on all data: 0.165717
Train acc on all data:  0.982596084119
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 2.95374e-06
Norm of the params: 20.0548
     Influence (LOO): fixed 492 labels. Loss 0.16572. Accuracy 0.965.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0557386
Train loss (w/o reg) on all data: 0.0281336
Test loss (w/o reg) on all data: 0.304673
Train acc on all data:  0.99879139473
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 1.15924e-05
Norm of the params: 23.4968
                Loss: fixed 438 labels. Loss 0.30467. Accuracy 0.939.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161346
Train loss (w/o reg) on all data: 0.119199
Test loss (w/o reg) on all data: 0.4495
Train acc on all data:  0.969784868262
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.41518e-05
Norm of the params: 29.0334
              Random: fixed 187 labels. Loss 0.44950. Accuracy 0.877.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205325
Train loss (w/o reg) on all data: 0.159634
Test loss (w/o reg) on all data: 0.851061
Train acc on all data:  0.954314720812
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 5.26711e-05
Norm of the params: 30.2296
Flipped loss: 0.85106. Accuracy: 0.859
### Flips: 618, rs: 22, checks: 206
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140483
Train loss (w/o reg) on all data: 0.105111
Test loss (w/o reg) on all data: 0.693211
Train acc on all data:  0.971235194585
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 7.552e-06
Norm of the params: 26.5974
     Influence (LOO): fixed 169 labels. Loss 0.69321. Accuracy 0.889.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112157
Train loss (w/o reg) on all data: 0.0655891
Test loss (w/o reg) on all data: 0.706445
Train acc on all data:  0.995165578922
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.10265e-05
Norm of the params: 30.518
                Loss: fixed 179 labels. Loss 0.70645. Accuracy 0.865.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197228
Train loss (w/o reg) on all data: 0.152138
Test loss (w/o reg) on all data: 0.79098
Train acc on all data:  0.958182257675
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 6.83465e-05
Norm of the params: 30.0299
              Random: fixed  34 labels. Loss 0.79098. Accuracy 0.865.
### Flips: 618, rs: 22, checks: 412
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12244
Train loss (w/o reg) on all data: 0.0930821
Test loss (w/o reg) on all data: 0.601206
Train acc on all data:  0.973652405124
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 4.76861e-06
Norm of the params: 24.2314
     Influence (LOO): fixed 283 labels. Loss 0.60121. Accuracy 0.913.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0882035
Train loss (w/o reg) on all data: 0.0471305
Test loss (w/o reg) on all data: 0.627297
Train acc on all data:  0.998307952623
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 1.50242e-05
Norm of the params: 28.6611
                Loss: fixed 258 labels. Loss 0.62730. Accuracy 0.885.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19018
Train loss (w/o reg) on all data: 0.14634
Test loss (w/o reg) on all data: 0.830859
Train acc on all data:  0.959390862944
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 3.34714e-05
Norm of the params: 29.6107
              Random: fixed  69 labels. Loss 0.83086. Accuracy 0.871.
### Flips: 618, rs: 22, checks: 618
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112691
Train loss (w/o reg) on all data: 0.0865003
Test loss (w/o reg) on all data: 0.513125
Train acc on all data:  0.97461928934
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 7.5651e-06
Norm of the params: 22.8869
     Influence (LOO): fixed 354 labels. Loss 0.51312. Accuracy 0.930.
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0756204
Train loss (w/o reg) on all data: 0.0390661
Test loss (w/o reg) on all data: 0.49865
Train acc on all data:  0.99879139473
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 6.5445e-06
Norm of the params: 27.0386
                Loss: fixed 319 labels. Loss 0.49865. Accuracy 0.898.
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184306
Train loss (w/o reg) on all data: 0.141154
Test loss (w/o reg) on all data: 0.754929
Train acc on all data:  0.960599468214
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 4.28881e-05
Norm of the params: 29.3774
              Random: fixed 102 labels. Loss 0.75493. Accuracy 0.880.
### Flips: 618, rs: 22, checks: 824
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102832
Train loss (w/o reg) on all data: 0.0793035
Test loss (w/o reg) on all data: 0.45021
Train acc on all data:  0.977036499879
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 5.42825e-06
Norm of the params: 21.6928
     Influence (LOO): fixed 411 labels. Loss 0.45021. Accuracy 0.950.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0690214
Train loss (w/o reg) on all data: 0.0352392
Test loss (w/o reg) on all data: 0.438129
Train acc on all data:  0.999274836838
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.45844e-05
Norm of the params: 25.9931
                Loss: fixed 352 labels. Loss 0.43813. Accuracy 0.905.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17665
Train loss (w/o reg) on all data: 0.134088
Test loss (w/o reg) on all data: 0.726077
Train acc on all data:  0.964467005076
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 3.28944e-05
Norm of the params: 29.1763
              Random: fixed 133 labels. Loss 0.72608. Accuracy 0.875.
### Flips: 618, rs: 22, checks: 1030
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0942553
Train loss (w/o reg) on all data: 0.0723455
Test loss (w/o reg) on all data: 0.393845
Train acc on all data:  0.978486826203
Test acc on all data:   0.955555555556
Norm of the mean of gradients: 4.61247e-06
Norm of the params: 20.9331
     Influence (LOO): fixed 446 labels. Loss 0.39384. Accuracy 0.956.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061995
Train loss (w/o reg) on all data: 0.0311287
Test loss (w/o reg) on all data: 0.513374
Train acc on all data:  0.999274836838
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 3.39162e-06
Norm of the params: 24.846
                Loss: fixed 379 labels. Loss 0.51337. Accuracy 0.908.
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.170313
Train loss (w/o reg) on all data: 0.128278
Test loss (w/o reg) on all data: 0.682417
Train acc on all data:  0.96470872613
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 4.20533e-05
Norm of the params: 28.9949
              Random: fixed 157 labels. Loss 0.68242. Accuracy 0.882.
### Flips: 618, rs: 22, checks: 1236
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0873695
Train loss (w/o reg) on all data: 0.0668933
Test loss (w/o reg) on all data: 0.394132
Train acc on all data:  0.979695431472
Test acc on all data:   0.966183574879
Norm of the mean of gradients: 8.81007e-06
Norm of the params: 20.2367
     Influence (LOO): fixed 474 labels. Loss 0.39413. Accuracy 0.966.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0536092
Train loss (w/o reg) on all data: 0.026201
Test loss (w/o reg) on all data: 0.412514
Train acc on all data:  0.999274836838
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 4.67016e-06
Norm of the params: 23.4129
                Loss: fixed 418 labels. Loss 0.41251. Accuracy 0.925.
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16383
Train loss (w/o reg) on all data: 0.122461
Test loss (w/o reg) on all data: 0.696837
Train acc on all data:  0.967609378777
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 7.90803e-06
Norm of the params: 28.7644
              Random: fixed 184 labels. Loss 0.69684. Accuracy 0.886.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211297
Train loss (w/o reg) on all data: 0.164231
Test loss (w/o reg) on all data: 0.556821
Train acc on all data:  0.950930626058
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 1.39186e-05
Norm of the params: 30.6808
Flipped loss: 0.55682. Accuracy: 0.849
### Flips: 618, rs: 23, checks: 206
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143682
Train loss (w/o reg) on all data: 0.10702
Test loss (w/o reg) on all data: 0.452317
Train acc on all data:  0.970751752478
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 6.31122e-06
Norm of the params: 27.0782
     Influence (LOO): fixed 164 labels. Loss 0.45232. Accuracy 0.895.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114597
Train loss (w/o reg) on all data: 0.0678598
Test loss (w/o reg) on all data: 0.44504
Train acc on all data:  0.995407299976
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 2.12022e-05
Norm of the params: 30.5735
                Loss: fixed 180 labels. Loss 0.44504. Accuracy 0.858.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206278
Train loss (w/o reg) on all data: 0.159333
Test loss (w/o reg) on all data: 0.541587
Train acc on all data:  0.954314720812
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 2.85319e-05
Norm of the params: 30.6416
              Random: fixed  24 labels. Loss 0.54159. Accuracy 0.860.
### Flips: 618, rs: 23, checks: 412
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121418
Train loss (w/o reg) on all data: 0.0914803
Test loss (w/o reg) on all data: 0.374465
Train acc on all data:  0.97582789461
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 4.13363e-06
Norm of the params: 24.4694
     Influence (LOO): fixed 288 labels. Loss 0.37447. Accuracy 0.924.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0919046
Train loss (w/o reg) on all data: 0.0498733
Test loss (w/o reg) on all data: 0.369412
Train acc on all data:  0.998066231569
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 8.45848e-06
Norm of the params: 28.9936
                Loss: fixed 256 labels. Loss 0.36941. Accuracy 0.896.
Using normal model
LBFGS training took [492] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200526
Train loss (w/o reg) on all data: 0.154607
Test loss (w/o reg) on all data: 0.520524
Train acc on all data:  0.956248489243
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 5.4357e-05
Norm of the params: 30.3047
              Random: fixed  54 labels. Loss 0.52052. Accuracy 0.860.
### Flips: 618, rs: 23, checks: 618
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111348
Train loss (w/o reg) on all data: 0.0847781
Test loss (w/o reg) on all data: 0.363997
Train acc on all data:  0.976553057771
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 4.39216e-06
Norm of the params: 23.0521
     Influence (LOO): fixed 348 labels. Loss 0.36400. Accuracy 0.934.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0759761
Train loss (w/o reg) on all data: 0.0392644
Test loss (w/o reg) on all data: 0.351953
Train acc on all data:  0.999033115784
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 4.98758e-06
Norm of the params: 27.0967
                Loss: fixed 319 labels. Loss 0.35195. Accuracy 0.902.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194247
Train loss (w/o reg) on all data: 0.148977
Test loss (w/o reg) on all data: 0.510059
Train acc on all data:  0.958665699782
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 2.52754e-05
Norm of the params: 30.09
              Random: fixed  89 labels. Loss 0.51006. Accuracy 0.867.
### Flips: 618, rs: 23, checks: 824
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984759
Train loss (w/o reg) on all data: 0.0750348
Test loss (w/o reg) on all data: 0.287793
Train acc on all data:  0.978245105149
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 6.32053e-06
Norm of the params: 21.6523
     Influence (LOO): fixed 407 labels. Loss 0.28779. Accuracy 0.960.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0667284
Train loss (w/o reg) on all data: 0.0336305
Test loss (w/o reg) on all data: 0.359306
Train acc on all data:  0.999274836838
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 9.27e-06
Norm of the params: 25.7286
                Loss: fixed 364 labels. Loss 0.35931. Accuracy 0.922.
Using normal model
LBFGS training took [485] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185955
Train loss (w/o reg) on all data: 0.141671
Test loss (w/o reg) on all data: 0.507679
Train acc on all data:  0.960841189268
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 2.26967e-05
Norm of the params: 29.7605
              Random: fixed 121 labels. Loss 0.50768. Accuracy 0.872.
### Flips: 618, rs: 23, checks: 1030
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0911684
Train loss (w/o reg) on all data: 0.0694196
Test loss (w/o reg) on all data: 0.317905
Train acc on all data:  0.979695431472
Test acc on all data:   0.961352657005
Norm of the mean of gradients: 1.82453e-06
Norm of the params: 20.8561
     Influence (LOO): fixed 445 labels. Loss 0.31791. Accuracy 0.961.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0587413
Train loss (w/o reg) on all data: 0.0293119
Test loss (w/o reg) on all data: 0.362976
Train acc on all data:  0.999033115784
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 9.80317e-06
Norm of the params: 24.2608
                Loss: fixed 406 labels. Loss 0.36298. Accuracy 0.926.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181226
Train loss (w/o reg) on all data: 0.137741
Test loss (w/o reg) on all data: 0.48619
Train acc on all data:  0.962774957699
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 3.67672e-05
Norm of the params: 29.4907
              Random: fixed 146 labels. Loss 0.48619. Accuracy 0.872.
### Flips: 618, rs: 23, checks: 1236
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088585
Train loss (w/o reg) on all data: 0.0678391
Test loss (w/o reg) on all data: 0.315137
Train acc on all data:  0.98017887358
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 4.32651e-06
Norm of the params: 20.3696
     Influence (LOO): fixed 469 labels. Loss 0.31514. Accuracy 0.965.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0550792
Train loss (w/o reg) on all data: 0.0272032
Test loss (w/o reg) on all data: 0.357894
Train acc on all data:  0.999033115784
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 5.00937e-06
Norm of the params: 23.6119
                Loss: fixed 431 labels. Loss 0.35789. Accuracy 0.931.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175044
Train loss (w/o reg) on all data: 0.132794
Test loss (w/o reg) on all data: 0.460998
Train acc on all data:  0.96470872613
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 5.05072e-05
Norm of the params: 29.0691
              Random: fixed 175 labels. Loss 0.46100. Accuracy 0.880.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207647
Train loss (w/o reg) on all data: 0.1603
Test loss (w/o reg) on all data: 0.670671
Train acc on all data:  0.952864394489
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 0.000168425
Norm of the params: 30.7725
Flipped loss: 0.67067. Accuracy: 0.826
### Flips: 618, rs: 24, checks: 206
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134469
Train loss (w/o reg) on all data: 0.098275
Test loss (w/o reg) on all data: 0.609027
Train acc on all data:  0.973652405124
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 1.77741e-05
Norm of the params: 26.9051
     Influence (LOO): fixed 177 labels. Loss 0.60903. Accuracy 0.885.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117975
Train loss (w/o reg) on all data: 0.0699317
Test loss (w/o reg) on all data: 0.703418
Train acc on all data:  0.990572878898
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.94802e-05
Norm of the params: 30.998
                Loss: fixed 166 labels. Loss 0.70342. Accuracy 0.851.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201961
Train loss (w/o reg) on all data: 0.155176
Test loss (w/o reg) on all data: 0.673161
Train acc on all data:  0.955281605028
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 2.55917e-05
Norm of the params: 30.5892
              Random: fixed  26 labels. Loss 0.67316. Accuracy 0.829.
### Flips: 618, rs: 24, checks: 412
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116761
Train loss (w/o reg) on all data: 0.0860226
Test loss (w/o reg) on all data: 0.45413
Train acc on all data:  0.977278220933
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 1.61928e-05
Norm of the params: 24.7945
     Influence (LOO): fixed 278 labels. Loss 0.45413. Accuracy 0.917.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0938046
Train loss (w/o reg) on all data: 0.0519292
Test loss (w/o reg) on all data: 0.63681
Train acc on all data:  0.997099347353
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.1036e-05
Norm of the params: 28.9397
                Loss: fixed 260 labels. Loss 0.63681. Accuracy 0.889.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196112
Train loss (w/o reg) on all data: 0.149854
Test loss (w/o reg) on all data: 0.682689
Train acc on all data:  0.956731931351
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 9.01378e-05
Norm of the params: 30.4164
              Random: fixed  54 labels. Loss 0.68269. Accuracy 0.845.
### Flips: 618, rs: 24, checks: 618
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106562
Train loss (w/o reg) on all data: 0.0795164
Test loss (w/o reg) on all data: 0.323498
Train acc on all data:  0.978003384095
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 6.70086e-06
Norm of the params: 23.2575
     Influence (LOO): fixed 354 labels. Loss 0.32350. Accuracy 0.940.
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0753967
Train loss (w/o reg) on all data: 0.0395343
Test loss (w/o reg) on all data: 0.457953
Train acc on all data:  0.99879139473
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.83818e-05
Norm of the params: 26.7815
                Loss: fixed 323 labels. Loss 0.45795. Accuracy 0.906.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19224
Train loss (w/o reg) on all data: 0.146623
Test loss (w/o reg) on all data: 0.665411
Train acc on all data:  0.957215373459
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 6.04909e-05
Norm of the params: 30.205
              Random: fixed  74 labels. Loss 0.66541. Accuracy 0.847.
### Flips: 618, rs: 24, checks: 824
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0996007
Train loss (w/o reg) on all data: 0.0751629
Test loss (w/o reg) on all data: 0.292606
Train acc on all data:  0.978728547256
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.52824e-05
Norm of the params: 22.1078
     Influence (LOO): fixed 409 labels. Loss 0.29261. Accuracy 0.945.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0680843
Train loss (w/o reg) on all data: 0.0350775
Test loss (w/o reg) on all data: 0.383121
Train acc on all data:  0.99879139473
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 2.50777e-05
Norm of the params: 25.6931
                Loss: fixed 357 labels. Loss 0.38312. Accuracy 0.916.
Using normal model
LBFGS training took [450] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180341
Train loss (w/o reg) on all data: 0.135845
Test loss (w/o reg) on all data: 0.708868
Train acc on all data:  0.963016678753
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.45629e-05
Norm of the params: 29.8314
              Random: fixed 111 labels. Loss 0.70887. Accuracy 0.858.
### Flips: 618, rs: 24, checks: 1030
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0926027
Train loss (w/o reg) on all data: 0.0698127
Test loss (w/o reg) on all data: 0.291396
Train acc on all data:  0.980662315688
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.0445e-05
Norm of the params: 21.3494
     Influence (LOO): fixed 450 labels. Loss 0.29140. Accuracy 0.955.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0626902
Train loss (w/o reg) on all data: 0.0319437
Test loss (w/o reg) on all data: 0.315852
Train acc on all data:  0.999516557892
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 4.83963e-05
Norm of the params: 24.7978
                Loss: fixed 389 labels. Loss 0.31585. Accuracy 0.930.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173629
Train loss (w/o reg) on all data: 0.130382
Test loss (w/o reg) on all data: 0.688885
Train acc on all data:  0.964950447184
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.83307e-05
Norm of the params: 29.4098
              Random: fixed 144 labels. Loss 0.68888. Accuracy 0.864.
### Flips: 618, rs: 24, checks: 1236
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084434
Train loss (w/o reg) on all data: 0.0639718
Test loss (w/o reg) on all data: 0.208002
Train acc on all data:  0.982112642011
Test acc on all data:   0.95652173913
Norm of the mean of gradients: 3.96918e-06
Norm of the params: 20.2298
     Influence (LOO): fixed 481 labels. Loss 0.20800. Accuracy 0.957.
Using normal model
LBFGS training took [502] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0571026
Train loss (w/o reg) on all data: 0.0285806
Test loss (w/o reg) on all data: 0.250558
Train acc on all data:  0.999274836838
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 3.35062e-05
Norm of the params: 23.8839
                Loss: fixed 420 labels. Loss 0.25056. Accuracy 0.932.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.165549
Train loss (w/o reg) on all data: 0.123542
Test loss (w/o reg) on all data: 0.65169
Train acc on all data:  0.966159052453
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 8.96979e-06
Norm of the params: 28.9852
              Random: fixed 179 labels. Loss 0.65169. Accuracy 0.870.
Using normal model
LBFGS training took [720] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207164
Train loss (w/o reg) on all data: 0.158895
Test loss (w/o reg) on all data: 0.590358
Train acc on all data:  0.949480299734
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 4.24881e-05
Norm of the params: 31.0707
Flipped loss: 0.59036. Accuracy: 0.852
### Flips: 618, rs: 25, checks: 206
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137517
Train loss (w/o reg) on all data: 0.101555
Test loss (w/o reg) on all data: 0.386028
Train acc on all data:  0.971718636693
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 1.18319e-05
Norm of the params: 26.8184
     Influence (LOO): fixed 170 labels. Loss 0.38603. Accuracy 0.898.
Using normal model
LBFGS training took [477] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113845
Train loss (w/o reg) on all data: 0.0670534
Test loss (w/o reg) on all data: 0.465784
Train acc on all data:  0.992990089437
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.94101e-05
Norm of the params: 30.5915
                Loss: fixed 168 labels. Loss 0.46578. Accuracy 0.882.
Using normal model
LBFGS training took [564] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201712
Train loss (w/o reg) on all data: 0.154647
Test loss (w/o reg) on all data: 0.533872
Train acc on all data:  0.951655789219
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 2.47444e-05
Norm of the params: 30.6805
              Random: fixed  29 labels. Loss 0.53387. Accuracy 0.853.
### Flips: 618, rs: 25, checks: 412
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116165
Train loss (w/o reg) on all data: 0.0861621
Test loss (w/o reg) on all data: 0.272605
Train acc on all data:  0.975102731448
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 9.34763e-06
Norm of the params: 24.496
     Influence (LOO): fixed 283 labels. Loss 0.27261. Accuracy 0.913.
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084469
Train loss (w/o reg) on all data: 0.0442896
Test loss (w/o reg) on all data: 0.390369
Train acc on all data:  0.998549673677
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 1.22313e-05
Norm of the params: 28.3476
                Loss: fixed 265 labels. Loss 0.39037. Accuracy 0.904.
Using normal model
LBFGS training took [625] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196659
Train loss (w/o reg) on all data: 0.150364
Test loss (w/o reg) on all data: 0.53018
Train acc on all data:  0.952864394489
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 4.95377e-05
Norm of the params: 30.4287
              Random: fixed  51 labels. Loss 0.53018. Accuracy 0.858.
### Flips: 618, rs: 25, checks: 618
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10309
Train loss (w/o reg) on all data: 0.0773329
Test loss (w/o reg) on all data: 0.244928
Train acc on all data:  0.977519941987
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 8.4933e-06
Norm of the params: 22.6967
     Influence (LOO): fixed 371 labels. Loss 0.24493. Accuracy 0.927.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0703732
Train loss (w/o reg) on all data: 0.035587
Test loss (w/o reg) on all data: 0.373304
Train acc on all data:  0.999516557892
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 9.25566e-06
Norm of the params: 26.3766
                Loss: fixed 322 labels. Loss 0.37330. Accuracy 0.914.
Using normal model
LBFGS training took [607] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188448
Train loss (w/o reg) on all data: 0.143161
Test loss (w/o reg) on all data: 0.5109
Train acc on all data:  0.956973652405
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 0.00023387
Norm of the params: 30.0954
              Random: fixed  75 labels. Loss 0.51090. Accuracy 0.864.
### Flips: 618, rs: 25, checks: 824
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0948846
Train loss (w/o reg) on all data: 0.0712814
Test loss (w/o reg) on all data: 0.200175
Train acc on all data:  0.977519941987
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 5.30811e-06
Norm of the params: 21.727
     Influence (LOO): fixed 416 labels. Loss 0.20018. Accuracy 0.940.
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0639302
Train loss (w/o reg) on all data: 0.0319504
Test loss (w/o reg) on all data: 0.303842
Train acc on all data:  0.999274836838
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 1.10457e-05
Norm of the params: 25.2903
                Loss: fixed 353 labels. Loss 0.30384. Accuracy 0.917.
Using normal model
LBFGS training took [575] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182397
Train loss (w/o reg) on all data: 0.138038
Test loss (w/o reg) on all data: 0.531391
Train acc on all data:  0.957940536621
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 5.97899e-05
Norm of the params: 29.7855
              Random: fixed 103 labels. Loss 0.53139. Accuracy 0.867.
### Flips: 618, rs: 25, checks: 1030
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0857239
Train loss (w/o reg) on all data: 0.0644518
Test loss (w/o reg) on all data: 0.183739
Train acc on all data:  0.980904036742
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 2.42062e-06
Norm of the params: 20.6262
     Influence (LOO): fixed 457 labels. Loss 0.18374. Accuracy 0.952.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0587955
Train loss (w/o reg) on all data: 0.0290527
Test loss (w/o reg) on all data: 0.270198
Train acc on all data:  0.999516557892
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 5.57832e-06
Norm of the params: 24.3897
                Loss: fixed 381 labels. Loss 0.27020. Accuracy 0.933.
Using normal model
LBFGS training took [600] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171958
Train loss (w/o reg) on all data: 0.128312
Test loss (w/o reg) on all data: 0.486852
Train acc on all data:  0.962291515591
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 1.82568e-05
Norm of the params: 29.5453
              Random: fixed 137 labels. Loss 0.48685. Accuracy 0.871.
### Flips: 618, rs: 25, checks: 1236
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082705
Train loss (w/o reg) on all data: 0.0625432
Test loss (w/o reg) on all data: 0.185602
Train acc on all data:  0.980662315688
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 9.9221e-07
Norm of the params: 20.0807
     Influence (LOO): fixed 478 labels. Loss 0.18560. Accuracy 0.949.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0558634
Train loss (w/o reg) on all data: 0.0274039
Test loss (w/o reg) on all data: 0.249066
Train acc on all data:  0.999516557892
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 2.99307e-06
Norm of the params: 23.8577
                Loss: fixed 403 labels. Loss 0.24907. Accuracy 0.932.
Using normal model
LBFGS training took [580] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164614
Train loss (w/o reg) on all data: 0.121685
Test loss (w/o reg) on all data: 0.473986
Train acc on all data:  0.968092820885
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 4.46289e-05
Norm of the params: 29.3015
              Random: fixed 168 labels. Loss 0.47399. Accuracy 0.877.
Using normal model
LBFGS training took [585] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202398
Train loss (w/o reg) on all data: 0.154456
Test loss (w/o reg) on all data: 0.719833
Train acc on all data:  0.954314720812
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 1.92419e-05
Norm of the params: 30.9651
Flipped loss: 0.71983. Accuracy: 0.847
### Flips: 618, rs: 26, checks: 206
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131736
Train loss (w/o reg) on all data: 0.0950278
Test loss (w/o reg) on all data: 0.419562
Train acc on all data:  0.974861010394
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 4.06058e-05
Norm of the params: 27.0954
     Influence (LOO): fixed 172 labels. Loss 0.41956. Accuracy 0.904.
Using normal model
LBFGS training took [477] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112145
Train loss (w/o reg) on all data: 0.0658184
Test loss (w/o reg) on all data: 0.702319
Train acc on all data:  0.99444041576
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 3.65034e-05
Norm of the params: 30.4391
                Loss: fixed 163 labels. Loss 0.70232. Accuracy 0.881.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19735
Train loss (w/o reg) on all data: 0.150113
Test loss (w/o reg) on all data: 0.715927
Train acc on all data:  0.956248489243
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 4.16909e-05
Norm of the params: 30.7366
              Random: fixed  25 labels. Loss 0.71593. Accuracy 0.844.
### Flips: 618, rs: 26, checks: 412
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110793
Train loss (w/o reg) on all data: 0.0801006
Test loss (w/o reg) on all data: 0.367514
Train acc on all data:  0.978245105149
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.33875e-05
Norm of the params: 24.776
     Influence (LOO): fixed 279 labels. Loss 0.36751. Accuracy 0.920.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0864306
Train loss (w/o reg) on all data: 0.046072
Test loss (w/o reg) on all data: 0.524307
Train acc on all data:  0.998307952623
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 3.11677e-05
Norm of the params: 28.4108
                Loss: fixed 256 labels. Loss 0.52431. Accuracy 0.888.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191345
Train loss (w/o reg) on all data: 0.144825
Test loss (w/o reg) on all data: 0.687351
Train acc on all data:  0.959632583998
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 2.71913e-05
Norm of the params: 30.5025
              Random: fixed  61 labels. Loss 0.68735. Accuracy 0.860.
### Flips: 618, rs: 26, checks: 618
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0996097
Train loss (w/o reg) on all data: 0.0720417
Test loss (w/o reg) on all data: 0.25571
Train acc on all data:  0.979695431472
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 3.65319e-06
Norm of the params: 23.481
     Influence (LOO): fixed 364 labels. Loss 0.25571. Accuracy 0.941.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0716371
Train loss (w/o reg) on all data: 0.0366391
Test loss (w/o reg) on all data: 0.320837
Train acc on all data:  0.99879139473
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 2.36952e-05
Norm of the params: 26.4567
                Loss: fixed 322 labels. Loss 0.32084. Accuracy 0.909.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187176
Train loss (w/o reg) on all data: 0.141245
Test loss (w/o reg) on all data: 0.673203
Train acc on all data:  0.961324631375
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 4.39618e-05
Norm of the params: 30.3088
              Random: fixed  85 labels. Loss 0.67320. Accuracy 0.866.
### Flips: 618, rs: 26, checks: 824
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0898425
Train loss (w/o reg) on all data: 0.06609
Test loss (w/o reg) on all data: 0.287758
Train acc on all data:  0.981870920957
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 3.1665e-06
Norm of the params: 21.7956
     Influence (LOO): fixed 427 labels. Loss 0.28776. Accuracy 0.953.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0637033
Train loss (w/o reg) on all data: 0.0317781
Test loss (w/o reg) on all data: 0.273526
Train acc on all data:  0.99879139473
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 2.1031e-05
Norm of the params: 25.2686
                Loss: fixed 362 labels. Loss 0.27353. Accuracy 0.914.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182039
Train loss (w/o reg) on all data: 0.136676
Test loss (w/o reg) on all data: 0.657569
Train acc on all data:  0.961082910321
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 2.37139e-05
Norm of the params: 30.1209
              Random: fixed 116 labels. Loss 0.65757. Accuracy 0.856.
### Flips: 618, rs: 26, checks: 1030
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0859642
Train loss (w/o reg) on all data: 0.0633642
Test loss (w/o reg) on all data: 0.237308
Train acc on all data:  0.982112642011
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 4.10228e-06
Norm of the params: 21.2603
     Influence (LOO): fixed 452 labels. Loss 0.23731. Accuracy 0.958.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055664
Train loss (w/o reg) on all data: 0.0271794
Test loss (w/o reg) on all data: 0.280804
Train acc on all data:  0.999033115784
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 5.39925e-06
Norm of the params: 23.8682
                Loss: fixed 407 labels. Loss 0.28080. Accuracy 0.934.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177161
Train loss (w/o reg) on all data: 0.132744
Test loss (w/o reg) on all data: 0.616073
Train acc on all data:  0.961566352429
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 3.96593e-05
Norm of the params: 29.8048
              Random: fixed 148 labels. Loss 0.61607. Accuracy 0.859.
### Flips: 618, rs: 26, checks: 1236
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0818039
Train loss (w/o reg) on all data: 0.0605908
Test loss (w/o reg) on all data: 0.181842
Train acc on all data:  0.983321247281
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 4.92774e-06
Norm of the params: 20.5976
     Influence (LOO): fixed 479 labels. Loss 0.18184. Accuracy 0.965.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0516665
Train loss (w/o reg) on all data: 0.0249969
Test loss (w/o reg) on all data: 0.238264
Train acc on all data:  0.999274836838
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.17562e-05
Norm of the params: 23.0953
                Loss: fixed 429 labels. Loss 0.23826. Accuracy 0.936.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.168545
Train loss (w/o reg) on all data: 0.124927
Test loss (w/o reg) on all data: 0.645957
Train acc on all data:  0.962774957699
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.70484e-05
Norm of the params: 29.5361
              Random: fixed 186 labels. Loss 0.64596. Accuracy 0.864.
Using normal model
LBFGS training took [471] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208791
Train loss (w/o reg) on all data: 0.159975
Test loss (w/o reg) on all data: 0.754191
Train acc on all data:  0.951172347111
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 1.68262e-05
Norm of the params: 31.2459
Flipped loss: 0.75419. Accuracy: 0.845
### Flips: 618, rs: 27, checks: 206
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136134
Train loss (w/o reg) on all data: 0.0992875
Test loss (w/o reg) on all data: 0.494997
Train acc on all data:  0.975344452502
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 8.53898e-06
Norm of the params: 27.1465
     Influence (LOO): fixed 170 labels. Loss 0.49500. Accuracy 0.891.
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113327
Train loss (w/o reg) on all data: 0.0667904
Test loss (w/o reg) on all data: 0.584599
Train acc on all data:  0.992990089437
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 1.46064e-05
Norm of the params: 30.5078
                Loss: fixed 175 labels. Loss 0.58460. Accuracy 0.867.
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204405
Train loss (w/o reg) on all data: 0.156152
Test loss (w/o reg) on all data: 0.752894
Train acc on all data:  0.952864394489
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 2.19778e-05
Norm of the params: 31.0653
              Random: fixed  21 labels. Loss 0.75289. Accuracy 0.843.
### Flips: 618, rs: 27, checks: 412
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113848
Train loss (w/o reg) on all data: 0.0835236
Test loss (w/o reg) on all data: 0.463142
Train acc on all data:  0.980662315688
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 2.27848e-05
Norm of the params: 24.6268
     Influence (LOO): fixed 284 labels. Loss 0.46314. Accuracy 0.926.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0915117
Train loss (w/o reg) on all data: 0.0498241
Test loss (w/o reg) on all data: 0.519876
Train acc on all data:  0.997582789461
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.32677e-05
Norm of the params: 28.8748
                Loss: fixed 255 labels. Loss 0.51988. Accuracy 0.894.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195323
Train loss (w/o reg) on all data: 0.148127
Test loss (w/o reg) on all data: 0.657583
Train acc on all data:  0.958423978729
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.06283e-05
Norm of the params: 30.7233
              Random: fixed  55 labels. Loss 0.65758. Accuracy 0.862.
### Flips: 618, rs: 27, checks: 618
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101923
Train loss (w/o reg) on all data: 0.0757624
Test loss (w/o reg) on all data: 0.420107
Train acc on all data:  0.981387478849
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 3.03686e-06
Norm of the params: 22.874
     Influence (LOO): fixed 362 labels. Loss 0.42011. Accuracy 0.940.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0744776
Train loss (w/o reg) on all data: 0.0386887
Test loss (w/o reg) on all data: 0.346712
Train acc on all data:  0.99879139473
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 9.83594e-06
Norm of the params: 26.7541
                Loss: fixed 330 labels. Loss 0.34671. Accuracy 0.915.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188899
Train loss (w/o reg) on all data: 0.143023
Test loss (w/o reg) on all data: 0.658232
Train acc on all data:  0.96035774716
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 3.70871e-05
Norm of the params: 30.2907
              Random: fixed  83 labels. Loss 0.65823. Accuracy 0.869.
### Flips: 618, rs: 27, checks: 824
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093508
Train loss (w/o reg) on all data: 0.0695683
Test loss (w/o reg) on all data: 0.332441
Train acc on all data:  0.983079526227
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.83983e-06
Norm of the params: 21.8814
     Influence (LOO): fixed 415 labels. Loss 0.33244. Accuracy 0.945.
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0658674
Train loss (w/o reg) on all data: 0.0335156
Test loss (w/o reg) on all data: 0.307079
Train acc on all data:  0.999033115784
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.19479e-05
Norm of the params: 25.4369
                Loss: fixed 373 labels. Loss 0.30708. Accuracy 0.933.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181868
Train loss (w/o reg) on all data: 0.136591
Test loss (w/o reg) on all data: 0.664061
Train acc on all data:  0.960116026106
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 5.49488e-05
Norm of the params: 30.0921
              Random: fixed 113 labels. Loss 0.66406. Accuracy 0.870.
### Flips: 618, rs: 27, checks: 1030
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0884549
Train loss (w/o reg) on all data: 0.0666485
Test loss (w/o reg) on all data: 0.230976
Train acc on all data:  0.982837805173
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 4.01848e-06
Norm of the params: 20.8837
     Influence (LOO): fixed 452 labels. Loss 0.23098. Accuracy 0.949.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0578672
Train loss (w/o reg) on all data: 0.0286252
Test loss (w/o reg) on all data: 0.240765
Train acc on all data:  0.999516557892
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 5.45784e-06
Norm of the params: 24.1835
                Loss: fixed 406 labels. Loss 0.24077. Accuracy 0.935.
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174168
Train loss (w/o reg) on all data: 0.129679
Test loss (w/o reg) on all data: 0.638492
Train acc on all data:  0.965675610346
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.50462e-05
Norm of the params: 29.829
              Random: fixed 141 labels. Loss 0.63849. Accuracy 0.882.
### Flips: 618, rs: 27, checks: 1236
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0846737
Train loss (w/o reg) on all data: 0.064022
Test loss (w/o reg) on all data: 0.207311
Train acc on all data:  0.982837805173
Test acc on all data:   0.950724637681
Norm of the mean of gradients: 1.22442e-06
Norm of the params: 20.3233
     Influence (LOO): fixed 476 labels. Loss 0.20731. Accuracy 0.951.
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0534185
Train loss (w/o reg) on all data: 0.0261721
Test loss (w/o reg) on all data: 0.270966
Train acc on all data:  0.999516557892
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 7.91742e-06
Norm of the params: 23.3437
                Loss: fixed 436 labels. Loss 0.27097. Accuracy 0.938.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.168598
Train loss (w/o reg) on all data: 0.124891
Test loss (w/o reg) on all data: 0.67471
Train acc on all data:  0.966400773507
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 3.06739e-05
Norm of the params: 29.5655
              Random: fixed 166 labels. Loss 0.67471. Accuracy 0.877.
Using normal model
LBFGS training took [674] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205209
Train loss (w/o reg) on all data: 0.155616
Test loss (w/o reg) on all data: 0.705033
Train acc on all data:  0.955765047136
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 4.97388e-05
Norm of the params: 31.494
Flipped loss: 0.70503. Accuracy: 0.849
### Flips: 618, rs: 28, checks: 206
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137959
Train loss (w/o reg) on all data: 0.100358
Test loss (w/o reg) on all data: 0.366362
Train acc on all data:  0.973410684071
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 8.87038e-06
Norm of the params: 27.4228
     Influence (LOO): fixed 174 labels. Loss 0.36636. Accuracy 0.908.
Using normal model
LBFGS training took [536] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117328
Train loss (w/o reg) on all data: 0.0680804
Test loss (w/o reg) on all data: 0.637638
Train acc on all data:  0.995407299976
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 1.96801e-05
Norm of the params: 31.3839
                Loss: fixed 171 labels. Loss 0.63764. Accuracy 0.867.
Using normal model
LBFGS training took [556] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196217
Train loss (w/o reg) on all data: 0.148135
Test loss (w/o reg) on all data: 0.700639
Train acc on all data:  0.958907420836
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 4.82571e-05
Norm of the params: 31.0103
              Random: fixed  38 labels. Loss 0.70064. Accuracy 0.855.
### Flips: 618, rs: 28, checks: 412
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118894
Train loss (w/o reg) on all data: 0.0870699
Test loss (w/o reg) on all data: 0.395828
Train acc on all data:  0.97582789461
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 7.24894e-06
Norm of the params: 25.2284
     Influence (LOO): fixed 282 labels. Loss 0.39583. Accuracy 0.921.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0947399
Train loss (w/o reg) on all data: 0.0513497
Test loss (w/o reg) on all data: 0.629612
Train acc on all data:  0.997341068407
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 3.566e-05
Norm of the params: 29.4585
                Loss: fixed 249 labels. Loss 0.62961. Accuracy 0.886.
Using normal model
LBFGS training took [608] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186387
Train loss (w/o reg) on all data: 0.139439
Test loss (w/o reg) on all data: 0.676601
Train acc on all data:  0.961082910321
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 3.2539e-05
Norm of the params: 30.6425
              Random: fixed  72 labels. Loss 0.67660. Accuracy 0.863.
### Flips: 618, rs: 28, checks: 618
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108666
Train loss (w/o reg) on all data: 0.0802019
Test loss (w/o reg) on all data: 0.305631
Train acc on all data:  0.978728547256
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 4.79688e-06
Norm of the params: 23.8598
     Influence (LOO): fixed 351 labels. Loss 0.30563. Accuracy 0.942.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0750355
Train loss (w/o reg) on all data: 0.0385269
Test loss (w/o reg) on all data: 0.506711
Train acc on all data:  0.998549673677
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 2.19488e-05
Norm of the params: 27.0217
                Loss: fixed 329 labels. Loss 0.50671. Accuracy 0.904.
Using normal model
LBFGS training took [565] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179255
Train loss (w/o reg) on all data: 0.133203
Test loss (w/o reg) on all data: 0.64277
Train acc on all data:  0.964225284022
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 2.07967e-05
Norm of the params: 30.3487
              Random: fixed 103 labels. Loss 0.64277. Accuracy 0.870.
### Flips: 618, rs: 28, checks: 824
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0967647
Train loss (w/o reg) on all data: 0.0712338
Test loss (w/o reg) on all data: 0.219664
Train acc on all data:  0.98017887358
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 4.71476e-06
Norm of the params: 22.5969
     Influence (LOO): fixed 408 labels. Loss 0.21966. Accuracy 0.955.
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069677
Train loss (w/o reg) on all data: 0.0352662
Test loss (w/o reg) on all data: 0.450525
Train acc on all data:  0.999033115784
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 8.15444e-06
Norm of the params: 26.2339
                Loss: fixed 360 labels. Loss 0.45053. Accuracy 0.922.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172812
Train loss (w/o reg) on all data: 0.128027
Test loss (w/o reg) on all data: 0.670328
Train acc on all data:  0.966642494561
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 0.000102823
Norm of the params: 29.9284
              Random: fixed 131 labels. Loss 0.67033. Accuracy 0.876.
### Flips: 618, rs: 28, checks: 1030
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092354
Train loss (w/o reg) on all data: 0.0686728
Test loss (w/o reg) on all data: 0.17122
Train acc on all data:  0.980420594634
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 5.67216e-06
Norm of the params: 21.7629
     Influence (LOO): fixed 441 labels. Loss 0.17122. Accuracy 0.955.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0625531
Train loss (w/o reg) on all data: 0.031228
Test loss (w/o reg) on all data: 0.447801
Train acc on all data:  0.99879139473
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 4.11281e-06
Norm of the params: 25.03
                Loss: fixed 393 labels. Loss 0.44780. Accuracy 0.928.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166738
Train loss (w/o reg) on all data: 0.122922
Test loss (w/o reg) on all data: 0.61278
Train acc on all data:  0.968334541939
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 1.39137e-05
Norm of the params: 29.6027
              Random: fixed 162 labels. Loss 0.61278. Accuracy 0.878.
### Flips: 618, rs: 28, checks: 1236
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0882156
Train loss (w/o reg) on all data: 0.0661014
Test loss (w/o reg) on all data: 0.174437
Train acc on all data:  0.980420594634
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 2.69329e-06
Norm of the params: 21.0306
     Influence (LOO): fixed 465 labels. Loss 0.17444. Accuracy 0.964.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0574938
Train loss (w/o reg) on all data: 0.0283251
Test loss (w/o reg) on all data: 0.385325
Train acc on all data:  0.99879139473
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 1.05586e-05
Norm of the params: 24.1532
                Loss: fixed 423 labels. Loss 0.38532. Accuracy 0.946.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15913
Train loss (w/o reg) on all data: 0.116451
Test loss (w/o reg) on all data: 0.52986
Train acc on all data:  0.971235194585
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 4.49902e-05
Norm of the params: 29.2162
              Random: fixed 192 labels. Loss 0.52986. Accuracy 0.889.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205903
Train loss (w/o reg) on all data: 0.158616
Test loss (w/o reg) on all data: 0.576165
Train acc on all data:  0.95044718395
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 1.55278e-05
Norm of the params: 30.7528
Flipped loss: 0.57617. Accuracy: 0.834
### Flips: 618, rs: 29, checks: 206
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141083
Train loss (w/o reg) on all data: 0.104483
Test loss (w/o reg) on all data: 0.369572
Train acc on all data:  0.971718636693
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 1.7984e-05
Norm of the params: 27.0558
     Influence (LOO): fixed 162 labels. Loss 0.36957. Accuracy 0.891.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116344
Train loss (w/o reg) on all data: 0.0694474
Test loss (w/o reg) on all data: 0.566897
Train acc on all data:  0.993956973652
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.57041e-05
Norm of the params: 30.6257
                Loss: fixed 171 labels. Loss 0.56690. Accuracy 0.860.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198787
Train loss (w/o reg) on all data: 0.15223
Test loss (w/o reg) on all data: 0.530454
Train acc on all data:  0.952622673435
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 2.90685e-05
Norm of the params: 30.5148
              Random: fixed  24 labels. Loss 0.53045. Accuracy 0.840.
### Flips: 618, rs: 29, checks: 412
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119767
Train loss (w/o reg) on all data: 0.089056
Test loss (w/o reg) on all data: 0.4221
Train acc on all data:  0.976794778825
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 2.6029e-05
Norm of the params: 24.7836
     Influence (LOO): fixed 274 labels. Loss 0.42210. Accuracy 0.918.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0927676
Train loss (w/o reg) on all data: 0.0509022
Test loss (w/o reg) on all data: 0.51768
Train acc on all data:  0.997099347353
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 6.33004e-05
Norm of the params: 28.9363
                Loss: fixed 246 labels. Loss 0.51768. Accuracy 0.879.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192012
Train loss (w/o reg) on all data: 0.146508
Test loss (w/o reg) on all data: 0.529669
Train acc on all data:  0.954072999758
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 7.23222e-05
Norm of the params: 30.1675
              Random: fixed  59 labels. Loss 0.52967. Accuracy 0.851.
### Flips: 618, rs: 29, checks: 618
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107884
Train loss (w/o reg) on all data: 0.0810636
Test loss (w/o reg) on all data: 0.407071
Train acc on all data:  0.978003384095
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 2.25693e-06
Norm of the params: 23.1604
     Influence (LOO): fixed 344 labels. Loss 0.40707. Accuracy 0.927.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0775777
Train loss (w/o reg) on all data: 0.0403869
Test loss (w/o reg) on all data: 0.508684
Train acc on all data:  0.998549673677
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 2.14801e-05
Norm of the params: 27.273
                Loss: fixed 310 labels. Loss 0.50868. Accuracy 0.897.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185628
Train loss (w/o reg) on all data: 0.141392
Test loss (w/o reg) on all data: 0.555937
Train acc on all data:  0.95914914189
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.49571e-05
Norm of the params: 29.7442
              Random: fixed  89 labels. Loss 0.55594. Accuracy 0.860.
### Flips: 618, rs: 29, checks: 824
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100225
Train loss (w/o reg) on all data: 0.0754313
Test loss (w/o reg) on all data: 0.361262
Train acc on all data:  0.980420594634
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 1.99203e-06
Norm of the params: 22.2682
     Influence (LOO): fixed 397 labels. Loss 0.36126. Accuracy 0.945.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0692201
Train loss (w/o reg) on all data: 0.0352944
Test loss (w/o reg) on all data: 0.456101
Train acc on all data:  0.99879139473
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 2.38268e-05
Norm of the params: 26.0483
                Loss: fixed 348 labels. Loss 0.45610. Accuracy 0.911.
Using normal model
LBFGS training took [544] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181278
Train loss (w/o reg) on all data: 0.137701
Test loss (w/o reg) on all data: 0.569415
Train acc on all data:  0.957457094513
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.52895e-05
Norm of the params: 29.5216
              Random: fixed 111 labels. Loss 0.56941. Accuracy 0.862.
### Flips: 618, rs: 29, checks: 1030
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0944166
Train loss (w/o reg) on all data: 0.0713068
Test loss (w/o reg) on all data: 0.384415
Train acc on all data:  0.980904036742
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 4.65863e-06
Norm of the params: 21.4987
     Influence (LOO): fixed 437 labels. Loss 0.38442. Accuracy 0.960.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0625993
Train loss (w/o reg) on all data: 0.031517
Test loss (w/o reg) on all data: 0.430181
Train acc on all data:  0.99879139473
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 5.9225e-06
Norm of the params: 24.9328
                Loss: fixed 387 labels. Loss 0.43018. Accuracy 0.932.
Using normal model
LBFGS training took [546] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172867
Train loss (w/o reg) on all data: 0.130471
Test loss (w/o reg) on all data: 0.522655
Train acc on all data:  0.960599468214
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.45841e-05
Norm of the params: 29.119
              Random: fixed 148 labels. Loss 0.52265. Accuracy 0.864.
### Flips: 618, rs: 29, checks: 1236
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0862951
Train loss (w/o reg) on all data: 0.0650627
Test loss (w/o reg) on all data: 0.275056
Train acc on all data:  0.981629199903
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 8.89806e-06
Norm of the params: 20.607
     Influence (LOO): fixed 473 labels. Loss 0.27506. Accuracy 0.965.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0578406
Train loss (w/o reg) on all data: 0.0287793
Test loss (w/o reg) on all data: 0.425066
Train acc on all data:  0.99879139473
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 5.46663e-06
Norm of the params: 24.1086
                Loss: fixed 415 labels. Loss 0.42507. Accuracy 0.935.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162786
Train loss (w/o reg) on all data: 0.122073
Test loss (w/o reg) on all data: 0.592561
Train acc on all data:  0.964225284022
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 4.30575e-05
Norm of the params: 28.5352
              Random: fixed 181 labels. Loss 0.59256. Accuracy 0.871.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20366
Train loss (w/o reg) on all data: 0.155595
Test loss (w/o reg) on all data: 0.609217
Train acc on all data:  0.952139231327
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 4.38831e-05
Norm of the params: 31.0049
Flipped loss: 0.60922. Accuracy: 0.824
### Flips: 618, rs: 30, checks: 206
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130918
Train loss (w/o reg) on all data: 0.0945976
Test loss (w/o reg) on all data: 0.476269
Train acc on all data:  0.975586173556
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 1.73272e-05
Norm of the params: 26.9519
     Influence (LOO): fixed 183 labels. Loss 0.47627. Accuracy 0.881.
Using normal model
LBFGS training took [411] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113461
Train loss (w/o reg) on all data: 0.0657225
Test loss (w/o reg) on all data: 0.533543
Train acc on all data:  0.994198694706
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 8.57997e-06
Norm of the params: 30.8992
                Loss: fixed 165 labels. Loss 0.53354. Accuracy 0.844.
Using normal model
LBFGS training took [404] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197321
Train loss (w/o reg) on all data: 0.150131
Test loss (w/o reg) on all data: 0.538882
Train acc on all data:  0.955039883974
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 4.42289e-05
Norm of the params: 30.7214
              Random: fixed  25 labels. Loss 0.53888. Accuracy 0.837.
### Flips: 618, rs: 30, checks: 412
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110988
Train loss (w/o reg) on all data: 0.0800451
Test loss (w/o reg) on all data: 0.33454
Train acc on all data:  0.979937152526
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 6.09551e-06
Norm of the params: 24.8768
     Influence (LOO): fixed 289 labels. Loss 0.33454. Accuracy 0.908.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0886079
Train loss (w/o reg) on all data: 0.047476
Test loss (w/o reg) on all data: 0.421914
Train acc on all data:  0.998066231569
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 3.17655e-05
Norm of the params: 28.6817
                Loss: fixed 251 labels. Loss 0.42191. Accuracy 0.871.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189673
Train loss (w/o reg) on all data: 0.143334
Test loss (w/o reg) on all data: 0.496541
Train acc on all data:  0.957698815567
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 6.37519e-05
Norm of the params: 30.443
              Random: fixed  59 labels. Loss 0.49654. Accuracy 0.847.
### Flips: 618, rs: 30, checks: 618
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101536
Train loss (w/o reg) on all data: 0.0738063
Test loss (w/o reg) on all data: 0.319835
Train acc on all data:  0.980420594634
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 1.40484e-05
Norm of the params: 23.55
     Influence (LOO): fixed 359 labels. Loss 0.31983. Accuracy 0.929.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0735449
Train loss (w/o reg) on all data: 0.0377469
Test loss (w/o reg) on all data: 0.364572
Train acc on all data:  0.998307952623
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 2.72057e-05
Norm of the params: 26.7575
                Loss: fixed 313 labels. Loss 0.36457. Accuracy 0.899.
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18177
Train loss (w/o reg) on all data: 0.136038
Test loss (w/o reg) on all data: 0.475697
Train acc on all data:  0.962291515591
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 1.47757e-05
Norm of the params: 30.2431
              Random: fixed  85 labels. Loss 0.47570. Accuracy 0.859.
### Flips: 618, rs: 30, checks: 824
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0920596
Train loss (w/o reg) on all data: 0.0671147
Test loss (w/o reg) on all data: 0.350351
Train acc on all data:  0.982596084119
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 2.63376e-05
Norm of the params: 22.336
     Influence (LOO): fixed 416 labels. Loss 0.35035. Accuracy 0.937.
Using normal model
LBFGS training took [351] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0661337
Train loss (w/o reg) on all data: 0.0332068
Test loss (w/o reg) on all data: 0.323256
Train acc on all data:  0.99879139473
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 7.17579e-06
Norm of the params: 25.662
                Loss: fixed 358 labels. Loss 0.32326. Accuracy 0.916.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173381
Train loss (w/o reg) on all data: 0.128938
Test loss (w/o reg) on all data: 0.463117
Train acc on all data:  0.966400773507
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 1.7802e-05
Norm of the params: 29.8139
              Random: fixed 113 labels. Loss 0.46312. Accuracy 0.870.
### Flips: 618, rs: 30, checks: 1030
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0854436
Train loss (w/o reg) on all data: 0.0628863
Test loss (w/o reg) on all data: 0.313507
Train acc on all data:  0.983321247281
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 5.01011e-06
Norm of the params: 21.2402
     Influence (LOO): fixed 458 labels. Loss 0.31351. Accuracy 0.948.
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0613903
Train loss (w/o reg) on all data: 0.0304827
Test loss (w/o reg) on all data: 0.3157
Train acc on all data:  0.999516557892
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 8.57488e-06
Norm of the params: 24.8627
                Loss: fixed 385 labels. Loss 0.31570. Accuracy 0.919.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166508
Train loss (w/o reg) on all data: 0.122714
Test loss (w/o reg) on all data: 0.481993
Train acc on all data:  0.968092820885
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 4.51129e-05
Norm of the params: 29.5952
              Random: fixed 147 labels. Loss 0.48199. Accuracy 0.866.
### Flips: 618, rs: 30, checks: 1236
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0801114
Train loss (w/o reg) on all data: 0.0592213
Test loss (w/o reg) on all data: 0.315216
Train acc on all data:  0.984771573604
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.57383e-05
Norm of the params: 20.4402
     Influence (LOO): fixed 486 labels. Loss 0.31522. Accuracy 0.952.
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0552036
Train loss (w/o reg) on all data: 0.0268759
Test loss (w/o reg) on all data: 0.274423
Train acc on all data:  0.999516557892
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 7.21055e-06
Norm of the params: 23.8024
                Loss: fixed 417 labels. Loss 0.27442. Accuracy 0.929.
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159597
Train loss (w/o reg) on all data: 0.116761
Test loss (w/o reg) on all data: 0.475334
Train acc on all data:  0.969784868262
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 2.26569e-05
Norm of the params: 29.27
              Random: fixed 179 labels. Loss 0.47533. Accuracy 0.873.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199075
Train loss (w/o reg) on all data: 0.149538
Test loss (w/o reg) on all data: 0.524738
Train acc on all data:  0.958423978729
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 2.14922e-05
Norm of the params: 31.4762
Flipped loss: 0.52474. Accuracy: 0.840
### Flips: 618, rs: 31, checks: 206
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131402
Train loss (w/o reg) on all data: 0.0940806
Test loss (w/o reg) on all data: 0.342909
Train acc on all data:  0.977278220933
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 2.83242e-05
Norm of the params: 27.3209
     Influence (LOO): fixed 170 labels. Loss 0.34291. Accuracy 0.905.
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116679
Train loss (w/o reg) on all data: 0.0679729
Test loss (w/o reg) on all data: 0.456078
Train acc on all data:  0.994923857868
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 2.38168e-05
Norm of the params: 31.2108
                Loss: fixed 158 labels. Loss 0.45608. Accuracy 0.863.
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192555
Train loss (w/o reg) on all data: 0.143753
Test loss (w/o reg) on all data: 0.507125
Train acc on all data:  0.959874305052
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.08012e-05
Norm of the params: 31.2416
              Random: fixed  32 labels. Loss 0.50713. Accuracy 0.851.
### Flips: 618, rs: 31, checks: 412
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112894
Train loss (w/o reg) on all data: 0.0810713
Test loss (w/o reg) on all data: 0.261587
Train acc on all data:  0.980662315688
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 9.09569e-06
Norm of the params: 25.2279
     Influence (LOO): fixed 282 labels. Loss 0.26159. Accuracy 0.912.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0914789
Train loss (w/o reg) on all data: 0.0493307
Test loss (w/o reg) on all data: 0.360901
Train acc on all data:  0.997099347353
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 8.37097e-06
Norm of the params: 29.0339
                Loss: fixed 248 labels. Loss 0.36090. Accuracy 0.887.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184476
Train loss (w/o reg) on all data: 0.136733
Test loss (w/o reg) on all data: 0.502464
Train acc on all data:  0.962291515591
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 5.2149e-05
Norm of the params: 30.9008
              Random: fixed  66 labels. Loss 0.50246. Accuracy 0.858.
### Flips: 618, rs: 31, checks: 618
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0994925
Train loss (w/o reg) on all data: 0.0714874
Test loss (w/o reg) on all data: 0.26419
Train acc on all data:  0.982596084119
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 8.93242e-06
Norm of the params: 23.6665
     Influence (LOO): fixed 363 labels. Loss 0.26419. Accuracy 0.932.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0756893
Train loss (w/o reg) on all data: 0.0392711
Test loss (w/o reg) on all data: 0.314277
Train acc on all data:  0.999033115784
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 7.27033e-06
Norm of the params: 26.9882
                Loss: fixed 302 labels. Loss 0.31428. Accuracy 0.903.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177771
Train loss (w/o reg) on all data: 0.130703
Test loss (w/o reg) on all data: 0.458116
Train acc on all data:  0.966884215615
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 0.000114859
Norm of the params: 30.6816
              Random: fixed  93 labels. Loss 0.45812. Accuracy 0.873.
### Flips: 618, rs: 31, checks: 824
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0890257
Train loss (w/o reg) on all data: 0.0635177
Test loss (w/o reg) on all data: 0.193239
Train acc on all data:  0.985013294658
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 6.42209e-06
Norm of the params: 22.5867
     Influence (LOO): fixed 418 labels. Loss 0.19324. Accuracy 0.940.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0644332
Train loss (w/o reg) on all data: 0.0323646
Test loss (w/o reg) on all data: 0.279214
Train acc on all data:  0.999033115784
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 6.07537e-06
Norm of the params: 25.3253
                Loss: fixed 358 labels. Loss 0.27921. Accuracy 0.917.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167996
Train loss (w/o reg) on all data: 0.121713
Test loss (w/o reg) on all data: 0.432959
Train acc on all data:  0.969301426154
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 5.68521e-05
Norm of the params: 30.4245
              Random: fixed 130 labels. Loss 0.43296. Accuracy 0.878.
### Flips: 618, rs: 31, checks: 1030
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081185
Train loss (w/o reg) on all data: 0.0583029
Test loss (w/o reg) on all data: 0.168029
Train acc on all data:  0.986221899927
Test acc on all data:   0.949758454106
Norm of the mean of gradients: 3.48221e-06
Norm of the params: 21.3926
     Influence (LOO): fixed 460 labels. Loss 0.16803. Accuracy 0.950.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0593693
Train loss (w/o reg) on all data: 0.0296375
Test loss (w/o reg) on all data: 0.278376
Train acc on all data:  0.999033115784
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 3.10296e-06
Norm of the params: 24.3852
                Loss: fixed 387 labels. Loss 0.27838. Accuracy 0.929.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161624
Train loss (w/o reg) on all data: 0.116321
Test loss (w/o reg) on all data: 0.488969
Train acc on all data:  0.971235194585
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 1.00653e-05
Norm of the params: 30.1008
              Random: fixed 170 labels. Loss 0.48897. Accuracy 0.884.
### Flips: 618, rs: 31, checks: 1236
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0766709
Train loss (w/o reg) on all data: 0.0554441
Test loss (w/o reg) on all data: 0.19469
Train acc on all data:  0.985980178874
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 2.65149e-06
Norm of the params: 20.6043
     Influence (LOO): fixed 492 labels. Loss 0.19469. Accuracy 0.954.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0532433
Train loss (w/o reg) on all data: 0.0261801
Test loss (w/o reg) on all data: 0.255514
Train acc on all data:  0.999516557892
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 7.96238e-06
Norm of the params: 23.2651
                Loss: fixed 429 labels. Loss 0.25551. Accuracy 0.934.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154104
Train loss (w/o reg) on all data: 0.110857
Test loss (w/o reg) on all data: 0.496887
Train acc on all data:  0.970751752478
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 9.59497e-06
Norm of the params: 29.4099
              Random: fixed 202 labels. Loss 0.49689. Accuracy 0.882.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202918
Train loss (w/o reg) on all data: 0.15541
Test loss (w/o reg) on all data: 0.729526
Train acc on all data:  0.951172347111
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 9.45032e-06
Norm of the params: 30.8245
Flipped loss: 0.72953. Accuracy: 0.837
### Flips: 618, rs: 32, checks: 206
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13471
Train loss (w/o reg) on all data: 0.0996341
Test loss (w/o reg) on all data: 0.470402
Train acc on all data:  0.972685520909
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 2.99146e-05
Norm of the params: 26.4861
     Influence (LOO): fixed 172 labels. Loss 0.47040. Accuracy 0.895.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113732
Train loss (w/o reg) on all data: 0.0672449
Test loss (w/o reg) on all data: 0.626656
Train acc on all data:  0.992990089437
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 2.30471e-05
Norm of the params: 30.4918
                Loss: fixed 170 labels. Loss 0.62666. Accuracy 0.859.
Using normal model
LBFGS training took [358] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194478
Train loss (w/o reg) on all data: 0.147949
Test loss (w/o reg) on all data: 0.715984
Train acc on all data:  0.95479816292
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 1.25938e-05
Norm of the params: 30.5053
              Random: fixed  41 labels. Loss 0.71598. Accuracy 0.845.
### Flips: 618, rs: 32, checks: 412
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11612
Train loss (w/o reg) on all data: 0.0861537
Test loss (w/o reg) on all data: 0.372633
Train acc on all data:  0.976553057771
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 4.61513e-06
Norm of the params: 24.4811
     Influence (LOO): fixed 287 labels. Loss 0.37263. Accuracy 0.923.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0867521
Train loss (w/o reg) on all data: 0.0462244
Test loss (w/o reg) on all data: 0.521245
Train acc on all data:  0.998549673677
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 2.66316e-05
Norm of the params: 28.4702
                Loss: fixed 268 labels. Loss 0.52125. Accuracy 0.885.
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184651
Train loss (w/o reg) on all data: 0.13893
Test loss (w/o reg) on all data: 0.736052
Train acc on all data:  0.95914914189
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.10493e-05
Norm of the params: 30.2395
              Random: fixed  77 labels. Loss 0.73605. Accuracy 0.851.
### Flips: 618, rs: 32, checks: 618
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105901
Train loss (w/o reg) on all data: 0.0793777
Test loss (w/o reg) on all data: 0.326179
Train acc on all data:  0.978245105149
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 3.71801e-06
Norm of the params: 23.0319
     Influence (LOO): fixed 358 labels. Loss 0.32618. Accuracy 0.932.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072334
Train loss (w/o reg) on all data: 0.037397
Test loss (w/o reg) on all data: 0.434954
Train acc on all data:  0.99879139473
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 8.70435e-06
Norm of the params: 26.4337
                Loss: fixed 322 labels. Loss 0.43495. Accuracy 0.914.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18012
Train loss (w/o reg) on all data: 0.13528
Test loss (w/o reg) on all data: 0.709958
Train acc on all data:  0.960841189268
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 1.44069e-05
Norm of the params: 29.9466
              Random: fixed 110 labels. Loss 0.70996. Accuracy 0.855.
### Flips: 618, rs: 32, checks: 824
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0983958
Train loss (w/o reg) on all data: 0.0742226
Test loss (w/o reg) on all data: 0.324126
Train acc on all data:  0.979211989364
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 7.23079e-06
Norm of the params: 21.9878
     Influence (LOO): fixed 404 labels. Loss 0.32413. Accuracy 0.933.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0634972
Train loss (w/o reg) on all data: 0.0320837
Test loss (w/o reg) on all data: 0.469251
Train acc on all data:  0.999033115784
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 7.69415e-06
Norm of the params: 25.0653
                Loss: fixed 366 labels. Loss 0.46925. Accuracy 0.924.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173494
Train loss (w/o reg) on all data: 0.129394
Test loss (w/o reg) on all data: 0.732397
Train acc on all data:  0.962533236645
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.11705e-05
Norm of the params: 29.6984
              Random: fixed 137 labels. Loss 0.73240. Accuracy 0.858.
### Flips: 618, rs: 32, checks: 1030
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0921318
Train loss (w/o reg) on all data: 0.0695813
Test loss (w/o reg) on all data: 0.338766
Train acc on all data:  0.981145757796
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 5.44046e-06
Norm of the params: 21.237
     Influence (LOO): fixed 443 labels. Loss 0.33877. Accuracy 0.945.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0584794
Train loss (w/o reg) on all data: 0.0294564
Test loss (w/o reg) on all data: 0.454604
Train acc on all data:  0.999274836838
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 2.95179e-06
Norm of the params: 24.0927
                Loss: fixed 402 labels. Loss 0.45460. Accuracy 0.929.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16807
Train loss (w/o reg) on all data: 0.124826
Test loss (w/o reg) on all data: 0.733883
Train acc on all data:  0.96470872613
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 1.41703e-05
Norm of the params: 29.4088
              Random: fixed 161 labels. Loss 0.73388. Accuracy 0.872.
### Flips: 618, rs: 32, checks: 1236
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0841281
Train loss (w/o reg) on all data: 0.0638261
Test loss (w/o reg) on all data: 0.343984
Train acc on all data:  0.982837805173
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 4.21712e-06
Norm of the params: 20.1504
     Influence (LOO): fixed 480 labels. Loss 0.34398. Accuracy 0.957.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0518993
Train loss (w/o reg) on all data: 0.0257254
Test loss (w/o reg) on all data: 0.428649
Train acc on all data:  0.999274836838
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 4.50852e-06
Norm of the params: 22.8797
                Loss: fixed 431 labels. Loss 0.42865. Accuracy 0.937.
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160931
Train loss (w/o reg) on all data: 0.118896
Test loss (w/o reg) on all data: 0.672809
Train acc on all data:  0.967125936669
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 4.9229e-05
Norm of the params: 28.9947
              Random: fixed 191 labels. Loss 0.67281. Accuracy 0.871.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204924
Train loss (w/o reg) on all data: 0.155737
Test loss (w/o reg) on all data: 0.543326
Train acc on all data:  0.952380952381
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 2.33959e-05
Norm of the params: 31.3645
Flipped loss: 0.54333. Accuracy: 0.842
### Flips: 618, rs: 33, checks: 206
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138335
Train loss (w/o reg) on all data: 0.10092
Test loss (w/o reg) on all data: 0.398151
Train acc on all data:  0.973894126178
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 3.16438e-05
Norm of the params: 27.3551
     Influence (LOO): fixed 173 labels. Loss 0.39815. Accuracy 0.899.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117811
Train loss (w/o reg) on all data: 0.0694656
Test loss (w/o reg) on all data: 0.476469
Train acc on all data:  0.990572878898
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 3.04844e-05
Norm of the params: 31.0952
                Loss: fixed 173 labels. Loss 0.47647. Accuracy 0.848.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199796
Train loss (w/o reg) on all data: 0.1514
Test loss (w/o reg) on all data: 0.508625
Train acc on all data:  0.955039883974
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 2.19649e-05
Norm of the params: 31.1113
              Random: fixed  24 labels. Loss 0.50863. Accuracy 0.838.
### Flips: 618, rs: 33, checks: 412
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118094
Train loss (w/o reg) on all data: 0.087111
Test loss (w/o reg) on all data: 0.318755
Train acc on all data:  0.975586173556
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 3.18752e-05
Norm of the params: 24.8928
     Influence (LOO): fixed 281 labels. Loss 0.31876. Accuracy 0.916.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0910209
Train loss (w/o reg) on all data: 0.0487607
Test loss (w/o reg) on all data: 0.475974
Train acc on all data:  0.998307952623
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 3.44958e-05
Norm of the params: 29.0724
                Loss: fixed 268 labels. Loss 0.47597. Accuracy 0.877.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194874
Train loss (w/o reg) on all data: 0.147314
Test loss (w/o reg) on all data: 0.483742
Train acc on all data:  0.955765047136
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 4.24644e-05
Norm of the params: 30.8415
              Random: fixed  56 labels. Loss 0.48374. Accuracy 0.844.
### Flips: 618, rs: 33, checks: 618
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103355
Train loss (w/o reg) on all data: 0.0766583
Test loss (w/o reg) on all data: 0.278105
Train acc on all data:  0.978245105149
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 7.20666e-06
Norm of the params: 23.1069
     Influence (LOO): fixed 366 labels. Loss 0.27810. Accuracy 0.936.
Using normal model
LBFGS training took [355] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0768156
Train loss (w/o reg) on all data: 0.0395637
Test loss (w/o reg) on all data: 0.471826
Train acc on all data:  0.99879139473
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 5.52408e-06
Norm of the params: 27.2954
                Loss: fixed 318 labels. Loss 0.47183. Accuracy 0.901.
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187029
Train loss (w/o reg) on all data: 0.14083
Test loss (w/o reg) on all data: 0.458824
Train acc on all data:  0.957940536621
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.23784e-05
Norm of the params: 30.3971
              Random: fixed  91 labels. Loss 0.45882. Accuracy 0.862.
### Flips: 618, rs: 33, checks: 824
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0949261
Train loss (w/o reg) on all data: 0.0706352
Test loss (w/o reg) on all data: 0.196412
Train acc on all data:  0.980904036742
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 7.9888e-06
Norm of the params: 22.0413
     Influence (LOO): fixed 414 labels. Loss 0.19641. Accuracy 0.947.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0693976
Train loss (w/o reg) on all data: 0.0351329
Test loss (w/o reg) on all data: 0.397106
Train acc on all data:  0.99879139473
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 9.16676e-06
Norm of the params: 26.1781
                Loss: fixed 352 labels. Loss 0.39711. Accuracy 0.912.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181618
Train loss (w/o reg) on all data: 0.136167
Test loss (w/o reg) on all data: 0.45344
Train acc on all data:  0.960116026106
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 4.83945e-05
Norm of the params: 30.1501
              Random: fixed 119 labels. Loss 0.45344. Accuracy 0.856.
### Flips: 618, rs: 33, checks: 1030
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088112
Train loss (w/o reg) on all data: 0.0662707
Test loss (w/o reg) on all data: 0.165434
Train acc on all data:  0.980420594634
Test acc on all data:   0.957487922705
Norm of the mean of gradients: 3.1526e-06
Norm of the params: 20.9004
     Influence (LOO): fixed 454 labels. Loss 0.16543. Accuracy 0.957.
Using normal model
LBFGS training took [355] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0643077
Train loss (w/o reg) on all data: 0.0320931
Test loss (w/o reg) on all data: 0.420344
Train acc on all data:  0.999033115784
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 4.87707e-06
Norm of the params: 25.3829
                Loss: fixed 384 labels. Loss 0.42034. Accuracy 0.916.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17414
Train loss (w/o reg) on all data: 0.130074
Test loss (w/o reg) on all data: 0.392641
Train acc on all data:  0.961082910321
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 4.97858e-05
Norm of the params: 29.687
              Random: fixed 156 labels. Loss 0.39264. Accuracy 0.872.
### Flips: 618, rs: 33, checks: 1236
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0823123
Train loss (w/o reg) on all data: 0.0616639
Test loss (w/o reg) on all data: 0.160255
Train acc on all data:  0.983079526227
Test acc on all data:   0.968115942029
Norm of the mean of gradients: 2.028e-05
Norm of the params: 20.3216
     Influence (LOO): fixed 483 labels. Loss 0.16026. Accuracy 0.968.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0575445
Train loss (w/o reg) on all data: 0.0283029
Test loss (w/o reg) on all data: 0.378841
Train acc on all data:  0.999033115784
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 3.80818e-05
Norm of the params: 24.1833
                Loss: fixed 420 labels. Loss 0.37884. Accuracy 0.933.
Using normal model
LBFGS training took [435] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167345
Train loss (w/o reg) on all data: 0.124152
Test loss (w/o reg) on all data: 0.378099
Train acc on all data:  0.963983562968
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 2.25744e-05
Norm of the params: 29.3918
              Random: fixed 187 labels. Loss 0.37810. Accuracy 0.875.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208684
Train loss (w/o reg) on all data: 0.160457
Test loss (w/o reg) on all data: 0.560203
Train acc on all data:  0.948513415518
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 4.09017e-05
Norm of the params: 31.0571
Flipped loss: 0.56020. Accuracy: 0.852
### Flips: 618, rs: 34, checks: 206
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143808
Train loss (w/o reg) on all data: 0.106701
Test loss (w/o reg) on all data: 0.480572
Train acc on all data:  0.970510031424
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.32359e-05
Norm of the params: 27.2419
     Influence (LOO): fixed 169 labels. Loss 0.48057. Accuracy 0.897.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118872
Train loss (w/o reg) on all data: 0.0706345
Test loss (w/o reg) on all data: 0.527158
Train acc on all data:  0.989847715736
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 9.65026e-05
Norm of the params: 31.0604
                Loss: fixed 175 labels. Loss 0.52716. Accuracy 0.867.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203671
Train loss (w/o reg) on all data: 0.156216
Test loss (w/o reg) on all data: 0.54252
Train acc on all data:  0.950205462896
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 4.06123e-05
Norm of the params: 30.8074
              Random: fixed  30 labels. Loss 0.54252. Accuracy 0.861.
### Flips: 618, rs: 34, checks: 412
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123117
Train loss (w/o reg) on all data: 0.0925048
Test loss (w/o reg) on all data: 0.438883
Train acc on all data:  0.972685520909
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 1.31414e-05
Norm of the params: 24.7435
     Influence (LOO): fixed 279 labels. Loss 0.43888. Accuracy 0.915.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888717
Train loss (w/o reg) on all data: 0.047684
Test loss (w/o reg) on all data: 0.49344
Train acc on all data:  0.99879139473
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 3.52697e-05
Norm of the params: 28.7011
                Loss: fixed 256 labels. Loss 0.49344. Accuracy 0.888.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197498
Train loss (w/o reg) on all data: 0.150732
Test loss (w/o reg) on all data: 0.577611
Train acc on all data:  0.952864394489
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 3.51549e-05
Norm of the params: 30.583
              Random: fixed  59 labels. Loss 0.57761. Accuracy 0.860.
### Flips: 618, rs: 34, checks: 618
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114924
Train loss (w/o reg) on all data: 0.0869291
Test loss (w/o reg) on all data: 0.414048
Train acc on all data:  0.974861010394
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 3.17248e-05
Norm of the params: 23.6619
     Influence (LOO): fixed 338 labels. Loss 0.41405. Accuracy 0.927.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0779579
Train loss (w/o reg) on all data: 0.0404741
Test loss (w/o reg) on all data: 0.515311
Train acc on all data:  0.999033115784
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 1.31843e-05
Norm of the params: 27.3802
                Loss: fixed 310 labels. Loss 0.51531. Accuracy 0.908.
Using normal model
LBFGS training took [508] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189459
Train loss (w/o reg) on all data: 0.143437
Test loss (w/o reg) on all data: 0.561311
Train acc on all data:  0.95600676819
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 3.51606e-05
Norm of the params: 30.3387
              Random: fixed  89 labels. Loss 0.56131. Accuracy 0.864.
### Flips: 618, rs: 34, checks: 824
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106383
Train loss (w/o reg) on all data: 0.0815015
Test loss (w/o reg) on all data: 0.307773
Train acc on all data:  0.976553057771
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 2.86489e-06
Norm of the params: 22.3075
     Influence (LOO): fixed 398 labels. Loss 0.30777. Accuracy 0.952.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0714181
Train loss (w/o reg) on all data: 0.0367743
Test loss (w/o reg) on all data: 0.468337
Train acc on all data:  0.99879139473
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 7.20286e-06
Norm of the params: 26.3225
                Loss: fixed 348 labels. Loss 0.46834. Accuracy 0.908.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183867
Train loss (w/o reg) on all data: 0.138755
Test loss (w/o reg) on all data: 0.573365
Train acc on all data:  0.957457094513
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 4.15619e-05
Norm of the params: 30.0374
              Random: fixed 117 labels. Loss 0.57337. Accuracy 0.867.
### Flips: 618, rs: 34, checks: 1030
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0986478
Train loss (w/o reg) on all data: 0.0759337
Test loss (w/o reg) on all data: 0.276896
Train acc on all data:  0.978003384095
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 4.04055e-06
Norm of the params: 21.3139
     Influence (LOO): fixed 437 labels. Loss 0.27690. Accuracy 0.955.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0644229
Train loss (w/o reg) on all data: 0.0326292
Test loss (w/o reg) on all data: 0.440023
Train acc on all data:  0.99879139473
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 7.9817e-06
Norm of the params: 25.2166
                Loss: fixed 382 labels. Loss 0.44002. Accuracy 0.921.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178212
Train loss (w/o reg) on all data: 0.133777
Test loss (w/o reg) on all data: 0.565287
Train acc on all data:  0.958907420836
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 4.12992e-05
Norm of the params: 29.811
              Random: fixed 148 labels. Loss 0.56529. Accuracy 0.869.
### Flips: 618, rs: 34, checks: 1236
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0905548
Train loss (w/o reg) on all data: 0.0700989
Test loss (w/o reg) on all data: 0.185467
Train acc on all data:  0.978728547256
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 1.09674e-05
Norm of the params: 20.2266
     Influence (LOO): fixed 470 labels. Loss 0.18547. Accuracy 0.958.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0581539
Train loss (w/o reg) on all data: 0.0292022
Test loss (w/o reg) on all data: 0.409764
Train acc on all data:  0.999274836838
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 8.07182e-06
Norm of the params: 24.0631
                Loss: fixed 420 labels. Loss 0.40976. Accuracy 0.932.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172882
Train loss (w/o reg) on all data: 0.129376
Test loss (w/o reg) on all data: 0.503043
Train acc on all data:  0.960599468214
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 1.70865e-05
Norm of the params: 29.4978
              Random: fixed 172 labels. Loss 0.50304. Accuracy 0.878.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205141
Train loss (w/o reg) on all data: 0.158071
Test loss (w/o reg) on all data: 0.563244
Train acc on all data:  0.953831278704
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 3.73289e-05
Norm of the params: 30.6822
Flipped loss: 0.56324. Accuracy: 0.842
### Flips: 618, rs: 35, checks: 206
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140331
Train loss (w/o reg) on all data: 0.105225
Test loss (w/o reg) on all data: 0.433363
Train acc on all data:  0.972202078801
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 2.37377e-05
Norm of the params: 26.4975
     Influence (LOO): fixed 169 labels. Loss 0.43336. Accuracy 0.900.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111609
Train loss (w/o reg) on all data: 0.065108
Test loss (w/o reg) on all data: 0.497975
Train acc on all data:  0.99564902103
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 1.52825e-05
Norm of the params: 30.4961
                Loss: fixed 176 labels. Loss 0.49798. Accuracy 0.861.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197916
Train loss (w/o reg) on all data: 0.151822
Test loss (w/o reg) on all data: 0.504631
Train acc on all data:  0.956731931351
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 9.59404e-06
Norm of the params: 30.3627
              Random: fixed  30 labels. Loss 0.50463. Accuracy 0.852.
### Flips: 618, rs: 35, checks: 412
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122934
Train loss (w/o reg) on all data: 0.0932465
Test loss (w/o reg) on all data: 0.342592
Train acc on all data:  0.975102731448
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 9.64782e-06
Norm of the params: 24.3668
     Influence (LOO): fixed 279 labels. Loss 0.34259. Accuracy 0.923.
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0903425
Train loss (w/o reg) on all data: 0.0486977
Test loss (w/o reg) on all data: 0.410695
Train acc on all data:  0.99879139473
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 3.77678e-05
Norm of the params: 28.86
                Loss: fixed 254 labels. Loss 0.41069. Accuracy 0.893.
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192194
Train loss (w/o reg) on all data: 0.14735
Test loss (w/o reg) on all data: 0.487537
Train acc on all data:  0.958182257675
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 4.02792e-05
Norm of the params: 29.9478
              Random: fixed  59 labels. Loss 0.48754. Accuracy 0.856.
### Flips: 618, rs: 35, checks: 618
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112726
Train loss (w/o reg) on all data: 0.0863557
Test loss (w/o reg) on all data: 0.305396
Train acc on all data:  0.976794778825
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 3.75364e-06
Norm of the params: 22.9652
     Influence (LOO): fixed 341 labels. Loss 0.30540. Accuracy 0.935.
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0781209
Train loss (w/o reg) on all data: 0.0409173
Test loss (w/o reg) on all data: 0.407203
Train acc on all data:  0.99879139473
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.46805e-05
Norm of the params: 27.2777
                Loss: fixed 313 labels. Loss 0.40720. Accuracy 0.905.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185896
Train loss (w/o reg) on all data: 0.142165
Test loss (w/o reg) on all data: 0.515509
Train acc on all data:  0.961082910321
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 6.48548e-05
Norm of the params: 29.5742
              Random: fixed  90 labels. Loss 0.51551. Accuracy 0.868.
### Flips: 618, rs: 35, checks: 824
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103425
Train loss (w/o reg) on all data: 0.0791269
Test loss (w/o reg) on all data: 0.313803
Train acc on all data:  0.978245105149
Test acc on all data:   0.942995169082
Norm of the mean of gradients: 1.36954e-05
Norm of the params: 22.0447
     Influence (LOO): fixed 393 labels. Loss 0.31380. Accuracy 0.943.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0700813
Train loss (w/o reg) on all data: 0.0359726
Test loss (w/o reg) on all data: 0.348574
Train acc on all data:  0.99879139473
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 1.35887e-05
Norm of the params: 26.1185
                Loss: fixed 355 labels. Loss 0.34857. Accuracy 0.914.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179521
Train loss (w/o reg) on all data: 0.136995
Test loss (w/o reg) on all data: 0.532373
Train acc on all data:  0.962774957699
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 1.83914e-05
Norm of the params: 29.1635
              Random: fixed 125 labels. Loss 0.53237. Accuracy 0.873.
### Flips: 618, rs: 35, checks: 1030
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0960978
Train loss (w/o reg) on all data: 0.074451
Test loss (w/o reg) on all data: 0.293381
Train acc on all data:  0.978728547256
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 3.42646e-06
Norm of the params: 20.8071
     Influence (LOO): fixed 436 labels. Loss 0.29338. Accuracy 0.952.
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0643211
Train loss (w/o reg) on all data: 0.0326135
Test loss (w/o reg) on all data: 0.333361
Train acc on all data:  0.998549673677
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 4.125e-06
Norm of the params: 25.1824
                Loss: fixed 393 labels. Loss 0.33336. Accuracy 0.924.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171886
Train loss (w/o reg) on all data: 0.130705
Test loss (w/o reg) on all data: 0.479104
Train acc on all data:  0.963500120861
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 3.50502e-05
Norm of the params: 28.6989
              Random: fixed 162 labels. Loss 0.47910. Accuracy 0.896.
### Flips: 618, rs: 35, checks: 1236
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0903102
Train loss (w/o reg) on all data: 0.070533
Test loss (w/o reg) on all data: 0.252054
Train acc on all data:  0.979695431472
Test acc on all data:   0.965217391304
Norm of the mean of gradients: 3.21786e-06
Norm of the params: 19.8883
     Influence (LOO): fixed 472 labels. Loss 0.25205. Accuracy 0.965.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0560392
Train loss (w/o reg) on all data: 0.0278281
Test loss (w/o reg) on all data: 0.33909
Train acc on all data:  0.998549673677
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 1.07069e-05
Norm of the params: 23.7533
                Loss: fixed 429 labels. Loss 0.33909. Accuracy 0.935.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164416
Train loss (w/o reg) on all data: 0.124236
Test loss (w/o reg) on all data: 0.464156
Train acc on all data:  0.964950447184
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.82514e-05
Norm of the params: 28.348
              Random: fixed 192 labels. Loss 0.46416. Accuracy 0.897.
Using normal model
LBFGS training took [648] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208921
Train loss (w/o reg) on all data: 0.160864
Test loss (w/o reg) on all data: 0.657072
Train acc on all data:  0.949480299734
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 5.87658e-05
Norm of the params: 31.0022
Flipped loss: 0.65707. Accuracy: 0.839
### Flips: 618, rs: 36, checks: 206
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134581
Train loss (w/o reg) on all data: 0.0969126
Test loss (w/o reg) on all data: 0.518407
Train acc on all data:  0.974861010394
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 2.23106e-05
Norm of the params: 27.4474
     Influence (LOO): fixed 167 labels. Loss 0.51841. Accuracy 0.894.
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120061
Train loss (w/o reg) on all data: 0.071731
Test loss (w/o reg) on all data: 0.720088
Train acc on all data:  0.991056321006
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.29886e-05
Norm of the params: 31.09
                Loss: fixed 161 labels. Loss 0.72009. Accuracy 0.865.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202697
Train loss (w/o reg) on all data: 0.15517
Test loss (w/o reg) on all data: 0.648782
Train acc on all data:  0.950930626058
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 3.06531e-05
Norm of the params: 30.8307
              Random: fixed  26 labels. Loss 0.64878. Accuracy 0.843.
### Flips: 618, rs: 36, checks: 412
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113055
Train loss (w/o reg) on all data: 0.082105
Test loss (w/o reg) on all data: 0.399105
Train acc on all data:  0.978728547256
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 2.99849e-06
Norm of the params: 24.8797
     Influence (LOO): fixed 283 labels. Loss 0.39910. Accuracy 0.913.
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0918635
Train loss (w/o reg) on all data: 0.0493613
Test loss (w/o reg) on all data: 0.6513
Train acc on all data:  0.999033115784
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.85073e-05
Norm of the params: 29.1555
                Loss: fixed 248 labels. Loss 0.65130. Accuracy 0.879.
Using normal model
LBFGS training took [582] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19878
Train loss (w/o reg) on all data: 0.151697
Test loss (w/o reg) on all data: 0.624485
Train acc on all data:  0.952622673435
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 3.38572e-05
Norm of the params: 30.6864
              Random: fixed  49 labels. Loss 0.62448. Accuracy 0.835.
### Flips: 618, rs: 36, checks: 618
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10113
Train loss (w/o reg) on all data: 0.0739534
Test loss (w/o reg) on all data: 0.336639
Train acc on all data:  0.979453710418
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 4.78335e-06
Norm of the params: 23.3138
     Influence (LOO): fixed 357 labels. Loss 0.33664. Accuracy 0.942.
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077482
Train loss (w/o reg) on all data: 0.0402191
Test loss (w/o reg) on all data: 0.560369
Train acc on all data:  0.999516557892
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 9.94075e-06
Norm of the params: 27.2994
                Loss: fixed 307 labels. Loss 0.56037. Accuracy 0.900.
Using normal model
LBFGS training took [590] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192082
Train loss (w/o reg) on all data: 0.145853
Test loss (w/o reg) on all data: 0.55247
Train acc on all data:  0.95479816292
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 2.13227e-05
Norm of the params: 30.4069
              Random: fixed  79 labels. Loss 0.55247. Accuracy 0.850.
### Flips: 618, rs: 36, checks: 824
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0916887
Train loss (w/o reg) on all data: 0.0675368
Test loss (w/o reg) on all data: 0.321181
Train acc on all data:  0.981387478849
Test acc on all data:   0.952657004831
Norm of the mean of gradients: 6.27206e-06
Norm of the params: 21.9781
     Influence (LOO): fixed 416 labels. Loss 0.32118. Accuracy 0.953.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0669135
Train loss (w/o reg) on all data: 0.0338859
Test loss (w/o reg) on all data: 0.45153
Train acc on all data:  0.999033115784
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 6.89107e-06
Norm of the params: 25.7012
                Loss: fixed 358 labels. Loss 0.45153. Accuracy 0.920.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1818
Train loss (w/o reg) on all data: 0.136658
Test loss (w/o reg) on all data: 0.497746
Train acc on all data:  0.957940536621
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 3.42754e-05
Norm of the params: 30.0473
              Random: fixed 116 labels. Loss 0.49775. Accuracy 0.862.
### Flips: 618, rs: 36, checks: 1030
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0866083
Train loss (w/o reg) on all data: 0.0642227
Test loss (w/o reg) on all data: 0.314447
Train acc on all data:  0.983079526227
Test acc on all data:   0.954589371981
Norm of the mean of gradients: 1.57691e-06
Norm of the params: 21.1592
     Influence (LOO): fixed 448 labels. Loss 0.31445. Accuracy 0.955.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0595336
Train loss (w/o reg) on all data: 0.0297265
Test loss (w/o reg) on all data: 0.44071
Train acc on all data:  0.999033115784
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 1.61903e-05
Norm of the params: 24.416
                Loss: fixed 394 labels. Loss 0.44071. Accuracy 0.929.
Using normal model
LBFGS training took [540] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174477
Train loss (w/o reg) on all data: 0.130263
Test loss (w/o reg) on all data: 0.556299
Train acc on all data:  0.963741841914
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 8.28286e-05
Norm of the params: 29.7368
              Random: fixed 152 labels. Loss 0.55630. Accuracy 0.873.
### Flips: 618, rs: 36, checks: 1236
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0827687
Train loss (w/o reg) on all data: 0.0615778
Test loss (w/o reg) on all data: 0.216199
Train acc on all data:  0.983562968335
Test acc on all data:   0.95845410628
Norm of the mean of gradients: 2.3473e-06
Norm of the params: 20.5868
     Influence (LOO): fixed 476 labels. Loss 0.21620. Accuracy 0.958.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0524923
Train loss (w/o reg) on all data: 0.0256637
Test loss (w/o reg) on all data: 0.415183
Train acc on all data:  0.999516557892
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 5.79429e-06
Norm of the params: 23.164
                Loss: fixed 431 labels. Loss 0.41518. Accuracy 0.935.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167502
Train loss (w/o reg) on all data: 0.124171
Test loss (w/o reg) on all data: 0.589043
Train acc on all data:  0.966642494561
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 6.1754e-05
Norm of the params: 29.4385
              Random: fixed 186 labels. Loss 0.58904. Accuracy 0.884.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204961
Train loss (w/o reg) on all data: 0.157066
Test loss (w/o reg) on all data: 0.653922
Train acc on all data:  0.955281605028
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 7.3414e-05
Norm of the params: 30.95
Flipped loss: 0.65392. Accuracy: 0.841
### Flips: 618, rs: 37, checks: 206
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136217
Train loss (w/o reg) on all data: 0.0992349
Test loss (w/o reg) on all data: 0.404788
Train acc on all data:  0.973410684071
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 6.19836e-05
Norm of the params: 27.1962
     Influence (LOO): fixed 171 labels. Loss 0.40479. Accuracy 0.900.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115319
Train loss (w/o reg) on all data: 0.0675023
Test loss (w/o reg) on all data: 0.548797
Train acc on all data:  0.993715252599
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 1.82251e-05
Norm of the params: 30.9245
                Loss: fixed 171 labels. Loss 0.54880. Accuracy 0.874.
Using normal model
LBFGS training took [535] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20068
Train loss (w/o reg) on all data: 0.153677
Test loss (w/o reg) on all data: 0.628855
Train acc on all data:  0.956490210297
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 7.12151e-05
Norm of the params: 30.6607
              Random: fixed  30 labels. Loss 0.62885. Accuracy 0.849.
### Flips: 618, rs: 37, checks: 412
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117796
Train loss (w/o reg) on all data: 0.0869278
Test loss (w/o reg) on all data: 0.327902
Train acc on all data:  0.976553057771
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.5259e-05
Norm of the params: 24.8469
     Influence (LOO): fixed 289 labels. Loss 0.32790. Accuracy 0.924.
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0866883
Train loss (w/o reg) on all data: 0.0458035
Test loss (w/o reg) on all data: 0.540069
Train acc on all data:  0.998307952623
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 9.11501e-06
Norm of the params: 28.5954
                Loss: fixed 260 labels. Loss 0.54007. Accuracy 0.890.
Using normal model
LBFGS training took [521] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19207
Train loss (w/o reg) on all data: 0.146543
Test loss (w/o reg) on all data: 0.608996
Train acc on all data:  0.958423978729
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 6.61401e-05
Norm of the params: 30.1753
              Random: fixed  68 labels. Loss 0.60900. Accuracy 0.853.
### Flips: 618, rs: 37, checks: 618
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107333
Train loss (w/o reg) on all data: 0.079321
Test loss (w/o reg) on all data: 0.227136
Train acc on all data:  0.977036499879
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 3.98065e-06
Norm of the params: 23.6694
     Influence (LOO): fixed 358 labels. Loss 0.22714. Accuracy 0.929.
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0744835
Train loss (w/o reg) on all data: 0.0379801
Test loss (w/o reg) on all data: 0.467711
Train acc on all data:  0.998549673677
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.1026e-05
Norm of the params: 27.0198
                Loss: fixed 316 labels. Loss 0.46771. Accuracy 0.903.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184511
Train loss (w/o reg) on all data: 0.139223
Test loss (w/o reg) on all data: 0.593371
Train acc on all data:  0.961566352429
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 5.38696e-05
Norm of the params: 30.0956
              Random: fixed  97 labels. Loss 0.59337. Accuracy 0.871.
### Flips: 618, rs: 37, checks: 824
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0976315
Train loss (w/o reg) on all data: 0.0729725
Test loss (w/o reg) on all data: 0.193936
Train acc on all data:  0.97897026831
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 4.44943e-06
Norm of the params: 22.2076
     Influence (LOO): fixed 412 labels. Loss 0.19394. Accuracy 0.946.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0668887
Train loss (w/o reg) on all data: 0.0335843
Test loss (w/o reg) on all data: 0.435383
Train acc on all data:  0.999033115784
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.48137e-05
Norm of the params: 25.8087
                Loss: fixed 356 labels. Loss 0.43538. Accuracy 0.910.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178795
Train loss (w/o reg) on all data: 0.134308
Test loss (w/o reg) on all data: 0.586908
Train acc on all data:  0.964467005076
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 3.37001e-05
Norm of the params: 29.8286
              Random: fixed 123 labels. Loss 0.58691. Accuracy 0.871.
### Flips: 618, rs: 37, checks: 1030
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0910336
Train loss (w/o reg) on all data: 0.067998
Test loss (w/o reg) on all data: 0.185336
Train acc on all data:  0.980662315688
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 4.90758e-06
Norm of the params: 21.4642
     Influence (LOO): fixed 455 labels. Loss 0.18534. Accuracy 0.949.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0599636
Train loss (w/o reg) on all data: 0.0295745
Test loss (w/o reg) on all data: 0.405931
Train acc on all data:  0.999516557892
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 4.84905e-06
Norm of the params: 24.6532
                Loss: fixed 390 labels. Loss 0.40593. Accuracy 0.925.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169573
Train loss (w/o reg) on all data: 0.125735
Test loss (w/o reg) on all data: 0.53322
Train acc on all data:  0.967125936669
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 1.80464e-05
Norm of the params: 29.6103
              Random: fixed 159 labels. Loss 0.53322. Accuracy 0.885.
### Flips: 618, rs: 37, checks: 1236
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0813525
Train loss (w/o reg) on all data: 0.0609121
Test loss (w/o reg) on all data: 0.154632
Train acc on all data:  0.981870920957
Test acc on all data:   0.959420289855
Norm of the mean of gradients: 6.70877e-06
Norm of the params: 20.219
     Influence (LOO): fixed 485 labels. Loss 0.15463. Accuracy 0.959.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055553
Train loss (w/o reg) on all data: 0.0272087
Test loss (w/o reg) on all data: 0.345589
Train acc on all data:  0.999516557892
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 9.25091e-06
Norm of the params: 23.8094
                Loss: fixed 422 labels. Loss 0.34559. Accuracy 0.928.
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16065
Train loss (w/o reg) on all data: 0.117941
Test loss (w/o reg) on all data: 0.535301
Train acc on all data:  0.971235194585
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 4.12558e-05
Norm of the params: 29.2263
              Random: fixed 198 labels. Loss 0.53530. Accuracy 0.887.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211305
Train loss (w/o reg) on all data: 0.16356
Test loss (w/o reg) on all data: 0.5487
Train acc on all data:  0.949480299734
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 7.16165e-05
Norm of the params: 30.9013
Flipped loss: 0.54870. Accuracy: 0.846
### Flips: 618, rs: 38, checks: 206
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145124
Train loss (w/o reg) on all data: 0.110148
Test loss (w/o reg) on all data: 0.39402
Train acc on all data:  0.970026589316
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 8.63013e-06
Norm of the params: 26.4482
     Influence (LOO): fixed 171 labels. Loss 0.39402. Accuracy 0.900.
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120403
Train loss (w/o reg) on all data: 0.0724697
Test loss (w/o reg) on all data: 0.591134
Train acc on all data:  0.992264926275
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 3.02959e-05
Norm of the params: 30.9622
                Loss: fixed 169 labels. Loss 0.59113. Accuracy 0.858.
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202179
Train loss (w/o reg) on all data: 0.155521
Test loss (w/o reg) on all data: 0.552754
Train acc on all data:  0.951897510273
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 2.03182e-05
Norm of the params: 30.5475
              Random: fixed  34 labels. Loss 0.55275. Accuracy 0.846.
### Flips: 618, rs: 38, checks: 412
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126583
Train loss (w/o reg) on all data: 0.0969106
Test loss (w/o reg) on all data: 0.261724
Train acc on all data:  0.973168963017
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 6.48804e-06
Norm of the params: 24.3606
     Influence (LOO): fixed 272 labels. Loss 0.26172. Accuracy 0.928.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0931729
Train loss (w/o reg) on all data: 0.0509294
Test loss (w/o reg) on all data: 0.432206
Train acc on all data:  0.998066231569
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 3.88244e-05
Norm of the params: 29.0666
                Loss: fixed 261 labels. Loss 0.43221. Accuracy 0.894.
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196218
Train loss (w/o reg) on all data: 0.150808
Test loss (w/o reg) on all data: 0.529842
Train acc on all data:  0.954314720812
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 1.81474e-05
Norm of the params: 30.1361
              Random: fixed  63 labels. Loss 0.52984. Accuracy 0.849.
### Flips: 618, rs: 38, checks: 618
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111227
Train loss (w/o reg) on all data: 0.0861499
Test loss (w/o reg) on all data: 0.238957
Train acc on all data:  0.974861010394
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 3.20206e-06
Norm of the params: 22.395
     Influence (LOO): fixed 352 labels. Loss 0.23896. Accuracy 0.937.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0758927
Train loss (w/o reg) on all data: 0.039231
Test loss (w/o reg) on all data: 0.378818
Train acc on all data:  0.999033115784
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 1.52602e-05
Norm of the params: 27.0783
                Loss: fixed 319 labels. Loss 0.37882. Accuracy 0.909.
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189993
Train loss (w/o reg) on all data: 0.145453
Test loss (w/o reg) on all data: 0.530191
Train acc on all data:  0.955765047136
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 0.000106742
Norm of the params: 29.8462
              Random: fixed  95 labels. Loss 0.53019. Accuracy 0.855.
### Flips: 618, rs: 38, checks: 824
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100777
Train loss (w/o reg) on all data: 0.0787885
Test loss (w/o reg) on all data: 0.214964
Train acc on all data:  0.976794778825
Test acc on all data:   0.947826086957
Norm of the mean of gradients: 3.21233e-06
Norm of the params: 20.9705
     Influence (LOO): fixed 407 labels. Loss 0.21496. Accuracy 0.948.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0658207
Train loss (w/o reg) on all data: 0.0330696
Test loss (w/o reg) on all data: 0.380162
Train acc on all data:  0.999033115784
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 1.99155e-05
Norm of the params: 25.5934
                Loss: fixed 370 labels. Loss 0.38016. Accuracy 0.922.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181352
Train loss (w/o reg) on all data: 0.138129
Test loss (w/o reg) on all data: 0.513123
Train acc on all data:  0.959390862944
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 5.48732e-05
Norm of the params: 29.4017
              Random: fixed 128 labels. Loss 0.51312. Accuracy 0.858.
### Flips: 618, rs: 38, checks: 1030
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0955345
Train loss (w/o reg) on all data: 0.0747682
Test loss (w/o reg) on all data: 0.200562
Train acc on all data:  0.978245105149
Test acc on all data:   0.953623188406
Norm of the mean of gradients: 2.62454e-06
Norm of the params: 20.3795
     Influence (LOO): fixed 439 labels. Loss 0.20056. Accuracy 0.954.
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0584553
Train loss (w/o reg) on all data: 0.0289751
Test loss (w/o reg) on all data: 0.338406
Train acc on all data:  0.999274836838
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 3.17391e-05
Norm of the params: 24.2817
                Loss: fixed 414 labels. Loss 0.33841. Accuracy 0.933.
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173471
Train loss (w/o reg) on all data: 0.130958
Test loss (w/o reg) on all data: 0.526643
Train acc on all data:  0.960599468214
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 1.8885e-05
Norm of the params: 29.159
              Random: fixed 154 labels. Loss 0.52664. Accuracy 0.869.
### Flips: 618, rs: 38, checks: 1236
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0865554
Train loss (w/o reg) on all data: 0.0676549
Test loss (w/o reg) on all data: 0.228658
Train acc on all data:  0.980420594634
Test acc on all data:   0.96038647343
Norm of the mean of gradients: 2.67435e-06
Norm of the params: 19.4425
     Influence (LOO): fixed 477 labels. Loss 0.22866. Accuracy 0.960.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0535538
Train loss (w/o reg) on all data: 0.0264597
Test loss (w/o reg) on all data: 0.320662
Train acc on all data:  0.999033115784
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 9.20202e-06
Norm of the params: 23.2784
                Loss: fixed 441 labels. Loss 0.32066. Accuracy 0.940.
Using normal model
LBFGS training took [371] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164859
Train loss (w/o reg) on all data: 0.122717
Test loss (w/o reg) on all data: 0.482463
Train acc on all data:  0.967851099831
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 4.32463e-05
Norm of the params: 29.0317
              Random: fixed 179 labels. Loss 0.48246. Accuracy 0.874.
Using normal model
LBFGS training took [434] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204693
Train loss (w/o reg) on all data: 0.156751
Test loss (w/o reg) on all data: 0.491295
Train acc on all data:  0.952380952381
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 2.34574e-05
Norm of the params: 30.965
Flipped loss: 0.49129. Accuracy: 0.853
### Flips: 618, rs: 39, checks: 206
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135204
Train loss (w/o reg) on all data: 0.101757
Test loss (w/o reg) on all data: 0.390984
Train acc on all data:  0.970026589316
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.93969e-05
Norm of the params: 25.8639
     Influence (LOO): fixed 171 labels. Loss 0.39098. Accuracy 0.900.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118933
Train loss (w/o reg) on all data: 0.0705464
Test loss (w/o reg) on all data: 0.473993
Train acc on all data:  0.99008943679
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 5.67788e-05
Norm of the params: 31.1085
                Loss: fixed 166 labels. Loss 0.47399. Accuracy 0.859.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197801
Train loss (w/o reg) on all data: 0.150766
Test loss (w/o reg) on all data: 0.485231
Train acc on all data:  0.953831278704
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 2.10042e-05
Norm of the params: 30.6705
              Random: fixed  28 labels. Loss 0.48523. Accuracy 0.855.
### Flips: 618, rs: 39, checks: 412
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117449
Train loss (w/o reg) on all data: 0.0884316
Test loss (w/o reg) on all data: 0.390318
Train acc on all data:  0.973894126178
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 9.65225e-06
Norm of the params: 24.0903
     Influence (LOO): fixed 281 labels. Loss 0.39032. Accuracy 0.915.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0872234
Train loss (w/o reg) on all data: 0.046998
Test loss (w/o reg) on all data: 0.479981
Train acc on all data:  0.998066231569
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 3.12337e-05
Norm of the params: 28.3639
                Loss: fixed 259 labels. Loss 0.47998. Accuracy 0.886.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189695
Train loss (w/o reg) on all data: 0.143248
Test loss (w/o reg) on all data: 0.4724
Train acc on all data:  0.957457094513
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 1.55517e-05
Norm of the params: 30.4787
              Random: fixed  62 labels. Loss 0.47240. Accuracy 0.871.
### Flips: 618, rs: 39, checks: 618
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106187
Train loss (w/o reg) on all data: 0.080743
Test loss (w/o reg) on all data: 0.316019
Train acc on all data:  0.976794778825
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 5.37281e-06
Norm of the params: 22.5582
     Influence (LOO): fixed 364 labels. Loss 0.31602. Accuracy 0.932.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0703591
Train loss (w/o reg) on all data: 0.036365
Test loss (w/o reg) on all data: 0.394132
Train acc on all data:  0.999033115784
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 3.99554e-06
Norm of the params: 26.0745
                Loss: fixed 323 labels. Loss 0.39413. Accuracy 0.907.
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184574
Train loss (w/o reg) on all data: 0.139063
Test loss (w/o reg) on all data: 0.462219
Train acc on all data:  0.958665699782
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 4.93698e-05
Norm of the params: 30.17
              Random: fixed  97 labels. Loss 0.46222. Accuracy 0.871.
### Flips: 618, rs: 39, checks: 824
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0981676
Train loss (w/o reg) on all data: 0.0754439
Test loss (w/o reg) on all data: 0.304069
Train acc on all data:  0.977278220933
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 3.00632e-06
Norm of the params: 21.3184
     Influence (LOO): fixed 417 labels. Loss 0.30407. Accuracy 0.942.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0627414
Train loss (w/o reg) on all data: 0.031988
Test loss (w/o reg) on all data: 0.311363
Train acc on all data:  0.999033115784
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 1.06558e-05
Norm of the params: 24.8006
                Loss: fixed 364 labels. Loss 0.31136. Accuracy 0.929.
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175461
Train loss (w/o reg) on all data: 0.131443
Test loss (w/o reg) on all data: 0.448441
Train acc on all data:  0.962533236645
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 1.99883e-05
Norm of the params: 29.6709
              Random: fixed 128 labels. Loss 0.44844. Accuracy 0.867.
### Flips: 618, rs: 39, checks: 1030
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0896489
Train loss (w/o reg) on all data: 0.0694596
Test loss (w/o reg) on all data: 0.344182
Train acc on all data:  0.979695431472
Test acc on all data:   0.951690821256
Norm of the mean of gradients: 1.42577e-05
Norm of the params: 20.0944
     Influence (LOO): fixed 457 labels. Loss 0.34418. Accuracy 0.952.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0579398
Train loss (w/o reg) on all data: 0.0289282
Test loss (w/o reg) on all data: 0.332754
Train acc on all data:  0.999516557892
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 2.0595e-05
Norm of the params: 24.088
                Loss: fixed 392 labels. Loss 0.33275. Accuracy 0.925.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171658
Train loss (w/o reg) on all data: 0.12826
Test loss (w/o reg) on all data: 0.423528
Train acc on all data:  0.963741841914
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 1.97321e-05
Norm of the params: 29.461
              Random: fixed 154 labels. Loss 0.42353. Accuracy 0.875.
### Flips: 618, rs: 39, checks: 1236
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0831732
Train loss (w/o reg) on all data: 0.0648175
Test loss (w/o reg) on all data: 0.271678
Train acc on all data:  0.980420594634
Test acc on all data:   0.964251207729
Norm of the mean of gradients: 8.19865e-06
Norm of the params: 19.1602
     Influence (LOO): fixed 488 labels. Loss 0.27168. Accuracy 0.964.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0516555
Train loss (w/o reg) on all data: 0.0254943
Test loss (w/o reg) on all data: 0.246
Train acc on all data:  0.999516557892
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 5.64434e-06
Norm of the params: 22.8741
                Loss: fixed 431 labels. Loss 0.24600. Accuracy 0.937.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166159
Train loss (w/o reg) on all data: 0.123691
Test loss (w/o reg) on all data: 0.405394
Train acc on all data:  0.963983562968
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 2.28169e-05
Norm of the params: 29.1438
              Random: fixed 183 labels. Loss 0.40539. Accuracy 0.882.
Using normal model
LBFGS training took [540] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24662
Train loss (w/o reg) on all data: 0.193723
Test loss (w/o reg) on all data: 0.713053
Train acc on all data:  0.937877689147
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 2.19279e-05
Norm of the params: 32.526
Flipped loss: 0.71305. Accuracy: 0.815
### Flips: 824, rs: 0, checks: 206
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180088
Train loss (w/o reg) on all data: 0.136506
Test loss (w/o reg) on all data: 0.728881
Train acc on all data:  0.958665699782
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 2.44564e-05
Norm of the params: 29.5237
     Influence (LOO): fixed 161 labels. Loss 0.72888. Accuracy 0.851.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153889
Train loss (w/o reg) on all data: 0.0986788
Test loss (w/o reg) on all data: 0.768815
Train acc on all data:  0.98017887358
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 6.17301e-05
Norm of the params: 33.2295
                Loss: fixed 187 labels. Loss 0.76882. Accuracy 0.822.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237047
Train loss (w/o reg) on all data: 0.185039
Test loss (w/o reg) on all data: 0.743684
Train acc on all data:  0.941261783901
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 7.06466e-05
Norm of the params: 32.2514
              Random: fixed  40 labels. Loss 0.74368. Accuracy 0.819.
### Flips: 824, rs: 0, checks: 412
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15922
Train loss (w/o reg) on all data: 0.121478
Test loss (w/o reg) on all data: 0.580185
Train acc on all data:  0.961566352429
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 1.15436e-05
Norm of the params: 27.4744
     Influence (LOO): fixed 282 labels. Loss 0.58019. Accuracy 0.881.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115433
Train loss (w/o reg) on all data: 0.06593
Test loss (w/o reg) on all data: 0.752266
Train acc on all data:  0.995407299976
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 6.74116e-05
Norm of the params: 31.4652
                Loss: fixed 291 labels. Loss 0.75227. Accuracy 0.852.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231454
Train loss (w/o reg) on all data: 0.179918
Test loss (w/o reg) on all data: 0.759243
Train acc on all data:  0.943920715494
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 8.69309e-05
Norm of the params: 32.1047
              Random: fixed  71 labels. Loss 0.75924. Accuracy 0.835.
### Flips: 824, rs: 0, checks: 618
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143211
Train loss (w/o reg) on all data: 0.109468
Test loss (w/o reg) on all data: 0.511238
Train acc on all data:  0.966642494561
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 9.27133e-06
Norm of the params: 25.978
     Influence (LOO): fixed 388 labels. Loss 0.51124. Accuracy 0.902.
Using normal model
LBFGS training took [411] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0979016
Train loss (w/o reg) on all data: 0.0529012
Test loss (w/o reg) on all data: 0.716896
Train acc on all data:  0.998549673677
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 4.20403e-05
Norm of the params: 30.0001
                Loss: fixed 350 labels. Loss 0.71690. Accuracy 0.866.
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223365
Train loss (w/o reg) on all data: 0.172825
Test loss (w/o reg) on all data: 0.725948
Train acc on all data:  0.946096204979
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 1.87045e-05
Norm of the params: 31.7929
              Random: fixed 110 labels. Loss 0.72595. Accuracy 0.833.
### Flips: 824, rs: 0, checks: 824
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132148
Train loss (w/o reg) on all data: 0.102135
Test loss (w/o reg) on all data: 0.469511
Train acc on all data:  0.968576262993
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 1.2048e-05
Norm of the params: 24.5001
     Influence (LOO): fixed 461 labels. Loss 0.46951. Accuracy 0.911.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888814
Train loss (w/o reg) on all data: 0.047131
Test loss (w/o reg) on all data: 0.737744
Train acc on all data:  0.99879139473
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 1.80018e-05
Norm of the params: 28.8965
                Loss: fixed 398 labels. Loss 0.73774. Accuracy 0.880.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213792
Train loss (w/o reg) on all data: 0.164071
Test loss (w/o reg) on all data: 0.714263
Train acc on all data:  0.951655789219
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 4.57829e-05
Norm of the params: 31.5345
              Random: fixed 155 labels. Loss 0.71426. Accuracy 0.838.
### Flips: 824, rs: 0, checks: 1030
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123671
Train loss (w/o reg) on all data: 0.0960493
Test loss (w/o reg) on all data: 0.403603
Train acc on all data:  0.969784868262
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 1.62901e-05
Norm of the params: 23.504
     Influence (LOO): fixed 511 labels. Loss 0.40360. Accuracy 0.928.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0779083
Train loss (w/o reg) on all data: 0.0400224
Test loss (w/o reg) on all data: 0.624207
Train acc on all data:  0.998549673677
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 2.85615e-05
Norm of the params: 27.5267
                Loss: fixed 461 labels. Loss 0.62421. Accuracy 0.892.
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207204
Train loss (w/o reg) on all data: 0.158344
Test loss (w/o reg) on all data: 0.776451
Train acc on all data:  0.953106115543
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 1.74245e-05
Norm of the params: 31.2603
              Random: fixed 197 labels. Loss 0.77645. Accuracy 0.841.
### Flips: 824, rs: 0, checks: 1236
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113773
Train loss (w/o reg) on all data: 0.0885207
Test loss (w/o reg) on all data: 0.347191
Train acc on all data:  0.971718636693
Test acc on all data:   0.946859903382
Norm of the mean of gradients: 2.44858e-05
Norm of the params: 22.4732
     Influence (LOO): fixed 561 labels. Loss 0.34719. Accuracy 0.947.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0729677
Train loss (w/o reg) on all data: 0.0368055
Test loss (w/o reg) on all data: 0.548781
Train acc on all data:  0.999274836838
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 2.31693e-05
Norm of the params: 26.8932
                Loss: fixed 494 labels. Loss 0.54878. Accuracy 0.905.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201479
Train loss (w/o reg) on all data: 0.153737
Test loss (w/o reg) on all data: 0.784207
Train acc on all data:  0.955039883974
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 2.12757e-05
Norm of the params: 30.9005
              Random: fixed 236 labels. Loss 0.78421. Accuracy 0.844.
Using normal model
LBFGS training took [529] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244039
Train loss (w/o reg) on all data: 0.192249
Test loss (w/o reg) on all data: 0.658685
Train acc on all data:  0.933284989123
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 1.97563e-05
Norm of the params: 32.1838
Flipped loss: 0.65869. Accuracy: 0.781
### Flips: 824, rs: 1, checks: 206
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185668
Train loss (w/o reg) on all data: 0.143448
Test loss (w/o reg) on all data: 0.595576
Train acc on all data:  0.956248489243
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 8.75818e-06
Norm of the params: 29.0583
     Influence (LOO): fixed 159 labels. Loss 0.59558. Accuracy 0.834.
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154362
Train loss (w/o reg) on all data: 0.10057
Test loss (w/o reg) on all data: 0.646176
Train acc on all data:  0.978245105149
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 2.32213e-05
Norm of the params: 32.8001
                Loss: fixed 189 labels. Loss 0.64618. Accuracy 0.803.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239023
Train loss (w/o reg) on all data: 0.188327
Test loss (w/o reg) on all data: 0.623089
Train acc on all data:  0.935460478608
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 2.13966e-05
Norm of the params: 31.8421
              Random: fixed  44 labels. Loss 0.62309. Accuracy 0.794.
### Flips: 824, rs: 1, checks: 412
Using normal model
LBFGS training took [416] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162929
Train loss (w/o reg) on all data: 0.125887
Test loss (w/o reg) on all data: 0.425887
Train acc on all data:  0.960599468214
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 2.65145e-05
Norm of the params: 27.2182
     Influence (LOO): fixed 273 labels. Loss 0.42589. Accuracy 0.858.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124133
Train loss (w/o reg) on all data: 0.0728904
Test loss (w/o reg) on all data: 0.605693
Train acc on all data:  0.990814599952
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 1.32721e-05
Norm of the params: 32.0135
                Loss: fixed 284 labels. Loss 0.60569. Accuracy 0.809.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230459
Train loss (w/o reg) on all data: 0.18035
Test loss (w/o reg) on all data: 0.579641
Train acc on all data:  0.939569736524
Test acc on all data:   0.8
Norm of the mean of gradients: 4.22633e-05
Norm of the params: 31.6573
              Random: fixed  77 labels. Loss 0.57964. Accuracy 0.800.
### Flips: 824, rs: 1, checks: 618
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149858
Train loss (w/o reg) on all data: 0.116906
Test loss (w/o reg) on all data: 0.342298
Train acc on all data:  0.962533236645
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 9.14542e-06
Norm of the params: 25.6719
     Influence (LOO): fixed 364 labels. Loss 0.34230. Accuracy 0.882.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104398
Train loss (w/o reg) on all data: 0.0583191
Test loss (w/o reg) on all data: 0.549808
Train acc on all data:  0.998549673677
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 2.47608e-05
Norm of the params: 30.3577
                Loss: fixed 348 labels. Loss 0.54981. Accuracy 0.841.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226613
Train loss (w/o reg) on all data: 0.176968
Test loss (w/o reg) on all data: 0.574072
Train acc on all data:  0.944404157602
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 3.75369e-05
Norm of the params: 31.5101
              Random: fixed 111 labels. Loss 0.57407. Accuracy 0.816.
### Flips: 824, rs: 1, checks: 824
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136424
Train loss (w/o reg) on all data: 0.107123
Test loss (w/o reg) on all data: 0.276331
Train acc on all data:  0.965675610346
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.80626e-05
Norm of the params: 24.208
     Influence (LOO): fixed 441 labels. Loss 0.27633. Accuracy 0.906.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888512
Train loss (w/o reg) on all data: 0.0475897
Test loss (w/o reg) on all data: 0.475241
Train acc on all data:  0.998549673677
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 1.42947e-05
Norm of the params: 28.7268
                Loss: fixed 410 labels. Loss 0.47524. Accuracy 0.868.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220735
Train loss (w/o reg) on all data: 0.171796
Test loss (w/o reg) on all data: 0.557671
Train acc on all data:  0.945371041818
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 3.64957e-05
Norm of the params: 31.2854
              Random: fixed 145 labels. Loss 0.55767. Accuracy 0.820.
### Flips: 824, rs: 1, checks: 1030
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128561
Train loss (w/o reg) on all data: 0.101914
Test loss (w/o reg) on all data: 0.23996
Train acc on all data:  0.966159052453
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 1.10406e-05
Norm of the params: 23.0852
     Influence (LOO): fixed 496 labels. Loss 0.23996. Accuracy 0.920.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0821399
Train loss (w/o reg) on all data: 0.0430766
Test loss (w/o reg) on all data: 0.420756
Train acc on all data:  0.99879139473
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 1.0714e-05
Norm of the params: 27.9512
                Loss: fixed 445 labels. Loss 0.42076. Accuracy 0.883.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211178
Train loss (w/o reg) on all data: 0.163636
Test loss (w/o reg) on all data: 0.516484
Train acc on all data:  0.949722020788
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 3.10625e-05
Norm of the params: 30.8357
              Random: fixed 192 labels. Loss 0.51648. Accuracy 0.837.
### Flips: 824, rs: 1, checks: 1236
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121438
Train loss (w/o reg) on all data: 0.0971133
Test loss (w/o reg) on all data: 0.219471
Train acc on all data:  0.967125936669
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 2.9124e-05
Norm of the params: 22.0567
     Influence (LOO): fixed 549 labels. Loss 0.21947. Accuracy 0.928.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073188
Train loss (w/o reg) on all data: 0.0375728
Test loss (w/o reg) on all data: 0.431975
Train acc on all data:  0.999516557892
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 2.95158e-05
Norm of the params: 26.689
                Loss: fixed 490 labels. Loss 0.43197. Accuracy 0.894.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200835
Train loss (w/o reg) on all data: 0.155052
Test loss (w/o reg) on all data: 0.460078
Train acc on all data:  0.950930626058
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 2.55359e-05
Norm of the params: 30.2598
              Random: fixed 241 labels. Loss 0.46008. Accuracy 0.860.
Using normal model
LBFGS training took [553] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241859
Train loss (w/o reg) on all data: 0.188611
Test loss (w/o reg) on all data: 0.740724
Train acc on all data:  0.940053178632
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 6.72792e-05
Norm of the params: 32.6336
Flipped loss: 0.74072. Accuracy: 0.790
### Flips: 824, rs: 2, checks: 206
Using normal model
LBFGS training took [439] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17985
Train loss (w/o reg) on all data: 0.13657
Test loss (w/o reg) on all data: 0.661347
Train acc on all data:  0.96035774716
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 2.9454e-05
Norm of the params: 29.4209
     Influence (LOO): fixed 165 labels. Loss 0.66135. Accuracy 0.836.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1537
Train loss (w/o reg) on all data: 0.0976633
Test loss (w/o reg) on all data: 0.75397
Train acc on all data:  0.981629199903
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 4.16453e-05
Norm of the params: 33.4775
                Loss: fixed 182 labels. Loss 0.75397. Accuracy 0.810.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237978
Train loss (w/o reg) on all data: 0.185293
Test loss (w/o reg) on all data: 0.747614
Train acc on all data:  0.941261783901
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 9.75874e-05
Norm of the params: 32.4606
              Random: fixed  27 labels. Loss 0.74761. Accuracy 0.797.
### Flips: 824, rs: 2, checks: 412
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15589
Train loss (w/o reg) on all data: 0.118461
Test loss (w/o reg) on all data: 0.49551
Train acc on all data:  0.964467005076
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.34211e-05
Norm of the params: 27.3602
     Influence (LOO): fixed 293 labels. Loss 0.49551. Accuracy 0.862.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122346
Train loss (w/o reg) on all data: 0.0709189
Test loss (w/o reg) on all data: 0.657516
Train acc on all data:  0.995165578922
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 2.79115e-05
Norm of the params: 32.0709
                Loss: fixed 276 labels. Loss 0.65752. Accuracy 0.823.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228954
Train loss (w/o reg) on all data: 0.177147
Test loss (w/o reg) on all data: 0.748814
Train acc on all data:  0.942228668117
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 4.40813e-05
Norm of the params: 32.1889
              Random: fixed  71 labels. Loss 0.74881. Accuracy 0.805.
### Flips: 824, rs: 2, checks: 618
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141972
Train loss (w/o reg) on all data: 0.108625
Test loss (w/o reg) on all data: 0.453197
Train acc on all data:  0.967609378777
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 1.31421e-05
Norm of the params: 25.825
     Influence (LOO): fixed 386 labels. Loss 0.45320. Accuracy 0.878.
Using normal model
LBFGS training took [485] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104064
Train loss (w/o reg) on all data: 0.0576524
Test loss (w/o reg) on all data: 0.669454
Train acc on all data:  0.996615905245
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 1.09319e-05
Norm of the params: 30.4667
                Loss: fixed 349 labels. Loss 0.66945. Accuracy 0.859.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223603
Train loss (w/o reg) on all data: 0.172798
Test loss (w/o reg) on all data: 0.711468
Train acc on all data:  0.945612762872
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 0.000103262
Norm of the params: 31.8763
              Random: fixed 112 labels. Loss 0.71147. Accuracy 0.818.
### Flips: 824, rs: 2, checks: 824
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130564
Train loss (w/o reg) on all data: 0.0999013
Test loss (w/o reg) on all data: 0.338367
Train acc on all data:  0.970510031424
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 9.48427e-06
Norm of the params: 24.7641
     Influence (LOO): fixed 456 labels. Loss 0.33837. Accuracy 0.901.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0928088
Train loss (w/o reg) on all data: 0.0499064
Test loss (w/o reg) on all data: 0.544677
Train acc on all data:  0.998549673677
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 8.22374e-05
Norm of the params: 29.2925
                Loss: fixed 395 labels. Loss 0.54468. Accuracy 0.868.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214537
Train loss (w/o reg) on all data: 0.164645
Test loss (w/o reg) on all data: 0.683821
Train acc on all data:  0.948513415518
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 4.77239e-05
Norm of the params: 31.5884
              Random: fixed 159 labels. Loss 0.68382. Accuracy 0.834.
### Flips: 824, rs: 2, checks: 1030
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121539
Train loss (w/o reg) on all data: 0.0932217
Test loss (w/o reg) on all data: 0.294068
Train acc on all data:  0.971235194585
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 1.32472e-05
Norm of the params: 23.7982
     Influence (LOO): fixed 512 labels. Loss 0.29407. Accuracy 0.912.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082482
Train loss (w/o reg) on all data: 0.0435562
Test loss (w/o reg) on all data: 0.588764
Train acc on all data:  0.99879139473
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.50444e-05
Norm of the params: 27.9019
                Loss: fixed 450 labels. Loss 0.58876. Accuracy 0.889.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208534
Train loss (w/o reg) on all data: 0.159112
Test loss (w/o reg) on all data: 0.665803
Train acc on all data:  0.949963741842
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 2.95769e-05
Norm of the params: 31.4393
              Random: fixed 198 labels. Loss 0.66580. Accuracy 0.828.
### Flips: 824, rs: 2, checks: 1236
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113485
Train loss (w/o reg) on all data: 0.0874613
Test loss (w/o reg) on all data: 0.279086
Train acc on all data:  0.973168963017
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 5.77299e-06
Norm of the params: 22.8137
     Influence (LOO): fixed 560 labels. Loss 0.27909. Accuracy 0.918.
Using normal model
LBFGS training took [408] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0751967
Train loss (w/o reg) on all data: 0.0391769
Test loss (w/o reg) on all data: 0.543198
Train acc on all data:  0.998549673677
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 2.01156e-05
Norm of the params: 26.8402
                Loss: fixed 485 labels. Loss 0.54320. Accuracy 0.894.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199341
Train loss (w/o reg) on all data: 0.151057
Test loss (w/o reg) on all data: 0.608501
Train acc on all data:  0.955281605028
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 4.52691e-05
Norm of the params: 31.0753
              Random: fixed 246 labels. Loss 0.60850. Accuracy 0.841.
Using normal model
LBFGS training took [602] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237738
Train loss (w/o reg) on all data: 0.184456
Test loss (w/o reg) on all data: 0.630027
Train acc on all data:  0.941503504955
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 9.04434e-05
Norm of the params: 32.6442
Flipped loss: 0.63003. Accuracy: 0.814
### Flips: 824, rs: 3, checks: 206
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174803
Train loss (w/o reg) on all data: 0.130798
Test loss (w/o reg) on all data: 0.513961
Train acc on all data:  0.962049794537
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 8.94415e-05
Norm of the params: 29.6667
     Influence (LOO): fixed 165 labels. Loss 0.51396. Accuracy 0.853.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149334
Train loss (w/o reg) on all data: 0.0940088
Test loss (w/o reg) on all data: 0.542279
Train acc on all data:  0.983562968335
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 3.75495e-05
Norm of the params: 33.2643
                Loss: fixed 187 labels. Loss 0.54228. Accuracy 0.821.
Using normal model
LBFGS training took [535] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.233522
Train loss (w/o reg) on all data: 0.180859
Test loss (w/o reg) on all data: 0.610729
Train acc on all data:  0.943195552333
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 4.62395e-05
Norm of the params: 32.454
              Random: fixed  37 labels. Loss 0.61073. Accuracy 0.814.
### Flips: 824, rs: 3, checks: 412
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153129
Train loss (w/o reg) on all data: 0.114193
Test loss (w/o reg) on all data: 0.435416
Train acc on all data:  0.966884215615
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 3.79348e-05
Norm of the params: 27.9056
     Influence (LOO): fixed 280 labels. Loss 0.43542. Accuracy 0.876.
Using normal model
LBFGS training took [526] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119972
Train loss (w/o reg) on all data: 0.0685267
Test loss (w/o reg) on all data: 0.497598
Train acc on all data:  0.992990089437
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 3.99516e-05
Norm of the params: 32.0767
                Loss: fixed 281 labels. Loss 0.49760. Accuracy 0.847.
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225812
Train loss (w/o reg) on all data: 0.173805
Test loss (w/o reg) on all data: 0.616753
Train acc on all data:  0.945612762872
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 2.37164e-05
Norm of the params: 32.2512
              Random: fixed  66 labels. Loss 0.61675. Accuracy 0.815.
### Flips: 824, rs: 3, checks: 618
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138889
Train loss (w/o reg) on all data: 0.104218
Test loss (w/o reg) on all data: 0.319467
Train acc on all data:  0.968334541939
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 1.50287e-05
Norm of the params: 26.333
     Influence (LOO): fixed 381 labels. Loss 0.31947. Accuracy 0.900.
Using normal model
LBFGS training took [512] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104414
Train loss (w/o reg) on all data: 0.0576929
Test loss (w/o reg) on all data: 0.440531
Train acc on all data:  0.992748368383
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 1.76673e-05
Norm of the params: 30.5685
                Loss: fixed 347 labels. Loss 0.44053. Accuracy 0.855.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219401
Train loss (w/o reg) on all data: 0.168463
Test loss (w/o reg) on all data: 0.60994
Train acc on all data:  0.948755136572
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 5.70374e-05
Norm of the params: 31.9181
              Random: fixed 108 labels. Loss 0.60994. Accuracy 0.820.
### Flips: 824, rs: 3, checks: 824
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12846
Train loss (w/o reg) on all data: 0.0968682
Test loss (w/o reg) on all data: 0.26808
Train acc on all data:  0.970026589316
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 4.45163e-05
Norm of the params: 25.1363
     Influence (LOO): fixed 452 labels. Loss 0.26808. Accuracy 0.925.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0943161
Train loss (w/o reg) on all data: 0.0507205
Test loss (w/o reg) on all data: 0.36774
Train acc on all data:  0.998307952623
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 2.54484e-05
Norm of the params: 29.5282
                Loss: fixed 391 labels. Loss 0.36774. Accuracy 0.886.
Using normal model
LBFGS training took [570] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.212919
Train loss (w/o reg) on all data: 0.162978
Test loss (w/o reg) on all data: 0.582349
Train acc on all data:  0.951172347111
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.29158e-05
Norm of the params: 31.6043
              Random: fixed 146 labels. Loss 0.58235. Accuracy 0.824.
### Flips: 824, rs: 3, checks: 1030
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120086
Train loss (w/o reg) on all data: 0.0910126
Test loss (w/o reg) on all data: 0.280647
Train acc on all data:  0.971718636693
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 2.4714e-05
Norm of the params: 24.1137
     Influence (LOO): fixed 508 labels. Loss 0.28065. Accuracy 0.929.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0830677
Train loss (w/o reg) on all data: 0.0431883
Test loss (w/o reg) on all data: 0.351113
Train acc on all data:  0.998549673677
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 3.40199e-05
Norm of the params: 28.2416
                Loss: fixed 433 labels. Loss 0.35111. Accuracy 0.900.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199607
Train loss (w/o reg) on all data: 0.150655
Test loss (w/o reg) on all data: 0.532421
Train acc on all data:  0.954556441866
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 2.81214e-05
Norm of the params: 31.2897
              Random: fixed 197 labels. Loss 0.53242. Accuracy 0.846.
### Flips: 824, rs: 3, checks: 1236
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116739
Train loss (w/o reg) on all data: 0.0888256
Test loss (w/o reg) on all data: 0.2706
Train acc on all data:  0.972443799855
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 1.95363e-05
Norm of the params: 23.6276
     Influence (LOO): fixed 540 labels. Loss 0.27060. Accuracy 0.933.
Using normal model
LBFGS training took [390] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074727
Train loss (w/o reg) on all data: 0.0383812
Test loss (w/o reg) on all data: 0.327833
Train acc on all data:  0.999033115784
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 1.51693e-05
Norm of the params: 26.9614
                Loss: fixed 479 labels. Loss 0.32783. Accuracy 0.902.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191122
Train loss (w/o reg) on all data: 0.142917
Test loss (w/o reg) on all data: 0.593428
Train acc on all data:  0.960116026106
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 9.17741e-05
Norm of the params: 31.0501
              Random: fixed 239 labels. Loss 0.59343. Accuracy 0.844.
Using normal model
LBFGS training took [560] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245641
Train loss (w/o reg) on all data: 0.194608
Test loss (w/o reg) on all data: 0.687436
Train acc on all data:  0.934251873338
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 3.3596e-05
Norm of the params: 31.9478
Flipped loss: 0.68744. Accuracy: 0.834
### Flips: 824, rs: 4, checks: 206
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180266
Train loss (w/o reg) on all data: 0.137961
Test loss (w/o reg) on all data: 0.706022
Train acc on all data:  0.955281605028
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 1.44519e-05
Norm of the params: 29.0877
     Influence (LOO): fixed 167 labels. Loss 0.70602. Accuracy 0.855.
Using normal model
LBFGS training took [492] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156919
Train loss (w/o reg) on all data: 0.102478
Test loss (w/o reg) on all data: 0.744771
Train acc on all data:  0.979211989364
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 2.03424e-05
Norm of the params: 32.9974
                Loss: fixed 179 labels. Loss 0.74477. Accuracy 0.837.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23785
Train loss (w/o reg) on all data: 0.187709
Test loss (w/o reg) on all data: 0.660337
Train acc on all data:  0.938361131255
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 0.000125328
Norm of the params: 31.6675
              Random: fixed  44 labels. Loss 0.66034. Accuracy 0.825.
### Flips: 824, rs: 4, checks: 412
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156244
Train loss (w/o reg) on all data: 0.119476
Test loss (w/o reg) on all data: 0.592943
Train acc on all data:  0.96035774716
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.84877e-05
Norm of the params: 27.1177
     Influence (LOO): fixed 288 labels. Loss 0.59294. Accuracy 0.879.
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122844
Train loss (w/o reg) on all data: 0.072225
Test loss (w/o reg) on all data: 0.75262
Train acc on all data:  0.991539763113
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 9.1597e-06
Norm of the params: 31.8179
                Loss: fixed 285 labels. Loss 0.75262. Accuracy 0.842.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231921
Train loss (w/o reg) on all data: 0.1827
Test loss (w/o reg) on all data: 0.59724
Train acc on all data:  0.940778341794
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 6.73601e-05
Norm of the params: 31.3754
              Random: fixed  84 labels. Loss 0.59724. Accuracy 0.837.
### Flips: 824, rs: 4, checks: 618
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141171
Train loss (w/o reg) on all data: 0.108477
Test loss (w/o reg) on all data: 0.471067
Train acc on all data:  0.962533236645
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 6.39116e-06
Norm of the params: 25.5709
     Influence (LOO): fixed 382 labels. Loss 0.47107. Accuracy 0.903.
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104509
Train loss (w/o reg) on all data: 0.0582186
Test loss (w/o reg) on all data: 0.771698
Train acc on all data:  0.997099347353
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 8.7932e-06
Norm of the params: 30.4272
                Loss: fixed 349 labels. Loss 0.77170. Accuracy 0.864.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226012
Train loss (w/o reg) on all data: 0.177648
Test loss (w/o reg) on all data: 0.61715
Train acc on all data:  0.943195552333
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 6.79404e-05
Norm of the params: 31.101
              Random: fixed 124 labels. Loss 0.61715. Accuracy 0.830.
### Flips: 824, rs: 4, checks: 824
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134458
Train loss (w/o reg) on all data: 0.104014
Test loss (w/o reg) on all data: 0.414241
Train acc on all data:  0.963983562968
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 4.49756e-06
Norm of the params: 24.6757
     Influence (LOO): fixed 446 labels. Loss 0.41424. Accuracy 0.918.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0931192
Train loss (w/o reg) on all data: 0.0499533
Test loss (w/o reg) on all data: 0.72135
Train acc on all data:  0.998549673677
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 5.18279e-06
Norm of the params: 29.3823
                Loss: fixed 394 labels. Loss 0.72135. Accuracy 0.877.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219677
Train loss (w/o reg) on all data: 0.172339
Test loss (w/o reg) on all data: 0.577017
Train acc on all data:  0.945371041818
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 9.11051e-05
Norm of the params: 30.7696
              Random: fixed 168 labels. Loss 0.57702. Accuracy 0.841.
### Flips: 824, rs: 4, checks: 1030
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126729
Train loss (w/o reg) on all data: 0.098254
Test loss (w/o reg) on all data: 0.430808
Train acc on all data:  0.967851099831
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 4.01267e-06
Norm of the params: 23.8644
     Influence (LOO): fixed 496 labels. Loss 0.43081. Accuracy 0.930.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0808051
Train loss (w/o reg) on all data: 0.0420779
Test loss (w/o reg) on all data: 0.672088
Train acc on all data:  0.999033115784
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 5.63998e-06
Norm of the params: 27.8306
                Loss: fixed 450 labels. Loss 0.67209. Accuracy 0.896.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209517
Train loss (w/o reg) on all data: 0.162708
Test loss (w/o reg) on all data: 0.558326
Train acc on all data:  0.949722020788
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 2.77048e-05
Norm of the params: 30.5969
              Random: fixed 205 labels. Loss 0.55833. Accuracy 0.846.
### Flips: 824, rs: 4, checks: 1236
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120072
Train loss (w/o reg) on all data: 0.0934898
Test loss (w/o reg) on all data: 0.406256
Train acc on all data:  0.968334541939
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 6.28592e-06
Norm of the params: 23.0574
     Influence (LOO): fixed 547 labels. Loss 0.40626. Accuracy 0.936.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0716583
Train loss (w/o reg) on all data: 0.0367297
Test loss (w/o reg) on all data: 0.626627
Train acc on all data:  0.999274836838
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 4.86512e-06
Norm of the params: 26.4305
                Loss: fixed 495 labels. Loss 0.62663. Accuracy 0.908.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202976
Train loss (w/o reg) on all data: 0.157243
Test loss (w/o reg) on all data: 0.518084
Train acc on all data:  0.950688905004
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 8.35421e-05
Norm of the params: 30.2434
              Random: fixed 244 labels. Loss 0.51808. Accuracy 0.854.
Using normal model
LBFGS training took [649] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242779
Train loss (w/o reg) on all data: 0.190817
Test loss (w/o reg) on all data: 0.709684
Train acc on all data:  0.935943920715
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 9.2461e-05
Norm of the params: 32.2371
Flipped loss: 0.70968. Accuracy: 0.783
### Flips: 824, rs: 5, checks: 206
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.170727
Train loss (w/o reg) on all data: 0.127503
Test loss (w/o reg) on all data: 0.599526
Train acc on all data:  0.965433889292
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 2.58921e-05
Norm of the params: 29.402
     Influence (LOO): fixed 171 labels. Loss 0.59953. Accuracy 0.833.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142469
Train loss (w/o reg) on all data: 0.0890785
Test loss (w/o reg) on all data: 0.69185
Train acc on all data:  0.985980178874
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 3.83958e-05
Norm of the params: 32.6773
                Loss: fixed 192 labels. Loss 0.69185. Accuracy 0.811.
Using normal model
LBFGS training took [549] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.236892
Train loss (w/o reg) on all data: 0.185967
Test loss (w/o reg) on all data: 0.665378
Train acc on all data:  0.937635968093
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 0.000139958
Norm of the params: 31.914
              Random: fixed  34 labels. Loss 0.66538. Accuracy 0.793.
### Flips: 824, rs: 5, checks: 412
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144584
Train loss (w/o reg) on all data: 0.107349
Test loss (w/o reg) on all data: 0.48455
Train acc on all data:  0.968334541939
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 1.31832e-05
Norm of the params: 27.2892
     Influence (LOO): fixed 304 labels. Loss 0.48455. Accuracy 0.880.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111359
Train loss (w/o reg) on all data: 0.0622407
Test loss (w/o reg) on all data: 0.628365
Train acc on all data:  0.997341068407
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 5.99487e-05
Norm of the params: 31.3428
                Loss: fixed 288 labels. Loss 0.62836. Accuracy 0.837.
Using normal model
LBFGS training took [615] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23088
Train loss (w/o reg) on all data: 0.18045
Test loss (w/o reg) on all data: 0.678424
Train acc on all data:  0.941020062847
Test acc on all data:   0.8
Norm of the mean of gradients: 2.94327e-05
Norm of the params: 31.7586
              Random: fixed  71 labels. Loss 0.67842. Accuracy 0.800.
### Flips: 824, rs: 5, checks: 618
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132256
Train loss (w/o reg) on all data: 0.0988783
Test loss (w/o reg) on all data: 0.431301
Train acc on all data:  0.971235194585
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 1.43282e-05
Norm of the params: 25.837
     Influence (LOO): fixed 390 labels. Loss 0.43130. Accuracy 0.890.
Using normal model
LBFGS training took [404] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0959604
Train loss (w/o reg) on all data: 0.0518055
Test loss (w/o reg) on all data: 0.589277
Train acc on all data:  0.997824510515
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 2.2241e-05
Norm of the params: 29.717
                Loss: fixed 356 labels. Loss 0.58928. Accuracy 0.843.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223057
Train loss (w/o reg) on all data: 0.173754
Test loss (w/o reg) on all data: 0.650848
Train acc on all data:  0.943437273387
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 3.32595e-05
Norm of the params: 31.4017
              Random: fixed 115 labels. Loss 0.65085. Accuracy 0.814.
### Flips: 824, rs: 5, checks: 824
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121204
Train loss (w/o reg) on all data: 0.0909359
Test loss (w/o reg) on all data: 0.3679
Train acc on all data:  0.973652405124
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 8.02327e-06
Norm of the params: 24.604
     Influence (LOO): fixed 465 labels. Loss 0.36790. Accuracy 0.905.
Using normal model
LBFGS training took [401] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0864087
Train loss (w/o reg) on all data: 0.0454262
Test loss (w/o reg) on all data: 0.55239
Train acc on all data:  0.998549673677
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 9.48552e-06
Norm of the params: 28.6295
                Loss: fixed 403 labels. Loss 0.55239. Accuracy 0.863.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.218295
Train loss (w/o reg) on all data: 0.169528
Test loss (w/o reg) on all data: 0.629975
Train acc on all data:  0.943920715494
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 7.72022e-05
Norm of the params: 31.2305
              Random: fixed 148 labels. Loss 0.62998. Accuracy 0.811.
### Flips: 824, rs: 5, checks: 1030
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111575
Train loss (w/o reg) on all data: 0.0841055
Test loss (w/o reg) on all data: 0.345651
Train acc on all data:  0.97461928934
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.43284e-05
Norm of the params: 23.439
     Influence (LOO): fixed 529 labels. Loss 0.34565. Accuracy 0.916.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0783407
Train loss (w/o reg) on all data: 0.0404111
Test loss (w/o reg) on all data: 0.466406
Train acc on all data:  0.999274836838
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 1.12214e-05
Norm of the params: 27.5425
                Loss: fixed 440 labels. Loss 0.46641. Accuracy 0.874.
Using normal model
LBFGS training took [570] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207672
Train loss (w/o reg) on all data: 0.159958
Test loss (w/o reg) on all data: 0.57896
Train acc on all data:  0.947788252357
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 0.000109915
Norm of the params: 30.8916
              Random: fixed 192 labels. Loss 0.57896. Accuracy 0.823.
### Flips: 824, rs: 5, checks: 1236
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10506
Train loss (w/o reg) on all data: 0.0794347
Test loss (w/o reg) on all data: 0.236375
Train acc on all data:  0.976069615664
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 1.77043e-05
Norm of the params: 22.6386
     Influence (LOO): fixed 574 labels. Loss 0.23638. Accuracy 0.925.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0730367
Train loss (w/o reg) on all data: 0.0373059
Test loss (w/o reg) on all data: 0.429152
Train acc on all data:  0.99879139473
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 8.68886e-06
Norm of the params: 26.7323
                Loss: fixed 476 labels. Loss 0.42915. Accuracy 0.881.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198783
Train loss (w/o reg) on all data: 0.151947
Test loss (w/o reg) on all data: 0.539961
Train acc on all data:  0.951655789219
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 2.71069e-05
Norm of the params: 30.6059
              Random: fixed 239 labels. Loss 0.53996. Accuracy 0.837.
Using normal model
LBFGS training took [606] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241214
Train loss (w/o reg) on all data: 0.189734
Test loss (w/o reg) on all data: 0.661928
Train acc on all data:  0.935702199662
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 2.14797e-05
Norm of the params: 32.0874
Flipped loss: 0.66193. Accuracy: 0.817
### Flips: 824, rs: 6, checks: 206
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17366
Train loss (w/o reg) on all data: 0.132138
Test loss (w/o reg) on all data: 0.497477
Train acc on all data:  0.960841189268
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 1.58421e-05
Norm of the params: 28.8173
     Influence (LOO): fixed 174 labels. Loss 0.49748. Accuracy 0.863.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143601
Train loss (w/o reg) on all data: 0.0902067
Test loss (w/o reg) on all data: 0.558189
Train acc on all data:  0.98573845782
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 1.76722e-05
Norm of the params: 32.6786
                Loss: fixed 194 labels. Loss 0.55819. Accuracy 0.832.
Using normal model
LBFGS training took [485] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234956
Train loss (w/o reg) on all data: 0.184415
Test loss (w/o reg) on all data: 0.645754
Train acc on all data:  0.937635968093
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 2.72331e-05
Norm of the params: 31.7933
              Random: fixed  39 labels. Loss 0.64575. Accuracy 0.827.
### Flips: 824, rs: 6, checks: 412
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152896
Train loss (w/o reg) on all data: 0.117053
Test loss (w/o reg) on all data: 0.437444
Train acc on all data:  0.963741841914
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 4.99304e-05
Norm of the params: 26.7742
     Influence (LOO): fixed 297 labels. Loss 0.43744. Accuracy 0.887.
Using normal model
LBFGS training took [474] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108931
Train loss (w/o reg) on all data: 0.0608097
Test loss (w/o reg) on all data: 0.490772
Train acc on all data:  0.997582789461
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 3.25662e-05
Norm of the params: 31.023
                Loss: fixed 291 labels. Loss 0.49077. Accuracy 0.853.
Using normal model
LBFGS training took [548] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227804
Train loss (w/o reg) on all data: 0.178156
Test loss (w/o reg) on all data: 0.628614
Train acc on all data:  0.94053662074
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 1.80226e-05
Norm of the params: 31.5115
              Random: fixed  83 labels. Loss 0.62861. Accuracy 0.829.
### Flips: 824, rs: 6, checks: 618
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139153
Train loss (w/o reg) on all data: 0.106906
Test loss (w/o reg) on all data: 0.361923
Train acc on all data:  0.967125936669
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 9.57202e-06
Norm of the params: 25.3956
     Influence (LOO): fixed 382 labels. Loss 0.36192. Accuracy 0.905.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0964866
Train loss (w/o reg) on all data: 0.0516765
Test loss (w/o reg) on all data: 0.44412
Train acc on all data:  0.998307952623
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 2.52495e-05
Norm of the params: 29.9367
                Loss: fixed 346 labels. Loss 0.44412. Accuracy 0.867.
Using normal model
LBFGS training took [548] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221116
Train loss (w/o reg) on all data: 0.172478
Test loss (w/o reg) on all data: 0.605105
Train acc on all data:  0.941745226009
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 3.08817e-05
Norm of the params: 31.1892
              Random: fixed 120 labels. Loss 0.60510. Accuracy 0.825.
### Flips: 824, rs: 6, checks: 824
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128109
Train loss (w/o reg) on all data: 0.0988635
Test loss (w/o reg) on all data: 0.283038
Train acc on all data:  0.9690597051
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 8.92992e-06
Norm of the params: 24.1847
     Influence (LOO): fixed 454 labels. Loss 0.28304. Accuracy 0.921.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888063
Train loss (w/o reg) on all data: 0.0465083
Test loss (w/o reg) on all data: 0.425112
Train acc on all data:  0.99879139473
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 3.03488e-05
Norm of the params: 29.0854
                Loss: fixed 385 labels. Loss 0.42511. Accuracy 0.882.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213335
Train loss (w/o reg) on all data: 0.165602
Test loss (w/o reg) on all data: 0.594126
Train acc on all data:  0.945129320764
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 5.40459e-05
Norm of the params: 30.8975
              Random: fixed 161 labels. Loss 0.59413. Accuracy 0.838.
### Flips: 824, rs: 6, checks: 1030
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121855
Train loss (w/o reg) on all data: 0.0943551
Test loss (w/o reg) on all data: 0.263875
Train acc on all data:  0.969301426154
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 1.57863e-05
Norm of the params: 23.4522
     Influence (LOO): fixed 501 labels. Loss 0.26388. Accuracy 0.929.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0818888
Train loss (w/o reg) on all data: 0.042318
Test loss (w/o reg) on all data: 0.406846
Train acc on all data:  0.998549673677
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 7.48116e-06
Norm of the params: 28.1321
                Loss: fixed 424 labels. Loss 0.40685. Accuracy 0.887.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206412
Train loss (w/o reg) on all data: 0.159851
Test loss (w/o reg) on all data: 0.546785
Train acc on all data:  0.951172347111
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 2.7129e-05
Norm of the params: 30.5159
              Random: fixed 198 labels. Loss 0.54679. Accuracy 0.848.
### Flips: 824, rs: 6, checks: 1236
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115844
Train loss (w/o reg) on all data: 0.0897529
Test loss (w/o reg) on all data: 0.242028
Train acc on all data:  0.971960357747
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 1.68158e-05
Norm of the params: 22.8435
     Influence (LOO): fixed 551 labels. Loss 0.24203. Accuracy 0.941.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0717406
Train loss (w/o reg) on all data: 0.0362218
Test loss (w/o reg) on all data: 0.355152
Train acc on all data:  0.999033115784
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 2.22611e-05
Norm of the params: 26.6529
                Loss: fixed 476 labels. Loss 0.35515. Accuracy 0.900.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197852
Train loss (w/o reg) on all data: 0.152512
Test loss (w/o reg) on all data: 0.556638
Train acc on all data:  0.951655789219
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 7.5332e-05
Norm of the params: 30.1133
              Random: fixed 241 labels. Loss 0.55664. Accuracy 0.854.
Using normal model
LBFGS training took [717] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242687
Train loss (w/o reg) on all data: 0.18916
Test loss (w/o reg) on all data: 0.87465
Train acc on all data:  0.938361131255
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 3.37539e-05
Norm of the params: 32.7192
Flipped loss: 0.87465. Accuracy: 0.787
### Flips: 824, rs: 7, checks: 206
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175861
Train loss (w/o reg) on all data: 0.131694
Test loss (w/o reg) on all data: 0.598373
Train acc on all data:  0.961566352429
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 1.69766e-05
Norm of the params: 29.721
     Influence (LOO): fixed 166 labels. Loss 0.59837. Accuracy 0.854.
Using normal model
LBFGS training took [586] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152258
Train loss (w/o reg) on all data: 0.0964163
Test loss (w/o reg) on all data: 0.867858
Train acc on all data:  0.982837805173
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 0.000142251
Norm of the params: 33.419
                Loss: fixed 186 labels. Loss 0.86786. Accuracy 0.813.
Using normal model
LBFGS training took [600] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235881
Train loss (w/o reg) on all data: 0.182492
Test loss (w/o reg) on all data: 0.816926
Train acc on all data:  0.940778341794
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 6.30594e-05
Norm of the params: 32.677
              Random: fixed  33 labels. Loss 0.81693. Accuracy 0.796.
### Flips: 824, rs: 7, checks: 412
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153623
Train loss (w/o reg) on all data: 0.114748
Test loss (w/o reg) on all data: 0.455634
Train acc on all data:  0.966400773507
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 1.49672e-05
Norm of the params: 27.8835
     Influence (LOO): fixed 283 labels. Loss 0.45563. Accuracy 0.888.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117439
Train loss (w/o reg) on all data: 0.0667821
Test loss (w/o reg) on all data: 0.716181
Train acc on all data:  0.995890742084
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 3.57242e-05
Norm of the params: 31.8298
                Loss: fixed 286 labels. Loss 0.71618. Accuracy 0.848.
Using normal model
LBFGS training took [619] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22883
Train loss (w/o reg) on all data: 0.176137
Test loss (w/o reg) on all data: 0.824828
Train acc on all data:  0.948029973411
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 3.48557e-05
Norm of the params: 32.463
              Random: fixed  80 labels. Loss 0.82483. Accuracy 0.811.
### Flips: 824, rs: 7, checks: 618
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141031
Train loss (w/o reg) on all data: 0.10487
Test loss (w/o reg) on all data: 0.428762
Train acc on all data:  0.969784868262
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 3.62284e-05
Norm of the params: 26.8926
     Influence (LOO): fixed 370 labels. Loss 0.42876. Accuracy 0.897.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0996672
Train loss (w/o reg) on all data: 0.0542127
Test loss (w/o reg) on all data: 0.592391
Train acc on all data:  0.998066231569
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 3.25285e-05
Norm of the params: 30.1511
                Loss: fixed 360 labels. Loss 0.59239. Accuracy 0.872.
Using normal model
LBFGS training took [632] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.218357
Train loss (w/o reg) on all data: 0.166822
Test loss (w/o reg) on all data: 0.796715
Train acc on all data:  0.951655789219
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 6.37302e-05
Norm of the params: 32.1045
              Random: fixed 131 labels. Loss 0.79672. Accuracy 0.822.
### Flips: 824, rs: 7, checks: 824
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127623
Train loss (w/o reg) on all data: 0.0965209
Test loss (w/o reg) on all data: 0.421457
Train acc on all data:  0.969543147208
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 6.88939e-06
Norm of the params: 24.9406
     Influence (LOO): fixed 458 labels. Loss 0.42146. Accuracy 0.920.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0878625
Train loss (w/o reg) on all data: 0.0462891
Test loss (w/o reg) on all data: 0.541158
Train acc on all data:  0.999033115784
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.14631e-05
Norm of the params: 28.8352
                Loss: fixed 411 labels. Loss 0.54116. Accuracy 0.879.
Using normal model
LBFGS training took [607] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.212152
Train loss (w/o reg) on all data: 0.16142
Test loss (w/o reg) on all data: 0.687398
Train acc on all data:  0.953831278704
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 0.000136963
Norm of the params: 31.8536
              Random: fixed 165 labels. Loss 0.68740. Accuracy 0.824.
### Flips: 824, rs: 7, checks: 1030
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118849
Train loss (w/o reg) on all data: 0.0906795
Test loss (w/o reg) on all data: 0.331965
Train acc on all data:  0.971960357747
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 1.59135e-05
Norm of the params: 23.7356
     Influence (LOO): fixed 523 labels. Loss 0.33196. Accuracy 0.936.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0810137
Train loss (w/o reg) on all data: 0.0419939
Test loss (w/o reg) on all data: 0.470487
Train acc on all data:  0.999033115784
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 2.80511e-05
Norm of the params: 27.9356
                Loss: fixed 449 labels. Loss 0.47049. Accuracy 0.899.
Using normal model
LBFGS training took [562] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205661
Train loss (w/o reg) on all data: 0.155928
Test loss (w/o reg) on all data: 0.655574
Train acc on all data:  0.952139231327
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 8.0639e-05
Norm of the params: 31.5381
              Random: fixed 204 labels. Loss 0.65557. Accuracy 0.825.
### Flips: 824, rs: 7, checks: 1236
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112428
Train loss (w/o reg) on all data: 0.0865398
Test loss (w/o reg) on all data: 0.259071
Train acc on all data:  0.973410684071
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 1.83246e-05
Norm of the params: 22.7546
     Influence (LOO): fixed 563 labels. Loss 0.25907. Accuracy 0.938.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0740009
Train loss (w/o reg) on all data: 0.0379103
Test loss (w/o reg) on all data: 0.445013
Train acc on all data:  0.99879139473
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 5.33055e-06
Norm of the params: 26.8666
                Loss: fixed 491 labels. Loss 0.44501. Accuracy 0.909.
Using normal model
LBFGS training took [554] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195692
Train loss (w/o reg) on all data: 0.147563
Test loss (w/o reg) on all data: 0.643119
Train acc on all data:  0.95479816292
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 1.94417e-05
Norm of the params: 31.0256
              Random: fixed 258 labels. Loss 0.64312. Accuracy 0.843.
Using normal model
LBFGS training took [698] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244189
Train loss (w/o reg) on all data: 0.190761
Test loss (w/o reg) on all data: 0.692341
Train acc on all data:  0.938602852308
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 9.46289e-05
Norm of the params: 32.689
Flipped loss: 0.69234. Accuracy: 0.812
### Flips: 824, rs: 8, checks: 206
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174765
Train loss (w/o reg) on all data: 0.131436
Test loss (w/o reg) on all data: 0.789711
Train acc on all data:  0.960599468214
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.41664e-05
Norm of the params: 29.4377
     Influence (LOO): fixed 172 labels. Loss 0.78971. Accuracy 0.862.
Using normal model
LBFGS training took [643] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149618
Train loss (w/o reg) on all data: 0.0952815
Test loss (w/o reg) on all data: 0.750592
Train acc on all data:  0.981870920957
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 1.42075e-05
Norm of the params: 32.9655
                Loss: fixed 181 labels. Loss 0.75059. Accuracy 0.817.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235786
Train loss (w/o reg) on all data: 0.183051
Test loss (w/o reg) on all data: 0.713959
Train acc on all data:  0.943437273387
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 9.2059e-05
Norm of the params: 32.4762
              Random: fixed  47 labels. Loss 0.71396. Accuracy 0.818.
### Flips: 824, rs: 8, checks: 412
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153402
Train loss (w/o reg) on all data: 0.115054
Test loss (w/o reg) on all data: 0.65183
Train acc on all data:  0.964950447184
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.29225e-05
Norm of the params: 27.694
     Influence (LOO): fixed 283 labels. Loss 0.65183. Accuracy 0.879.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117063
Train loss (w/o reg) on all data: 0.0667584
Test loss (w/o reg) on all data: 0.878906
Train acc on all data:  0.995890742084
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 9.61995e-05
Norm of the params: 31.7189
                Loss: fixed 286 labels. Loss 0.87891. Accuracy 0.848.
Using normal model
LBFGS training took [611] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232509
Train loss (w/o reg) on all data: 0.180749
Test loss (w/o reg) on all data: 0.684171
Train acc on all data:  0.944404157602
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 7.67166e-05
Norm of the params: 32.1744
              Random: fixed  80 labels. Loss 0.68417. Accuracy 0.830.
### Flips: 824, rs: 8, checks: 618
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138036
Train loss (w/o reg) on all data: 0.10394
Test loss (w/o reg) on all data: 0.584945
Train acc on all data:  0.969301426154
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 7.29789e-06
Norm of the params: 26.1137
     Influence (LOO): fixed 377 labels. Loss 0.58494. Accuracy 0.915.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0959675
Train loss (w/o reg) on all data: 0.0517275
Test loss (w/o reg) on all data: 0.760678
Train acc on all data:  0.998066231569
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 2.94154e-05
Norm of the params: 29.7456
                Loss: fixed 364 labels. Loss 0.76068. Accuracy 0.883.
Using normal model
LBFGS training took [565] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227838
Train loss (w/o reg) on all data: 0.176814
Test loss (w/o reg) on all data: 0.647355
Train acc on all data:  0.945612762872
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 5.48774e-05
Norm of the params: 31.945
              Random: fixed 111 labels. Loss 0.64735. Accuracy 0.836.
### Flips: 824, rs: 8, checks: 824
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128825
Train loss (w/o reg) on all data: 0.0982553
Test loss (w/o reg) on all data: 0.481205
Train acc on all data:  0.970993473532
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 5.4486e-05
Norm of the params: 24.7265
     Influence (LOO): fixed 454 labels. Loss 0.48121. Accuracy 0.927.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0872235
Train loss (w/o reg) on all data: 0.0458405
Test loss (w/o reg) on all data: 0.796581
Train acc on all data:  0.99879139473
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 1.12697e-05
Norm of the params: 28.7691
                Loss: fixed 407 labels. Loss 0.79658. Accuracy 0.886.
Using normal model
LBFGS training took [612] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219677
Train loss (w/o reg) on all data: 0.170038
Test loss (w/o reg) on all data: 0.59772
Train acc on all data:  0.948755136572
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 2.5843e-05
Norm of the params: 31.5084
              Random: fixed 156 labels. Loss 0.59772. Accuracy 0.843.
### Flips: 824, rs: 8, checks: 1030
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121531
Train loss (w/o reg) on all data: 0.0928097
Test loss (w/o reg) on all data: 0.449266
Train acc on all data:  0.970993473532
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 2.86064e-05
Norm of the params: 23.9672
     Influence (LOO): fixed 504 labels. Loss 0.44927. Accuracy 0.934.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0792079
Train loss (w/o reg) on all data: 0.0408193
Test loss (w/o reg) on all data: 0.644342
Train acc on all data:  0.999033115784
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.9105e-05
Norm of the params: 27.7087
                Loss: fixed 446 labels. Loss 0.64434. Accuracy 0.903.
Using normal model
LBFGS training took [600] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.212749
Train loss (w/o reg) on all data: 0.164062
Test loss (w/o reg) on all data: 0.578381
Train acc on all data:  0.950205462896
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 0.000141174
Norm of the params: 31.2046
              Random: fixed 195 labels. Loss 0.57838. Accuracy 0.849.
### Flips: 824, rs: 8, checks: 1236
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111138
Train loss (w/o reg) on all data: 0.0850111
Test loss (w/o reg) on all data: 0.499118
Train acc on all data:  0.973652405124
Test acc on all data:   0.948792270531
Norm of the mean of gradients: 7.43779e-06
Norm of the params: 22.859
     Influence (LOO): fixed 563 labels. Loss 0.49912. Accuracy 0.949.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071125
Train loss (w/o reg) on all data: 0.0359861
Test loss (w/o reg) on all data: 0.684005
Train acc on all data:  0.999274836838
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 3.40496e-05
Norm of the params: 26.5099
                Loss: fixed 478 labels. Loss 0.68400. Accuracy 0.914.
Using normal model
LBFGS training took [606] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204036
Train loss (w/o reg) on all data: 0.156114
Test loss (w/o reg) on all data: 0.518902
Train acc on all data:  0.953347836597
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 3.12953e-05
Norm of the params: 30.9587
              Random: fixed 230 labels. Loss 0.51890. Accuracy 0.854.
Using normal model
LBFGS training took [583] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240815
Train loss (w/o reg) on all data: 0.188742
Test loss (w/o reg) on all data: 0.917862
Train acc on all data:  0.94053662074
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 3.1122e-05
Norm of the params: 32.2715
Flipped loss: 0.91786. Accuracy: 0.809
### Flips: 824, rs: 9, checks: 206
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175954
Train loss (w/o reg) on all data: 0.13282
Test loss (w/o reg) on all data: 0.767944
Train acc on all data:  0.958423978729
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 1.41746e-05
Norm of the params: 29.3715
     Influence (LOO): fixed 169 labels. Loss 0.76794. Accuracy 0.857.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146675
Train loss (w/o reg) on all data: 0.0928697
Test loss (w/o reg) on all data: 0.85396
Train acc on all data:  0.983321247281
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 2.16166e-05
Norm of the params: 32.8041
                Loss: fixed 180 labels. Loss 0.85396. Accuracy 0.824.
Using normal model
LBFGS training took [502] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232593
Train loss (w/o reg) on all data: 0.180971
Test loss (w/o reg) on all data: 0.877236
Train acc on all data:  0.943920715494
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 7.7411e-05
Norm of the params: 32.1316
              Random: fixed  31 labels. Loss 0.87724. Accuracy 0.809.
### Flips: 824, rs: 9, checks: 412
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155421
Train loss (w/o reg) on all data: 0.118156
Test loss (w/o reg) on all data: 0.709907
Train acc on all data:  0.965192168238
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 4.9312e-06
Norm of the params: 27.3001
     Influence (LOO): fixed 291 labels. Loss 0.70991. Accuracy 0.880.
Using normal model
LBFGS training took [434] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114856
Train loss (w/o reg) on all data: 0.0657033
Test loss (w/o reg) on all data: 0.719009
Train acc on all data:  0.992506647329
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 9.7582e-06
Norm of the params: 31.3537
                Loss: fixed 289 labels. Loss 0.71901. Accuracy 0.853.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226666
Train loss (w/o reg) on all data: 0.17553
Test loss (w/o reg) on all data: 0.847501
Train acc on all data:  0.946337926033
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 3.26918e-05
Norm of the params: 31.98
              Random: fixed  71 labels. Loss 0.84750. Accuracy 0.821.
### Flips: 824, rs: 9, checks: 618
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137733
Train loss (w/o reg) on all data: 0.104364
Test loss (w/o reg) on all data: 0.637773
Train acc on all data:  0.969543147208
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 8.26992e-06
Norm of the params: 25.8336
     Influence (LOO): fixed 382 labels. Loss 0.63777. Accuracy 0.891.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100447
Train loss (w/o reg) on all data: 0.0551293
Test loss (w/o reg) on all data: 0.683599
Train acc on all data:  0.997582789461
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 1.6497e-05
Norm of the params: 30.1058
                Loss: fixed 348 labels. Loss 0.68360. Accuracy 0.870.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217034
Train loss (w/o reg) on all data: 0.167066
Test loss (w/o reg) on all data: 0.770461
Train acc on all data:  0.947788252357
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 2.56488e-05
Norm of the params: 31.6127
              Random: fixed 118 labels. Loss 0.77046. Accuracy 0.824.
### Flips: 824, rs: 9, checks: 824
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127235
Train loss (w/o reg) on all data: 0.0966427
Test loss (w/o reg) on all data: 0.574449
Train acc on all data:  0.970993473532
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 9.18839e-06
Norm of the params: 24.7356
     Influence (LOO): fixed 451 labels. Loss 0.57445. Accuracy 0.917.
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0881883
Train loss (w/o reg) on all data: 0.0466343
Test loss (w/o reg) on all data: 0.624862
Train acc on all data:  0.99879139473
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.04341e-05
Norm of the params: 28.8285
                Loss: fixed 403 labels. Loss 0.62486. Accuracy 0.879.
Using normal model
LBFGS training took [512] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210742
Train loss (w/o reg) on all data: 0.162003
Test loss (w/o reg) on all data: 0.659315
Train acc on all data:  0.951172347111
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 2.80638e-05
Norm of the params: 31.2212
              Random: fixed 160 labels. Loss 0.65932. Accuracy 0.839.
### Flips: 824, rs: 9, checks: 1030
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121671
Train loss (w/o reg) on all data: 0.0925525
Test loss (w/o reg) on all data: 0.607908
Train acc on all data:  0.972443799855
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 1.68816e-05
Norm of the params: 24.1324
     Influence (LOO): fixed 500 labels. Loss 0.60791. Accuracy 0.927.
Using normal model
LBFGS training took [415] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0812699
Train loss (w/o reg) on all data: 0.0422884
Test loss (w/o reg) on all data: 0.597069
Train acc on all data:  0.998549673677
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 2.33723e-05
Norm of the params: 27.9219
                Loss: fixed 441 labels. Loss 0.59707. Accuracy 0.889.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201093
Train loss (w/o reg) on all data: 0.153779
Test loss (w/o reg) on all data: 0.649461
Train acc on all data:  0.95600676819
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 6.18301e-05
Norm of the params: 30.7617
              Random: fixed 196 labels. Loss 0.64946. Accuracy 0.845.
### Flips: 824, rs: 9, checks: 1236
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112923
Train loss (w/o reg) on all data: 0.0861037
Test loss (w/o reg) on all data: 0.498308
Train acc on all data:  0.97461928934
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 4.4752e-06
Norm of the params: 23.16
     Influence (LOO): fixed 549 labels. Loss 0.49831. Accuracy 0.940.
Using normal model
LBFGS training took [415] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072262
Train loss (w/o reg) on all data: 0.0367078
Test loss (w/o reg) on all data: 0.442731
Train acc on all data:  0.999033115784
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 1.53601e-05
Norm of the params: 26.6661
                Loss: fixed 489 labels. Loss 0.44273. Accuracy 0.900.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19502
Train loss (w/o reg) on all data: 0.148438
Test loss (w/o reg) on all data: 0.600635
Train acc on all data:  0.957698815567
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.91952e-05
Norm of the params: 30.5226
              Random: fixed 232 labels. Loss 0.60064. Accuracy 0.843.
Using normal model
LBFGS training took [620] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246905
Train loss (w/o reg) on all data: 0.193921
Test loss (w/o reg) on all data: 0.70874
Train acc on all data:  0.937635968093
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 2.4453e-05
Norm of the params: 32.5525
Flipped loss: 0.70874. Accuracy: 0.794
### Flips: 824, rs: 10, checks: 206
Using normal model
LBFGS training took [434] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175549
Train loss (w/o reg) on all data: 0.133541
Test loss (w/o reg) on all data: 0.566818
Train acc on all data:  0.95914914189
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 1.1439e-05
Norm of the params: 28.9856
     Influence (LOO): fixed 172 labels. Loss 0.56682. Accuracy 0.850.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14609
Train loss (w/o reg) on all data: 0.0917843
Test loss (w/o reg) on all data: 0.648391
Train acc on all data:  0.983804689388
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 7.78173e-05
Norm of the params: 32.9564
                Loss: fixed 194 labels. Loss 0.64839. Accuracy 0.824.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238692
Train loss (w/o reg) on all data: 0.186587
Test loss (w/o reg) on all data: 0.621464
Train acc on all data:  0.940053178632
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 1.69543e-05
Norm of the params: 32.2816
              Random: fixed  41 labels. Loss 0.62146. Accuracy 0.806.
### Flips: 824, rs: 10, checks: 412
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155595
Train loss (w/o reg) on all data: 0.118498
Test loss (w/o reg) on all data: 0.393744
Train acc on all data:  0.964225284022
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 2.41145e-05
Norm of the params: 27.2386
     Influence (LOO): fixed 284 labels. Loss 0.39374. Accuracy 0.890.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111537
Train loss (w/o reg) on all data: 0.0630462
Test loss (w/o reg) on all data: 0.586286
Train acc on all data:  0.996615905245
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 7.33643e-06
Norm of the params: 31.1419
                Loss: fixed 295 labels. Loss 0.58629. Accuracy 0.849.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.233045
Train loss (w/o reg) on all data: 0.181592
Test loss (w/o reg) on all data: 0.602009
Train acc on all data:  0.942470389171
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 5.61921e-05
Norm of the params: 32.079
              Random: fixed  79 labels. Loss 0.60201. Accuracy 0.811.
### Flips: 824, rs: 10, checks: 618
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141752
Train loss (w/o reg) on all data: 0.108397
Test loss (w/o reg) on all data: 0.329101
Train acc on all data:  0.967609378777
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 7.24629e-06
Norm of the params: 25.8283
     Influence (LOO): fixed 371 labels. Loss 0.32910. Accuracy 0.906.
Using normal model
LBFGS training took [401] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0942281
Train loss (w/o reg) on all data: 0.0506716
Test loss (w/o reg) on all data: 0.478719
Train acc on all data:  0.998549673677
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 1.66336e-05
Norm of the params: 29.5149
                Loss: fixed 358 labels. Loss 0.47872. Accuracy 0.868.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225188
Train loss (w/o reg) on all data: 0.174764
Test loss (w/o reg) on all data: 0.565703
Train acc on all data:  0.94488759971
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 5.70681e-05
Norm of the params: 31.7569
              Random: fixed 116 labels. Loss 0.56570. Accuracy 0.824.
### Flips: 824, rs: 10, checks: 824
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129692
Train loss (w/o reg) on all data: 0.0993896
Test loss (w/o reg) on all data: 0.336984
Train acc on all data:  0.969784868262
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 6.01095e-06
Norm of the params: 24.6182
     Influence (LOO): fixed 450 labels. Loss 0.33698. Accuracy 0.919.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0855849
Train loss (w/o reg) on all data: 0.0448898
Test loss (w/o reg) on all data: 0.450341
Train acc on all data:  0.99879139473
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 8.99283e-06
Norm of the params: 28.529
                Loss: fixed 403 labels. Loss 0.45034. Accuracy 0.879.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216716
Train loss (w/o reg) on all data: 0.167711
Test loss (w/o reg) on all data: 0.51669
Train acc on all data:  0.947788252357
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 2.01205e-05
Norm of the params: 31.3068
              Random: fixed 166 labels. Loss 0.51669. Accuracy 0.842.
### Flips: 824, rs: 10, checks: 1030
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120912
Train loss (w/o reg) on all data: 0.0934799
Test loss (w/o reg) on all data: 0.282549
Train acc on all data:  0.970993473532
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 1.7327e-05
Norm of the params: 23.4231
     Influence (LOO): fixed 514 labels. Loss 0.28255. Accuracy 0.932.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0806738
Train loss (w/o reg) on all data: 0.0418362
Test loss (w/o reg) on all data: 0.428643
Train acc on all data:  0.999033115784
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 2.05189e-05
Norm of the params: 27.8703
                Loss: fixed 432 labels. Loss 0.42864. Accuracy 0.900.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205077
Train loss (w/o reg) on all data: 0.157197
Test loss (w/o reg) on all data: 0.495625
Train acc on all data:  0.952139231327
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 3.21411e-05
Norm of the params: 30.9452
              Random: fixed 211 labels. Loss 0.49563. Accuracy 0.845.
### Flips: 824, rs: 10, checks: 1236
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110307
Train loss (w/o reg) on all data: 0.0851071
Test loss (w/o reg) on all data: 0.199623
Train acc on all data:  0.974135847232
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 4.56784e-06
Norm of the params: 22.45
     Influence (LOO): fixed 573 labels. Loss 0.19962. Accuracy 0.936.
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0719952
Train loss (w/o reg) on all data: 0.0365749
Test loss (w/o reg) on all data: 0.370489
Train acc on all data:  0.999274836838
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 7.3275e-06
Norm of the params: 26.6159
                Loss: fixed 485 labels. Loss 0.37049. Accuracy 0.904.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196823
Train loss (w/o reg) on all data: 0.149487
Test loss (w/o reg) on all data: 0.494865
Train acc on all data:  0.955523326082
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 3.70582e-05
Norm of the params: 30.769
              Random: fixed 246 labels. Loss 0.49486. Accuracy 0.850.
Using normal model
LBFGS training took [644] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243304
Train loss (w/o reg) on all data: 0.191431
Test loss (w/o reg) on all data: 0.584087
Train acc on all data:  0.936427362823
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 6.06792e-05
Norm of the params: 32.2094
Flipped loss: 0.58409. Accuracy: 0.804
### Flips: 824, rs: 11, checks: 206
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177496
Train loss (w/o reg) on all data: 0.135772
Test loss (w/o reg) on all data: 0.496625
Train acc on all data:  0.957940536621
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 1.00703e-05
Norm of the params: 28.8874
     Influence (LOO): fixed 169 labels. Loss 0.49662. Accuracy 0.871.
Using normal model
LBFGS training took [577] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147882
Train loss (w/o reg) on all data: 0.0940766
Test loss (w/o reg) on all data: 0.553765
Train acc on all data:  0.982837805173
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 1.47287e-05
Norm of the params: 32.804
                Loss: fixed 187 labels. Loss 0.55377. Accuracy 0.841.
Using normal model
LBFGS training took [551] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237284
Train loss (w/o reg) on all data: 0.18688
Test loss (w/o reg) on all data: 0.54338
Train acc on all data:  0.937877689147
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 2.41878e-05
Norm of the params: 31.7503
              Random: fixed  46 labels. Loss 0.54338. Accuracy 0.817.
### Flips: 824, rs: 11, checks: 412
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156212
Train loss (w/o reg) on all data: 0.119813
Test loss (w/o reg) on all data: 0.381761
Train acc on all data:  0.962291515591
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 1.71212e-05
Norm of the params: 26.9811
     Influence (LOO): fixed 287 labels. Loss 0.38176. Accuracy 0.888.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113485
Train loss (w/o reg) on all data: 0.0647237
Test loss (w/o reg) on all data: 0.534339
Train acc on all data:  0.996615905245
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 3.95184e-05
Norm of the params: 31.2285
                Loss: fixed 289 labels. Loss 0.53434. Accuracy 0.851.
Using normal model
LBFGS training took [581] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230894
Train loss (w/o reg) on all data: 0.181295
Test loss (w/o reg) on all data: 0.526222
Train acc on all data:  0.942470389171
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 3.15549e-05
Norm of the params: 31.4955
              Random: fixed  86 labels. Loss 0.52622. Accuracy 0.826.
### Flips: 824, rs: 11, checks: 618
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141798
Train loss (w/o reg) on all data: 0.109328
Test loss (w/o reg) on all data: 0.298408
Train acc on all data:  0.965675610346
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.06263e-05
Norm of the params: 25.4832
     Influence (LOO): fixed 383 labels. Loss 0.29841. Accuracy 0.916.
Using normal model
LBFGS training took [408] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0981955
Train loss (w/o reg) on all data: 0.0533509
Test loss (w/o reg) on all data: 0.501769
Train acc on all data:  0.997341068407
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 2.52901e-05
Norm of the params: 29.9482
                Loss: fixed 350 labels. Loss 0.50177. Accuracy 0.867.
Using normal model
LBFGS training took [586] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224282
Train loss (w/o reg) on all data: 0.175451
Test loss (w/o reg) on all data: 0.532588
Train acc on all data:  0.942953831279
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 3.07327e-05
Norm of the params: 31.2509
              Random: fixed 122 labels. Loss 0.53259. Accuracy 0.823.
### Flips: 824, rs: 11, checks: 824
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130974
Train loss (w/o reg) on all data: 0.101122
Test loss (w/o reg) on all data: 0.260203
Train acc on all data:  0.968576262993
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 4.24223e-06
Norm of the params: 24.4344
     Influence (LOO): fixed 450 labels. Loss 0.26020. Accuracy 0.924.
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0856274
Train loss (w/o reg) on all data: 0.0446912
Test loss (w/o reg) on all data: 0.427992
Train acc on all data:  0.998307952623
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 2.98308e-05
Norm of the params: 28.6134
                Loss: fixed 403 labels. Loss 0.42799. Accuracy 0.889.
Using normal model
LBFGS training took [572] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214477
Train loss (w/o reg) on all data: 0.166452
Test loss (w/o reg) on all data: 0.522755
Train acc on all data:  0.949480299734
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 1.73387e-05
Norm of the params: 30.9919
              Random: fixed 174 labels. Loss 0.52276. Accuracy 0.842.
### Flips: 824, rs: 11, checks: 1030
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121676
Train loss (w/o reg) on all data: 0.0938788
Test loss (w/o reg) on all data: 0.214785
Train acc on all data:  0.970510031424
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 7.86846e-06
Norm of the params: 23.5783
     Influence (LOO): fixed 513 labels. Loss 0.21479. Accuracy 0.927.
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0784664
Train loss (w/o reg) on all data: 0.0401856
Test loss (w/o reg) on all data: 0.426287
Train acc on all data:  0.99879139473
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.78993e-05
Norm of the params: 27.6698
                Loss: fixed 441 labels. Loss 0.42629. Accuracy 0.905.
Using normal model
LBFGS training took [594] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209284
Train loss (w/o reg) on all data: 0.161803
Test loss (w/o reg) on all data: 0.502204
Train acc on all data:  0.951897510273
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 7.82098e-05
Norm of the params: 30.8162
              Random: fixed 201 labels. Loss 0.50220. Accuracy 0.844.
### Flips: 824, rs: 11, checks: 1236
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114043
Train loss (w/o reg) on all data: 0.0890281
Test loss (w/o reg) on all data: 0.199549
Train acc on all data:  0.970993473532
Test acc on all data:   0.944927536232
Norm of the mean of gradients: 9.12163e-06
Norm of the params: 22.3672
     Influence (LOO): fixed 567 labels. Loss 0.19955. Accuracy 0.945.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0701718
Train loss (w/o reg) on all data: 0.0352596
Test loss (w/o reg) on all data: 0.384225
Train acc on all data:  0.999033115784
Test acc on all data:   0.909178743961
Norm of the mean of gradients: 8.90878e-06
Norm of the params: 26.4243
                Loss: fixed 486 labels. Loss 0.38423. Accuracy 0.909.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198154
Train loss (w/o reg) on all data: 0.151147
Test loss (w/o reg) on all data: 0.500622
Train acc on all data:  0.956490210297
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 3.74108e-05
Norm of the params: 30.6616
              Random: fixed 245 labels. Loss 0.50062. Accuracy 0.845.
Using normal model
LBFGS training took [575] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237151
Train loss (w/o reg) on all data: 0.185012
Test loss (w/o reg) on all data: 0.668002
Train acc on all data:  0.943437273387
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 3.58453e-05
Norm of the params: 32.2921
Flipped loss: 0.66800. Accuracy: 0.824
### Flips: 824, rs: 12, checks: 206
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175551
Train loss (w/o reg) on all data: 0.132083
Test loss (w/o reg) on all data: 0.568084
Train acc on all data:  0.961566352429
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 2.09784e-05
Norm of the params: 29.4848
     Influence (LOO): fixed 162 labels. Loss 0.56808. Accuracy 0.863.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14662
Train loss (w/o reg) on all data: 0.0921648
Test loss (w/o reg) on all data: 0.652405
Train acc on all data:  0.985496736766
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 1.68632e-05
Norm of the params: 33.0016
                Loss: fixed 179 labels. Loss 0.65240. Accuracy 0.826.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231441
Train loss (w/o reg) on all data: 0.179933
Test loss (w/o reg) on all data: 0.674974
Train acc on all data:  0.945371041818
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 3.29791e-05
Norm of the params: 32.0961
              Random: fixed  36 labels. Loss 0.67497. Accuracy 0.830.
### Flips: 824, rs: 12, checks: 412
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153065
Train loss (w/o reg) on all data: 0.115503
Test loss (w/o reg) on all data: 0.470854
Train acc on all data:  0.966642494561
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 6.11976e-06
Norm of the params: 27.4086
     Influence (LOO): fixed 286 labels. Loss 0.47085. Accuracy 0.883.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113704
Train loss (w/o reg) on all data: 0.0645351
Test loss (w/o reg) on all data: 0.570322
Train acc on all data:  0.992506647329
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.09808e-05
Norm of the params: 31.3588
                Loss: fixed 285 labels. Loss 0.57032. Accuracy 0.840.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222114
Train loss (w/o reg) on all data: 0.172064
Test loss (w/o reg) on all data: 0.671099
Train acc on all data:  0.948996857626
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 0.000116187
Norm of the params: 31.6386
              Random: fixed  81 labels. Loss 0.67110. Accuracy 0.826.
### Flips: 824, rs: 12, checks: 618
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139681
Train loss (w/o reg) on all data: 0.106406
Test loss (w/o reg) on all data: 0.346145
Train acc on all data:  0.967851099831
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 4.2076e-06
Norm of the params: 25.797
     Influence (LOO): fixed 383 labels. Loss 0.34614. Accuracy 0.894.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0991802
Train loss (w/o reg) on all data: 0.0541966
Test loss (w/o reg) on all data: 0.514479
Train acc on all data:  0.998066231569
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 2.93801e-05
Norm of the params: 29.9945
                Loss: fixed 349 labels. Loss 0.51448. Accuracy 0.873.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216978
Train loss (w/o reg) on all data: 0.167791
Test loss (w/o reg) on all data: 0.704773
Train acc on all data:  0.950205462896
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 5.53409e-05
Norm of the params: 31.3647
              Random: fixed 128 labels. Loss 0.70477. Accuracy 0.838.
### Flips: 824, rs: 12, checks: 824
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129181
Train loss (w/o reg) on all data: 0.0988398
Test loss (w/o reg) on all data: 0.263059
Train acc on all data:  0.97026831037
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 1.27684e-05
Norm of the params: 24.6336
     Influence (LOO): fixed 455 labels. Loss 0.26306. Accuracy 0.917.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087012
Train loss (w/o reg) on all data: 0.0460402
Test loss (w/o reg) on all data: 0.501733
Train acc on all data:  0.999274836838
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 6.7506e-06
Norm of the params: 28.6258
                Loss: fixed 400 labels. Loss 0.50173. Accuracy 0.882.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210638
Train loss (w/o reg) on all data: 0.162201
Test loss (w/o reg) on all data: 0.662284
Train acc on all data:  0.952380952381
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 1.67123e-05
Norm of the params: 31.1245
              Random: fixed 167 labels. Loss 0.66228. Accuracy 0.847.
### Flips: 824, rs: 12, checks: 1030
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120696
Train loss (w/o reg) on all data: 0.092393
Test loss (w/o reg) on all data: 0.289396
Train acc on all data:  0.971960357747
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 4.36175e-06
Norm of the params: 23.7922
     Influence (LOO): fixed 509 labels. Loss 0.28940. Accuracy 0.930.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0800388
Train loss (w/o reg) on all data: 0.0416536
Test loss (w/o reg) on all data: 0.47126
Train acc on all data:  0.999516557892
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 7.94499e-06
Norm of the params: 27.7075
                Loss: fixed 441 labels. Loss 0.47126. Accuracy 0.891.
Using normal model
LBFGS training took [502] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203287
Train loss (w/o reg) on all data: 0.155824
Test loss (w/o reg) on all data: 0.639375
Train acc on all data:  0.953831278704
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 2.98929e-05
Norm of the params: 30.8102
              Random: fixed 205 labels. Loss 0.63937. Accuracy 0.855.
### Flips: 824, rs: 12, checks: 1236
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112906
Train loss (w/o reg) on all data: 0.0870085
Test loss (w/o reg) on all data: 0.237039
Train acc on all data:  0.974135847232
Test acc on all data:   0.943961352657
Norm of the mean of gradients: 4.01893e-06
Norm of the params: 22.7585
     Influence (LOO): fixed 548 labels. Loss 0.23704. Accuracy 0.944.
Using normal model
LBFGS training took [351] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073265
Train loss (w/o reg) on all data: 0.0374402
Test loss (w/o reg) on all data: 0.387226
Train acc on all data:  0.999274836838
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 9.29676e-06
Norm of the params: 26.7674
                Loss: fixed 487 labels. Loss 0.38723. Accuracy 0.907.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197545
Train loss (w/o reg) on all data: 0.150861
Test loss (w/o reg) on all data: 0.602166
Train acc on all data:  0.954556441866
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 1.73727e-05
Norm of the params: 30.5559
              Random: fixed 234 labels. Loss 0.60217. Accuracy 0.863.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245545
Train loss (w/o reg) on all data: 0.192165
Test loss (w/o reg) on all data: 0.665699
Train acc on all data:  0.939811457578
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 1.89226e-05
Norm of the params: 32.6741
Flipped loss: 0.66570. Accuracy: 0.786
### Flips: 824, rs: 13, checks: 206
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177579
Train loss (w/o reg) on all data: 0.134073
Test loss (w/o reg) on all data: 0.548729
Train acc on all data:  0.959632583998
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 2.09257e-05
Norm of the params: 29.4975
     Influence (LOO): fixed 169 labels. Loss 0.54873. Accuracy 0.844.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152957
Train loss (w/o reg) on all data: 0.097762
Test loss (w/o reg) on all data: 0.633454
Train acc on all data:  0.983079526227
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 1.31355e-05
Norm of the params: 33.2249
                Loss: fixed 182 labels. Loss 0.63345. Accuracy 0.802.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237106
Train loss (w/o reg) on all data: 0.184633
Test loss (w/o reg) on all data: 0.642338
Train acc on all data:  0.941503504955
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 2.22746e-05
Norm of the params: 32.3953
              Random: fixed  36 labels. Loss 0.64234. Accuracy 0.797.
### Flips: 824, rs: 13, checks: 412
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151508
Train loss (w/o reg) on all data: 0.114152
Test loss (w/o reg) on all data: 0.427093
Train acc on all data:  0.966159052453
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 2.08839e-05
Norm of the params: 27.3335
     Influence (LOO): fixed 296 labels. Loss 0.42709. Accuracy 0.884.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117249
Train loss (w/o reg) on all data: 0.066929
Test loss (w/o reg) on all data: 0.591548
Train acc on all data:  0.995407299976
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 1.59317e-05
Norm of the params: 31.7237
                Loss: fixed 296 labels. Loss 0.59155. Accuracy 0.844.
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228569
Train loss (w/o reg) on all data: 0.17732
Test loss (w/o reg) on all data: 0.638861
Train acc on all data:  0.945854483926
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 7.33205e-05
Norm of the params: 32.0152
              Random: fixed  78 labels. Loss 0.63886. Accuracy 0.803.
### Flips: 824, rs: 13, checks: 618
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138313
Train loss (w/o reg) on all data: 0.104636
Test loss (w/o reg) on all data: 0.358959
Train acc on all data:  0.968334541939
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 4.44284e-05
Norm of the params: 25.9523
     Influence (LOO): fixed 378 labels. Loss 0.35896. Accuracy 0.892.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0953956
Train loss (w/o reg) on all data: 0.0511251
Test loss (w/o reg) on all data: 0.588725
Train acc on all data:  0.998307952623
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 2.89358e-05
Norm of the params: 29.7558
                Loss: fixed 370 labels. Loss 0.58872. Accuracy 0.873.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221005
Train loss (w/o reg) on all data: 0.170335
Test loss (w/o reg) on all data: 0.60823
Train acc on all data:  0.948271694465
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 0.000101073
Norm of the params: 31.8341
              Random: fixed 113 labels. Loss 0.60823. Accuracy 0.811.
### Flips: 824, rs: 13, checks: 824
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127232
Train loss (w/o reg) on all data: 0.0968698
Test loss (w/o reg) on all data: 0.386962
Train acc on all data:  0.969301426154
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 1.01258e-05
Norm of the params: 24.6424
     Influence (LOO): fixed 458 labels. Loss 0.38696. Accuracy 0.919.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0849428
Train loss (w/o reg) on all data: 0.0443175
Test loss (w/o reg) on all data: 0.449204
Train acc on all data:  0.999033115784
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.87209e-05
Norm of the params: 28.5045
                Loss: fixed 414 labels. Loss 0.44920. Accuracy 0.882.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213019
Train loss (w/o reg) on all data: 0.163188
Test loss (w/o reg) on all data: 0.60425
Train acc on all data:  0.950930626058
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 2.24011e-05
Norm of the params: 31.5696
              Random: fixed 148 labels. Loss 0.60425. Accuracy 0.818.
### Flips: 824, rs: 13, checks: 1030
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120322
Train loss (w/o reg) on all data: 0.0916454
Test loss (w/o reg) on all data: 0.367407
Train acc on all data:  0.970751752478
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.93691e-05
Norm of the params: 23.9485
     Influence (LOO): fixed 502 labels. Loss 0.36741. Accuracy 0.924.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0773956
Train loss (w/o reg) on all data: 0.0398829
Test loss (w/o reg) on all data: 0.400689
Train acc on all data:  0.999274836838
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 2.17469e-05
Norm of the params: 27.3908
                Loss: fixed 462 labels. Loss 0.40069. Accuracy 0.896.
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207668
Train loss (w/o reg) on all data: 0.158629
Test loss (w/o reg) on all data: 0.550677
Train acc on all data:  0.95358955765
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.78092e-05
Norm of the params: 31.3173
              Random: fixed 181 labels. Loss 0.55068. Accuracy 0.824.
### Flips: 824, rs: 13, checks: 1236
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113364
Train loss (w/o reg) on all data: 0.0872292
Test loss (w/o reg) on all data: 0.381637
Train acc on all data:  0.972202078801
Test acc on all data:   0.941062801932
Norm of the mean of gradients: 4.45506e-06
Norm of the params: 22.8626
     Influence (LOO): fixed 549 labels. Loss 0.38164. Accuracy 0.941.
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0705483
Train loss (w/o reg) on all data: 0.0356523
Test loss (w/o reg) on all data: 0.367482
Train acc on all data:  0.99879139473
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 4.90584e-06
Norm of the params: 26.4181
                Loss: fixed 498 labels. Loss 0.36748. Accuracy 0.899.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200744
Train loss (w/o reg) on all data: 0.15282
Test loss (w/o reg) on all data: 0.531901
Train acc on all data:  0.956490210297
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 3.46482e-05
Norm of the params: 30.9592
              Random: fixed 228 labels. Loss 0.53190. Accuracy 0.830.
Using normal model
LBFGS training took [556] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247563
Train loss (w/o reg) on all data: 0.194341
Test loss (w/o reg) on all data: 0.732587
Train acc on all data:  0.938602852308
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 5.95502e-05
Norm of the params: 32.6258
Flipped loss: 0.73259. Accuracy: 0.810
### Flips: 824, rs: 14, checks: 206
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181478
Train loss (w/o reg) on all data: 0.138361
Test loss (w/o reg) on all data: 0.468447
Train acc on all data:  0.959632583998
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 2.87524e-05
Norm of the params: 29.3656
     Influence (LOO): fixed 169 labels. Loss 0.46845. Accuracy 0.866.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160314
Train loss (w/o reg) on all data: 0.104309
Test loss (w/o reg) on all data: 0.735057
Train acc on all data:  0.979453710418
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 1.84392e-05
Norm of the params: 33.4679
                Loss: fixed 175 labels. Loss 0.73506. Accuracy 0.813.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239042
Train loss (w/o reg) on all data: 0.186813
Test loss (w/o reg) on all data: 0.639843
Train acc on all data:  0.941261783901
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 8.05428e-05
Norm of the params: 32.3202
              Random: fixed  42 labels. Loss 0.63984. Accuracy 0.808.
### Flips: 824, rs: 14, checks: 412
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161303
Train loss (w/o reg) on all data: 0.122636
Test loss (w/o reg) on all data: 0.389149
Train acc on all data:  0.963258399807
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 1.00594e-05
Norm of the params: 27.8089
     Influence (LOO): fixed 283 labels. Loss 0.38915. Accuracy 0.885.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123681
Train loss (w/o reg) on all data: 0.072294
Test loss (w/o reg) on all data: 0.609659
Train acc on all data:  0.991056321006
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 3.65609e-05
Norm of the params: 32.0584
                Loss: fixed 288 labels. Loss 0.60966. Accuracy 0.830.
Using normal model
LBFGS training took [485] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231672
Train loss (w/o reg) on all data: 0.18014
Test loss (w/o reg) on all data: 0.592035
Train acc on all data:  0.94367899444
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 2.53427e-05
Norm of the params: 32.1035
              Random: fixed  82 labels. Loss 0.59204. Accuracy 0.813.
### Flips: 824, rs: 14, checks: 618
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149224
Train loss (w/o reg) on all data: 0.114559
Test loss (w/o reg) on all data: 0.41268
Train acc on all data:  0.96470872613
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 5.39612e-06
Norm of the params: 26.3304
     Influence (LOO): fixed 371 labels. Loss 0.41268. Accuracy 0.900.
Using normal model
LBFGS training took [439] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105691
Train loss (w/o reg) on all data: 0.0583481
Test loss (w/o reg) on all data: 0.552078
Train acc on all data:  0.997824510515
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.65441e-05
Norm of the params: 30.7711
                Loss: fixed 357 labels. Loss 0.55208. Accuracy 0.858.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226665
Train loss (w/o reg) on all data: 0.175595
Test loss (w/o reg) on all data: 0.599627
Train acc on all data:  0.944645878656
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 7.00669e-05
Norm of the params: 31.9592
              Random: fixed 113 labels. Loss 0.59963. Accuracy 0.814.
### Flips: 824, rs: 14, checks: 824
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138113
Train loss (w/o reg) on all data: 0.106746
Test loss (w/o reg) on all data: 0.425841
Train acc on all data:  0.966400773507
Test acc on all data:   0.92077294686
Norm of the mean of gradients: 5.71104e-06
Norm of the params: 25.047
     Influence (LOO): fixed 444 labels. Loss 0.42584. Accuracy 0.921.
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0939283
Train loss (w/o reg) on all data: 0.0504304
Test loss (w/o reg) on all data: 0.447693
Train acc on all data:  0.998549673677
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 2.42715e-05
Norm of the params: 29.4951
                Loss: fixed 404 labels. Loss 0.44769. Accuracy 0.869.
Using normal model
LBFGS training took [528] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217451
Train loss (w/o reg) on all data: 0.16788
Test loss (w/o reg) on all data: 0.574224
Train acc on all data:  0.946096204979
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 6.73294e-05
Norm of the params: 31.4867
              Random: fixed 161 labels. Loss 0.57422. Accuracy 0.836.
### Flips: 824, rs: 14, checks: 1030
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131908
Train loss (w/o reg) on all data: 0.102799
Test loss (w/o reg) on all data: 0.330793
Train acc on all data:  0.966642494561
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 6.69123e-06
Norm of the params: 24.1284
     Influence (LOO): fixed 493 labels. Loss 0.33079. Accuracy 0.937.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0865528
Train loss (w/o reg) on all data: 0.0457677
Test loss (w/o reg) on all data: 0.455706
Train acc on all data:  0.99879139473
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 1.69582e-05
Norm of the params: 28.5605
                Loss: fixed 452 labels. Loss 0.45571. Accuracy 0.884.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208187
Train loss (w/o reg) on all data: 0.159467
Test loss (w/o reg) on all data: 0.581397
Train acc on all data:  0.950688905004
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 1.79222e-05
Norm of the params: 31.2156
              Random: fixed 201 labels. Loss 0.58140. Accuracy 0.837.
### Flips: 824, rs: 14, checks: 1236
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122521
Train loss (w/o reg) on all data: 0.0957679
Test loss (w/o reg) on all data: 0.245116
Train acc on all data:  0.968334541939
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 6.12704e-06
Norm of the params: 23.1314
     Influence (LOO): fixed 548 labels. Loss 0.24512. Accuracy 0.946.
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0763353
Train loss (w/o reg) on all data: 0.0393638
Test loss (w/o reg) on all data: 0.445989
Train acc on all data:  0.99879139473
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 6.53919e-06
Norm of the params: 27.1924
                Loss: fixed 504 labels. Loss 0.44599. Accuracy 0.893.
Using normal model
LBFGS training took [471] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201702
Train loss (w/o reg) on all data: 0.153754
Test loss (w/o reg) on all data: 0.54866
Train acc on all data:  0.954072999758
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 4.80554e-05
Norm of the params: 30.9671
              Random: fixed 234 labels. Loss 0.54866. Accuracy 0.843.
Using normal model
LBFGS training took [661] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242407
Train loss (w/o reg) on all data: 0.190683
Test loss (w/o reg) on all data: 0.785335
Train acc on all data:  0.937394247039
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 9.62968e-05
Norm of the params: 32.1633
Flipped loss: 0.78534. Accuracy: 0.803
### Flips: 824, rs: 15, checks: 206
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173766
Train loss (w/o reg) on all data: 0.131028
Test loss (w/o reg) on all data: 0.706999
Train acc on all data:  0.960841189268
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 2.49052e-05
Norm of the params: 29.2364
     Influence (LOO): fixed 163 labels. Loss 0.70700. Accuracy 0.842.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142848
Train loss (w/o reg) on all data: 0.0902273
Test loss (w/o reg) on all data: 0.745092
Train acc on all data:  0.983804689388
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 2.09671e-05
Norm of the params: 32.4409
                Loss: fixed 186 labels. Loss 0.74509. Accuracy 0.810.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.236826
Train loss (w/o reg) on all data: 0.186013
Test loss (w/o reg) on all data: 0.721293
Train acc on all data:  0.938119410201
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 7.94998e-05
Norm of the params: 31.8789
              Random: fixed  42 labels. Loss 0.72129. Accuracy 0.811.
### Flips: 824, rs: 15, checks: 412
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152093
Train loss (w/o reg) on all data: 0.115257
Test loss (w/o reg) on all data: 0.544613
Train acc on all data:  0.964225284022
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 1.04032e-05
Norm of the params: 27.1428
     Influence (LOO): fixed 285 labels. Loss 0.54461. Accuracy 0.867.
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1119
Train loss (w/o reg) on all data: 0.0631981
Test loss (w/o reg) on all data: 0.780863
Train acc on all data:  0.99564902103
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.88544e-05
Norm of the params: 31.2095
                Loss: fixed 289 labels. Loss 0.78086. Accuracy 0.840.
Using normal model
LBFGS training took [609] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231321
Train loss (w/o reg) on all data: 0.181347
Test loss (w/o reg) on all data: 0.676884
Train acc on all data:  0.939569736524
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 2.4508e-05
Norm of the params: 31.6143
              Random: fixed  79 labels. Loss 0.67688. Accuracy 0.828.
### Flips: 824, rs: 15, checks: 618
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139539
Train loss (w/o reg) on all data: 0.106073
Test loss (w/o reg) on all data: 0.509014
Train acc on all data:  0.968576262993
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.31366e-05
Norm of the params: 25.8711
     Influence (LOO): fixed 367 labels. Loss 0.50901. Accuracy 0.894.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0972897
Train loss (w/o reg) on all data: 0.0527907
Test loss (w/o reg) on all data: 0.723593
Train acc on all data:  0.997099347353
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 3.03163e-05
Norm of the params: 29.8325
                Loss: fixed 355 labels. Loss 0.72359. Accuracy 0.858.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21985
Train loss (w/o reg) on all data: 0.170923
Test loss (w/o reg) on all data: 0.703186
Train acc on all data:  0.945612762872
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 3.53256e-05
Norm of the params: 31.2816
              Random: fixed 127 labels. Loss 0.70319. Accuracy 0.832.
### Flips: 824, rs: 15, checks: 824
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131083
Train loss (w/o reg) on all data: 0.100393
Test loss (w/o reg) on all data: 0.44605
Train acc on all data:  0.971718636693
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.342e-05
Norm of the params: 24.775
     Influence (LOO): fixed 445 labels. Loss 0.44605. Accuracy 0.905.
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0848269
Train loss (w/o reg) on all data: 0.0446185
Test loss (w/o reg) on all data: 0.584066
Train acc on all data:  0.998066231569
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 2.83886e-05
Norm of the params: 28.3579
                Loss: fixed 411 labels. Loss 0.58407. Accuracy 0.868.
Using normal model
LBFGS training took [548] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210058
Train loss (w/o reg) on all data: 0.162493
Test loss (w/o reg) on all data: 0.670794
Train acc on all data:  0.947546531303
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 0.000132444
Norm of the params: 30.8433
              Random: fixed 170 labels. Loss 0.67079. Accuracy 0.840.
### Flips: 824, rs: 15, checks: 1030
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122202
Train loss (w/o reg) on all data: 0.0938807
Test loss (w/o reg) on all data: 0.341372
Train acc on all data:  0.972443799855
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 1.06664e-05
Norm of the params: 23.7998
     Influence (LOO): fixed 505 labels. Loss 0.34137. Accuracy 0.923.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0786728
Train loss (w/o reg) on all data: 0.0407196
Test loss (w/o reg) on all data: 0.519506
Train acc on all data:  0.998307952623
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 1.28916e-05
Norm of the params: 27.5511
                Loss: fixed 451 labels. Loss 0.51951. Accuracy 0.888.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205403
Train loss (w/o reg) on all data: 0.158515
Test loss (w/o reg) on all data: 0.697039
Train acc on all data:  0.948513415518
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 2.82117e-05
Norm of the params: 30.6229
              Random: fixed 202 labels. Loss 0.69704. Accuracy 0.846.
### Flips: 824, rs: 15, checks: 1236
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112091
Train loss (w/o reg) on all data: 0.0857502
Test loss (w/o reg) on all data: 0.309937
Train acc on all data:  0.974861010394
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 2.41703e-05
Norm of the params: 22.9526
     Influence (LOO): fixed 552 labels. Loss 0.30994. Accuracy 0.927.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0733727
Train loss (w/o reg) on all data: 0.037347
Test loss (w/o reg) on all data: 0.406308
Train acc on all data:  0.99879139473
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 9.74673e-06
Norm of the params: 26.8424
                Loss: fixed 480 labels. Loss 0.40631. Accuracy 0.881.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196422
Train loss (w/o reg) on all data: 0.150695
Test loss (w/o reg) on all data: 0.701232
Train acc on all data:  0.951414068165
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 0.000105316
Norm of the params: 30.2411
              Random: fixed 247 labels. Loss 0.70123. Accuracy 0.845.
Using normal model
LBFGS training took [586] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241297
Train loss (w/o reg) on all data: 0.1881
Test loss (w/o reg) on all data: 0.991849
Train acc on all data:  0.937877689147
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 3.15023e-05
Norm of the params: 32.6182
Flipped loss: 0.99185. Accuracy: 0.778
### Flips: 824, rs: 16, checks: 206
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179313
Train loss (w/o reg) on all data: 0.135383
Test loss (w/o reg) on all data: 0.868552
Train acc on all data:  0.958423978729
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 1.15917e-05
Norm of the params: 29.641
     Influence (LOO): fixed 154 labels. Loss 0.86855. Accuracy 0.817.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156562
Train loss (w/o reg) on all data: 0.100171
Test loss (w/o reg) on all data: 0.939299
Train acc on all data:  0.981387478849
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 0.000103655
Norm of the params: 33.5831
                Loss: fixed 171 labels. Loss 0.93930. Accuracy 0.784.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235353
Train loss (w/o reg) on all data: 0.182844
Test loss (w/o reg) on all data: 0.942892
Train acc on all data:  0.941020062847
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 4.47245e-05
Norm of the params: 32.4065
              Random: fixed  45 labels. Loss 0.94289. Accuracy 0.794.
### Flips: 824, rs: 16, checks: 412
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155481
Train loss (w/o reg) on all data: 0.117943
Test loss (w/o reg) on all data: 0.73057
Train acc on all data:  0.96470872613
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 2.10506e-05
Norm of the params: 27.3998
     Influence (LOO): fixed 288 labels. Loss 0.73057. Accuracy 0.868.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124876
Train loss (w/o reg) on all data: 0.0727761
Test loss (w/o reg) on all data: 0.82763
Train acc on all data:  0.991298042059
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 1.21146e-05
Norm of the params: 32.2802
                Loss: fixed 268 labels. Loss 0.82763. Accuracy 0.812.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229205
Train loss (w/o reg) on all data: 0.177536
Test loss (w/o reg) on all data: 0.86385
Train acc on all data:  0.941503504955
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 4.24644e-05
Norm of the params: 32.1462
              Random: fixed  85 labels. Loss 0.86385. Accuracy 0.807.
### Flips: 824, rs: 16, checks: 618
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141327
Train loss (w/o reg) on all data: 0.107464
Test loss (w/o reg) on all data: 0.710272
Train acc on all data:  0.966642494561
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 2.01763e-05
Norm of the params: 26.0244
     Influence (LOO): fixed 386 labels. Loss 0.71027. Accuracy 0.884.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103096
Train loss (w/o reg) on all data: 0.0568312
Test loss (w/o reg) on all data: 0.677238
Train acc on all data:  0.997582789461
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 5.63031e-05
Norm of the params: 30.4187
                Loss: fixed 338 labels. Loss 0.67724. Accuracy 0.853.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221979
Train loss (w/o reg) on all data: 0.17148
Test loss (w/o reg) on all data: 0.890585
Train acc on all data:  0.944645878656
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 1.74172e-05
Norm of the params: 31.7802
              Random: fixed 125 labels. Loss 0.89059. Accuracy 0.818.
### Flips: 824, rs: 16, checks: 824
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131666
Train loss (w/o reg) on all data: 0.100178
Test loss (w/o reg) on all data: 0.550418
Train acc on all data:  0.97026831037
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 5.66015e-06
Norm of the params: 25.0948
     Influence (LOO): fixed 455 labels. Loss 0.55042. Accuracy 0.904.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0867024
Train loss (w/o reg) on all data: 0.0456056
Test loss (w/o reg) on all data: 0.588488
Train acc on all data:  0.99879139473
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.31918e-05
Norm of the params: 28.6694
                Loss: fixed 406 labels. Loss 0.58849. Accuracy 0.865.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215611
Train loss (w/o reg) on all data: 0.166507
Test loss (w/o reg) on all data: 0.888178
Train acc on all data:  0.947546531303
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 2.36282e-05
Norm of the params: 31.338
              Random: fixed 170 labels. Loss 0.88818. Accuracy 0.818.
### Flips: 824, rs: 16, checks: 1030
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123023
Train loss (w/o reg) on all data: 0.0939362
Test loss (w/o reg) on all data: 0.56994
Train acc on all data:  0.972443799855
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 1.1807e-05
Norm of the params: 24.119
     Influence (LOO): fixed 509 labels. Loss 0.56994. Accuracy 0.918.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0804761
Train loss (w/o reg) on all data: 0.0416746
Test loss (w/o reg) on all data: 0.551814
Train acc on all data:  0.999274836838
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 5.25553e-06
Norm of the params: 27.8573
                Loss: fixed 438 labels. Loss 0.55181. Accuracy 0.889.
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204721
Train loss (w/o reg) on all data: 0.156757
Test loss (w/o reg) on all data: 0.75155
Train acc on all data:  0.951897510273
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 1.56151e-05
Norm of the params: 30.972
              Random: fixed 220 labels. Loss 0.75155. Accuracy 0.839.
### Flips: 824, rs: 16, checks: 1236
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115719
Train loss (w/o reg) on all data: 0.0889631
Test loss (w/o reg) on all data: 0.501375
Train acc on all data:  0.973168963017
Test acc on all data:   0.933333333333
Norm of the mean of gradients: 8.23014e-06
Norm of the params: 23.1325
     Influence (LOO): fixed 561 labels. Loss 0.50137. Accuracy 0.933.
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0742957
Train loss (w/o reg) on all data: 0.0380094
Test loss (w/o reg) on all data: 0.525437
Train acc on all data:  0.999033115784
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 6.4802e-06
Norm of the params: 26.9393
                Loss: fixed 475 labels. Loss 0.52544. Accuracy 0.885.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194058
Train loss (w/o reg) on all data: 0.147049
Test loss (w/o reg) on all data: 0.748418
Train acc on all data:  0.955765047136
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 2.31598e-05
Norm of the params: 30.6624
              Random: fixed 264 labels. Loss 0.74842. Accuracy 0.844.
Using normal model
LBFGS training took [562] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244084
Train loss (w/o reg) on all data: 0.191295
Test loss (w/o reg) on all data: 0.667211
Train acc on all data:  0.93932801547
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 3.38679e-05
Norm of the params: 32.4926
Flipped loss: 0.66721. Accuracy: 0.777
### Flips: 824, rs: 17, checks: 206
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177698
Train loss (w/o reg) on all data: 0.13391
Test loss (w/o reg) on all data: 0.595274
Train acc on all data:  0.960841189268
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 1.34368e-05
Norm of the params: 29.5934
     Influence (LOO): fixed 170 labels. Loss 0.59527. Accuracy 0.834.
Using normal model
LBFGS training took [506] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157916
Train loss (w/o reg) on all data: 0.102006
Test loss (w/o reg) on all data: 0.626585
Train acc on all data:  0.981145757796
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 0.000114962
Norm of the params: 33.4394
                Loss: fixed 178 labels. Loss 0.62658. Accuracy 0.797.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235114
Train loss (w/o reg) on all data: 0.183009
Test loss (w/o reg) on all data: 0.64644
Train acc on all data:  0.94488759971
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 0.000109746
Norm of the params: 32.2817
              Random: fixed  42 labels. Loss 0.64644. Accuracy 0.783.
### Flips: 824, rs: 17, checks: 412
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155313
Train loss (w/o reg) on all data: 0.11705
Test loss (w/o reg) on all data: 0.505393
Train acc on all data:  0.966159052453
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 1.2001e-05
Norm of the params: 27.6634
     Influence (LOO): fixed 290 labels. Loss 0.50539. Accuracy 0.854.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12271
Train loss (w/o reg) on all data: 0.0711868
Test loss (w/o reg) on all data: 0.57157
Train acc on all data:  0.993715252599
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 1.26358e-05
Norm of the params: 32.1009
                Loss: fixed 280 labels. Loss 0.57157. Accuracy 0.832.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229166
Train loss (w/o reg) on all data: 0.178049
Test loss (w/o reg) on all data: 0.643057
Train acc on all data:  0.946821368141
Test acc on all data:   0.8
Norm of the mean of gradients: 2.35774e-05
Norm of the params: 31.9741
              Random: fixed  75 labels. Loss 0.64306. Accuracy 0.800.
### Flips: 824, rs: 17, checks: 618
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14607
Train loss (w/o reg) on all data: 0.110863
Test loss (w/o reg) on all data: 0.453374
Train acc on all data:  0.967367657723
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 2.02366e-05
Norm of the params: 26.5357
     Influence (LOO): fixed 367 labels. Loss 0.45337. Accuracy 0.872.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105332
Train loss (w/o reg) on all data: 0.0582794
Test loss (w/o reg) on all data: 0.515607
Train acc on all data:  0.996857626299
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 9.93353e-06
Norm of the params: 30.6767
                Loss: fixed 354 labels. Loss 0.51561. Accuracy 0.861.
Using normal model
LBFGS training took [404] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220575
Train loss (w/o reg) on all data: 0.170367
Test loss (w/o reg) on all data: 0.608361
Train acc on all data:  0.950205462896
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 2.0907e-05
Norm of the params: 31.6886
              Random: fixed 115 labels. Loss 0.60836. Accuracy 0.818.
### Flips: 824, rs: 17, checks: 824
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13604
Train loss (w/o reg) on all data: 0.10343
Test loss (w/o reg) on all data: 0.4007
Train acc on all data:  0.968576262993
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 4.65548e-06
Norm of the params: 25.5383
     Influence (LOO): fixed 438 labels. Loss 0.40070. Accuracy 0.891.
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0921484
Train loss (w/o reg) on all data: 0.0495183
Test loss (w/o reg) on all data: 0.500895
Train acc on all data:  0.998066231569
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 7.59943e-06
Norm of the params: 29.1993
                Loss: fixed 415 labels. Loss 0.50089. Accuracy 0.874.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214631
Train loss (w/o reg) on all data: 0.164976
Test loss (w/o reg) on all data: 0.610258
Train acc on all data:  0.953106115543
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 4.43164e-05
Norm of the params: 31.5136
              Random: fixed 159 labels. Loss 0.61026. Accuracy 0.826.
### Flips: 824, rs: 17, checks: 1030
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129266
Train loss (w/o reg) on all data: 0.098463
Test loss (w/o reg) on all data: 0.334833
Train acc on all data:  0.969301426154
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.93355e-05
Norm of the params: 24.8204
     Influence (LOO): fixed 487 labels. Loss 0.33483. Accuracy 0.900.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0845525
Train loss (w/o reg) on all data: 0.0445793
Test loss (w/o reg) on all data: 0.470136
Train acc on all data:  0.997824510515
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 1.43096e-05
Norm of the params: 28.2748
                Loss: fixed 449 labels. Loss 0.47014. Accuracy 0.890.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208812
Train loss (w/o reg) on all data: 0.159958
Test loss (w/o reg) on all data: 0.582915
Train acc on all data:  0.95479816292
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 5.3004e-05
Norm of the params: 31.2583
              Random: fixed 191 labels. Loss 0.58291. Accuracy 0.834.
### Flips: 824, rs: 17, checks: 1236
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122399
Train loss (w/o reg) on all data: 0.0939826
Test loss (w/o reg) on all data: 0.38638
Train acc on all data:  0.971476915639
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 3.43075e-06
Norm of the params: 23.8396
     Influence (LOO): fixed 522 labels. Loss 0.38638. Accuracy 0.919.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0807727
Train loss (w/o reg) on all data: 0.0421818
Test loss (w/o reg) on all data: 0.45532
Train acc on all data:  0.998549673677
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 1.75489e-05
Norm of the params: 27.7816
                Loss: fixed 478 labels. Loss 0.45532. Accuracy 0.900.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200359
Train loss (w/o reg) on all data: 0.152161
Test loss (w/o reg) on all data: 0.576146
Train acc on all data:  0.957940536621
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 5.12833e-05
Norm of the params: 31.0475
              Random: fixed 229 labels. Loss 0.57615. Accuracy 0.834.
Using normal model
LBFGS training took [766] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242617
Train loss (w/o reg) on all data: 0.189756
Test loss (w/o reg) on all data: 0.892621
Train acc on all data:  0.935943920715
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 5.51594e-05
Norm of the params: 32.5149
Flipped loss: 0.89262. Accuracy: 0.766
### Flips: 824, rs: 18, checks: 206
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175342
Train loss (w/o reg) on all data: 0.131958
Test loss (w/o reg) on all data: 0.649935
Train acc on all data:  0.961324631375
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 2.37668e-05
Norm of the params: 29.4562
     Influence (LOO): fixed 170 labels. Loss 0.64994. Accuracy 0.831.
Using normal model
LBFGS training took [564] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151654
Train loss (w/o reg) on all data: 0.0959321
Test loss (w/o reg) on all data: 0.865536
Train acc on all data:  0.982354363065
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 6.54117e-05
Norm of the params: 33.3831
                Loss: fixed 186 labels. Loss 0.86554. Accuracy 0.795.
Using normal model
LBFGS training took [582] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23503
Train loss (w/o reg) on all data: 0.18324
Test loss (w/o reg) on all data: 0.819199
Train acc on all data:  0.943195552333
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 9.38584e-05
Norm of the params: 32.1838
              Random: fixed  41 labels. Loss 0.81920. Accuracy 0.772.
### Flips: 824, rs: 18, checks: 412
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154075
Train loss (w/o reg) on all data: 0.115728
Test loss (w/o reg) on all data: 0.522488
Train acc on all data:  0.9659173314
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.65564e-05
Norm of the params: 27.6939
     Influence (LOO): fixed 282 labels. Loss 0.52249. Accuracy 0.864.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123369
Train loss (w/o reg) on all data: 0.0717167
Test loss (w/o reg) on all data: 0.834291
Train acc on all data:  0.991056321006
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 9.99524e-05
Norm of the params: 32.141
                Loss: fixed 274 labels. Loss 0.83429. Accuracy 0.823.
Using normal model
LBFGS training took [627] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225458
Train loss (w/o reg) on all data: 0.17465
Test loss (w/o reg) on all data: 0.801727
Train acc on all data:  0.945612762872
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 0.000128126
Norm of the params: 31.8775
              Random: fixed  78 labels. Loss 0.80173. Accuracy 0.777.
### Flips: 824, rs: 18, checks: 618
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14113
Train loss (w/o reg) on all data: 0.106753
Test loss (w/o reg) on all data: 0.445345
Train acc on all data:  0.967851099831
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 3.40405e-05
Norm of the params: 26.2209
     Influence (LOO): fixed 376 labels. Loss 0.44535. Accuracy 0.879.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102206
Train loss (w/o reg) on all data: 0.0556804
Test loss (w/o reg) on all data: 0.758214
Train acc on all data:  0.997824510515
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 1.12222e-05
Norm of the params: 30.5043
                Loss: fixed 362 labels. Loss 0.75821. Accuracy 0.857.
Using normal model
LBFGS training took [610] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217095
Train loss (w/o reg) on all data: 0.166767
Test loss (w/o reg) on all data: 0.827894
Train acc on all data:  0.949480299734
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 4.81445e-05
Norm of the params: 31.7264
              Random: fixed 122 labels. Loss 0.82789. Accuracy 0.796.
### Flips: 824, rs: 18, checks: 824
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131021
Train loss (w/o reg) on all data: 0.0998709
Test loss (w/o reg) on all data: 0.369088
Train acc on all data:  0.968092820885
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 2.23742e-05
Norm of the params: 24.9601
     Influence (LOO): fixed 451 labels. Loss 0.36909. Accuracy 0.892.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093032
Train loss (w/o reg) on all data: 0.049786
Test loss (w/o reg) on all data: 0.651008
Train acc on all data:  0.996857626299
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 2.8621e-05
Norm of the params: 29.4095
                Loss: fixed 408 labels. Loss 0.65101. Accuracy 0.864.
Using normal model
LBFGS training took [593] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208259
Train loss (w/o reg) on all data: 0.159135
Test loss (w/o reg) on all data: 0.777812
Train acc on all data:  0.952380952381
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 9.5585e-05
Norm of the params: 31.3445
              Random: fixed 157 labels. Loss 0.77781. Accuracy 0.799.
### Flips: 824, rs: 18, checks: 1030
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122727
Train loss (w/o reg) on all data: 0.0933708
Test loss (w/o reg) on all data: 0.356433
Train acc on all data:  0.969784868262
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.4683e-05
Norm of the params: 24.2305
     Influence (LOO): fixed 510 labels. Loss 0.35643. Accuracy 0.903.
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0843092
Train loss (w/o reg) on all data: 0.044507
Test loss (w/o reg) on all data: 0.629765
Train acc on all data:  0.998307952623
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 3.79425e-05
Norm of the params: 28.2142
                Loss: fixed 453 labels. Loss 0.62977. Accuracy 0.878.
Using normal model
LBFGS training took [614] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200273
Train loss (w/o reg) on all data: 0.151877
Test loss (w/o reg) on all data: 0.786821
Train acc on all data:  0.955281605028
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 6.71138e-05
Norm of the params: 31.1116
              Random: fixed 196 labels. Loss 0.78682. Accuracy 0.816.
### Flips: 824, rs: 18, checks: 1236
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115736
Train loss (w/o reg) on all data: 0.0888211
Test loss (w/o reg) on all data: 0.287904
Train acc on all data:  0.97026831037
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.4806e-05
Norm of the params: 23.2012
     Influence (LOO): fixed 555 labels. Loss 0.28790. Accuracy 0.924.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0754507
Train loss (w/o reg) on all data: 0.0390294
Test loss (w/o reg) on all data: 0.549547
Train acc on all data:  0.99879139473
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 3.28591e-05
Norm of the params: 26.9893
                Loss: fixed 492 labels. Loss 0.54955. Accuracy 0.894.
Using normal model
LBFGS training took [648] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191739
Train loss (w/o reg) on all data: 0.144417
Test loss (w/o reg) on all data: 0.725241
Train acc on all data:  0.956490210297
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 3.92539e-05
Norm of the params: 30.7643
              Random: fixed 234 labels. Loss 0.72524. Accuracy 0.832.
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248693
Train loss (w/o reg) on all data: 0.195348
Test loss (w/o reg) on all data: 0.69993
Train acc on all data:  0.936910804931
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 2.70327e-05
Norm of the params: 32.6634
Flipped loss: 0.69993. Accuracy: 0.808
### Flips: 824, rs: 19, checks: 206
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178931
Train loss (w/o reg) on all data: 0.134931
Test loss (w/o reg) on all data: 0.579891
Train acc on all data:  0.960599468214
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 1.28414e-05
Norm of the params: 29.6646
     Influence (LOO): fixed 171 labels. Loss 0.57989. Accuracy 0.853.
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153903
Train loss (w/o reg) on all data: 0.0977981
Test loss (w/o reg) on all data: 0.755316
Train acc on all data:  0.983079526227
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 9.17354e-06
Norm of the params: 33.4976
                Loss: fixed 190 labels. Loss 0.75532. Accuracy 0.829.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239921
Train loss (w/o reg) on all data: 0.1871
Test loss (w/o reg) on all data: 0.673625
Train acc on all data:  0.940053178632
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 1.85303e-05
Norm of the params: 32.5024
              Random: fixed  46 labels. Loss 0.67362. Accuracy 0.808.
### Flips: 824, rs: 19, checks: 412
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150873
Train loss (w/o reg) on all data: 0.114049
Test loss (w/o reg) on all data: 0.484014
Train acc on all data:  0.9659173314
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 4.67775e-06
Norm of the params: 27.1382
     Influence (LOO): fixed 302 labels. Loss 0.48401. Accuracy 0.885.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122456
Train loss (w/o reg) on all data: 0.0704094
Test loss (w/o reg) on all data: 0.670129
Train acc on all data:  0.995165578922
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 4.85956e-06
Norm of the params: 32.2633
                Loss: fixed 286 labels. Loss 0.67013. Accuracy 0.846.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234537
Train loss (w/o reg) on all data: 0.182678
Test loss (w/o reg) on all data: 0.65118
Train acc on all data:  0.944404157602
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 4.93189e-05
Norm of the params: 32.2052
              Random: fixed  86 labels. Loss 0.65118. Accuracy 0.809.
### Flips: 824, rs: 19, checks: 618
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138932
Train loss (w/o reg) on all data: 0.105776
Test loss (w/o reg) on all data: 0.406631
Train acc on all data:  0.967851099831
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 8.30061e-06
Norm of the params: 25.7511
     Influence (LOO): fixed 386 labels. Loss 0.40663. Accuracy 0.893.
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101459
Train loss (w/o reg) on all data: 0.0552951
Test loss (w/o reg) on all data: 0.558706
Train acc on all data:  0.997582789461
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 1.77579e-05
Norm of the params: 30.3856
                Loss: fixed 362 labels. Loss 0.55871. Accuracy 0.871.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228257
Train loss (w/o reg) on all data: 0.177628
Test loss (w/o reg) on all data: 0.6023
Train acc on all data:  0.946337926033
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 1.55785e-05
Norm of the params: 31.8212
              Random: fixed 124 labels. Loss 0.60230. Accuracy 0.814.
### Flips: 824, rs: 19, checks: 824
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129723
Train loss (w/o reg) on all data: 0.0989885
Test loss (w/o reg) on all data: 0.374605
Train acc on all data:  0.971235194585
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 1.43059e-05
Norm of the params: 24.793
     Influence (LOO): fixed 452 labels. Loss 0.37461. Accuracy 0.912.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0870589
Train loss (w/o reg) on all data: 0.0458803
Test loss (w/o reg) on all data: 0.472693
Train acc on all data:  0.998066231569
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 8.14696e-06
Norm of the params: 28.698
                Loss: fixed 414 labels. Loss 0.47269. Accuracy 0.892.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220961
Train loss (w/o reg) on all data: 0.171038
Test loss (w/o reg) on all data: 0.615753
Train acc on all data:  0.947304810249
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 4.20074e-05
Norm of the params: 31.5984
              Random: fixed 160 labels. Loss 0.61575. Accuracy 0.831.
### Flips: 824, rs: 19, checks: 1030
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121973
Train loss (w/o reg) on all data: 0.0934176
Test loss (w/o reg) on all data: 0.337913
Train acc on all data:  0.971718636693
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 4.85065e-06
Norm of the params: 23.8978
     Influence (LOO): fixed 511 labels. Loss 0.33791. Accuracy 0.920.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08097
Train loss (w/o reg) on all data: 0.0419364
Test loss (w/o reg) on all data: 0.522006
Train acc on all data:  0.99879139473
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 4.4764e-06
Norm of the params: 27.9405
                Loss: fixed 451 labels. Loss 0.52201. Accuracy 0.895.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209964
Train loss (w/o reg) on all data: 0.161529
Test loss (w/o reg) on all data: 0.56877
Train acc on all data:  0.951655789219
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.50279e-05
Norm of the params: 31.1236
              Random: fixed 212 labels. Loss 0.56877. Accuracy 0.843.
### Flips: 824, rs: 19, checks: 1236
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114559
Train loss (w/o reg) on all data: 0.0879577
Test loss (w/o reg) on all data: 0.373255
Train acc on all data:  0.973894126178
Test acc on all data:   0.927536231884
Norm of the mean of gradients: 1.85232e-05
Norm of the params: 23.0658
     Influence (LOO): fixed 556 labels. Loss 0.37326. Accuracy 0.928.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0730964
Train loss (w/o reg) on all data: 0.037492
Test loss (w/o reg) on all data: 0.533121
Train acc on all data:  0.999033115784
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 1.56668e-05
Norm of the params: 26.685
                Loss: fixed 494 labels. Loss 0.53312. Accuracy 0.900.
Using normal model
LBFGS training took [370] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201286
Train loss (w/o reg) on all data: 0.153997
Test loss (w/o reg) on all data: 0.533408
Train acc on all data:  0.956248489243
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 3.66662e-05
Norm of the params: 30.7535
              Random: fixed 249 labels. Loss 0.53341. Accuracy 0.853.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244333
Train loss (w/o reg) on all data: 0.192706
Test loss (w/o reg) on all data: 0.655245
Train acc on all data:  0.936185641769
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 1.99053e-05
Norm of the params: 32.1331
Flipped loss: 0.65525. Accuracy: 0.826
### Flips: 824, rs: 20, checks: 206
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177485
Train loss (w/o reg) on all data: 0.133838
Test loss (w/o reg) on all data: 0.581319
Train acc on all data:  0.959632583998
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 3.55975e-05
Norm of the params: 29.5456
     Influence (LOO): fixed 163 labels. Loss 0.58132. Accuracy 0.860.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144339
Train loss (w/o reg) on all data: 0.0904256
Test loss (w/o reg) on all data: 0.632534
Train acc on all data:  0.986947063089
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 5.82426e-05
Norm of the params: 32.8369
                Loss: fixed 194 labels. Loss 0.63253. Accuracy 0.839.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234615
Train loss (w/o reg) on all data: 0.18399
Test loss (w/o reg) on all data: 0.652796
Train acc on all data:  0.941745226009
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.83293e-05
Norm of the params: 31.8198
              Random: fixed  49 labels. Loss 0.65280. Accuracy 0.830.
### Flips: 824, rs: 20, checks: 412
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154507
Train loss (w/o reg) on all data: 0.117021
Test loss (w/o reg) on all data: 0.487631
Train acc on all data:  0.963983562968
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 1.05432e-05
Norm of the params: 27.381
     Influence (LOO): fixed 289 labels. Loss 0.48763. Accuracy 0.872.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112796
Train loss (w/o reg) on all data: 0.0629974
Test loss (w/o reg) on all data: 0.598679
Train acc on all data:  0.996857626299
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.16939e-05
Norm of the params: 31.5589
                Loss: fixed 282 labels. Loss 0.59868. Accuracy 0.851.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227075
Train loss (w/o reg) on all data: 0.177164
Test loss (w/o reg) on all data: 0.563991
Train acc on all data:  0.942470389171
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 1.21651e-05
Norm of the params: 31.5947
              Random: fixed  88 labels. Loss 0.56399. Accuracy 0.826.
### Flips: 824, rs: 20, checks: 618
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137852
Train loss (w/o reg) on all data: 0.10412
Test loss (w/o reg) on all data: 0.399901
Train acc on all data:  0.967609378777
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 5.59239e-06
Norm of the params: 25.9737
     Influence (LOO): fixed 386 labels. Loss 0.39990. Accuracy 0.900.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0974106
Train loss (w/o reg) on all data: 0.0517985
Test loss (w/o reg) on all data: 0.507043
Train acc on all data:  0.998066231569
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 1.63989e-05
Norm of the params: 30.2034
                Loss: fixed 344 labels. Loss 0.50704. Accuracy 0.866.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220544
Train loss (w/o reg) on all data: 0.171276
Test loss (w/o reg) on all data: 0.541021
Train acc on all data:  0.946337926033
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 8.52455e-05
Norm of the params: 31.3905
              Random: fixed 133 labels. Loss 0.54102. Accuracy 0.839.
### Flips: 824, rs: 20, checks: 824
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125639
Train loss (w/o reg) on all data: 0.095768
Test loss (w/o reg) on all data: 0.33025
Train acc on all data:  0.968817984046
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 1.42385e-05
Norm of the params: 24.442
     Influence (LOO): fixed 471 labels. Loss 0.33025. Accuracy 0.912.
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.085859
Train loss (w/o reg) on all data: 0.0445294
Test loss (w/o reg) on all data: 0.460254
Train acc on all data:  0.998549673677
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 9.77439e-06
Norm of the params: 28.7505
                Loss: fixed 396 labels. Loss 0.46025. Accuracy 0.879.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213007
Train loss (w/o reg) on all data: 0.164552
Test loss (w/o reg) on all data: 0.530504
Train acc on all data:  0.948271694465
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.98761e-05
Norm of the params: 31.1304
              Random: fixed 175 labels. Loss 0.53050. Accuracy 0.843.
### Flips: 824, rs: 20, checks: 1030
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118254
Train loss (w/o reg) on all data: 0.0905508
Test loss (w/o reg) on all data: 0.300214
Train acc on all data:  0.97026831037
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 1.89718e-05
Norm of the params: 23.5384
     Influence (LOO): fixed 532 labels. Loss 0.30021. Accuracy 0.923.
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0805421
Train loss (w/o reg) on all data: 0.0414291
Test loss (w/o reg) on all data: 0.427202
Train acc on all data:  0.999033115784
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 1.7071e-05
Norm of the params: 27.9689
                Loss: fixed 429 labels. Loss 0.42720. Accuracy 0.896.
Using normal model
LBFGS training took [502] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205213
Train loss (w/o reg) on all data: 0.157692
Test loss (w/o reg) on all data: 0.505438
Train acc on all data:  0.94923857868
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 1.33715e-05
Norm of the params: 30.8287
              Random: fixed 221 labels. Loss 0.50544. Accuracy 0.855.
### Flips: 824, rs: 20, checks: 1236
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112011
Train loss (w/o reg) on all data: 0.086587
Test loss (w/o reg) on all data: 0.246599
Train acc on all data:  0.971718636693
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 1.23683e-05
Norm of the params: 22.5494
     Influence (LOO): fixed 578 labels. Loss 0.24660. Accuracy 0.934.
Using normal model
LBFGS training took [348] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0758019
Train loss (w/o reg) on all data: 0.0385206
Test loss (w/o reg) on all data: 0.379907
Train acc on all data:  0.999033115784
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.36805e-05
Norm of the params: 27.3061
                Loss: fixed 459 labels. Loss 0.37991. Accuracy 0.897.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198336
Train loss (w/o reg) on all data: 0.151672
Test loss (w/o reg) on all data: 0.489073
Train acc on all data:  0.954314720812
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.85576e-05
Norm of the params: 30.5496
              Random: fixed 256 labels. Loss 0.48907. Accuracy 0.862.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241832
Train loss (w/o reg) on all data: 0.189377
Test loss (w/o reg) on all data: 0.8231
Train acc on all data:  0.936185641769
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 3.71393e-05
Norm of the params: 32.3899
Flipped loss: 0.82310. Accuracy: 0.809
### Flips: 824, rs: 21, checks: 206
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178331
Train loss (w/o reg) on all data: 0.135624
Test loss (w/o reg) on all data: 0.618437
Train acc on all data:  0.957215373459
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 6.50264e-05
Norm of the params: 29.2254
     Influence (LOO): fixed 158 labels. Loss 0.61844. Accuracy 0.859.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150974
Train loss (w/o reg) on all data: 0.0963104
Test loss (w/o reg) on all data: 0.708053
Train acc on all data:  0.982596084119
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 1.44691e-05
Norm of the params: 33.0646
                Loss: fixed 188 labels. Loss 0.70805. Accuracy 0.829.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235865
Train loss (w/o reg) on all data: 0.184144
Test loss (w/o reg) on all data: 0.822474
Train acc on all data:  0.937394247039
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 9.56592e-05
Norm of the params: 32.1625
              Random: fixed  31 labels. Loss 0.82247. Accuracy 0.811.
### Flips: 824, rs: 21, checks: 412
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154632
Train loss (w/o reg) on all data: 0.11817
Test loss (w/o reg) on all data: 0.454128
Train acc on all data:  0.961566352429
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 3.41913e-05
Norm of the params: 27.0042
     Influence (LOO): fixed 288 labels. Loss 0.45413. Accuracy 0.889.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116878
Train loss (w/o reg) on all data: 0.0666557
Test loss (w/o reg) on all data: 0.665654
Train acc on all data:  0.992506647329
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.41759e-05
Norm of the params: 31.6929
                Loss: fixed 299 labels. Loss 0.66565. Accuracy 0.840.
Using normal model
LBFGS training took [464] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230513
Train loss (w/o reg) on all data: 0.179306
Test loss (w/o reg) on all data: 0.809197
Train acc on all data:  0.941020062847
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 5.7337e-05
Norm of the params: 32.0023
              Random: fixed  69 labels. Loss 0.80920. Accuracy 0.818.
### Flips: 824, rs: 21, checks: 618
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137788
Train loss (w/o reg) on all data: 0.105878
Test loss (w/o reg) on all data: 0.410063
Train acc on all data:  0.96470872613
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 7.70846e-06
Norm of the params: 25.2628
     Influence (LOO): fixed 384 labels. Loss 0.41006. Accuracy 0.903.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100641
Train loss (w/o reg) on all data: 0.0548362
Test loss (w/o reg) on all data: 0.577223
Train acc on all data:  0.997341068407
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 1.19161e-05
Norm of the params: 30.267
                Loss: fixed 363 labels. Loss 0.57722. Accuracy 0.871.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22196
Train loss (w/o reg) on all data: 0.171901
Test loss (w/o reg) on all data: 0.82071
Train acc on all data:  0.946337926033
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 1.964e-05
Norm of the params: 31.6412
              Random: fixed 108 labels. Loss 0.82071. Accuracy 0.829.
### Flips: 824, rs: 21, checks: 824
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130493
Train loss (w/o reg) on all data: 0.100554
Test loss (w/o reg) on all data: 0.36625
Train acc on all data:  0.967609378777
Test acc on all data:   0.926570048309
Norm of the mean of gradients: 6.66677e-06
Norm of the params: 24.4697
     Influence (LOO): fixed 447 labels. Loss 0.36625. Accuracy 0.927.
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0884504
Train loss (w/o reg) on all data: 0.0469796
Test loss (w/o reg) on all data: 0.577426
Train acc on all data:  0.997341068407
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 2.4156e-05
Norm of the params: 28.7996
                Loss: fixed 413 labels. Loss 0.57743. Accuracy 0.883.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21362
Train loss (w/o reg) on all data: 0.16453
Test loss (w/o reg) on all data: 0.815388
Train acc on all data:  0.947788252357
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 2.7034e-05
Norm of the params: 31.3338
              Random: fixed 150 labels. Loss 0.81539. Accuracy 0.837.
### Flips: 824, rs: 21, checks: 1030
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122776
Train loss (w/o reg) on all data: 0.0950177
Test loss (w/o reg) on all data: 0.31514
Train acc on all data:  0.969543147208
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 1.3816e-05
Norm of the params: 23.5622
     Influence (LOO): fixed 502 labels. Loss 0.31514. Accuracy 0.939.
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0804218
Train loss (w/o reg) on all data: 0.0418826
Test loss (w/o reg) on all data: 0.551589
Train acc on all data:  0.998307952623
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 2.33315e-05
Norm of the params: 27.763
                Loss: fixed 453 labels. Loss 0.55159. Accuracy 0.886.
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206476
Train loss (w/o reg) on all data: 0.158631
Test loss (w/o reg) on all data: 0.787444
Train acc on all data:  0.948029973411
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 2.86372e-05
Norm of the params: 30.9336
              Random: fixed 187 labels. Loss 0.78744. Accuracy 0.841.
### Flips: 824, rs: 21, checks: 1236
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115512
Train loss (w/o reg) on all data: 0.0895306
Test loss (w/o reg) on all data: 0.258945
Train acc on all data:  0.971235194585
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 2.25887e-05
Norm of the params: 22.7955
     Influence (LOO): fixed 548 labels. Loss 0.25894. Accuracy 0.946.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0724853
Train loss (w/o reg) on all data: 0.0370156
Test loss (w/o reg) on all data: 0.562026
Train acc on all data:  0.99879139473
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 5.42092e-06
Norm of the params: 26.6345
                Loss: fixed 489 labels. Loss 0.56203. Accuracy 0.897.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196163
Train loss (w/o reg) on all data: 0.149256
Test loss (w/o reg) on all data: 0.72925
Train acc on all data:  0.955523326082
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 3.19894e-05
Norm of the params: 30.629
              Random: fixed 231 labels. Loss 0.72925. Accuracy 0.853.
Using normal model
LBFGS training took [588] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241992
Train loss (w/o reg) on all data: 0.189158
Test loss (w/o reg) on all data: 0.741809
Train acc on all data:  0.94053662074
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 7.78019e-05
Norm of the params: 32.5068
Flipped loss: 0.74181. Accuracy: 0.805
### Flips: 824, rs: 22, checks: 206
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17923
Train loss (w/o reg) on all data: 0.136092
Test loss (w/o reg) on all data: 0.490702
Train acc on all data:  0.962049794537
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 2.66424e-05
Norm of the params: 29.3727
     Influence (LOO): fixed 170 labels. Loss 0.49070. Accuracy 0.840.
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151434
Train loss (w/o reg) on all data: 0.0954566
Test loss (w/o reg) on all data: 0.729757
Train acc on all data:  0.981629199903
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 3.1901e-05
Norm of the params: 33.4595
                Loss: fixed 178 labels. Loss 0.72976. Accuracy 0.812.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235783
Train loss (w/o reg) on all data: 0.183952
Test loss (w/o reg) on all data: 0.681914
Train acc on all data:  0.943437273387
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 0.000118904
Norm of the params: 32.1967
              Random: fixed  42 labels. Loss 0.68191. Accuracy 0.804.
### Flips: 824, rs: 22, checks: 412
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155726
Train loss (w/o reg) on all data: 0.118204
Test loss (w/o reg) on all data: 0.430979
Train acc on all data:  0.966400773507
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 1.3852e-05
Norm of the params: 27.3943
     Influence (LOO): fixed 291 labels. Loss 0.43098. Accuracy 0.885.
Using normal model
LBFGS training took [491] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119376
Train loss (w/o reg) on all data: 0.0678025
Test loss (w/o reg) on all data: 0.568122
Train acc on all data:  0.992023205221
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 2.69553e-05
Norm of the params: 32.1165
                Loss: fixed 284 labels. Loss 0.56812. Accuracy 0.835.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229271
Train loss (w/o reg) on all data: 0.178368
Test loss (w/o reg) on all data: 0.67306
Train acc on all data:  0.945854483926
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 8.27921e-05
Norm of the params: 31.9072
              Random: fixed  78 labels. Loss 0.67306. Accuracy 0.809.
### Flips: 824, rs: 22, checks: 618
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139165
Train loss (w/o reg) on all data: 0.105579
Test loss (w/o reg) on all data: 0.336378
Train acc on all data:  0.970993473532
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.02574e-05
Norm of the params: 25.9173
     Influence (LOO): fixed 383 labels. Loss 0.33638. Accuracy 0.905.
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0961652
Train loss (w/o reg) on all data: 0.0514789
Test loss (w/o reg) on all data: 0.526076
Train acc on all data:  0.998307952623
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 2.11667e-05
Norm of the params: 29.8952
                Loss: fixed 365 labels. Loss 0.52608. Accuracy 0.865.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222286
Train loss (w/o reg) on all data: 0.172465
Test loss (w/o reg) on all data: 0.636932
Train acc on all data:  0.947788252357
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 0.000102889
Norm of the params: 31.566
              Random: fixed 128 labels. Loss 0.63693. Accuracy 0.823.
### Flips: 824, rs: 22, checks: 824
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127603
Train loss (w/o reg) on all data: 0.0964848
Test loss (w/o reg) on all data: 0.289957
Train acc on all data:  0.972685520909
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 7.17797e-06
Norm of the params: 24.9473
     Influence (LOO): fixed 456 labels. Loss 0.28996. Accuracy 0.916.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089249
Train loss (w/o reg) on all data: 0.0470071
Test loss (w/o reg) on all data: 0.487438
Train acc on all data:  0.999033115784
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 2.06401e-05
Norm of the params: 29.0661
                Loss: fixed 400 labels. Loss 0.48744. Accuracy 0.866.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213295
Train loss (w/o reg) on all data: 0.164729
Test loss (w/o reg) on all data: 0.592561
Train acc on all data:  0.950930626058
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 4.46259e-05
Norm of the params: 31.166
              Random: fixed 178 labels. Loss 0.59256. Accuracy 0.856.
### Flips: 824, rs: 22, checks: 1030
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117177
Train loss (w/o reg) on all data: 0.0894211
Test loss (w/o reg) on all data: 0.254019
Train acc on all data:  0.973168963017
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 3.30341e-06
Norm of the params: 23.5612
     Influence (LOO): fixed 521 labels. Loss 0.25402. Accuracy 0.929.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0811847
Train loss (w/o reg) on all data: 0.0417829
Test loss (w/o reg) on all data: 0.433419
Train acc on all data:  0.999033115784
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 2.61688e-05
Norm of the params: 28.072
                Loss: fixed 441 labels. Loss 0.43342. Accuracy 0.882.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20193
Train loss (w/o reg) on all data: 0.15443
Test loss (w/o reg) on all data: 0.537802
Train acc on all data:  0.952864394489
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 5.41722e-05
Norm of the params: 30.8219
              Random: fixed 230 labels. Loss 0.53780. Accuracy 0.861.
### Flips: 824, rs: 22, checks: 1236
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108929
Train loss (w/o reg) on all data: 0.0837522
Test loss (w/o reg) on all data: 0.247686
Train acc on all data:  0.97461928934
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 1.8432e-05
Norm of the params: 22.4395
     Influence (LOO): fixed 570 labels. Loss 0.24769. Accuracy 0.940.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0747062
Train loss (w/o reg) on all data: 0.0378245
Test loss (w/o reg) on all data: 0.527463
Train acc on all data:  0.999274836838
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 1.32114e-05
Norm of the params: 27.1594
                Loss: fixed 481 labels. Loss 0.52746. Accuracy 0.898.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193044
Train loss (w/o reg) on all data: 0.146595
Test loss (w/o reg) on all data: 0.499759
Train acc on all data:  0.957698815567
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 6.25265e-05
Norm of the params: 30.4792
              Random: fixed 272 labels. Loss 0.49976. Accuracy 0.869.
Using normal model
LBFGS training took [586] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24404
Train loss (w/o reg) on all data: 0.191508
Test loss (w/o reg) on all data: 0.938819
Train acc on all data:  0.937877689147
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 1.93683e-05
Norm of the params: 32.4136
Flipped loss: 0.93882. Accuracy: 0.793
### Flips: 824, rs: 23, checks: 206
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179874
Train loss (w/o reg) on all data: 0.136287
Test loss (w/o reg) on all data: 0.861978
Train acc on all data:  0.959874305052
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 1.96896e-05
Norm of the params: 29.5252
     Influence (LOO): fixed 160 labels. Loss 0.86198. Accuracy 0.843.
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150353
Train loss (w/o reg) on all data: 0.0953377
Test loss (w/o reg) on all data: 0.886778
Train acc on all data:  0.981629199903
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 8.32906e-05
Norm of the params: 33.1709
                Loss: fixed 190 labels. Loss 0.88678. Accuracy 0.814.
Using normal model
LBFGS training took [508] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239456
Train loss (w/o reg) on all data: 0.187197
Test loss (w/o reg) on all data: 0.935499
Train acc on all data:  0.940053178632
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 3.18078e-05
Norm of the params: 32.3293
              Random: fixed  36 labels. Loss 0.93550. Accuracy 0.794.
### Flips: 824, rs: 23, checks: 412
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158785
Train loss (w/o reg) on all data: 0.121222
Test loss (w/o reg) on all data: 0.667279
Train acc on all data:  0.963500120861
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 2.73369e-05
Norm of the params: 27.4092
     Influence (LOO): fixed 285 labels. Loss 0.66728. Accuracy 0.861.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119827
Train loss (w/o reg) on all data: 0.069955
Test loss (w/o reg) on all data: 0.785771
Train acc on all data:  0.993231810491
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 8.94733e-06
Norm of the params: 31.5821
                Loss: fixed 289 labels. Loss 0.78577. Accuracy 0.835.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232253
Train loss (w/o reg) on all data: 0.181329
Test loss (w/o reg) on all data: 0.959644
Train acc on all data:  0.941261783901
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 1.50651e-05
Norm of the params: 31.9136
              Random: fixed  77 labels. Loss 0.95964. Accuracy 0.805.
### Flips: 824, rs: 23, checks: 618
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145137
Train loss (w/o reg) on all data: 0.110795
Test loss (w/o reg) on all data: 0.615718
Train acc on all data:  0.966400773507
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 6.92141e-06
Norm of the params: 26.2075
     Influence (LOO): fixed 362 labels. Loss 0.61572. Accuracy 0.879.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098664
Train loss (w/o reg) on all data: 0.0545012
Test loss (w/o reg) on all data: 0.708539
Train acc on all data:  0.996857626299
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 7.85552e-05
Norm of the params: 29.7196
                Loss: fixed 357 labels. Loss 0.70854. Accuracy 0.877.
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225733
Train loss (w/o reg) on all data: 0.17594
Test loss (w/o reg) on all data: 0.923328
Train acc on all data:  0.942470389171
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 1.98181e-05
Norm of the params: 31.5573
              Random: fixed 118 labels. Loss 0.92333. Accuracy 0.818.
### Flips: 824, rs: 23, checks: 824
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13176
Train loss (w/o reg) on all data: 0.101334
Test loss (w/o reg) on all data: 0.447378
Train acc on all data:  0.968092820885
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.02172e-05
Norm of the params: 24.6679
     Influence (LOO): fixed 454 labels. Loss 0.44738. Accuracy 0.910.
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0881675
Train loss (w/o reg) on all data: 0.046806
Test loss (w/o reg) on all data: 0.668975
Train acc on all data:  0.997824510515
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 8.97147e-06
Norm of the params: 28.7616
                Loss: fixed 395 labels. Loss 0.66898. Accuracy 0.878.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216816
Train loss (w/o reg) on all data: 0.168337
Test loss (w/o reg) on all data: 0.884458
Train acc on all data:  0.948029973411
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 3.19839e-05
Norm of the params: 31.1382
              Random: fixed 160 labels. Loss 0.88446. Accuracy 0.821.
### Flips: 824, rs: 23, checks: 1030
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122338
Train loss (w/o reg) on all data: 0.0943953
Test loss (w/o reg) on all data: 0.539737
Train acc on all data:  0.969301426154
Test acc on all data:   0.91690821256
Norm of the mean of gradients: 2.65424e-06
Norm of the params: 23.6399
     Influence (LOO): fixed 508 labels. Loss 0.53974. Accuracy 0.917.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0797267
Train loss (w/o reg) on all data: 0.0417135
Test loss (w/o reg) on all data: 0.60901
Train acc on all data:  0.998307952623
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 9.55854e-06
Norm of the params: 27.5729
                Loss: fixed 435 labels. Loss 0.60901. Accuracy 0.893.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209078
Train loss (w/o reg) on all data: 0.161361
Test loss (w/o reg) on all data: 0.841309
Train acc on all data:  0.952139231327
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 5.76866e-05
Norm of the params: 30.8925
              Random: fixed 203 labels. Loss 0.84131. Accuracy 0.825.
### Flips: 824, rs: 23, checks: 1236
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113981
Train loss (w/o reg) on all data: 0.0877033
Test loss (w/o reg) on all data: 0.493491
Train acc on all data:  0.97026831037
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 2.18064e-06
Norm of the params: 22.925
     Influence (LOO): fixed 547 labels. Loss 0.49349. Accuracy 0.930.
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0716127
Train loss (w/o reg) on all data: 0.0368549
Test loss (w/o reg) on all data: 0.570548
Train acc on all data:  0.998549673677
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 5.05208e-06
Norm of the params: 26.3658
                Loss: fixed 483 labels. Loss 0.57055. Accuracy 0.907.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202481
Train loss (w/o reg) on all data: 0.155664
Test loss (w/o reg) on all data: 0.803362
Train acc on all data:  0.952380952381
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.78269e-05
Norm of the params: 30.5995
              Random: fixed 238 labels. Loss 0.80336. Accuracy 0.830.
Using normal model
LBFGS training took [510] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247726
Train loss (w/o reg) on all data: 0.195471
Test loss (w/o reg) on all data: 0.679881
Train acc on all data:  0.935943920715
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 0.000103959
Norm of the params: 32.328
Flipped loss: 0.67988. Accuracy: 0.819
### Flips: 824, rs: 24, checks: 206
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180412
Train loss (w/o reg) on all data: 0.137306
Test loss (w/o reg) on all data: 0.64782
Train acc on all data:  0.958423978729
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.65166e-05
Norm of the params: 29.3617
     Influence (LOO): fixed 167 labels. Loss 0.64782. Accuracy 0.851.
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151539
Train loss (w/o reg) on all data: 0.0963231
Test loss (w/o reg) on all data: 0.687934
Train acc on all data:  0.982596084119
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 4.46517e-05
Norm of the params: 33.2314
                Loss: fixed 194 labels. Loss 0.68793. Accuracy 0.813.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240056
Train loss (w/o reg) on all data: 0.188886
Test loss (w/o reg) on all data: 0.852046
Train acc on all data:  0.938602852308
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 2.88834e-05
Norm of the params: 31.9906
              Random: fixed  46 labels. Loss 0.85205. Accuracy 0.824.
### Flips: 824, rs: 24, checks: 412
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158496
Train loss (w/o reg) on all data: 0.121115
Test loss (w/o reg) on all data: 0.439009
Train acc on all data:  0.961566352429
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 1.27585e-05
Norm of the params: 27.3426
     Influence (LOO): fixed 283 labels. Loss 0.43901. Accuracy 0.878.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120479
Train loss (w/o reg) on all data: 0.0698257
Test loss (w/o reg) on all data: 0.701695
Train acc on all data:  0.991539763113
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 3.082e-05
Norm of the params: 31.8288
                Loss: fixed 290 labels. Loss 0.70169. Accuracy 0.845.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229857
Train loss (w/o reg) on all data: 0.179256
Test loss (w/o reg) on all data: 0.838738
Train acc on all data:  0.945129320764
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 2.24287e-05
Norm of the params: 31.8124
              Random: fixed  95 labels. Loss 0.83874. Accuracy 0.837.
### Flips: 824, rs: 24, checks: 618
Using normal model
LBFGS training took [330] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142141
Train loss (w/o reg) on all data: 0.10922
Test loss (w/o reg) on all data: 0.339447
Train acc on all data:  0.963983562968
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 2.10238e-05
Norm of the params: 25.6596
     Influence (LOO): fixed 382 labels. Loss 0.33945. Accuracy 0.907.
Using normal model
LBFGS training took [360] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100503
Train loss (w/o reg) on all data: 0.055095
Test loss (w/o reg) on all data: 0.580811
Train acc on all data:  0.997582789461
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 8.0723e-06
Norm of the params: 30.1355
                Loss: fixed 359 labels. Loss 0.58081. Accuracy 0.869.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220148
Train loss (w/o reg) on all data: 0.17097
Test loss (w/o reg) on all data: 0.863971
Train acc on all data:  0.949480299734
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.53865e-05
Norm of the params: 31.3616
              Random: fixed 137 labels. Loss 0.86397. Accuracy 0.840.
### Flips: 824, rs: 24, checks: 824
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13261
Train loss (w/o reg) on all data: 0.102332
Test loss (w/o reg) on all data: 0.423146
Train acc on all data:  0.966884215615
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.59874e-05
Norm of the params: 24.6084
     Influence (LOO): fixed 456 labels. Loss 0.42315. Accuracy 0.916.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0933721
Train loss (w/o reg) on all data: 0.050232
Test loss (w/o reg) on all data: 0.530008
Train acc on all data:  0.998066231569
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.28336e-05
Norm of the params: 29.3735
                Loss: fixed 394 labels. Loss 0.53001. Accuracy 0.877.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210919
Train loss (w/o reg) on all data: 0.162231
Test loss (w/o reg) on all data: 0.836485
Train acc on all data:  0.951414068165
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 2.72631e-05
Norm of the params: 31.2053
              Random: fixed 181 labels. Loss 0.83649. Accuracy 0.859.
### Flips: 824, rs: 24, checks: 1030
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121982
Train loss (w/o reg) on all data: 0.0939946
Test loss (w/o reg) on all data: 0.411285
Train acc on all data:  0.969784868262
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 1.93917e-05
Norm of the params: 23.659
     Influence (LOO): fixed 516 labels. Loss 0.41129. Accuracy 0.922.
Using normal model
LBFGS training took [360] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0874495
Train loss (w/o reg) on all data: 0.0464995
Test loss (w/o reg) on all data: 0.506074
Train acc on all data:  0.997582789461
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 2.20223e-05
Norm of the params: 28.6182
                Loss: fixed 436 labels. Loss 0.50607. Accuracy 0.890.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203958
Train loss (w/o reg) on all data: 0.156222
Test loss (w/o reg) on all data: 0.832139
Train acc on all data:  0.954556441866
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 3.15523e-05
Norm of the params: 30.8987
              Random: fixed 216 labels. Loss 0.83214. Accuracy 0.843.
### Flips: 824, rs: 24, checks: 1236
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113455
Train loss (w/o reg) on all data: 0.0875564
Test loss (w/o reg) on all data: 0.322149
Train acc on all data:  0.970751752478
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 1.86816e-05
Norm of the params: 22.7588
     Influence (LOO): fixed 561 labels. Loss 0.32215. Accuracy 0.929.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0769324
Train loss (w/o reg) on all data: 0.0397285
Test loss (w/o reg) on all data: 0.476143
Train acc on all data:  0.99879139473
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 2.64113e-05
Norm of the params: 27.2778
                Loss: fixed 484 labels. Loss 0.47614. Accuracy 0.903.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196797
Train loss (w/o reg) on all data: 0.149463
Test loss (w/o reg) on all data: 0.812837
Train acc on all data:  0.956731931351
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 4.67464e-05
Norm of the params: 30.768
              Random: fixed 251 labels. Loss 0.81284. Accuracy 0.852.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24637
Train loss (w/o reg) on all data: 0.195141
Test loss (w/o reg) on all data: 0.672862
Train acc on all data:  0.939811457578
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 8.44878e-05
Norm of the params: 32.0089
Flipped loss: 0.67286. Accuracy: 0.822
### Flips: 824, rs: 25, checks: 206
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181361
Train loss (w/o reg) on all data: 0.139282
Test loss (w/o reg) on all data: 0.550757
Train acc on all data:  0.957215373459
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 2.12769e-05
Norm of the params: 29.0102
     Influence (LOO): fixed 163 labels. Loss 0.55076. Accuracy 0.861.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149975
Train loss (w/o reg) on all data: 0.0963593
Test loss (w/o reg) on all data: 0.649832
Train acc on all data:  0.981629199903
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 2.86125e-05
Norm of the params: 32.7462
                Loss: fixed 190 labels. Loss 0.64983. Accuracy 0.825.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235473
Train loss (w/o reg) on all data: 0.185276
Test loss (w/o reg) on all data: 0.60898
Train acc on all data:  0.944162436548
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 2.70636e-05
Norm of the params: 31.6847
              Random: fixed  43 labels. Loss 0.60898. Accuracy 0.824.
### Flips: 824, rs: 25, checks: 412
Using normal model
LBFGS training took [348] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156916
Train loss (w/o reg) on all data: 0.12071
Test loss (w/o reg) on all data: 0.509477
Train acc on all data:  0.963016678753
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 1.07269e-05
Norm of the params: 26.9096
     Influence (LOO): fixed 300 labels. Loss 0.50948. Accuracy 0.896.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113853
Train loss (w/o reg) on all data: 0.0646834
Test loss (w/o reg) on all data: 0.647085
Train acc on all data:  0.995165578922
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 1.84827e-05
Norm of the params: 31.3591
                Loss: fixed 298 labels. Loss 0.64709. Accuracy 0.837.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226778
Train loss (w/o reg) on all data: 0.177391
Test loss (w/o reg) on all data: 0.517192
Train acc on all data:  0.948271694465
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 2.43844e-05
Norm of the params: 31.4285
              Random: fixed  92 labels. Loss 0.51719. Accuracy 0.846.
### Flips: 824, rs: 25, checks: 618
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144447
Train loss (w/o reg) on all data: 0.111642
Test loss (w/o reg) on all data: 0.446143
Train acc on all data:  0.963983562968
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 7.04403e-06
Norm of the params: 25.6147
     Influence (LOO): fixed 392 labels. Loss 0.44614. Accuracy 0.908.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0973954
Train loss (w/o reg) on all data: 0.0526095
Test loss (w/o reg) on all data: 0.553363
Train acc on all data:  0.998066231569
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.34592e-05
Norm of the params: 29.9286
                Loss: fixed 365 labels. Loss 0.55336. Accuracy 0.862.
Using normal model
LBFGS training took [490] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221579
Train loss (w/o reg) on all data: 0.173219
Test loss (w/o reg) on all data: 0.532521
Train acc on all data:  0.949722020788
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 1.53975e-05
Norm of the params: 31.0999
              Random: fixed 134 labels. Loss 0.53252. Accuracy 0.842.
### Flips: 824, rs: 25, checks: 824
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13496
Train loss (w/o reg) on all data: 0.105522
Test loss (w/o reg) on all data: 0.342168
Train acc on all data:  0.965433889292
Test acc on all data:   0.932367149758
Norm of the mean of gradients: 5.90122e-06
Norm of the params: 24.2643
     Influence (LOO): fixed 462 labels. Loss 0.34217. Accuracy 0.932.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0863203
Train loss (w/o reg) on all data: 0.0452711
Test loss (w/o reg) on all data: 0.503073
Train acc on all data:  0.999274836838
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.35951e-05
Norm of the params: 28.6528
                Loss: fixed 413 labels. Loss 0.50307. Accuracy 0.876.
Using normal model
LBFGS training took [462] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211993
Train loss (w/o reg) on all data: 0.164687
Test loss (w/o reg) on all data: 0.519549
Train acc on all data:  0.952139231327
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 0.00010589
Norm of the params: 30.759
              Random: fixed 179 labels. Loss 0.51955. Accuracy 0.853.
### Flips: 824, rs: 25, checks: 1030
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124468
Train loss (w/o reg) on all data: 0.0972898
Test loss (w/o reg) on all data: 0.222718
Train acc on all data:  0.968334541939
Test acc on all data:   0.935265700483
Norm of the mean of gradients: 6.78671e-06
Norm of the params: 23.3145
     Influence (LOO): fixed 517 labels. Loss 0.22272. Accuracy 0.935.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0785001
Train loss (w/o reg) on all data: 0.0404712
Test loss (w/o reg) on all data: 0.453992
Train acc on all data:  0.999033115784
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 2.48504e-05
Norm of the params: 27.5786
                Loss: fixed 451 labels. Loss 0.45399. Accuracy 0.880.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199664
Train loss (w/o reg) on all data: 0.153463
Test loss (w/o reg) on all data: 0.514305
Train acc on all data:  0.956490210297
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 3.74449e-05
Norm of the params: 30.3974
              Random: fixed 230 labels. Loss 0.51430. Accuracy 0.860.
### Flips: 824, rs: 25, checks: 1236
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117773
Train loss (w/o reg) on all data: 0.0926468
Test loss (w/o reg) on all data: 0.206519
Train acc on all data:  0.970510031424
Test acc on all data:   0.942028985507
Norm of the mean of gradients: 2.42921e-05
Norm of the params: 22.4172
     Influence (LOO): fixed 556 labels. Loss 0.20652. Accuracy 0.942.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0702996
Train loss (w/o reg) on all data: 0.0357435
Test loss (w/o reg) on all data: 0.416373
Train acc on all data:  0.999033115784
Test acc on all data:   0.910144927536
Norm of the mean of gradients: 1.39484e-05
Norm of the params: 26.2892
                Loss: fixed 502 labels. Loss 0.41637. Accuracy 0.910.
Using normal model
LBFGS training took [445] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194957
Train loss (w/o reg) on all data: 0.149695
Test loss (w/o reg) on all data: 0.505947
Train acc on all data:  0.957457094513
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 2.00079e-05
Norm of the params: 30.0874
              Random: fixed 263 labels. Loss 0.50595. Accuracy 0.875.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253251
Train loss (w/o reg) on all data: 0.200723
Test loss (w/o reg) on all data: 0.779022
Train acc on all data:  0.933043268069
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 2.72087e-05
Norm of the params: 32.4125
Flipped loss: 0.77902. Accuracy: 0.807
### Flips: 824, rs: 26, checks: 206
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180268
Train loss (w/o reg) on all data: 0.137703
Test loss (w/o reg) on all data: 0.579537
Train acc on all data:  0.957698815567
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 3.38645e-05
Norm of the params: 29.1772
     Influence (LOO): fixed 177 labels. Loss 0.57954. Accuracy 0.874.
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154388
Train loss (w/o reg) on all data: 0.0984969
Test loss (w/o reg) on all data: 0.801277
Train acc on all data:  0.982596084119
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.75772e-05
Norm of the params: 33.4339
                Loss: fixed 195 labels. Loss 0.80128. Accuracy 0.830.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245133
Train loss (w/o reg) on all data: 0.193917
Test loss (w/o reg) on all data: 0.622776
Train acc on all data:  0.933526710176
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 7.73811e-05
Norm of the params: 32.005
              Random: fixed  44 labels. Loss 0.62278. Accuracy 0.828.
### Flips: 824, rs: 26, checks: 412
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157494
Train loss (w/o reg) on all data: 0.120015
Test loss (w/o reg) on all data: 0.405246
Train acc on all data:  0.963983562968
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 1.16578e-05
Norm of the params: 27.3782
     Influence (LOO): fixed 301 labels. Loss 0.40525. Accuracy 0.892.
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117105
Train loss (w/o reg) on all data: 0.0668484
Test loss (w/o reg) on all data: 0.690211
Train acc on all data:  0.99564902103
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 1.40705e-05
Norm of the params: 31.704
                Loss: fixed 300 labels. Loss 0.69021. Accuracy 0.861.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24157
Train loss (w/o reg) on all data: 0.191142
Test loss (w/o reg) on all data: 0.639489
Train acc on all data:  0.936185641769
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 8.74543e-05
Norm of the params: 31.7578
              Random: fixed  82 labels. Loss 0.63949. Accuracy 0.830.
### Flips: 824, rs: 26, checks: 618
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143147
Train loss (w/o reg) on all data: 0.10954
Test loss (w/o reg) on all data: 0.33139
Train acc on all data:  0.9659173314
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 5.45078e-05
Norm of the params: 25.9258
     Influence (LOO): fixed 385 labels. Loss 0.33139. Accuracy 0.901.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0968135
Train loss (w/o reg) on all data: 0.0522278
Test loss (w/o reg) on all data: 0.576019
Train acc on all data:  0.997824510515
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 2.13046e-05
Norm of the params: 29.8616
                Loss: fixed 376 labels. Loss 0.57602. Accuracy 0.871.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235165
Train loss (w/o reg) on all data: 0.185647
Test loss (w/o reg) on all data: 0.662093
Train acc on all data:  0.939086294416
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 5.77462e-05
Norm of the params: 31.4699
              Random: fixed 112 labels. Loss 0.66209. Accuracy 0.846.
### Flips: 824, rs: 26, checks: 824
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131877
Train loss (w/o reg) on all data: 0.10124
Test loss (w/o reg) on all data: 0.291677
Train acc on all data:  0.968092820885
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 7.89281e-06
Norm of the params: 24.7535
     Influence (LOO): fixed 459 labels. Loss 0.29168. Accuracy 0.918.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0883305
Train loss (w/o reg) on all data: 0.0467866
Test loss (w/o reg) on all data: 0.568665
Train acc on all data:  0.99879139473
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 2.80195e-05
Norm of the params: 28.8249
                Loss: fixed 413 labels. Loss 0.56867. Accuracy 0.882.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226392
Train loss (w/o reg) on all data: 0.177295
Test loss (w/o reg) on all data: 0.629176
Train acc on all data:  0.943920715494
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 7.50142e-05
Norm of the params: 31.3361
              Random: fixed 156 labels. Loss 0.62918. Accuracy 0.839.
### Flips: 824, rs: 26, checks: 1030
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12405
Train loss (w/o reg) on all data: 0.0961669
Test loss (w/o reg) on all data: 0.248105
Train acc on all data:  0.969301426154
Test acc on all data:   0.934299516908
Norm of the mean of gradients: 6.38841e-06
Norm of the params: 23.6149
     Influence (LOO): fixed 516 labels. Loss 0.24810. Accuracy 0.934.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0799768
Train loss (w/o reg) on all data: 0.0412361
Test loss (w/o reg) on all data: 0.510652
Train acc on all data:  0.999274836838
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 7.5217e-06
Norm of the params: 27.8355
                Loss: fixed 449 labels. Loss 0.51065. Accuracy 0.892.
Using normal model
LBFGS training took [491] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.218446
Train loss (w/o reg) on all data: 0.170234
Test loss (w/o reg) on all data: 0.633253
Train acc on all data:  0.946337926033
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 1.72582e-05
Norm of the params: 31.052
              Random: fixed 198 labels. Loss 0.63325. Accuracy 0.854.
### Flips: 824, rs: 26, checks: 1236
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116204
Train loss (w/o reg) on all data: 0.0907816
Test loss (w/o reg) on all data: 0.21406
Train acc on all data:  0.970993473532
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 9.34928e-06
Norm of the params: 22.5489
     Influence (LOO): fixed 562 labels. Loss 0.21406. Accuracy 0.940.
Using normal model
LBFGS training took [370] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0743589
Train loss (w/o reg) on all data: 0.0377919
Test loss (w/o reg) on all data: 0.430643
Train acc on all data:  0.999274836838
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 2.07411e-05
Norm of the params: 27.0433
                Loss: fixed 486 labels. Loss 0.43064. Accuracy 0.885.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208825
Train loss (w/o reg) on all data: 0.161994
Test loss (w/o reg) on all data: 0.587463
Train acc on all data:  0.948513415518
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 4.1232e-05
Norm of the params: 30.6044
              Random: fixed 238 labels. Loss 0.58746. Accuracy 0.862.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238484
Train loss (w/o reg) on all data: 0.187169
Test loss (w/o reg) on all data: 0.570173
Train acc on all data:  0.940053178632
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 7.0654e-05
Norm of the params: 32.0358
Flipped loss: 0.57017. Accuracy: 0.832
### Flips: 824, rs: 27, checks: 206
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172575
Train loss (w/o reg) on all data: 0.130411
Test loss (w/o reg) on all data: 0.483433
Train acc on all data:  0.959390862944
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.15037e-05
Norm of the params: 29.0392
     Influence (LOO): fixed 168 labels. Loss 0.48343. Accuracy 0.865.
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139823
Train loss (w/o reg) on all data: 0.0869795
Test loss (w/o reg) on all data: 0.596808
Train acc on all data:  0.986947063089
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 3.08563e-05
Norm of the params: 32.5097
                Loss: fixed 193 labels. Loss 0.59681. Accuracy 0.834.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231025
Train loss (w/o reg) on all data: 0.180029
Test loss (w/o reg) on all data: 0.551019
Train acc on all data:  0.942228668117
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.37518e-05
Norm of the params: 31.936
              Random: fixed  40 labels. Loss 0.55102. Accuracy 0.840.
### Flips: 824, rs: 27, checks: 412
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151733
Train loss (w/o reg) on all data: 0.114931
Test loss (w/o reg) on all data: 0.34573
Train acc on all data:  0.964950447184
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 4.62576e-06
Norm of the params: 27.1303
     Influence (LOO): fixed 283 labels. Loss 0.34573. Accuracy 0.894.
Using normal model
LBFGS training took [408] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106271
Train loss (w/o reg) on all data: 0.0587412
Test loss (w/o reg) on all data: 0.493219
Train acc on all data:  0.998066231569
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 6.66807e-06
Norm of the params: 30.8318
                Loss: fixed 286 labels. Loss 0.49322. Accuracy 0.862.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225775
Train loss (w/o reg) on all data: 0.175409
Test loss (w/o reg) on all data: 0.547781
Train acc on all data:  0.942953831279
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 0.000132104
Norm of the params: 31.7384
              Random: fixed  79 labels. Loss 0.54778. Accuracy 0.840.
### Flips: 824, rs: 27, checks: 618
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13977
Train loss (w/o reg) on all data: 0.106817
Test loss (w/o reg) on all data: 0.294858
Train acc on all data:  0.966884215615
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 6.10878e-06
Norm of the params: 25.6721
     Influence (LOO): fixed 368 labels. Loss 0.29486. Accuracy 0.915.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0951984
Train loss (w/o reg) on all data: 0.0510144
Test loss (w/o reg) on all data: 0.452621
Train acc on all data:  0.997824510515
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 2.59358e-05
Norm of the params: 29.7268
                Loss: fixed 340 labels. Loss 0.45262. Accuracy 0.877.
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221169
Train loss (w/o reg) on all data: 0.17177
Test loss (w/o reg) on all data: 0.500078
Train acc on all data:  0.947063089195
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 5.07621e-05
Norm of the params: 31.4319
              Random: fixed 114 labels. Loss 0.50008. Accuracy 0.856.
### Flips: 824, rs: 27, checks: 824
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128985
Train loss (w/o reg) on all data: 0.099368
Test loss (w/o reg) on all data: 0.311044
Train acc on all data:  0.968334541939
Test acc on all data:   0.92270531401
Norm of the mean of gradients: 1.3065e-05
Norm of the params: 24.3379
     Influence (LOO): fixed 445 labels. Loss 0.31104. Accuracy 0.923.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0835028
Train loss (w/o reg) on all data: 0.0434352
Test loss (w/o reg) on all data: 0.409148
Train acc on all data:  0.999516557892
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 1.05488e-05
Norm of the params: 28.3082
                Loss: fixed 397 labels. Loss 0.40915. Accuracy 0.890.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210296
Train loss (w/o reg) on all data: 0.16153
Test loss (w/o reg) on all data: 0.461982
Train acc on all data:  0.953347836597
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 4.24405e-05
Norm of the params: 31.2304
              Random: fixed 167 labels. Loss 0.46198. Accuracy 0.857.
### Flips: 824, rs: 27, checks: 1030
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117485
Train loss (w/o reg) on all data: 0.0910057
Test loss (w/o reg) on all data: 0.216035
Train acc on all data:  0.97026831037
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 8.77394e-06
Norm of the params: 23.0129
     Influence (LOO): fixed 516 labels. Loss 0.21603. Accuracy 0.939.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0785893
Train loss (w/o reg) on all data: 0.0403192
Test loss (w/o reg) on all data: 0.38309
Train acc on all data:  0.999516557892
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 1.71753e-05
Norm of the params: 27.6659
                Loss: fixed 432 labels. Loss 0.38309. Accuracy 0.898.
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202701
Train loss (w/o reg) on all data: 0.15476
Test loss (w/o reg) on all data: 0.473959
Train acc on all data:  0.95600676819
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.0941e-05
Norm of the params: 30.9649
              Random: fixed 208 labels. Loss 0.47396. Accuracy 0.862.
### Flips: 824, rs: 27, checks: 1236
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110277
Train loss (w/o reg) on all data: 0.0856356
Test loss (w/o reg) on all data: 0.184948
Train acc on all data:  0.972202078801
Test acc on all data:   0.945893719807
Norm of the mean of gradients: 1.65076e-05
Norm of the params: 22.1999
     Influence (LOO): fixed 557 labels. Loss 0.18495. Accuracy 0.946.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0721801
Train loss (w/o reg) on all data: 0.0365588
Test loss (w/o reg) on all data: 0.3634
Train acc on all data:  0.99879139473
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 2.64165e-05
Norm of the params: 26.6913
                Loss: fixed 470 labels. Loss 0.36340. Accuracy 0.904.
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193953
Train loss (w/o reg) on all data: 0.147215
Test loss (w/o reg) on all data: 0.439279
Train acc on all data:  0.958423978729
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.25459e-05
Norm of the params: 30.5739
              Random: fixed 244 labels. Loss 0.43928. Accuracy 0.862.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244867
Train loss (w/o reg) on all data: 0.193095
Test loss (w/o reg) on all data: 0.978787
Train acc on all data:  0.937152525985
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 2.56407e-05
Norm of the params: 32.1782
Flipped loss: 0.97879. Accuracy: 0.802
### Flips: 824, rs: 28, checks: 206
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177509
Train loss (w/o reg) on all data: 0.135682
Test loss (w/o reg) on all data: 0.837072
Train acc on all data:  0.953831278704
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 1.03027e-05
Norm of the params: 28.9229
     Influence (LOO): fixed 173 labels. Loss 0.83707. Accuracy 0.845.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156166
Train loss (w/o reg) on all data: 0.101782
Test loss (w/o reg) on all data: 0.860174
Train acc on all data:  0.978728547256
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 3.19549e-05
Norm of the params: 32.9799
                Loss: fixed 184 labels. Loss 0.86017. Accuracy 0.817.
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239126
Train loss (w/o reg) on all data: 0.188728
Test loss (w/o reg) on all data: 0.947281
Train acc on all data:  0.939569736524
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 2.06133e-05
Norm of the params: 31.7484
              Random: fixed  35 labels. Loss 0.94728. Accuracy 0.818.
### Flips: 824, rs: 28, checks: 412
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15787
Train loss (w/o reg) on all data: 0.121116
Test loss (w/o reg) on all data: 0.626798
Train acc on all data:  0.960599468214
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 2.22289e-05
Norm of the params: 27.1124
     Influence (LOO): fixed 282 labels. Loss 0.62680. Accuracy 0.874.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117418
Train loss (w/o reg) on all data: 0.0679565
Test loss (w/o reg) on all data: 0.73815
Train acc on all data:  0.99564902103
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.46217e-05
Norm of the params: 31.4521
                Loss: fixed 292 labels. Loss 0.73815. Accuracy 0.843.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232709
Train loss (w/o reg) on all data: 0.183581
Test loss (w/o reg) on all data: 0.87086
Train acc on all data:  0.938361131255
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 1.87331e-05
Norm of the params: 31.3458
              Random: fixed  76 labels. Loss 0.87086. Accuracy 0.821.
### Flips: 824, rs: 28, checks: 618
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145437
Train loss (w/o reg) on all data: 0.112015
Test loss (w/o reg) on all data: 0.495738
Train acc on all data:  0.9659173314
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 1.82739e-05
Norm of the params: 25.8541
     Influence (LOO): fixed 371 labels. Loss 0.49574. Accuracy 0.892.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100707
Train loss (w/o reg) on all data: 0.055576
Test loss (w/o reg) on all data: 0.737027
Train acc on all data:  0.997341068407
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.69198e-05
Norm of the params: 30.0438
                Loss: fixed 357 labels. Loss 0.73703. Accuracy 0.860.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227072
Train loss (w/o reg) on all data: 0.178665
Test loss (w/o reg) on all data: 0.812386
Train acc on all data:  0.939811457578
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 0.000103939
Norm of the params: 31.1149
              Random: fixed 115 labels. Loss 0.81239. Accuracy 0.823.
### Flips: 824, rs: 28, checks: 824
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135685
Train loss (w/o reg) on all data: 0.104735
Test loss (w/o reg) on all data: 0.511814
Train acc on all data:  0.967125936669
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 5.28195e-06
Norm of the params: 24.8796
     Influence (LOO): fixed 443 labels. Loss 0.51181. Accuracy 0.908.
Using normal model
LBFGS training took [360] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0894406
Train loss (w/o reg) on all data: 0.047713
Test loss (w/o reg) on all data: 0.759655
Train acc on all data:  0.998066231569
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 7.60796e-06
Norm of the params: 28.8886
                Loss: fixed 401 labels. Loss 0.75966. Accuracy 0.864.
Using normal model
LBFGS training took [506] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221058
Train loss (w/o reg) on all data: 0.17379
Test loss (w/o reg) on all data: 0.724039
Train acc on all data:  0.941986947063
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 1.57002e-05
Norm of the params: 30.7466
              Random: fixed 157 labels. Loss 0.72404. Accuracy 0.843.
### Flips: 824, rs: 28, checks: 1030
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128345
Train loss (w/o reg) on all data: 0.0998921
Test loss (w/o reg) on all data: 0.450343
Train acc on all data:  0.967125936669
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 2.80577e-05
Norm of the params: 23.8548
     Influence (LOO): fixed 497 labels. Loss 0.45034. Accuracy 0.920.
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0782928
Train loss (w/o reg) on all data: 0.0409858
Test loss (w/o reg) on all data: 0.682027
Train acc on all data:  0.997824510515
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 2.87017e-05
Norm of the params: 27.3156
                Loss: fixed 452 labels. Loss 0.68203. Accuracy 0.878.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209825
Train loss (w/o reg) on all data: 0.163078
Test loss (w/o reg) on all data: 0.683679
Train acc on all data:  0.946821368141
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 1.42839e-05
Norm of the params: 30.5767
              Random: fixed 207 labels. Loss 0.68368. Accuracy 0.853.
### Flips: 824, rs: 28, checks: 1236
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119741
Train loss (w/o reg) on all data: 0.0933293
Test loss (w/o reg) on all data: 0.48589
Train acc on all data:  0.9690597051
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.827e-05
Norm of the params: 22.9833
     Influence (LOO): fixed 544 labels. Loss 0.48589. Accuracy 0.930.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0711377
Train loss (w/o reg) on all data: 0.0363751
Test loss (w/o reg) on all data: 0.774305
Train acc on all data:  0.999033115784
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 1.61023e-05
Norm of the params: 26.3676
                Loss: fixed 496 labels. Loss 0.77431. Accuracy 0.886.
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197837
Train loss (w/o reg) on all data: 0.152158
Test loss (w/o reg) on all data: 0.665367
Train acc on all data:  0.951172347111
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 2.87708e-05
Norm of the params: 30.2255
              Random: fixed 259 labels. Loss 0.66537. Accuracy 0.858.
Using normal model
LBFGS training took [594] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243209
Train loss (w/o reg) on all data: 0.190979
Test loss (w/o reg) on all data: 0.634211
Train acc on all data:  0.939086294416
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 2.1161e-05
Norm of the params: 32.3205
Flipped loss: 0.63421. Accuracy: 0.806
### Flips: 824, rs: 29, checks: 206
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173378
Train loss (w/o reg) on all data: 0.131231
Test loss (w/o reg) on all data: 0.514683
Train acc on all data:  0.962049794537
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 2.32454e-05
Norm of the params: 29.0334
     Influence (LOO): fixed 167 labels. Loss 0.51468. Accuracy 0.843.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153129
Train loss (w/o reg) on all data: 0.0981505
Test loss (w/o reg) on all data: 0.62002
Train acc on all data:  0.982112642011
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 6.21387e-05
Norm of the params: 33.1596
                Loss: fixed 172 labels. Loss 0.62002. Accuracy 0.819.
Using normal model
LBFGS training took [574] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.236289
Train loss (w/o reg) on all data: 0.1849
Test loss (w/o reg) on all data: 0.609398
Train acc on all data:  0.940053178632
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 2.65653e-05
Norm of the params: 32.0592
              Random: fixed  39 labels. Loss 0.60940. Accuracy 0.814.
### Flips: 824, rs: 29, checks: 412
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148597
Train loss (w/o reg) on all data: 0.112401
Test loss (w/o reg) on all data: 0.40019
Train acc on all data:  0.966642494561
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 7.46639e-06
Norm of the params: 26.9058
     Influence (LOO): fixed 291 labels. Loss 0.40019. Accuracy 0.872.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116083
Train loss (w/o reg) on all data: 0.0664258
Test loss (w/o reg) on all data: 0.584491
Train acc on all data:  0.992990089437
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 2.23432e-05
Norm of the params: 31.5141
                Loss: fixed 279 labels. Loss 0.58449. Accuracy 0.841.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230616
Train loss (w/o reg) on all data: 0.179951
Test loss (w/o reg) on all data: 0.581967
Train acc on all data:  0.94367899444
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 4.74578e-05
Norm of the params: 31.8325
              Random: fixed  76 labels. Loss 0.58197. Accuracy 0.823.
### Flips: 824, rs: 29, checks: 618
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133202
Train loss (w/o reg) on all data: 0.101131
Test loss (w/o reg) on all data: 0.293532
Train acc on all data:  0.971476915639
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 1.38273e-05
Norm of the params: 25.326
     Influence (LOO): fixed 386 labels. Loss 0.29353. Accuracy 0.904.
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0971429
Train loss (w/o reg) on all data: 0.0528325
Test loss (w/o reg) on all data: 0.524978
Train acc on all data:  0.997824510515
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 4.42824e-05
Norm of the params: 29.7693
                Loss: fixed 353 labels. Loss 0.52498. Accuracy 0.869.
Using normal model
LBFGS training took [606] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223585
Train loss (w/o reg) on all data: 0.173654
Test loss (w/o reg) on all data: 0.574616
Train acc on all data:  0.946579647087
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 0.000105822
Norm of the params: 31.6008
              Random: fixed 118 labels. Loss 0.57462. Accuracy 0.822.
### Flips: 824, rs: 29, checks: 824
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123892
Train loss (w/o reg) on all data: 0.0942903
Test loss (w/o reg) on all data: 0.259482
Train acc on all data:  0.972202078801
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 2.50824e-05
Norm of the params: 24.3318
     Influence (LOO): fixed 450 labels. Loss 0.25948. Accuracy 0.914.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0860092
Train loss (w/o reg) on all data: 0.0453425
Test loss (w/o reg) on all data: 0.503289
Train acc on all data:  0.99879139473
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 7.46597e-06
Norm of the params: 28.519
                Loss: fixed 414 labels. Loss 0.50329. Accuracy 0.872.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216708
Train loss (w/o reg) on all data: 0.167552
Test loss (w/o reg) on all data: 0.548033
Train acc on all data:  0.949963741842
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 4.10834e-05
Norm of the params: 31.3549
              Random: fixed 154 labels. Loss 0.54803. Accuracy 0.829.
### Flips: 824, rs: 29, checks: 1030
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114314
Train loss (w/o reg) on all data: 0.0869976
Test loss (w/o reg) on all data: 0.229774
Train acc on all data:  0.97461928934
Test acc on all data:   0.931400966184
Norm of the mean of gradients: 1.12521e-05
Norm of the params: 23.3737
     Influence (LOO): fixed 506 labels. Loss 0.22977. Accuracy 0.931.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0789993
Train loss (w/o reg) on all data: 0.0410551
Test loss (w/o reg) on all data: 0.428722
Train acc on all data:  0.99879139473
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 5.33702e-06
Norm of the params: 27.5478
                Loss: fixed 456 labels. Loss 0.42872. Accuracy 0.885.
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206874
Train loss (w/o reg) on all data: 0.159062
Test loss (w/o reg) on all data: 0.478383
Train acc on all data:  0.951655789219
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 1.39916e-05
Norm of the params: 30.9229
              Random: fixed 198 labels. Loss 0.47838. Accuracy 0.842.
### Flips: 824, rs: 29, checks: 1236
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107531
Train loss (w/o reg) on all data: 0.0819688
Test loss (w/o reg) on all data: 0.224173
Train acc on all data:  0.97582789461
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 3.75648e-06
Norm of the params: 22.6109
     Influence (LOO): fixed 551 labels. Loss 0.22417. Accuracy 0.930.
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0736992
Train loss (w/o reg) on all data: 0.0377996
Test loss (w/o reg) on all data: 0.391503
Train acc on all data:  0.999033115784
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 1.61715e-05
Norm of the params: 26.7954
                Loss: fixed 498 labels. Loss 0.39150. Accuracy 0.898.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198357
Train loss (w/o reg) on all data: 0.152486
Test loss (w/o reg) on all data: 0.458883
Train acc on all data:  0.954556441866
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 3.64909e-05
Norm of the params: 30.2889
              Random: fixed 245 labels. Loss 0.45888. Accuracy 0.855.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243245
Train loss (w/o reg) on all data: 0.18981
Test loss (w/o reg) on all data: 0.765597
Train acc on all data:  0.940778341794
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 8.40186e-05
Norm of the params: 32.691
Flipped loss: 0.76560. Accuracy: 0.785
### Flips: 824, rs: 30, checks: 206
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178873
Train loss (w/o reg) on all data: 0.135288
Test loss (w/o reg) on all data: 0.658895
Train acc on all data:  0.958182257675
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 4.19706e-05
Norm of the params: 29.5245
     Influence (LOO): fixed 161 labels. Loss 0.65890. Accuracy 0.847.
Using normal model
LBFGS training took [448] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154226
Train loss (w/o reg) on all data: 0.0983416
Test loss (w/o reg) on all data: 0.778908
Train acc on all data:  0.983079526227
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 1.75537e-05
Norm of the params: 33.4319
                Loss: fixed 176 labels. Loss 0.77891. Accuracy 0.797.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23669
Train loss (w/o reg) on all data: 0.184264
Test loss (w/o reg) on all data: 0.71302
Train acc on all data:  0.941745226009
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 3.48427e-05
Norm of the params: 32.3808
              Random: fixed  38 labels. Loss 0.71302. Accuracy 0.799.
### Flips: 824, rs: 30, checks: 412
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158647
Train loss (w/o reg) on all data: 0.120063
Test loss (w/o reg) on all data: 0.614347
Train acc on all data:  0.962291515591
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 1.69054e-05
Norm of the params: 27.7793
     Influence (LOO): fixed 274 labels. Loss 0.61435. Accuracy 0.872.
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123289
Train loss (w/o reg) on all data: 0.0718411
Test loss (w/o reg) on all data: 0.593806
Train acc on all data:  0.990814599952
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 4.92271e-05
Norm of the params: 32.0773
                Loss: fixed 284 labels. Loss 0.59381. Accuracy 0.824.
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230447
Train loss (w/o reg) on all data: 0.178584
Test loss (w/o reg) on all data: 0.71528
Train acc on all data:  0.943920715494
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 4.26303e-05
Norm of the params: 32.2063
              Random: fixed  70 labels. Loss 0.71528. Accuracy 0.803.
### Flips: 824, rs: 30, checks: 618
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14288
Train loss (w/o reg) on all data: 0.107672
Test loss (w/o reg) on all data: 0.605749
Train acc on all data:  0.966642494561
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 1.16287e-05
Norm of the params: 26.5363
     Influence (LOO): fixed 371 labels. Loss 0.60575. Accuracy 0.893.
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102091
Train loss (w/o reg) on all data: 0.0555928
Test loss (w/o reg) on all data: 0.513579
Train acc on all data:  0.994923857868
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 1.88757e-05
Norm of the params: 30.4953
                Loss: fixed 350 labels. Loss 0.51358. Accuracy 0.855.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221298
Train loss (w/o reg) on all data: 0.17019
Test loss (w/o reg) on all data: 0.672263
Train acc on all data:  0.948029973411
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 1.67668e-05
Norm of the params: 31.9711
              Random: fixed 115 labels. Loss 0.67226. Accuracy 0.817.
### Flips: 824, rs: 30, checks: 824
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132441
Train loss (w/o reg) on all data: 0.0999574
Test loss (w/o reg) on all data: 0.416175
Train acc on all data:  0.968334541939
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 7.69555e-06
Norm of the params: 25.4886
     Influence (LOO): fixed 449 labels. Loss 0.41618. Accuracy 0.907.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0890721
Train loss (w/o reg) on all data: 0.0469634
Test loss (w/o reg) on all data: 0.456055
Train acc on all data:  0.998549673677
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 4.9605e-06
Norm of the params: 29.0202
                Loss: fixed 413 labels. Loss 0.45605. Accuracy 0.885.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21135
Train loss (w/o reg) on all data: 0.161623
Test loss (w/o reg) on all data: 0.611943
Train acc on all data:  0.951172347111
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 4.5716e-05
Norm of the params: 31.5362
              Random: fixed 158 labels. Loss 0.61194. Accuracy 0.820.
### Flips: 824, rs: 30, checks: 1030
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120586
Train loss (w/o reg) on all data: 0.091058
Test loss (w/o reg) on all data: 0.376377
Train acc on all data:  0.971476915639
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.77687e-05
Norm of the params: 24.3013
     Influence (LOO): fixed 513 labels. Loss 0.37638. Accuracy 0.913.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080428
Train loss (w/o reg) on all data: 0.0416843
Test loss (w/o reg) on all data: 0.5151
Train acc on all data:  0.999033115784
Test acc on all data:   0.880193236715
Norm of the mean of gradients: 2.1618e-05
Norm of the params: 27.8365
                Loss: fixed 464 labels. Loss 0.51510. Accuracy 0.880.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200215
Train loss (w/o reg) on all data: 0.152026
Test loss (w/o reg) on all data: 0.61778
Train acc on all data:  0.953831278704
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 3.01456e-05
Norm of the params: 31.045
              Random: fixed 198 labels. Loss 0.61778. Accuracy 0.828.
### Flips: 824, rs: 30, checks: 1236
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110702
Train loss (w/o reg) on all data: 0.0836602
Test loss (w/o reg) on all data: 0.306969
Train acc on all data:  0.972443799855
Test acc on all data:   0.937198067633
Norm of the mean of gradients: 6.2498e-06
Norm of the params: 23.2561
     Influence (LOO): fixed 569 labels. Loss 0.30697. Accuracy 0.937.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0737612
Train loss (w/o reg) on all data: 0.0375541
Test loss (w/o reg) on all data: 0.501163
Train acc on all data:  0.999033115784
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 9.21531e-06
Norm of the params: 26.9099
                Loss: fixed 495 labels. Loss 0.50116. Accuracy 0.894.
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193971
Train loss (w/o reg) on all data: 0.147428
Test loss (w/o reg) on all data: 0.55052
Train acc on all data:  0.954072999758
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 2.41193e-05
Norm of the params: 30.5101
              Random: fixed 243 labels. Loss 0.55052. Accuracy 0.838.
Using normal model
LBFGS training took [626] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241827
Train loss (w/o reg) on all data: 0.18806
Test loss (w/o reg) on all data: 0.730648
Train acc on all data:  0.941020062847
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 2.8821e-05
Norm of the params: 32.7924
Flipped loss: 0.73065. Accuracy: 0.786
### Flips: 824, rs: 31, checks: 206
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176019
Train loss (w/o reg) on all data: 0.132041
Test loss (w/o reg) on all data: 0.658387
Train acc on all data:  0.960841189268
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 1.28033e-05
Norm of the params: 29.6576
     Influence (LOO): fixed 165 labels. Loss 0.65839. Accuracy 0.823.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154098
Train loss (w/o reg) on all data: 0.0982522
Test loss (w/o reg) on all data: 0.665706
Train acc on all data:  0.982354363065
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 2.97437e-05
Norm of the params: 33.4202
                Loss: fixed 172 labels. Loss 0.66571. Accuracy 0.809.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235857
Train loss (w/o reg) on all data: 0.183079
Test loss (w/o reg) on all data: 0.678399
Train acc on all data:  0.942228668117
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 7.07685e-05
Norm of the params: 32.4893
              Random: fixed  42 labels. Loss 0.67840. Accuracy 0.798.
### Flips: 824, rs: 31, checks: 412
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149557
Train loss (w/o reg) on all data: 0.111712
Test loss (w/o reg) on all data: 0.468862
Train acc on all data:  0.968092820885
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 4.07826e-05
Norm of the params: 27.5119
     Influence (LOO): fixed 290 labels. Loss 0.46886. Accuracy 0.851.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120844
Train loss (w/o reg) on all data: 0.0693232
Test loss (w/o reg) on all data: 0.650598
Train acc on all data:  0.992023205221
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 4.96763e-05
Norm of the params: 32.1002
                Loss: fixed 277 labels. Loss 0.65060. Accuracy 0.821.
Using normal model
LBFGS training took [585] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225905
Train loss (w/o reg) on all data: 0.173916
Test loss (w/o reg) on all data: 0.652412
Train acc on all data:  0.946579647087
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 5.81439e-05
Norm of the params: 32.2454
              Random: fixed  89 labels. Loss 0.65241. Accuracy 0.814.
### Flips: 824, rs: 31, checks: 618
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135365
Train loss (w/o reg) on all data: 0.101652
Test loss (w/o reg) on all data: 0.414192
Train acc on all data:  0.970026589316
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 9.24518e-06
Norm of the params: 25.9665
     Influence (LOO): fixed 384 labels. Loss 0.41419. Accuracy 0.865.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984187
Train loss (w/o reg) on all data: 0.0531987
Test loss (w/o reg) on all data: 0.611991
Train acc on all data:  0.997582789461
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 1.31782e-05
Norm of the params: 30.0733
                Loss: fixed 361 labels. Loss 0.61199. Accuracy 0.842.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21754
Train loss (w/o reg) on all data: 0.166324
Test loss (w/o reg) on all data: 0.610514
Train acc on all data:  0.950930626058
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 4.54472e-05
Norm of the params: 32.005
              Random: fixed 128 labels. Loss 0.61051. Accuracy 0.816.
### Flips: 824, rs: 31, checks: 824
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124134
Train loss (w/o reg) on all data: 0.0930971
Test loss (w/o reg) on all data: 0.39482
Train acc on all data:  0.972927241963
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 1.15973e-05
Norm of the params: 24.9147
     Influence (LOO): fixed 457 labels. Loss 0.39482. Accuracy 0.885.
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0877697
Train loss (w/o reg) on all data: 0.0459836
Test loss (w/o reg) on all data: 0.610892
Train acc on all data:  0.998307952623
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 2.39583e-05
Norm of the params: 28.9088
                Loss: fixed 406 labels. Loss 0.61089. Accuracy 0.850.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209722
Train loss (w/o reg) on all data: 0.159953
Test loss (w/o reg) on all data: 0.54686
Train acc on all data:  0.953347836597
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 0.000143464
Norm of the params: 31.5497
              Random: fixed 176 labels. Loss 0.54686. Accuracy 0.828.
### Flips: 824, rs: 31, checks: 1030
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118458
Train loss (w/o reg) on all data: 0.0892724
Test loss (w/o reg) on all data: 0.333741
Train acc on all data:  0.973410684071
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 4.38855e-06
Norm of the params: 24.1601
     Influence (LOO): fixed 508 labels. Loss 0.33374. Accuracy 0.905.
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0778563
Train loss (w/o reg) on all data: 0.0400143
Test loss (w/o reg) on all data: 0.563796
Train acc on all data:  0.998307952623
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.64153e-05
Norm of the params: 27.5107
                Loss: fixed 462 labels. Loss 0.56380. Accuracy 0.864.
Using normal model
LBFGS training took [529] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202018
Train loss (w/o reg) on all data: 0.152965
Test loss (w/o reg) on all data: 0.524638
Train acc on all data:  0.95600676819
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 4.9347e-05
Norm of the params: 31.3217
              Random: fixed 219 labels. Loss 0.52464. Accuracy 0.841.
### Flips: 824, rs: 31, checks: 1236
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110595
Train loss (w/o reg) on all data: 0.0832467
Test loss (w/o reg) on all data: 0.275234
Train acc on all data:  0.974377568286
Test acc on all data:   0.91884057971
Norm of the mean of gradients: 3.67793e-05
Norm of the params: 23.3872
     Influence (LOO): fixed 558 labels. Loss 0.27523. Accuracy 0.919.
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0714254
Train loss (w/o reg) on all data: 0.036189
Test loss (w/o reg) on all data: 0.517217
Train acc on all data:  0.998549673677
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 7.78622e-06
Norm of the params: 26.5467
                Loss: fixed 497 labels. Loss 0.51722. Accuracy 0.871.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194949
Train loss (w/o reg) on all data: 0.146529
Test loss (w/o reg) on all data: 0.492416
Train acc on all data:  0.958907420836
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 4.76893e-05
Norm of the params: 31.1192
              Random: fixed 257 labels. Loss 0.49242. Accuracy 0.843.
Using normal model
LBFGS training took [506] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253318
Train loss (w/o reg) on all data: 0.198719
Test loss (w/o reg) on all data: 0.776423
Train acc on all data:  0.935943920715
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 0.00011555
Norm of the params: 33.0451
Flipped loss: 0.77642. Accuracy: 0.793
### Flips: 824, rs: 32, checks: 206
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184651
Train loss (w/o reg) on all data: 0.139462
Test loss (w/o reg) on all data: 0.82926
Train acc on all data:  0.957457094513
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 3.34729e-05
Norm of the params: 30.063
     Influence (LOO): fixed 168 labels. Loss 0.82926. Accuracy 0.837.
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157082
Train loss (w/o reg) on all data: 0.100751
Test loss (w/o reg) on all data: 0.830396
Train acc on all data:  0.980904036742
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 1.17001e-05
Norm of the params: 33.5653
                Loss: fixed 189 labels. Loss 0.83040. Accuracy 0.808.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245072
Train loss (w/o reg) on all data: 0.191413
Test loss (w/o reg) on all data: 0.750897
Train acc on all data:  0.939811457578
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 0.000232039
Norm of the params: 32.7597
              Random: fixed  44 labels. Loss 0.75090. Accuracy 0.804.
### Flips: 824, rs: 32, checks: 412
Using normal model
LBFGS training took [371] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157452
Train loss (w/o reg) on all data: 0.118407
Test loss (w/o reg) on all data: 0.68447
Train acc on all data:  0.96470872613
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 7.09801e-06
Norm of the params: 27.9445
     Influence (LOO): fixed 289 labels. Loss 0.68447. Accuracy 0.870.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11736
Train loss (w/o reg) on all data: 0.0663698
Test loss (w/o reg) on all data: 0.882337
Train acc on all data:  0.995890742084
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 1.64191e-05
Norm of the params: 31.9344
                Loss: fixed 306 labels. Loss 0.88234. Accuracy 0.844.
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237438
Train loss (w/o reg) on all data: 0.184849
Test loss (w/o reg) on all data: 0.776115
Train acc on all data:  0.941986947063
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 2.87303e-05
Norm of the params: 32.4311
              Random: fixed  81 labels. Loss 0.77612. Accuracy 0.804.
### Flips: 824, rs: 32, checks: 618
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142672
Train loss (w/o reg) on all data: 0.108136
Test loss (w/o reg) on all data: 0.596859
Train acc on all data:  0.967851099831
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 2.59986e-05
Norm of the params: 26.2816
     Influence (LOO): fixed 387 labels. Loss 0.59686. Accuracy 0.892.
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101706
Train loss (w/o reg) on all data: 0.0553025
Test loss (w/o reg) on all data: 0.767925
Train acc on all data:  0.996857626299
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.9712e-05
Norm of the params: 30.4641
                Loss: fixed 369 labels. Loss 0.76792. Accuracy 0.860.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228298
Train loss (w/o reg) on all data: 0.177139
Test loss (w/o reg) on all data: 0.772062
Train acc on all data:  0.94488759971
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 2.73442e-05
Norm of the params: 31.9873
              Random: fixed 123 labels. Loss 0.77206. Accuracy 0.816.
### Flips: 824, rs: 32, checks: 824
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132419
Train loss (w/o reg) on all data: 0.100455
Test loss (w/o reg) on all data: 0.593459
Train acc on all data:  0.97026831037
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.71755e-05
Norm of the params: 25.2839
     Influence (LOO): fixed 444 labels. Loss 0.59346. Accuracy 0.905.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0928997
Train loss (w/o reg) on all data: 0.0495098
Test loss (w/o reg) on all data: 0.701065
Train acc on all data:  0.997582789461
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.6094e-05
Norm of the params: 29.4584
                Loss: fixed 412 labels. Loss 0.70107. Accuracy 0.858.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.218678
Train loss (w/o reg) on all data: 0.168498
Test loss (w/o reg) on all data: 0.776967
Train acc on all data:  0.950688905004
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 5.17963e-05
Norm of the params: 31.6797
              Random: fixed 170 labels. Loss 0.77697. Accuracy 0.815.
### Flips: 824, rs: 32, checks: 1030
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122597
Train loss (w/o reg) on all data: 0.0928694
Test loss (w/o reg) on all data: 0.490171
Train acc on all data:  0.971960357747
Test acc on all data:   0.917874396135
Norm of the mean of gradients: 1.46323e-05
Norm of the params: 24.3834
     Influence (LOO): fixed 502 labels. Loss 0.49017. Accuracy 0.918.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08606
Train loss (w/o reg) on all data: 0.0453179
Test loss (w/o reg) on all data: 0.659476
Train acc on all data:  0.998307952623
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 1.12067e-05
Norm of the params: 28.5454
                Loss: fixed 448 labels. Loss 0.65948. Accuracy 0.871.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21323
Train loss (w/o reg) on all data: 0.16374
Test loss (w/o reg) on all data: 0.722935
Train acc on all data:  0.952139231327
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 1.48491e-05
Norm of the params: 31.4611
              Random: fixed 205 labels. Loss 0.72293. Accuracy 0.829.
### Flips: 824, rs: 32, checks: 1236
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113355
Train loss (w/o reg) on all data: 0.0863975
Test loss (w/o reg) on all data: 0.401189
Train acc on all data:  0.973894126178
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 3.50809e-06
Norm of the params: 23.2196
     Influence (LOO): fixed 553 labels. Loss 0.40119. Accuracy 0.936.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0806127
Train loss (w/o reg) on all data: 0.0420265
Test loss (w/o reg) on all data: 0.623494
Train acc on all data:  0.998307952623
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 6.4267e-06
Norm of the params: 27.7799
                Loss: fixed 481 labels. Loss 0.62349. Accuracy 0.874.
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206655
Train loss (w/o reg) on all data: 0.158295
Test loss (w/o reg) on all data: 0.673982
Train acc on all data:  0.953831278704
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 2.05141e-05
Norm of the params: 31.0996
              Random: fixed 242 labels. Loss 0.67398. Accuracy 0.844.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251352
Train loss (w/o reg) on all data: 0.199735
Test loss (w/o reg) on all data: 0.638062
Train acc on all data:  0.932318104907
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 2.60382e-05
Norm of the params: 32.1301
Flipped loss: 0.63806. Accuracy: 0.804
### Flips: 824, rs: 33, checks: 206
Using normal model
LBFGS training took [335] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180206
Train loss (w/o reg) on all data: 0.137495
Test loss (w/o reg) on all data: 0.45751
Train acc on all data:  0.957940536621
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 1.6036e-05
Norm of the params: 29.2272
     Influence (LOO): fixed 176 labels. Loss 0.45751. Accuracy 0.863.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153096
Train loss (w/o reg) on all data: 0.0990711
Test loss (w/o reg) on all data: 0.582684
Train acc on all data:  0.979937152526
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 3.59252e-05
Norm of the params: 32.8709
                Loss: fixed 189 labels. Loss 0.58268. Accuracy 0.811.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24189
Train loss (w/o reg) on all data: 0.190801
Test loss (w/o reg) on all data: 0.615369
Train acc on all data:  0.937394247039
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 1.74876e-05
Norm of the params: 31.9654
              Random: fixed  47 labels. Loss 0.61537. Accuracy 0.816.
### Flips: 824, rs: 33, checks: 412
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158145
Train loss (w/o reg) on all data: 0.120769
Test loss (w/o reg) on all data: 0.39282
Train acc on all data:  0.962049794537
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.05588e-05
Norm of the params: 27.3409
     Influence (LOO): fixed 297 labels. Loss 0.39282. Accuracy 0.877.
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117763
Train loss (w/o reg) on all data: 0.0675419
Test loss (w/o reg) on all data: 0.484406
Train acc on all data:  0.995890742084
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 3.26601e-05
Norm of the params: 31.6925
                Loss: fixed 302 labels. Loss 0.48441. Accuracy 0.859.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23354
Train loss (w/o reg) on all data: 0.183425
Test loss (w/o reg) on all data: 0.627733
Train acc on all data:  0.940778341794
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 3.17328e-05
Norm of the params: 31.659
              Random: fixed  95 labels. Loss 0.62773. Accuracy 0.817.
### Flips: 824, rs: 33, checks: 618
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146684
Train loss (w/o reg) on all data: 0.112584
Test loss (w/o reg) on all data: 0.341076
Train acc on all data:  0.962774957699
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.02404e-05
Norm of the params: 26.1152
     Influence (LOO): fixed 367 labels. Loss 0.34108. Accuracy 0.894.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0988546
Train loss (w/o reg) on all data: 0.0537461
Test loss (w/o reg) on all data: 0.52173
Train acc on all data:  0.997341068407
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.17761e-05
Norm of the params: 30.0361
                Loss: fixed 373 labels. Loss 0.52173. Accuracy 0.864.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227084
Train loss (w/o reg) on all data: 0.177967
Test loss (w/o reg) on all data: 0.626724
Train acc on all data:  0.94367899444
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 1.84876e-05
Norm of the params: 31.3422
              Random: fixed 131 labels. Loss 0.62672. Accuracy 0.836.
### Flips: 824, rs: 33, checks: 824
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13747
Train loss (w/o reg) on all data: 0.10604
Test loss (w/o reg) on all data: 0.297693
Train acc on all data:  0.964225284022
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 1.25318e-05
Norm of the params: 25.0721
     Influence (LOO): fixed 439 labels. Loss 0.29769. Accuracy 0.911.
Using normal model
LBFGS training took [348] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0877402
Train loss (w/o reg) on all data: 0.0463149
Test loss (w/o reg) on all data: 0.521241
Train acc on all data:  0.998066231569
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.89813e-05
Norm of the params: 28.7838
                Loss: fixed 424 labels. Loss 0.52124. Accuracy 0.877.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217995
Train loss (w/o reg) on all data: 0.170199
Test loss (w/o reg) on all data: 0.581003
Train acc on all data:  0.947304810249
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 4.1999e-05
Norm of the params: 30.918
              Random: fixed 180 labels. Loss 0.58100. Accuracy 0.841.
### Flips: 824, rs: 33, checks: 1030
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12966
Train loss (w/o reg) on all data: 0.100393
Test loss (w/o reg) on all data: 0.242282
Train acc on all data:  0.968092820885
Test acc on all data:   0.930434782609
Norm of the mean of gradients: 1.03472e-05
Norm of the params: 24.194
     Influence (LOO): fixed 493 labels. Loss 0.24228. Accuracy 0.930.
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0813235
Train loss (w/o reg) on all data: 0.042369
Test loss (w/o reg) on all data: 0.484526
Train acc on all data:  0.998307952623
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 3.4569e-05
Norm of the params: 27.9122
                Loss: fixed 454 labels. Loss 0.48453. Accuracy 0.888.
Using normal model
LBFGS training took [526] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207142
Train loss (w/o reg) on all data: 0.160493
Test loss (w/o reg) on all data: 0.543182
Train acc on all data:  0.951897510273
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 6.67826e-05
Norm of the params: 30.5446
              Random: fixed 227 labels. Loss 0.54318. Accuracy 0.845.
### Flips: 824, rs: 33, checks: 1236
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120067
Train loss (w/o reg) on all data: 0.0932236
Test loss (w/o reg) on all data: 0.21106
Train acc on all data:  0.968576262993
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 1.34173e-05
Norm of the params: 23.1703
     Influence (LOO): fixed 535 labels. Loss 0.21106. Accuracy 0.939.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0731327
Train loss (w/o reg) on all data: 0.037533
Test loss (w/o reg) on all data: 0.426463
Train acc on all data:  0.998307952623
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 8.4182e-06
Norm of the params: 26.6832
                Loss: fixed 494 labels. Loss 0.42646. Accuracy 0.896.
Using normal model
LBFGS training took [491] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197297
Train loss (w/o reg) on all data: 0.152439
Test loss (w/o reg) on all data: 0.486675
Train acc on all data:  0.954072999758
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 4.48202e-05
Norm of the params: 29.9526
              Random: fixed 273 labels. Loss 0.48668. Accuracy 0.861.
Using normal model
LBFGS training took [528] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241849
Train loss (w/o reg) on all data: 0.189377
Test loss (w/o reg) on all data: 0.783057
Train acc on all data:  0.935702199662
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 3.76374e-05
Norm of the params: 32.3949
Flipped loss: 0.78306. Accuracy: 0.809
### Flips: 824, rs: 34, checks: 206
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178819
Train loss (w/o reg) on all data: 0.135104
Test loss (w/o reg) on all data: 0.72523
Train acc on all data:  0.957940536621
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 2.21757e-05
Norm of the params: 29.5688
     Influence (LOO): fixed 170 labels. Loss 0.72523. Accuracy 0.844.
Using normal model
LBFGS training took [484] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149046
Train loss (w/o reg) on all data: 0.0950324
Test loss (w/o reg) on all data: 0.771734
Train acc on all data:  0.981870920957
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 7.67917e-05
Norm of the params: 32.8674
                Loss: fixed 190 labels. Loss 0.77173. Accuracy 0.822.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234633
Train loss (w/o reg) on all data: 0.18289
Test loss (w/o reg) on all data: 0.767642
Train acc on all data:  0.940053178632
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 9.21308e-05
Norm of the params: 32.1693
              Random: fixed  41 labels. Loss 0.76764. Accuracy 0.816.
### Flips: 824, rs: 34, checks: 412
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156028
Train loss (w/o reg) on all data: 0.118119
Test loss (w/o reg) on all data: 0.429258
Train acc on all data:  0.963016678753
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 2.14938e-05
Norm of the params: 27.5349
     Influence (LOO): fixed 294 labels. Loss 0.42926. Accuracy 0.887.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117406
Train loss (w/o reg) on all data: 0.0669164
Test loss (w/o reg) on all data: 0.798167
Train acc on all data:  0.993231810491
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 2.31581e-05
Norm of the params: 31.7771
                Loss: fixed 289 labels. Loss 0.79817. Accuracy 0.839.
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228094
Train loss (w/o reg) on all data: 0.177272
Test loss (w/o reg) on all data: 0.774753
Train acc on all data:  0.941503504955
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 8.67951e-05
Norm of the params: 31.8817
              Random: fixed  81 labels. Loss 0.77475. Accuracy 0.813.
### Flips: 824, rs: 34, checks: 618
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14379
Train loss (w/o reg) on all data: 0.109488
Test loss (w/o reg) on all data: 0.371263
Train acc on all data:  0.965675610346
Test acc on all data:   0.900483091787
Norm of the mean of gradients: 5.87071e-06
Norm of the params: 26.1921
     Influence (LOO): fixed 380 labels. Loss 0.37126. Accuracy 0.900.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100557
Train loss (w/o reg) on all data: 0.0550216
Test loss (w/o reg) on all data: 0.808662
Train acc on all data:  0.997341068407
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 8.87185e-06
Norm of the params: 30.1778
                Loss: fixed 356 labels. Loss 0.80866. Accuracy 0.863.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219107
Train loss (w/o reg) on all data: 0.16928
Test loss (w/o reg) on all data: 0.719109
Train acc on all data:  0.945129320764
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 6.68939e-05
Norm of the params: 31.568
              Random: fixed 125 labels. Loss 0.71911. Accuracy 0.817.
### Flips: 824, rs: 34, checks: 824
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133423
Train loss (w/o reg) on all data: 0.102108
Test loss (w/o reg) on all data: 0.336136
Train acc on all data:  0.967609378777
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 5.6234e-06
Norm of the params: 25.0261
     Influence (LOO): fixed 447 labels. Loss 0.33614. Accuracy 0.914.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0925357
Train loss (w/o reg) on all data: 0.0495514
Test loss (w/o reg) on all data: 0.755125
Train acc on all data:  0.997824510515
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 2.1375e-05
Norm of the params: 29.3204
                Loss: fixed 400 labels. Loss 0.75513. Accuracy 0.878.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211333
Train loss (w/o reg) on all data: 0.161965
Test loss (w/o reg) on all data: 0.712775
Train acc on all data:  0.947546531303
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 2.02535e-05
Norm of the params: 31.4222
              Random: fixed 164 labels. Loss 0.71277. Accuracy 0.833.
### Flips: 824, rs: 34, checks: 1030
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125595
Train loss (w/o reg) on all data: 0.0968815
Test loss (w/o reg) on all data: 0.322938
Train acc on all data:  0.9690597051
Test acc on all data:   0.923671497585
Norm of the mean of gradients: 1.67532e-05
Norm of the params: 23.964
     Influence (LOO): fixed 499 labels. Loss 0.32294. Accuracy 0.924.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0844111
Train loss (w/o reg) on all data: 0.04435
Test loss (w/o reg) on all data: 0.640356
Train acc on all data:  0.998307952623
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 6.91284e-06
Norm of the params: 28.3059
                Loss: fixed 435 labels. Loss 0.64036. Accuracy 0.889.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204735
Train loss (w/o reg) on all data: 0.156064
Test loss (w/o reg) on all data: 0.655082
Train acc on all data:  0.951172347111
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 2.35268e-05
Norm of the params: 31.1996
              Random: fixed 204 labels. Loss 0.65508. Accuracy 0.838.
### Flips: 824, rs: 34, checks: 1236
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118451
Train loss (w/o reg) on all data: 0.0914526
Test loss (w/o reg) on all data: 0.307769
Train acc on all data:  0.970510031424
Test acc on all data:   0.928502415459
Norm of the mean of gradients: 6.08796e-06
Norm of the params: 23.2371
     Influence (LOO): fixed 541 labels. Loss 0.30777. Accuracy 0.929.
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0766933
Train loss (w/o reg) on all data: 0.0399131
Test loss (w/o reg) on all data: 0.656541
Train acc on all data:  0.997824510515
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 9.56426e-06
Norm of the params: 27.122
                Loss: fixed 490 labels. Loss 0.65654. Accuracy 0.900.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197464
Train loss (w/o reg) on all data: 0.150346
Test loss (w/o reg) on all data: 0.616003
Train acc on all data:  0.95358955765
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 2.95378e-05
Norm of the params: 30.6979
              Random: fixed 249 labels. Loss 0.61600. Accuracy 0.847.
Using normal model
LBFGS training took [556] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238512
Train loss (w/o reg) on all data: 0.185091
Test loss (w/o reg) on all data: 0.826129
Train acc on all data:  0.943920715494
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 5.56993e-05
Norm of the params: 32.6868
Flipped loss: 0.82613. Accuracy: 0.795
### Flips: 824, rs: 35, checks: 206
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177408
Train loss (w/o reg) on all data: 0.133696
Test loss (w/o reg) on all data: 0.565207
Train acc on all data:  0.96035774716
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 4.38267e-05
Norm of the params: 29.5678
     Influence (LOO): fixed 155 labels. Loss 0.56521. Accuracy 0.833.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151431
Train loss (w/o reg) on all data: 0.0965898
Test loss (w/o reg) on all data: 0.841479
Train acc on all data:  0.983321247281
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 6.56023e-05
Norm of the params: 33.1185
                Loss: fixed 168 labels. Loss 0.84148. Accuracy 0.797.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232386
Train loss (w/o reg) on all data: 0.179782
Test loss (w/o reg) on all data: 0.781862
Train acc on all data:  0.946579647087
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 4.30483e-05
Norm of the params: 32.4358
              Random: fixed  42 labels. Loss 0.78186. Accuracy 0.798.
### Flips: 824, rs: 35, checks: 412
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155978
Train loss (w/o reg) on all data: 0.117262
Test loss (w/o reg) on all data: 0.472282
Train acc on all data:  0.965192168238
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.72882e-05
Norm of the params: 27.8265
     Influence (LOO): fixed 285 labels. Loss 0.47228. Accuracy 0.860.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12299
Train loss (w/o reg) on all data: 0.0717221
Test loss (w/o reg) on all data: 0.715329
Train acc on all data:  0.991539763113
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 2.16755e-05
Norm of the params: 32.0212
                Loss: fixed 267 labels. Loss 0.71533. Accuracy 0.820.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225378
Train loss (w/o reg) on all data: 0.173795
Test loss (w/o reg) on all data: 0.725472
Train acc on all data:  0.949963741842
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 3.36821e-05
Norm of the params: 32.1195
              Random: fixed  84 labels. Loss 0.72547. Accuracy 0.810.
### Flips: 824, rs: 35, checks: 618
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139419
Train loss (w/o reg) on all data: 0.105063
Test loss (w/o reg) on all data: 0.393525
Train acc on all data:  0.969301426154
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 4.77148e-05
Norm of the params: 26.2129
     Influence (LOO): fixed 386 labels. Loss 0.39353. Accuracy 0.886.
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104478
Train loss (w/o reg) on all data: 0.0576723
Test loss (w/o reg) on all data: 0.611402
Train acc on all data:  0.997341068407
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 3.52892e-05
Norm of the params: 30.5959
                Loss: fixed 344 labels. Loss 0.61140. Accuracy 0.851.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220337
Train loss (w/o reg) on all data: 0.170073
Test loss (w/o reg) on all data: 0.698556
Train acc on all data:  0.94923857868
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 6.71238e-05
Norm of the params: 31.7061
              Random: fixed 122 labels. Loss 0.69856. Accuracy 0.807.
### Flips: 824, rs: 35, checks: 824
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128183
Train loss (w/o reg) on all data: 0.0972637
Test loss (w/o reg) on all data: 0.337968
Train acc on all data:  0.971718636693
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 8.13882e-06
Norm of the params: 24.8674
     Influence (LOO): fixed 468 labels. Loss 0.33797. Accuracy 0.908.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0896566
Train loss (w/o reg) on all data: 0.0472426
Test loss (w/o reg) on all data: 0.585159
Train acc on all data:  0.998307952623
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 9.17783e-05
Norm of the params: 29.1253
                Loss: fixed 412 labels. Loss 0.58516. Accuracy 0.855.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21262
Train loss (w/o reg) on all data: 0.163393
Test loss (w/o reg) on all data: 0.683335
Train acc on all data:  0.951897510273
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 4.54673e-05
Norm of the params: 31.3774
              Random: fixed 166 labels. Loss 0.68333. Accuracy 0.815.
### Flips: 824, rs: 35, checks: 1030
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118799
Train loss (w/o reg) on all data: 0.0902638
Test loss (w/o reg) on all data: 0.289407
Train acc on all data:  0.973652405124
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 1.65682e-05
Norm of the params: 23.8892
     Influence (LOO): fixed 524 labels. Loss 0.28941. Accuracy 0.926.
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0794305
Train loss (w/o reg) on all data: 0.0409171
Test loss (w/o reg) on all data: 0.51308
Train acc on all data:  0.998549673677
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.38094e-05
Norm of the params: 27.7537
                Loss: fixed 461 labels. Loss 0.51308. Accuracy 0.879.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206254
Train loss (w/o reg) on all data: 0.158215
Test loss (w/o reg) on all data: 0.65907
Train acc on all data:  0.954072999758
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 4.64256e-05
Norm of the params: 30.9962
              Random: fixed 208 labels. Loss 0.65907. Accuracy 0.830.
### Flips: 824, rs: 35, checks: 1236
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113337
Train loss (w/o reg) on all data: 0.0864572
Test loss (w/o reg) on all data: 0.229875
Train acc on all data:  0.974861010394
Test acc on all data:   0.938164251208
Norm of the mean of gradients: 4.28248e-06
Norm of the params: 23.186
     Influence (LOO): fixed 564 labels. Loss 0.22988. Accuracy 0.938.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0739035
Train loss (w/o reg) on all data: 0.0376525
Test loss (w/o reg) on all data: 0.455199
Train acc on all data:  0.99879139473
Test acc on all data:   0.890821256039
Norm of the mean of gradients: 2.46741e-05
Norm of the params: 26.9262
                Loss: fixed 498 labels. Loss 0.45520. Accuracy 0.891.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198728
Train loss (w/o reg) on all data: 0.151558
Test loss (w/o reg) on all data: 0.590333
Train acc on all data:  0.958423978729
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 2.77175e-05
Norm of the params: 30.7146
              Random: fixed 240 labels. Loss 0.59033. Accuracy 0.826.
Using normal model
LBFGS training took [709] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240466
Train loss (w/o reg) on all data: 0.188519
Test loss (w/o reg) on all data: 0.894152
Train acc on all data:  0.940053178632
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 2.72854e-05
Norm of the params: 32.2325
Flipped loss: 0.89415. Accuracy: 0.795
### Flips: 824, rs: 36, checks: 206
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172767
Train loss (w/o reg) on all data: 0.130574
Test loss (w/o reg) on all data: 0.756283
Train acc on all data:  0.958423978729
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 1.75163e-05
Norm of the params: 29.0492
     Influence (LOO): fixed 167 labels. Loss 0.75628. Accuracy 0.833.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147264
Train loss (w/o reg) on all data: 0.0924194
Test loss (w/o reg) on all data: 0.898156
Train acc on all data:  0.985013294658
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 3.08572e-05
Norm of the params: 33.1194
                Loss: fixed 194 labels. Loss 0.89816. Accuracy 0.806.
Using normal model
LBFGS training took [540] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232924
Train loss (w/o reg) on all data: 0.181568
Test loss (w/o reg) on all data: 0.814214
Train acc on all data:  0.940294899686
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 6.58769e-05
Norm of the params: 32.0486
              Random: fixed  33 labels. Loss 0.81421. Accuracy 0.789.
### Flips: 824, rs: 36, checks: 412
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154105
Train loss (w/o reg) on all data: 0.11651
Test loss (w/o reg) on all data: 0.577415
Train acc on all data:  0.963016678753
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 7.60739e-06
Norm of the params: 27.4208
     Influence (LOO): fixed 275 labels. Loss 0.57741. Accuracy 0.851.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119503
Train loss (w/o reg) on all data: 0.0686059
Test loss (w/o reg) on all data: 0.804086
Train acc on all data:  0.996374184191
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 1.72102e-05
Norm of the params: 31.9052
                Loss: fixed 276 labels. Loss 0.80409. Accuracy 0.817.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227703
Train loss (w/o reg) on all data: 0.176808
Test loss (w/o reg) on all data: 0.705635
Train acc on all data:  0.941745226009
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 2.19005e-05
Norm of the params: 31.9048
              Random: fixed  73 labels. Loss 0.70564. Accuracy 0.793.
### Flips: 824, rs: 36, checks: 618
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142253
Train loss (w/o reg) on all data: 0.108026
Test loss (w/o reg) on all data: 0.360174
Train acc on all data:  0.965192168238
Test acc on all data:   0.87922705314
Norm of the mean of gradients: 1.57838e-05
Norm of the params: 26.1637
     Influence (LOO): fixed 365 labels. Loss 0.36017. Accuracy 0.879.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101198
Train loss (w/o reg) on all data: 0.0554803
Test loss (w/o reg) on all data: 0.744941
Train acc on all data:  0.997341068407
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 4.34232e-05
Norm of the params: 30.2384
                Loss: fixed 341 labels. Loss 0.74494. Accuracy 0.843.
Using normal model
LBFGS training took [635] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220626
Train loss (w/o reg) on all data: 0.170735
Test loss (w/o reg) on all data: 0.708395
Train acc on all data:  0.945371041818
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 2.26776e-05
Norm of the params: 31.5881
              Random: fixed 111 labels. Loss 0.70840. Accuracy 0.810.
### Flips: 824, rs: 36, checks: 824
Using normal model
LBFGS training took [365] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131342
Train loss (w/o reg) on all data: 0.0995644
Test loss (w/o reg) on all data: 0.331094
Train acc on all data:  0.968334541939
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 2.43689e-05
Norm of the params: 25.2103
     Influence (LOO): fixed 438 labels. Loss 0.33109. Accuracy 0.903.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0891912
Train loss (w/o reg) on all data: 0.047352
Test loss (w/o reg) on all data: 0.611782
Train acc on all data:  0.997341068407
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.26301e-05
Norm of the params: 28.9272
                Loss: fixed 404 labels. Loss 0.61178. Accuracy 0.862.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213338
Train loss (w/o reg) on all data: 0.164257
Test loss (w/o reg) on all data: 0.679966
Train acc on all data:  0.948271694465
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 6.58694e-05
Norm of the params: 31.3308
              Random: fixed 146 labels. Loss 0.67997. Accuracy 0.821.
### Flips: 824, rs: 36, checks: 1030
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120655
Train loss (w/o reg) on all data: 0.092435
Test loss (w/o reg) on all data: 0.292702
Train acc on all data:  0.970026589316
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 7.50287e-06
Norm of the params: 23.7573
     Influence (LOO): fixed 509 labels. Loss 0.29270. Accuracy 0.925.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078888
Train loss (w/o reg) on all data: 0.0409236
Test loss (w/o reg) on all data: 0.586253
Train acc on all data:  0.998066231569
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 1.37306e-05
Norm of the params: 27.5552
                Loss: fixed 460 labels. Loss 0.58625. Accuracy 0.874.
Using normal model
LBFGS training took [526] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207172
Train loss (w/o reg) on all data: 0.158309
Test loss (w/o reg) on all data: 0.679138
Train acc on all data:  0.954556441866
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 2.00568e-05
Norm of the params: 31.261
              Random: fixed 174 labels. Loss 0.67914. Accuracy 0.836.
### Flips: 824, rs: 36, checks: 1236
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11367
Train loss (w/o reg) on all data: 0.0871819
Test loss (w/o reg) on all data: 0.260172
Train acc on all data:  0.971476915639
Test acc on all data:   0.940096618357
Norm of the mean of gradients: 1.67483e-05
Norm of the params: 23.0164
     Influence (LOO): fixed 557 labels. Loss 0.26017. Accuracy 0.940.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0708966
Train loss (w/o reg) on all data: 0.0359315
Test loss (w/o reg) on all data: 0.52062
Train acc on all data:  0.99879139473
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 6.84583e-06
Norm of the params: 26.4443
                Loss: fixed 499 labels. Loss 0.52062. Accuracy 0.903.
Using normal model
LBFGS training took [545] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200014
Train loss (w/o reg) on all data: 0.152437
Test loss (w/o reg) on all data: 0.634193
Train acc on all data:  0.956248489243
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 6.49727e-05
Norm of the params: 30.8468
              Random: fixed 225 labels. Loss 0.63419. Accuracy 0.829.
Using normal model
LBFGS training took [606] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243928
Train loss (w/o reg) on all data: 0.192518
Test loss (w/o reg) on all data: 0.759946
Train acc on all data:  0.934010152284
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 6.58752e-05
Norm of the params: 32.0654
Flipped loss: 0.75995. Accuracy: 0.811
### Flips: 824, rs: 37, checks: 206
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180092
Train loss (w/o reg) on all data: 0.137982
Test loss (w/o reg) on all data: 0.568204
Train acc on all data:  0.955281605028
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.915e-05
Norm of the params: 29.0208
     Influence (LOO): fixed 171 labels. Loss 0.56820. Accuracy 0.858.
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152584
Train loss (w/o reg) on all data: 0.0988883
Test loss (w/o reg) on all data: 0.672379
Train acc on all data:  0.979211989364
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 1.83729e-05
Norm of the params: 32.7706
                Loss: fixed 190 labels. Loss 0.67238. Accuracy 0.821.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2384
Train loss (w/o reg) on all data: 0.187807
Test loss (w/o reg) on all data: 0.732369
Train acc on all data:  0.935943920715
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 7.84052e-05
Norm of the params: 31.8098
              Random: fixed  35 labels. Loss 0.73237. Accuracy 0.816.
### Flips: 824, rs: 37, checks: 412
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160381
Train loss (w/o reg) on all data: 0.123519
Test loss (w/o reg) on all data: 0.511025
Train acc on all data:  0.959632583998
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 1.56365e-05
Norm of the params: 27.152
     Influence (LOO): fixed 296 labels. Loss 0.51103. Accuracy 0.875.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11838
Train loss (w/o reg) on all data: 0.068305
Test loss (w/o reg) on all data: 0.529922
Train acc on all data:  0.991539763113
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 2.16276e-05
Norm of the params: 31.6464
                Loss: fixed 296 labels. Loss 0.52992. Accuracy 0.845.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229413
Train loss (w/o reg) on all data: 0.179781
Test loss (w/o reg) on all data: 0.693522
Train acc on all data:  0.938602852308
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 2.34112e-05
Norm of the params: 31.506
              Random: fixed  81 labels. Loss 0.69352. Accuracy 0.831.
### Flips: 824, rs: 37, checks: 618
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14837
Train loss (w/o reg) on all data: 0.115303
Test loss (w/o reg) on all data: 0.490673
Train acc on all data:  0.963016678753
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 6.50872e-06
Norm of the params: 25.7164
     Influence (LOO): fixed 371 labels. Loss 0.49067. Accuracy 0.893.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101331
Train loss (w/o reg) on all data: 0.055566
Test loss (w/o reg) on all data: 0.535604
Train acc on all data:  0.992990089437
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 1.18511e-05
Norm of the params: 30.2538
                Loss: fixed 363 labels. Loss 0.53560. Accuracy 0.869.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220824
Train loss (w/o reg) on all data: 0.172197
Test loss (w/o reg) on all data: 0.647145
Train acc on all data:  0.941986947063
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 0.000156438
Norm of the params: 31.1855
              Random: fixed 123 labels. Loss 0.64715. Accuracy 0.842.
### Flips: 824, rs: 37, checks: 824
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13788
Train loss (w/o reg) on all data: 0.107382
Test loss (w/o reg) on all data: 0.391117
Train acc on all data:  0.964225284022
Test acc on all data:   0.919806763285
Norm of the mean of gradients: 6.47676e-06
Norm of the params: 24.6975
     Influence (LOO): fixed 437 labels. Loss 0.39112. Accuracy 0.920.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0892928
Train loss (w/o reg) on all data: 0.0476312
Test loss (w/o reg) on all data: 0.526004
Train acc on all data:  0.997582789461
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 6.82055e-06
Norm of the params: 28.8658
                Loss: fixed 409 labels. Loss 0.52600. Accuracy 0.884.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.212095
Train loss (w/o reg) on all data: 0.164244
Test loss (w/o reg) on all data: 0.600974
Train acc on all data:  0.946337926033
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 4.26678e-05
Norm of the params: 30.9357
              Random: fixed 168 labels. Loss 0.60097. Accuracy 0.857.
### Flips: 824, rs: 37, checks: 1030
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12484
Train loss (w/o reg) on all data: 0.0980537
Test loss (w/o reg) on all data: 0.437622
Train acc on all data:  0.967609378777
Test acc on all data:   0.929468599034
Norm of the mean of gradients: 3.86486e-06
Norm of the params: 23.1458
     Influence (LOO): fixed 506 labels. Loss 0.43762. Accuracy 0.929.
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0800672
Train loss (w/o reg) on all data: 0.0415638
Test loss (w/o reg) on all data: 0.496868
Train acc on all data:  0.997824510515
Test acc on all data:   0.895652173913
Norm of the mean of gradients: 3.15066e-05
Norm of the params: 27.7501
                Loss: fixed 466 labels. Loss 0.49687. Accuracy 0.896.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203283
Train loss (w/o reg) on all data: 0.156707
Test loss (w/o reg) on all data: 0.614942
Train acc on all data:  0.947304810249
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 6.00566e-05
Norm of the params: 30.5208
              Random: fixed 215 labels. Loss 0.61494. Accuracy 0.855.
### Flips: 824, rs: 37, checks: 1236
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116753
Train loss (w/o reg) on all data: 0.0918335
Test loss (w/o reg) on all data: 0.351074
Train acc on all data:  0.968817984046
Test acc on all data:   0.939130434783
Norm of the mean of gradients: 5.51546e-06
Norm of the params: 22.3248
     Influence (LOO): fixed 546 labels. Loss 0.35107. Accuracy 0.939.
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0754097
Train loss (w/o reg) on all data: 0.0388388
Test loss (w/o reg) on all data: 0.449385
Train acc on all data:  0.998307952623
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 7.97256e-06
Norm of the params: 27.0448
                Loss: fixed 501 labels. Loss 0.44938. Accuracy 0.901.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193851
Train loss (w/o reg) on all data: 0.148267
Test loss (w/o reg) on all data: 0.586756
Train acc on all data:  0.950930626058
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.97744e-05
Norm of the params: 30.1941
              Random: fixed 259 labels. Loss 0.58676. Accuracy 0.865.
Using normal model
LBFGS training took [614] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246972
Train loss (w/o reg) on all data: 0.194983
Test loss (w/o reg) on all data: 0.925557
Train acc on all data:  0.933043268069
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 5.6697e-05
Norm of the params: 32.2457
Flipped loss: 0.92556. Accuracy: 0.789
### Flips: 824, rs: 38, checks: 206
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176843
Train loss (w/o reg) on all data: 0.13506
Test loss (w/o reg) on all data: 0.701426
Train acc on all data:  0.957698815567
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 1.5379e-05
Norm of the params: 28.9077
     Influence (LOO): fixed 170 labels. Loss 0.70143. Accuracy 0.842.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152996
Train loss (w/o reg) on all data: 0.0989082
Test loss (w/o reg) on all data: 0.923056
Train acc on all data:  0.980420594634
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 7.91547e-05
Norm of the params: 32.8901
                Loss: fixed 185 labels. Loss 0.92306. Accuracy 0.803.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239117
Train loss (w/o reg) on all data: 0.188317
Test loss (w/o reg) on all data: 0.873971
Train acc on all data:  0.938361131255
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 2.38889e-05
Norm of the params: 31.8748
              Random: fixed  39 labels. Loss 0.87397. Accuracy 0.807.
### Flips: 824, rs: 38, checks: 412
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15544
Train loss (w/o reg) on all data: 0.11947
Test loss (w/o reg) on all data: 0.631824
Train acc on all data:  0.959874305052
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 2.44419e-05
Norm of the params: 26.8216
     Influence (LOO): fixed 285 labels. Loss 0.63182. Accuracy 0.869.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114303
Train loss (w/o reg) on all data: 0.0656723
Test loss (w/o reg) on all data: 0.88721
Train acc on all data:  0.992990089437
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 1.2932e-05
Norm of the params: 31.1866
                Loss: fixed 296 labels. Loss 0.88721. Accuracy 0.818.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231472
Train loss (w/o reg) on all data: 0.181402
Test loss (w/o reg) on all data: 0.849469
Train acc on all data:  0.942228668117
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 4.7763e-05
Norm of the params: 31.6447
              Random: fixed  81 labels. Loss 0.84947. Accuracy 0.818.
### Flips: 824, rs: 38, checks: 618
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141664
Train loss (w/o reg) on all data: 0.109645
Test loss (w/o reg) on all data: 0.527081
Train acc on all data:  0.964225284022
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.25286e-05
Norm of the params: 25.306
     Influence (LOO): fixed 378 labels. Loss 0.52708. Accuracy 0.899.
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0955294
Train loss (w/o reg) on all data: 0.0514838
Test loss (w/o reg) on all data: 0.736439
Train acc on all data:  0.998549673677
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.83611e-05
Norm of the params: 29.6802
                Loss: fixed 366 labels. Loss 0.73644. Accuracy 0.851.
Using normal model
LBFGS training took [567] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221406
Train loss (w/o reg) on all data: 0.172177
Test loss (w/o reg) on all data: 0.84328
Train acc on all data:  0.946096204979
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 9.57202e-05
Norm of the params: 31.3781
              Random: fixed 120 labels. Loss 0.84328. Accuracy 0.828.
### Flips: 824, rs: 38, checks: 824
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131663
Train loss (w/o reg) on all data: 0.101799
Test loss (w/o reg) on all data: 0.444765
Train acc on all data:  0.967125936669
Test acc on all data:   0.908212560386
Norm of the mean of gradients: 4.67474e-06
Norm of the params: 24.4392
     Influence (LOO): fixed 442 labels. Loss 0.44476. Accuracy 0.908.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0835603
Train loss (w/o reg) on all data: 0.0438995
Test loss (w/o reg) on all data: 0.665503
Train acc on all data:  0.999033115784
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 7.82947e-06
Norm of the params: 28.1641
                Loss: fixed 425 labels. Loss 0.66550. Accuracy 0.872.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21182
Train loss (w/o reg) on all data: 0.163355
Test loss (w/o reg) on all data: 0.772551
Train acc on all data:  0.949963741842
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 9.99497e-05
Norm of the params: 31.1338
              Random: fixed 176 labels. Loss 0.77255. Accuracy 0.826.
### Flips: 824, rs: 38, checks: 1030
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121254
Train loss (w/o reg) on all data: 0.0943421
Test loss (w/o reg) on all data: 0.503952
Train acc on all data:  0.969301426154
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.28671e-05
Norm of the params: 23.1998
     Influence (LOO): fixed 509 labels. Loss 0.50395. Accuracy 0.913.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0767608
Train loss (w/o reg) on all data: 0.0399374
Test loss (w/o reg) on all data: 0.611924
Train acc on all data:  0.998549673677
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 6.77205e-06
Norm of the params: 27.1379
                Loss: fixed 464 labels. Loss 0.61192. Accuracy 0.876.
Using normal model
LBFGS training took [522] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203916
Train loss (w/o reg) on all data: 0.156655
Test loss (w/o reg) on all data: 0.683903
Train acc on all data:  0.951655789219
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 0.00013744
Norm of the params: 30.7445
              Random: fixed 221 labels. Loss 0.68390. Accuracy 0.842.
### Flips: 824, rs: 38, checks: 1236
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113269
Train loss (w/o reg) on all data: 0.0885533
Test loss (w/o reg) on all data: 0.402094
Train acc on all data:  0.971235194585
Test acc on all data:   0.924637681159
Norm of the mean of gradients: 2.87723e-06
Norm of the params: 22.233
     Influence (LOO): fixed 555 labels. Loss 0.40209. Accuracy 0.925.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0710349
Train loss (w/o reg) on all data: 0.036372
Test loss (w/o reg) on all data: 0.56315
Train acc on all data:  0.99879139473
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 3.89831e-06
Norm of the params: 26.3298
                Loss: fixed 490 labels. Loss 0.56315. Accuracy 0.875.
Using normal model
LBFGS training took [554] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195247
Train loss (w/o reg) on all data: 0.149296
Test loss (w/o reg) on all data: 0.636782
Train acc on all data:  0.954072999758
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 2.6991e-05
Norm of the params: 30.3152
              Random: fixed 259 labels. Loss 0.63678. Accuracy 0.849.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241372
Train loss (w/o reg) on all data: 0.190355
Test loss (w/o reg) on all data: 0.640284
Train acc on all data:  0.936185641769
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 7.30839e-05
Norm of the params: 31.9428
Flipped loss: 0.64028. Accuracy: 0.805
### Flips: 824, rs: 39, checks: 206
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175926
Train loss (w/o reg) on all data: 0.133199
Test loss (w/o reg) on all data: 0.526336
Train acc on all data:  0.959874305052
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 8.21613e-06
Norm of the params: 29.2328
     Influence (LOO): fixed 163 labels. Loss 0.52634. Accuracy 0.846.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146137
Train loss (w/o reg) on all data: 0.0918153
Test loss (w/o reg) on all data: 0.601669
Train acc on all data:  0.983804689388
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 2.34345e-05
Norm of the params: 32.9611
                Loss: fixed 187 labels. Loss 0.60167. Accuracy 0.814.
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235997
Train loss (w/o reg) on all data: 0.185665
Test loss (w/o reg) on all data: 0.602965
Train acc on all data:  0.937635968093
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 1.33076e-05
Norm of the params: 31.7276
              Random: fixed  31 labels. Loss 0.60297. Accuracy 0.820.
### Flips: 824, rs: 39, checks: 412
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153562
Train loss (w/o reg) on all data: 0.115363
Test loss (w/o reg) on all data: 0.430454
Train acc on all data:  0.967125936669
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 6.07029e-06
Norm of the params: 27.6402
     Influence (LOO): fixed 282 labels. Loss 0.43045. Accuracy 0.866.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115835
Train loss (w/o reg) on all data: 0.0660957
Test loss (w/o reg) on all data: 0.564933
Train acc on all data:  0.994198694706
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 2.25695e-05
Norm of the params: 31.5404
                Loss: fixed 280 labels. Loss 0.56493. Accuracy 0.845.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2307
Train loss (w/o reg) on all data: 0.181249
Test loss (w/o reg) on all data: 0.570708
Train acc on all data:  0.940294899686
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 5.07244e-05
Norm of the params: 31.4486
              Random: fixed  66 labels. Loss 0.57071. Accuracy 0.822.
### Flips: 824, rs: 39, checks: 618
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140554
Train loss (w/o reg) on all data: 0.105865
Test loss (w/o reg) on all data: 0.417186
Train acc on all data:  0.970751752478
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 4.50943e-06
Norm of the params: 26.3395
     Influence (LOO): fixed 372 labels. Loss 0.41719. Accuracy 0.882.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103069
Train loss (w/o reg) on all data: 0.0569723
Test loss (w/o reg) on all data: 0.530019
Train acc on all data:  0.997099347353
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 1.47198e-05
Norm of the params: 30.3634
                Loss: fixed 343 labels. Loss 0.53002. Accuracy 0.863.
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224666
Train loss (w/o reg) on all data: 0.176064
Test loss (w/o reg) on all data: 0.547708
Train acc on all data:  0.942953831279
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 1.24295e-05
Norm of the params: 31.1777
              Random: fixed 112 labels. Loss 0.54771. Accuracy 0.828.
### Flips: 824, rs: 39, checks: 824
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130596
Train loss (w/o reg) on all data: 0.0992744
Test loss (w/o reg) on all data: 0.360582
Train acc on all data:  0.972202078801
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.25485e-05
Norm of the params: 25.0285
     Influence (LOO): fixed 444 labels. Loss 0.36058. Accuracy 0.903.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0916589
Train loss (w/o reg) on all data: 0.0493875
Test loss (w/o reg) on all data: 0.472949
Train acc on all data:  0.997582789461
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 2.16479e-05
Norm of the params: 29.0762
                Loss: fixed 398 labels. Loss 0.47295. Accuracy 0.874.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215725
Train loss (w/o reg) on all data: 0.168002
Test loss (w/o reg) on all data: 0.522442
Train acc on all data:  0.946579647087
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 8.92847e-06
Norm of the params: 30.8946
              Random: fixed 157 labels. Loss 0.52244. Accuracy 0.832.
### Flips: 824, rs: 39, checks: 1030
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124418
Train loss (w/o reg) on all data: 0.0948712
Test loss (w/o reg) on all data: 0.270053
Train acc on all data:  0.972443799855
Test acc on all data:   0.925603864734
Norm of the mean of gradients: 3.48564e-06
Norm of the params: 24.3093
     Influence (LOO): fixed 496 labels. Loss 0.27005. Accuracy 0.926.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0846697
Train loss (w/o reg) on all data: 0.0449081
Test loss (w/o reg) on all data: 0.448593
Train acc on all data:  0.997824510515
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 1.27834e-05
Norm of the params: 28.1999
                Loss: fixed 439 labels. Loss 0.44859. Accuracy 0.893.
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206756
Train loss (w/o reg) on all data: 0.160073
Test loss (w/o reg) on all data: 0.509596
Train acc on all data:  0.949722020788
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 1.8337e-05
Norm of the params: 30.5561
              Random: fixed 193 labels. Loss 0.50960. Accuracy 0.833.
### Flips: 824, rs: 39, checks: 1236
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116937
Train loss (w/o reg) on all data: 0.0896977
Test loss (w/o reg) on all data: 0.215327
Train acc on all data:  0.973410684071
Test acc on all data:   0.936231884058
Norm of the mean of gradients: 5.76243e-06
Norm of the params: 23.3405
     Influence (LOO): fixed 551 labels. Loss 0.21533. Accuracy 0.936.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0760543
Train loss (w/o reg) on all data: 0.0396248
Test loss (w/o reg) on all data: 0.367073
Train acc on all data:  0.998307952623
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 3.87455e-06
Norm of the params: 26.9924
                Loss: fixed 487 labels. Loss 0.36707. Accuracy 0.894.
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196737
Train loss (w/o reg) on all data: 0.151445
Test loss (w/o reg) on all data: 0.483262
Train acc on all data:  0.953831278704
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 6.13332e-05
Norm of the params: 30.0971
              Random: fixed 243 labels. Loss 0.48326. Accuracy 0.837.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27499
Train loss (w/o reg) on all data: 0.220061
Test loss (w/o reg) on all data: 0.700192
Train acc on all data:  0.92385786802
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 0.00012097
Norm of the params: 33.1449
Flipped loss: 0.70019. Accuracy: 0.764
### Flips: 1030, rs: 0, checks: 206
Using normal model
LBFGS training took [416] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211771
Train loss (w/o reg) on all data: 0.163167
Test loss (w/o reg) on all data: 0.613251
Train acc on all data:  0.945129320764
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 4.87279e-05
Norm of the params: 31.1782
     Influence (LOO): fixed 157 labels. Loss 0.61325. Accuracy 0.811.
Using normal model
LBFGS training took [555] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18614
Train loss (w/o reg) on all data: 0.12698
Test loss (w/o reg) on all data: 0.68324
Train acc on all data:  0.967609378777
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 2.1854e-05
Norm of the params: 34.3977
                Loss: fixed 186 labels. Loss 0.68324. Accuracy 0.787.
Using normal model
LBFGS training took [535] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265716
Train loss (w/o reg) on all data: 0.211247
Test loss (w/o reg) on all data: 0.679312
Train acc on all data:  0.927000241721
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 2.28448e-05
Norm of the params: 33.0058
              Random: fixed  53 labels. Loss 0.67931. Accuracy 0.775.
### Flips: 1030, rs: 0, checks: 412
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187461
Train loss (w/o reg) on all data: 0.144397
Test loss (w/o reg) on all data: 0.471822
Train acc on all data:  0.95358955765
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.7924e-05
Norm of the params: 29.3476
     Influence (LOO): fixed 285 labels. Loss 0.47182. Accuracy 0.851.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144066
Train loss (w/o reg) on all data: 0.0882151
Test loss (w/o reg) on all data: 0.624661
Train acc on all data:  0.986705342035
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.60191e-05
Norm of the params: 33.4219
                Loss: fixed 310 labels. Loss 0.62466. Accuracy 0.803.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260047
Train loss (w/o reg) on all data: 0.206487
Test loss (w/o reg) on all data: 0.669803
Train acc on all data:  0.929659173314
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 3.30526e-05
Norm of the params: 32.7292
              Random: fixed 101 labels. Loss 0.66980. Accuracy 0.788.
### Flips: 1030, rs: 0, checks: 618
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172551
Train loss (w/o reg) on all data: 0.133285
Test loss (w/o reg) on all data: 0.413149
Train acc on all data:  0.957698815567
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 3.10885e-05
Norm of the params: 28.0234
     Influence (LOO): fixed 394 labels. Loss 0.41315. Accuracy 0.871.
Using normal model
LBFGS training took [537] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12316
Train loss (w/o reg) on all data: 0.0716897
Test loss (w/o reg) on all data: 0.584055
Train acc on all data:  0.99008943679
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 1.58855e-05
Norm of the params: 32.0842
                Loss: fixed 391 labels. Loss 0.58405. Accuracy 0.821.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253543
Train loss (w/o reg) on all data: 0.200817
Test loss (w/o reg) on all data: 0.713415
Train acc on all data:  0.933284989123
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 2.73524e-05
Norm of the params: 32.4734
              Random: fixed 149 labels. Loss 0.71342. Accuracy 0.814.
### Flips: 1030, rs: 0, checks: 824
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162201
Train loss (w/o reg) on all data: 0.125869
Test loss (w/o reg) on all data: 0.407977
Train acc on all data:  0.959390862944
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 9.51018e-06
Norm of the params: 26.9561
     Influence (LOO): fixed 470 labels. Loss 0.40798. Accuracy 0.882.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110115
Train loss (w/o reg) on all data: 0.0618746
Test loss (w/o reg) on all data: 0.558242
Train acc on all data:  0.992264926275
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 1.74015e-05
Norm of the params: 31.0615
                Loss: fixed 434 labels. Loss 0.55824. Accuracy 0.849.
Using normal model
LBFGS training took [548] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245135
Train loss (w/o reg) on all data: 0.192515
Test loss (w/o reg) on all data: 0.704802
Train acc on all data:  0.935702199662
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 6.46225e-05
Norm of the params: 32.4407
              Random: fixed 189 labels. Loss 0.70480. Accuracy 0.818.
### Flips: 1030, rs: 0, checks: 1030
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150935
Train loss (w/o reg) on all data: 0.116897
Test loss (w/o reg) on all data: 0.447529
Train acc on all data:  0.963741841914
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 1.03282e-05
Norm of the params: 26.0913
     Influence (LOO): fixed 550 labels. Loss 0.44753. Accuracy 0.892.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101096
Train loss (w/o reg) on all data: 0.0551794
Test loss (w/o reg) on all data: 0.510463
Train acc on all data:  0.996374184191
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.33269e-05
Norm of the params: 30.3041
                Loss: fixed 486 labels. Loss 0.51046. Accuracy 0.862.
Using normal model
LBFGS training took [577] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235774
Train loss (w/o reg) on all data: 0.183929
Test loss (w/o reg) on all data: 0.655014
Train acc on all data:  0.94053662074
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 2.41778e-05
Norm of the params: 32.2009
              Random: fixed 241 labels. Loss 0.65501. Accuracy 0.824.
### Flips: 1030, rs: 0, checks: 1236
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142049
Train loss (w/o reg) on all data: 0.110494
Test loss (w/o reg) on all data: 0.402577
Train acc on all data:  0.965433889292
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 1.75621e-05
Norm of the params: 25.1214
     Influence (LOO): fixed 614 labels. Loss 0.40258. Accuracy 0.902.
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0935802
Train loss (w/o reg) on all data: 0.0499603
Test loss (w/o reg) on all data: 0.502541
Train acc on all data:  0.997341068407
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 1.33708e-05
Norm of the params: 29.5364
                Loss: fixed 524 labels. Loss 0.50254. Accuracy 0.869.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224774
Train loss (w/o reg) on all data: 0.174838
Test loss (w/o reg) on all data: 0.683428
Train acc on all data:  0.942228668117
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 3.54965e-05
Norm of the params: 31.6026
              Random: fixed 305 labels. Loss 0.68343. Accuracy 0.840.
Using normal model
LBFGS training took [621] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272928
Train loss (w/o reg) on all data: 0.217175
Test loss (w/o reg) on all data: 0.895258
Train acc on all data:  0.925308194344
Test acc on all data:   0.727536231884
Norm of the mean of gradients: 5.94465e-05
Norm of the params: 33.3924
Flipped loss: 0.89526. Accuracy: 0.728
### Flips: 1030, rs: 1, checks: 206
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211061
Train loss (w/o reg) on all data: 0.1623
Test loss (w/o reg) on all data: 0.805194
Train acc on all data:  0.947788252357
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 8.28944e-05
Norm of the params: 31.2284
     Influence (LOO): fixed 161 labels. Loss 0.80519. Accuracy 0.759.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187318
Train loss (w/o reg) on all data: 0.126859
Test loss (w/o reg) on all data: 0.880006
Train acc on all data:  0.969301426154
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 4.38968e-05
Norm of the params: 34.7733
                Loss: fixed 183 labels. Loss 0.88001. Accuracy 0.756.
Using normal model
LBFGS training took [577] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263601
Train loss (w/o reg) on all data: 0.208103
Test loss (w/o reg) on all data: 0.896547
Train acc on all data:  0.928934010152
Test acc on all data:   0.743961352657
Norm of the mean of gradients: 2.38506e-05
Norm of the params: 33.3162
              Random: fixed  50 labels. Loss 0.89655. Accuracy 0.744.
### Flips: 1030, rs: 1, checks: 412
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188073
Train loss (w/o reg) on all data: 0.143761
Test loss (w/o reg) on all data: 0.64514
Train acc on all data:  0.956490210297
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 3.50968e-05
Norm of the params: 29.7699
     Influence (LOO): fixed 280 labels. Loss 0.64514. Accuracy 0.810.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149839
Train loss (w/o reg) on all data: 0.0918282
Test loss (w/o reg) on all data: 0.854854
Train acc on all data:  0.988155668359
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 5.98811e-05
Norm of the params: 34.0618
                Loss: fixed 289 labels. Loss 0.85485. Accuracy 0.769.
Using normal model
LBFGS training took [528] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259901
Train loss (w/o reg) on all data: 0.204846
Test loss (w/o reg) on all data: 0.877876
Train acc on all data:  0.932318104907
Test acc on all data:   0.750724637681
Norm of the mean of gradients: 8.8497e-05
Norm of the params: 33.1828
              Random: fixed  98 labels. Loss 0.87788. Accuracy 0.751.
### Flips: 1030, rs: 1, checks: 618
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172523
Train loss (w/o reg) on all data: 0.132171
Test loss (w/o reg) on all data: 0.494773
Train acc on all data:  0.958423978729
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 4.20165e-05
Norm of the params: 28.4086
     Influence (LOO): fixed 391 labels. Loss 0.49477. Accuracy 0.836.
Using normal model
LBFGS training took [537] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12844
Train loss (w/o reg) on all data: 0.0746802
Test loss (w/o reg) on all data: 0.765552
Train acc on all data:  0.991298042059
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 1.49613e-05
Norm of the params: 32.7902
                Loss: fixed 369 labels. Loss 0.76555. Accuracy 0.779.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249978
Train loss (w/o reg) on all data: 0.196205
Test loss (w/o reg) on all data: 0.866218
Train acc on all data:  0.934251873338
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 5.13699e-05
Norm of the params: 32.794
              Random: fixed 148 labels. Loss 0.86622. Accuracy 0.753.
### Flips: 1030, rs: 1, checks: 824
Using normal model
LBFGS training took [508] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162493
Train loss (w/o reg) on all data: 0.125036
Test loss (w/o reg) on all data: 0.444362
Train acc on all data:  0.96035774716
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.27579e-05
Norm of the params: 27.3703
     Influence (LOO): fixed 468 labels. Loss 0.44436. Accuracy 0.862.
Using normal model
LBFGS training took [573] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112327
Train loss (w/o reg) on all data: 0.0626285
Test loss (w/o reg) on all data: 0.749343
Train acc on all data:  0.99564902103
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 4.2788e-05
Norm of the params: 31.5271
                Loss: fixed 428 labels. Loss 0.74934. Accuracy 0.802.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240157
Train loss (w/o reg) on all data: 0.187152
Test loss (w/o reg) on all data: 0.811875
Train acc on all data:  0.939086294416
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 3.80108e-05
Norm of the params: 32.5589
              Random: fixed 199 labels. Loss 0.81187. Accuracy 0.760.
### Flips: 1030, rs: 1, checks: 1030
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152384
Train loss (w/o reg) on all data: 0.117428
Test loss (w/o reg) on all data: 0.3955
Train acc on all data:  0.963016678753
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 1.21483e-05
Norm of the params: 26.4406
     Influence (LOO): fixed 535 labels. Loss 0.39550. Accuracy 0.874.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100372
Train loss (w/o reg) on all data: 0.0542741
Test loss (w/o reg) on all data: 0.682177
Train acc on all data:  0.996374184191
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 2.48781e-05
Norm of the params: 30.3638
                Loss: fixed 480 labels. Loss 0.68218. Accuracy 0.818.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232323
Train loss (w/o reg) on all data: 0.179921
Test loss (w/o reg) on all data: 0.772307
Train acc on all data:  0.944162436548
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 6.087e-05
Norm of the params: 32.3737
              Random: fixed 250 labels. Loss 0.77231. Accuracy 0.769.
### Flips: 1030, rs: 1, checks: 1236
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143574
Train loss (w/o reg) on all data: 0.111062
Test loss (w/o reg) on all data: 0.370467
Train acc on all data:  0.964950447184
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.48699e-05
Norm of the params: 25.4997
     Influence (LOO): fixed 603 labels. Loss 0.37047. Accuracy 0.889.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0942907
Train loss (w/o reg) on all data: 0.0504565
Test loss (w/o reg) on all data: 0.6091
Train acc on all data:  0.997341068407
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 4.22279e-05
Norm of the params: 29.6089
                Loss: fixed 521 labels. Loss 0.60910. Accuracy 0.841.
Using normal model
LBFGS training took [448] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222209
Train loss (w/o reg) on all data: 0.171835
Test loss (w/o reg) on all data: 0.716482
Train acc on all data:  0.946096204979
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 1.61562e-05
Norm of the params: 31.7409
              Random: fixed 305 labels. Loss 0.71648. Accuracy 0.786.
Using normal model
LBFGS training took [723] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27612
Train loss (w/o reg) on all data: 0.220229
Test loss (w/o reg) on all data: 0.76611
Train acc on all data:  0.925308194344
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 3.37973e-05
Norm of the params: 33.4339
Flipped loss: 0.76611. Accuracy: 0.764
### Flips: 1030, rs: 2, checks: 206
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217664
Train loss (w/o reg) on all data: 0.168953
Test loss (w/o reg) on all data: 0.717131
Train acc on all data:  0.944162436548
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 1.64042e-05
Norm of the params: 31.2127
     Influence (LOO): fixed 158 labels. Loss 0.71713. Accuracy 0.804.
Using normal model
LBFGS training took [642] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192093
Train loss (w/o reg) on all data: 0.131227
Test loss (w/o reg) on all data: 0.786066
Train acc on all data:  0.968817984046
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 1.91437e-05
Norm of the params: 34.8902
                Loss: fixed 176 labels. Loss 0.78607. Accuracy 0.777.
Using normal model
LBFGS training took [590] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268259
Train loss (w/o reg) on all data: 0.212983
Test loss (w/o reg) on all data: 0.663715
Train acc on all data:  0.928208846991
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 0.000119631
Norm of the params: 33.2492
              Random: fixed  54 labels. Loss 0.66372. Accuracy 0.784.
### Flips: 1030, rs: 2, checks: 412
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195804
Train loss (w/o reg) on all data: 0.151723
Test loss (w/o reg) on all data: 0.571796
Train acc on all data:  0.949722020788
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 2.63167e-05
Norm of the params: 29.692
     Influence (LOO): fixed 283 labels. Loss 0.57180. Accuracy 0.843.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152993
Train loss (w/o reg) on all data: 0.0941006
Test loss (w/o reg) on all data: 0.836389
Train acc on all data:  0.987913947305
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 6.93995e-05
Norm of the params: 34.3197
                Loss: fixed 300 labels. Loss 0.83639. Accuracy 0.783.
Using normal model
LBFGS training took [576] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260447
Train loss (w/o reg) on all data: 0.206168
Test loss (w/o reg) on all data: 0.667451
Train acc on all data:  0.932801547015
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 5.93104e-05
Norm of the params: 32.9481
              Random: fixed 105 labels. Loss 0.66745. Accuracy 0.788.
### Flips: 1030, rs: 2, checks: 618
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183742
Train loss (w/o reg) on all data: 0.143069
Test loss (w/o reg) on all data: 0.541277
Train acc on all data:  0.951414068165
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 1.32674e-05
Norm of the params: 28.5212
     Influence (LOO): fixed 375 labels. Loss 0.54128. Accuracy 0.854.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129979
Train loss (w/o reg) on all data: 0.0754218
Test loss (w/o reg) on all data: 0.732939
Train acc on all data:  0.990814599952
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 2.93892e-05
Norm of the params: 33.0324
                Loss: fixed 386 labels. Loss 0.73294. Accuracy 0.799.
Using normal model
LBFGS training took [612] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253913
Train loss (w/o reg) on all data: 0.200421
Test loss (w/o reg) on all data: 0.637122
Train acc on all data:  0.936185641769
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 4.26955e-05
Norm of the params: 32.7084
              Random: fixed 155 labels. Loss 0.63712. Accuracy 0.795.
### Flips: 1030, rs: 2, checks: 824
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171874
Train loss (w/o reg) on all data: 0.134647
Test loss (w/o reg) on all data: 0.528478
Train acc on all data:  0.955039883974
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 7.6693e-06
Norm of the params: 27.2861
     Influence (LOO): fixed 460 labels. Loss 0.52848. Accuracy 0.882.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115155
Train loss (w/o reg) on all data: 0.0644842
Test loss (w/o reg) on all data: 0.704944
Train acc on all data:  0.996132463138
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 1.19422e-05
Norm of the params: 31.8343
                Loss: fixed 449 labels. Loss 0.70494. Accuracy 0.828.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246767
Train loss (w/o reg) on all data: 0.194303
Test loss (w/o reg) on all data: 0.643776
Train acc on all data:  0.937152525985
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 5.39541e-05
Norm of the params: 32.3924
              Random: fixed 205 labels. Loss 0.64378. Accuracy 0.806.
### Flips: 1030, rs: 2, checks: 1030
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159444
Train loss (w/o reg) on all data: 0.12539
Test loss (w/o reg) on all data: 0.513733
Train acc on all data:  0.957457094513
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 9.88485e-06
Norm of the params: 26.0973
     Influence (LOO): fixed 531 labels. Loss 0.51373. Accuracy 0.886.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104498
Train loss (w/o reg) on all data: 0.0570647
Test loss (w/o reg) on all data: 0.595285
Train acc on all data:  0.996132463138
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 1.1902e-05
Norm of the params: 30.8005
                Loss: fixed 502 labels. Loss 0.59528. Accuracy 0.847.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239086
Train loss (w/o reg) on all data: 0.18696
Test loss (w/o reg) on all data: 0.660903
Train acc on all data:  0.940294899686
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 7.2407e-05
Norm of the params: 32.2879
              Random: fixed 250 labels. Loss 0.66090. Accuracy 0.817.
### Flips: 1030, rs: 2, checks: 1236
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147166
Train loss (w/o reg) on all data: 0.115656
Test loss (w/o reg) on all data: 0.385815
Train acc on all data:  0.960116026106
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 5.52509e-06
Norm of the params: 25.1039
     Influence (LOO): fixed 594 labels. Loss 0.38582. Accuracy 0.903.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0974483
Train loss (w/o reg) on all data: 0.0524249
Test loss (w/o reg) on all data: 0.523678
Train acc on all data:  0.996615905245
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 2.56768e-05
Norm of the params: 30.0078
                Loss: fixed 540 labels. Loss 0.52368. Accuracy 0.855.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23052
Train loss (w/o reg) on all data: 0.17858
Test loss (w/o reg) on all data: 0.587885
Train acc on all data:  0.94488759971
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 6.17867e-05
Norm of the params: 32.2303
              Random: fixed 300 labels. Loss 0.58789. Accuracy 0.824.
Using normal model
LBFGS training took [592] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28009
Train loss (w/o reg) on all data: 0.223812
Test loss (w/o reg) on all data: 1.05699
Train acc on all data:  0.922407541697
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 2.21742e-05
Norm of the params: 33.5494
Flipped loss: 1.05699. Accuracy: 0.753
### Flips: 1030, rs: 3, checks: 206
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22057
Train loss (w/o reg) on all data: 0.171607
Test loss (w/o reg) on all data: 0.850964
Train acc on all data:  0.941261783901
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 2.8537e-05
Norm of the params: 31.2931
     Influence (LOO): fixed 156 labels. Loss 0.85096. Accuracy 0.793.
Using normal model
LBFGS training took [491] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191977
Train loss (w/o reg) on all data: 0.131804
Test loss (w/o reg) on all data: 1.05171
Train acc on all data:  0.969543147208
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 2.38907e-05
Norm of the params: 34.6907
                Loss: fixed 188 labels. Loss 1.05171. Accuracy 0.768.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271204
Train loss (w/o reg) on all data: 0.216182
Test loss (w/o reg) on all data: 1.03811
Train acc on all data:  0.927967125937
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 3.268e-05
Norm of the params: 33.1731
              Random: fixed  53 labels. Loss 1.03811. Accuracy 0.772.
### Flips: 1030, rs: 3, checks: 412
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198231
Train loss (w/o reg) on all data: 0.154189
Test loss (w/o reg) on all data: 0.726902
Train acc on all data:  0.947788252357
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.17209e-05
Norm of the params: 29.6789
     Influence (LOO): fixed 278 labels. Loss 0.72690. Accuracy 0.824.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147771
Train loss (w/o reg) on all data: 0.0896757
Test loss (w/o reg) on all data: 1.0687
Train acc on all data:  0.991056321006
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 4.18506e-05
Norm of the params: 34.0866
                Loss: fixed 309 labels. Loss 1.06870. Accuracy 0.791.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261489
Train loss (w/o reg) on all data: 0.207222
Test loss (w/o reg) on all data: 0.969025
Train acc on all data:  0.930142615422
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 4.94042e-05
Norm of the params: 32.9444
              Random: fixed  99 labels. Loss 0.96903. Accuracy 0.781.
### Flips: 1030, rs: 3, checks: 618
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182306
Train loss (w/o reg) on all data: 0.142189
Test loss (w/o reg) on all data: 0.622741
Train acc on all data:  0.949480299734
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 3.57194e-05
Norm of the params: 28.3255
     Influence (LOO): fixed 380 labels. Loss 0.62274. Accuracy 0.857.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130919
Train loss (w/o reg) on all data: 0.0761322
Test loss (w/o reg) on all data: 1.02619
Train acc on all data:  0.992748368383
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 0.000100969
Norm of the params: 33.1019
                Loss: fixed 383 labels. Loss 1.02619. Accuracy 0.805.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254681
Train loss (w/o reg) on all data: 0.201028
Test loss (w/o reg) on all data: 0.924117
Train acc on all data:  0.9349770365
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 0.000172975
Norm of the params: 32.7577
              Random: fixed 154 labels. Loss 0.92412. Accuracy 0.801.
### Flips: 1030, rs: 3, checks: 824
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169689
Train loss (w/o reg) on all data: 0.132547
Test loss (w/o reg) on all data: 0.570954
Train acc on all data:  0.952622673435
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 8.68251e-06
Norm of the params: 27.2554
     Influence (LOO): fixed 467 labels. Loss 0.57095. Accuracy 0.873.
Using normal model
LBFGS training took [450] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119336
Train loss (w/o reg) on all data: 0.067274
Test loss (w/o reg) on all data: 0.865733
Train acc on all data:  0.993231810491
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 2.60385e-05
Norm of the params: 32.2682
                Loss: fixed 436 labels. Loss 0.86573. Accuracy 0.812.
Using normal model
LBFGS training took [506] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245067
Train loss (w/o reg) on all data: 0.192707
Test loss (w/o reg) on all data: 0.859843
Train acc on all data:  0.936427362823
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 5.98151e-05
Norm of the params: 32.3605
              Random: fixed 213 labels. Loss 0.85984. Accuracy 0.802.
### Flips: 1030, rs: 3, checks: 1030
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157884
Train loss (w/o reg) on all data: 0.123427
Test loss (w/o reg) on all data: 0.523537
Train acc on all data:  0.955765047136
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.2517e-05
Norm of the params: 26.2512
     Influence (LOO): fixed 541 labels. Loss 0.52354. Accuracy 0.882.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112782
Train loss (w/o reg) on all data: 0.0625349
Test loss (w/o reg) on all data: 0.857302
Train acc on all data:  0.993473531545
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 2.50164e-05
Norm of the params: 31.7009
                Loss: fixed 479 labels. Loss 0.85730. Accuracy 0.834.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.233748
Train loss (w/o reg) on all data: 0.182878
Test loss (w/o reg) on all data: 0.788237
Train acc on all data:  0.941986947063
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 2.29091e-05
Norm of the params: 31.8966
              Random: fixed 265 labels. Loss 0.78824. Accuracy 0.813.
### Flips: 1030, rs: 3, checks: 1236
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147031
Train loss (w/o reg) on all data: 0.115228
Test loss (w/o reg) on all data: 0.515742
Train acc on all data:  0.960841189268
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 7.59925e-06
Norm of the params: 25.2202
     Influence (LOO): fixed 613 labels. Loss 0.51574. Accuracy 0.901.
Using normal model
LBFGS training took [371] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0991506
Train loss (w/o reg) on all data: 0.0532904
Test loss (w/o reg) on all data: 0.713814
Train acc on all data:  0.995165578922
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 1.18572e-05
Norm of the params: 30.2854
                Loss: fixed 532 labels. Loss 0.71381. Accuracy 0.850.
Using normal model
LBFGS training took [537] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226986
Train loss (w/o reg) on all data: 0.177526
Test loss (w/o reg) on all data: 0.78517
Train acc on all data:  0.941745226009
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 5.05671e-05
Norm of the params: 31.4518
              Random: fixed 311 labels. Loss 0.78517. Accuracy 0.829.
Using normal model
LBFGS training took [556] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.278413
Train loss (w/o reg) on all data: 0.223419
Test loss (w/o reg) on all data: 0.807108
Train acc on all data:  0.923132704859
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 1.80038e-05
Norm of the params: 33.1643
Flipped loss: 0.80711. Accuracy: 0.778
### Flips: 1030, rs: 4, checks: 206
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21127
Train loss (w/o reg) on all data: 0.16388
Test loss (w/o reg) on all data: 0.729808
Train acc on all data:  0.946821368141
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 4.16259e-05
Norm of the params: 30.7863
     Influence (LOO): fixed 161 labels. Loss 0.72981. Accuracy 0.814.
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190051
Train loss (w/o reg) on all data: 0.131036
Test loss (w/o reg) on all data: 0.889641
Train acc on all data:  0.9690597051
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 1.37173e-05
Norm of the params: 34.3553
                Loss: fixed 190 labels. Loss 0.88964. Accuracy 0.778.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26875
Train loss (w/o reg) on all data: 0.214653
Test loss (w/o reg) on all data: 0.753602
Train acc on all data:  0.924341310128
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 2.72783e-05
Norm of the params: 32.8929
              Random: fixed  50 labels. Loss 0.75360. Accuracy 0.778.
### Flips: 1030, rs: 4, checks: 412
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188208
Train loss (w/o reg) on all data: 0.146154
Test loss (w/o reg) on all data: 0.589379
Train acc on all data:  0.951655789219
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 8.90393e-06
Norm of the params: 29.0013
     Influence (LOO): fixed 292 labels. Loss 0.58938. Accuracy 0.853.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146228
Train loss (w/o reg) on all data: 0.0909711
Test loss (w/o reg) on all data: 0.727949
Train acc on all data:  0.983804689388
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 7.35444e-05
Norm of the params: 33.2437
                Loss: fixed 319 labels. Loss 0.72795. Accuracy 0.806.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260354
Train loss (w/o reg) on all data: 0.206415
Test loss (w/o reg) on all data: 0.708118
Train acc on all data:  0.930384336476
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 2.14729e-05
Norm of the params: 32.8449
              Random: fixed  95 labels. Loss 0.70812. Accuracy 0.789.
### Flips: 1030, rs: 4, checks: 618
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175936
Train loss (w/o reg) on all data: 0.136574
Test loss (w/o reg) on all data: 0.464787
Train acc on all data:  0.953347836597
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.56284e-05
Norm of the params: 28.0576
     Influence (LOO): fixed 385 labels. Loss 0.46479. Accuracy 0.860.
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122014
Train loss (w/o reg) on all data: 0.0705959
Test loss (w/o reg) on all data: 0.592327
Train acc on all data:  0.994198694706
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 1.00055e-05
Norm of the params: 32.068
                Loss: fixed 397 labels. Loss 0.59233. Accuracy 0.834.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254909
Train loss (w/o reg) on all data: 0.201862
Test loss (w/o reg) on all data: 0.647404
Train acc on all data:  0.928450568044
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 4.93695e-05
Norm of the params: 32.5721
              Random: fixed 141 labels. Loss 0.64740. Accuracy 0.792.
### Flips: 1030, rs: 4, checks: 824
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164125
Train loss (w/o reg) on all data: 0.127494
Test loss (w/o reg) on all data: 0.40392
Train acc on all data:  0.955523326082
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 2.04231e-05
Norm of the params: 27.0669
     Influence (LOO): fixed 463 labels. Loss 0.40392. Accuracy 0.882.
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108744
Train loss (w/o reg) on all data: 0.0606172
Test loss (w/o reg) on all data: 0.598457
Train acc on all data:  0.995407299976
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 3.6915e-05
Norm of the params: 31.0247
                Loss: fixed 458 labels. Loss 0.59846. Accuracy 0.849.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246941
Train loss (w/o reg) on all data: 0.194898
Test loss (w/o reg) on all data: 0.606271
Train acc on all data:  0.930142615422
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 5.48501e-05
Norm of the params: 32.2625
              Random: fixed 193 labels. Loss 0.60627. Accuracy 0.807.
### Flips: 1030, rs: 4, checks: 1030
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156075
Train loss (w/o reg) on all data: 0.121502
Test loss (w/o reg) on all data: 0.344091
Train acc on all data:  0.957940536621
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 5.28951e-06
Norm of the params: 26.2954
     Influence (LOO): fixed 524 labels. Loss 0.34409. Accuracy 0.901.
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0993063
Train loss (w/o reg) on all data: 0.0543469
Test loss (w/o reg) on all data: 0.52939
Train acc on all data:  0.996615905245
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.04938e-05
Norm of the params: 29.9864
                Loss: fixed 503 labels. Loss 0.52939. Accuracy 0.862.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23612
Train loss (w/o reg) on all data: 0.184834
Test loss (w/o reg) on all data: 0.601995
Train acc on all data:  0.936669083877
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 2.42806e-05
Norm of the params: 32.0267
              Random: fixed 239 labels. Loss 0.60199. Accuracy 0.814.
### Flips: 1030, rs: 4, checks: 1236
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146881
Train loss (w/o reg) on all data: 0.115013
Test loss (w/o reg) on all data: 0.311132
Train acc on all data:  0.961566352429
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 5.16987e-06
Norm of the params: 25.246
     Influence (LOO): fixed 584 labels. Loss 0.31113. Accuracy 0.906.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0888102
Train loss (w/o reg) on all data: 0.0471838
Test loss (w/o reg) on all data: 0.497392
Train acc on all data:  0.997341068407
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 1.06008e-05
Norm of the params: 28.8536
                Loss: fixed 558 labels. Loss 0.49739. Accuracy 0.871.
Using normal model
LBFGS training took [450] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224401
Train loss (w/o reg) on all data: 0.174458
Test loss (w/o reg) on all data: 0.575435
Train acc on all data:  0.942712110225
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 3.44815e-05
Norm of the params: 31.6049
              Random: fixed 295 labels. Loss 0.57543. Accuracy 0.826.
Using normal model
LBFGS training took [628] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276409
Train loss (w/o reg) on all data: 0.220342
Test loss (w/o reg) on all data: 0.771975
Train acc on all data:  0.928208846991
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 2.23939e-05
Norm of the params: 33.4862
Flipped loss: 0.77198. Accuracy: 0.769
### Flips: 1030, rs: 5, checks: 206
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215999
Train loss (w/o reg) on all data: 0.167597
Test loss (w/o reg) on all data: 0.738376
Train acc on all data:  0.946096204979
Test acc on all data:   0.8
Norm of the mean of gradients: 5.02556e-05
Norm of the params: 31.1133
     Influence (LOO): fixed 164 labels. Loss 0.73838. Accuracy 0.800.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184938
Train loss (w/o reg) on all data: 0.124536
Test loss (w/o reg) on all data: 0.771978
Train acc on all data:  0.972685520909
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 3.9328e-05
Norm of the params: 34.7569
                Loss: fixed 193 labels. Loss 0.77198. Accuracy 0.772.
Using normal model
LBFGS training took [508] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268471
Train loss (w/o reg) on all data: 0.213184
Test loss (w/o reg) on all data: 0.77357
Train acc on all data:  0.927483683829
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 1.94457e-05
Norm of the params: 33.2526
              Random: fixed  44 labels. Loss 0.77357. Accuracy 0.767.
### Flips: 1030, rs: 5, checks: 412
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191898
Train loss (w/o reg) on all data: 0.148728
Test loss (w/o reg) on all data: 0.546751
Train acc on all data:  0.950688905004
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 2.52879e-05
Norm of the params: 29.3836
     Influence (LOO): fixed 285 labels. Loss 0.54675. Accuracy 0.837.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149116
Train loss (w/o reg) on all data: 0.0901581
Test loss (w/o reg) on all data: 0.767521
Train acc on all data:  0.990814599952
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 2.56842e-05
Norm of the params: 34.3387
                Loss: fixed 300 labels. Loss 0.76752. Accuracy 0.794.
Using normal model
LBFGS training took [523] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260177
Train loss (w/o reg) on all data: 0.205687
Test loss (w/o reg) on all data: 0.70603
Train acc on all data:  0.933043268069
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 1.84217e-05
Norm of the params: 33.0121
              Random: fixed 100 labels. Loss 0.70603. Accuracy 0.784.
### Flips: 1030, rs: 5, checks: 618
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179454
Train loss (w/o reg) on all data: 0.139519
Test loss (w/o reg) on all data: 0.47339
Train acc on all data:  0.954072999758
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.46344e-05
Norm of the params: 28.2612
     Influence (LOO): fixed 384 labels. Loss 0.47339. Accuracy 0.862.
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129234
Train loss (w/o reg) on all data: 0.073485
Test loss (w/o reg) on all data: 0.691138
Train acc on all data:  0.997099347353
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 6.75367e-05
Norm of the params: 33.3914
                Loss: fixed 371 labels. Loss 0.69114. Accuracy 0.810.
Using normal model
LBFGS training took [490] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25431
Train loss (w/o reg) on all data: 0.200617
Test loss (w/o reg) on all data: 0.680854
Train acc on all data:  0.936185641769
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 6.33164e-05
Norm of the params: 32.7699
              Random: fixed 144 labels. Loss 0.68085. Accuracy 0.793.
### Flips: 1030, rs: 5, checks: 824
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166069
Train loss (w/o reg) on all data: 0.128894
Test loss (w/o reg) on all data: 0.465055
Train acc on all data:  0.956490210297
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 3.22232e-05
Norm of the params: 27.2673
     Influence (LOO): fixed 459 labels. Loss 0.46506. Accuracy 0.878.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111272
Train loss (w/o reg) on all data: 0.0607007
Test loss (w/o reg) on all data: 0.647196
Train acc on all data:  0.998066231569
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 1.46289e-05
Norm of the params: 31.8029
                Loss: fixed 441 labels. Loss 0.64720. Accuracy 0.827.
Using normal model
LBFGS training took [485] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246624
Train loss (w/o reg) on all data: 0.194106
Test loss (w/o reg) on all data: 0.652928
Train acc on all data:  0.935702199662
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 5.51886e-05
Norm of the params: 32.4095
              Random: fixed 193 labels. Loss 0.65293. Accuracy 0.798.
### Flips: 1030, rs: 5, checks: 1030
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154075
Train loss (w/o reg) on all data: 0.119636
Test loss (w/o reg) on all data: 0.448996
Train acc on all data:  0.96035774716
Test acc on all data:   0.878260869565
Norm of the mean of gradients: 6.32953e-06
Norm of the params: 26.2449
     Influence (LOO): fixed 538 labels. Loss 0.44900. Accuracy 0.878.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100021
Train loss (w/o reg) on all data: 0.0534046
Test loss (w/o reg) on all data: 0.569883
Train acc on all data:  0.998549673677
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 1.3735e-05
Norm of the params: 30.5339
                Loss: fixed 490 labels. Loss 0.56988. Accuracy 0.852.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239775
Train loss (w/o reg) on all data: 0.18804
Test loss (w/o reg) on all data: 0.648252
Train acc on all data:  0.937635968093
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 3.85037e-05
Norm of the params: 32.1667
              Random: fixed 247 labels. Loss 0.64825. Accuracy 0.809.
### Flips: 1030, rs: 5, checks: 1236
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147567
Train loss (w/o reg) on all data: 0.114627
Test loss (w/o reg) on all data: 0.365868
Train acc on all data:  0.961808073483
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 1.08472e-05
Norm of the params: 25.667
     Influence (LOO): fixed 595 labels. Loss 0.36587. Accuracy 0.898.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0905741
Train loss (w/o reg) on all data: 0.0473364
Test loss (w/o reg) on all data: 0.51987
Train acc on all data:  0.998307952623
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 8.49138e-06
Norm of the params: 29.4067
                Loss: fixed 546 labels. Loss 0.51987. Accuracy 0.866.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231384
Train loss (w/o reg) on all data: 0.180848
Test loss (w/o reg) on all data: 0.608482
Train acc on all data:  0.941503504955
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 2.47407e-05
Norm of the params: 31.7918
              Random: fixed 291 labels. Loss 0.60848. Accuracy 0.816.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.274995
Train loss (w/o reg) on all data: 0.217961
Test loss (w/o reg) on all data: 0.814451
Train acc on all data:  0.929659173314
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 4.88822e-05
Norm of the params: 33.774
Flipped loss: 0.81445. Accuracy: 0.771
### Flips: 1030, rs: 6, checks: 206
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208752
Train loss (w/o reg) on all data: 0.158876
Test loss (w/o reg) on all data: 0.733129
Train acc on all data:  0.95044718395
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 5.20003e-05
Norm of the params: 31.5835
     Influence (LOO): fixed 155 labels. Loss 0.73313. Accuracy 0.814.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183912
Train loss (w/o reg) on all data: 0.123143
Test loss (w/o reg) on all data: 0.849457
Train acc on all data:  0.976794778825
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 5.9866e-05
Norm of the params: 34.8623
                Loss: fixed 186 labels. Loss 0.84946. Accuracy 0.786.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267975
Train loss (w/o reg) on all data: 0.211511
Test loss (w/o reg) on all data: 0.794416
Train acc on all data:  0.934251873338
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 6.2392e-05
Norm of the params: 33.6047
              Random: fixed  42 labels. Loss 0.79442. Accuracy 0.778.
### Flips: 1030, rs: 6, checks: 412
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183271
Train loss (w/o reg) on all data: 0.139164
Test loss (w/o reg) on all data: 0.635113
Train acc on all data:  0.956973652405
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 1.22791e-05
Norm of the params: 29.7006
     Influence (LOO): fixed 287 labels. Loss 0.63511. Accuracy 0.826.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146872
Train loss (w/o reg) on all data: 0.0887911
Test loss (w/o reg) on all data: 0.793634
Train acc on all data:  0.992023205221
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 2.11135e-05
Norm of the params: 34.0826
                Loss: fixed 299 labels. Loss 0.79363. Accuracy 0.809.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261173
Train loss (w/o reg) on all data: 0.205455
Test loss (w/o reg) on all data: 0.781512
Train acc on all data:  0.936910804931
Test acc on all data:   0.8
Norm of the mean of gradients: 7.17779e-05
Norm of the params: 33.3819
              Random: fixed  91 labels. Loss 0.78151. Accuracy 0.800.
### Flips: 1030, rs: 6, checks: 618
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169222
Train loss (w/o reg) on all data: 0.129437
Test loss (w/o reg) on all data: 0.514418
Train acc on all data:  0.959632583998
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 1.0407e-05
Norm of the params: 28.2079
     Influence (LOO): fixed 394 labels. Loss 0.51442. Accuracy 0.847.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119164
Train loss (w/o reg) on all data: 0.0668623
Test loss (w/o reg) on all data: 0.717378
Train acc on all data:  0.996374184191
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 1.08616e-05
Norm of the params: 32.3424
                Loss: fixed 398 labels. Loss 0.71738. Accuracy 0.826.
Using normal model
LBFGS training took [535] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25287
Train loss (w/o reg) on all data: 0.197915
Test loss (w/o reg) on all data: 0.725651
Train acc on all data:  0.941261783901
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.59959e-05
Norm of the params: 33.1525
              Random: fixed 142 labels. Loss 0.72565. Accuracy 0.803.
### Flips: 1030, rs: 6, checks: 824
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159347
Train loss (w/o reg) on all data: 0.12231
Test loss (w/o reg) on all data: 0.460294
Train acc on all data:  0.96035774716
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 2.18031e-05
Norm of the params: 27.2164
     Influence (LOO): fixed 469 labels. Loss 0.46029. Accuracy 0.870.
Using normal model
LBFGS training took [457] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109049
Train loss (w/o reg) on all data: 0.0596232
Test loss (w/o reg) on all data: 0.783841
Train acc on all data:  0.997341068407
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 3.69724e-05
Norm of the params: 31.4406
                Loss: fixed 450 labels. Loss 0.78384. Accuracy 0.823.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242441
Train loss (w/o reg) on all data: 0.188566
Test loss (w/o reg) on all data: 0.675789
Train acc on all data:  0.945371041818
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 0.000129615
Norm of the params: 32.8253
              Random: fixed 194 labels. Loss 0.67579. Accuracy 0.813.
### Flips: 1030, rs: 6, checks: 1030
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149128
Train loss (w/o reg) on all data: 0.114828
Test loss (w/o reg) on all data: 0.42327
Train acc on all data:  0.961566352429
Test acc on all data:   0.901449275362
Norm of the mean of gradients: 8.1305e-06
Norm of the params: 26.1916
     Influence (LOO): fixed 549 labels. Loss 0.42327. Accuracy 0.901.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0990293
Train loss (w/o reg) on all data: 0.0531564
Test loss (w/o reg) on all data: 0.722853
Train acc on all data:  0.998307952623
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 9.13834e-06
Norm of the params: 30.2896
                Loss: fixed 500 labels. Loss 0.72285. Accuracy 0.847.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232624
Train loss (w/o reg) on all data: 0.179723
Test loss (w/o reg) on all data: 0.608662
Train acc on all data:  0.948996857626
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 5.41214e-05
Norm of the params: 32.5275
              Random: fixed 245 labels. Loss 0.60866. Accuracy 0.820.
### Flips: 1030, rs: 6, checks: 1236
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139017
Train loss (w/o reg) on all data: 0.10721
Test loss (w/o reg) on all data: 0.345834
Train acc on all data:  0.96470872613
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 1.72854e-05
Norm of the params: 25.2216
     Influence (LOO): fixed 613 labels. Loss 0.34583. Accuracy 0.913.
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0909958
Train loss (w/o reg) on all data: 0.04807
Test loss (w/o reg) on all data: 0.722196
Train acc on all data:  0.998307952623
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 6.14985e-05
Norm of the params: 29.3004
                Loss: fixed 545 labels. Loss 0.72220. Accuracy 0.865.
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221737
Train loss (w/o reg) on all data: 0.169331
Test loss (w/o reg) on all data: 0.552338
Train acc on all data:  0.951172347111
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 3.59086e-05
Norm of the params: 32.3745
              Random: fixed 295 labels. Loss 0.55234. Accuracy 0.840.
Using normal model
LBFGS training took [608] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28228
Train loss (w/o reg) on all data: 0.225024
Test loss (w/o reg) on all data: 0.857484
Train acc on all data:  0.922649262751
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 0.000104257
Norm of the params: 33.8396
Flipped loss: 0.85748. Accuracy: 0.796
### Flips: 1030, rs: 7, checks: 206
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21869
Train loss (w/o reg) on all data: 0.168371
Test loss (w/o reg) on all data: 0.709576
Train acc on all data:  0.947063089195
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 1.67451e-05
Norm of the params: 31.7236
     Influence (LOO): fixed 159 labels. Loss 0.70958. Accuracy 0.820.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194231
Train loss (w/o reg) on all data: 0.132019
Test loss (w/o reg) on all data: 0.89072
Train acc on all data:  0.971235194585
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 4.3668e-05
Norm of the params: 35.2739
                Loss: fixed 184 labels. Loss 0.89072. Accuracy 0.783.
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272731
Train loss (w/o reg) on all data: 0.216066
Test loss (w/o reg) on all data: 0.785979
Train acc on all data:  0.925791636452
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 2.4184e-05
Norm of the params: 33.6645
              Random: fixed  58 labels. Loss 0.78598. Accuracy 0.807.
### Flips: 1030, rs: 7, checks: 412
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19476
Train loss (w/o reg) on all data: 0.150053
Test loss (w/o reg) on all data: 0.619392
Train acc on all data:  0.949480299734
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.88668e-05
Norm of the params: 29.9022
     Influence (LOO): fixed 281 labels. Loss 0.61939. Accuracy 0.851.
Using normal model
LBFGS training took [474] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151004
Train loss (w/o reg) on all data: 0.0914142
Test loss (w/o reg) on all data: 0.914877
Train acc on all data:  0.988639110467
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 4.6929e-05
Norm of the params: 34.5223
                Loss: fixed 308 labels. Loss 0.91488. Accuracy 0.799.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263372
Train loss (w/o reg) on all data: 0.207462
Test loss (w/o reg) on all data: 0.742098
Train acc on all data:  0.930867778584
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 2.76181e-05
Norm of the params: 33.4395
              Random: fixed 115 labels. Loss 0.74210. Accuracy 0.814.
### Flips: 1030, rs: 7, checks: 618
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179351
Train loss (w/o reg) on all data: 0.138364
Test loss (w/o reg) on all data: 0.500218
Train acc on all data:  0.954314720812
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 3.25083e-05
Norm of the params: 28.6312
     Influence (LOO): fixed 391 labels. Loss 0.50022. Accuracy 0.871.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12859
Train loss (w/o reg) on all data: 0.0736475
Test loss (w/o reg) on all data: 0.765389
Train acc on all data:  0.991781484167
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 8.76366e-05
Norm of the params: 33.1488
                Loss: fixed 397 labels. Loss 0.76539. Accuracy 0.810.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.255468
Train loss (w/o reg) on all data: 0.20028
Test loss (w/o reg) on all data: 0.717025
Train acc on all data:  0.931834662799
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 1.99951e-05
Norm of the params: 33.2227
              Random: fixed 159 labels. Loss 0.71702. Accuracy 0.812.
### Flips: 1030, rs: 7, checks: 824
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166265
Train loss (w/o reg) on all data: 0.128853
Test loss (w/o reg) on all data: 0.437661
Train acc on all data:  0.957215373459
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 1.78361e-05
Norm of the params: 27.354
     Influence (LOO): fixed 486 labels. Loss 0.43766. Accuracy 0.888.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116507
Train loss (w/o reg) on all data: 0.0650399
Test loss (w/o reg) on all data: 0.63648
Train acc on all data:  0.996615905245
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 9.33514e-06
Norm of the params: 32.0834
                Loss: fixed 449 labels. Loss 0.63648. Accuracy 0.826.
Using normal model
LBFGS training took [491] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245287
Train loss (w/o reg) on all data: 0.19067
Test loss (w/o reg) on all data: 0.665825
Train acc on all data:  0.938361131255
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 3.60041e-05
Norm of the params: 33.0507
              Random: fixed 210 labels. Loss 0.66582. Accuracy 0.824.
### Flips: 1030, rs: 7, checks: 1030
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153924
Train loss (w/o reg) on all data: 0.119305
Test loss (w/o reg) on all data: 0.364897
Train acc on all data:  0.95914914189
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 5.2058e-06
Norm of the params: 26.3129
     Influence (LOO): fixed 560 labels. Loss 0.36490. Accuracy 0.899.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104997
Train loss (w/o reg) on all data: 0.0573073
Test loss (w/o reg) on all data: 0.587698
Train acc on all data:  0.996857626299
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 3.68093e-05
Norm of the params: 30.8834
                Loss: fixed 495 labels. Loss 0.58770. Accuracy 0.848.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234104
Train loss (w/o reg) on all data: 0.180496
Test loss (w/o reg) on all data: 0.645717
Train acc on all data:  0.942953831279
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 3.82453e-05
Norm of the params: 32.744
              Random: fixed 261 labels. Loss 0.64572. Accuracy 0.828.
### Flips: 1030, rs: 7, checks: 1236
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144599
Train loss (w/o reg) on all data: 0.112649
Test loss (w/o reg) on all data: 0.325988
Train acc on all data:  0.961082910321
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 1.3148e-05
Norm of the params: 25.2785
     Influence (LOO): fixed 627 labels. Loss 0.32599. Accuracy 0.915.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094501
Train loss (w/o reg) on all data: 0.0502809
Test loss (w/o reg) on all data: 0.589004
Train acc on all data:  0.998066231569
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 2.93657e-05
Norm of the params: 29.7389
                Loss: fixed 541 labels. Loss 0.58900. Accuracy 0.859.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223676
Train loss (w/o reg) on all data: 0.171192
Test loss (w/o reg) on all data: 0.634861
Train acc on all data:  0.948271694465
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 5.35241e-05
Norm of the params: 32.3986
              Random: fixed 311 labels. Loss 0.63486. Accuracy 0.826.
Using normal model
LBFGS training took [574] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27219
Train loss (w/o reg) on all data: 0.217025
Test loss (w/o reg) on all data: 0.923458
Train acc on all data:  0.926033357505
Test acc on all data:   0.734299516908
Norm of the mean of gradients: 6.71387e-05
Norm of the params: 33.2159
Flipped loss: 0.92346. Accuracy: 0.734
### Flips: 1030, rs: 8, checks: 206
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207972
Train loss (w/o reg) on all data: 0.15969
Test loss (w/o reg) on all data: 0.769481
Train acc on all data:  0.949480299734
Test acc on all data:   0.772946859903
Norm of the mean of gradients: 1.19116e-05
Norm of the params: 31.0747
     Influence (LOO): fixed 158 labels. Loss 0.76948. Accuracy 0.773.
Using normal model
LBFGS training took [549] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17952
Train loss (w/o reg) on all data: 0.120105
Test loss (w/o reg) on all data: 0.85484
Train acc on all data:  0.974135847232
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 5.59421e-05
Norm of the params: 34.4718
                Loss: fixed 189 labels. Loss 0.85484. Accuracy 0.759.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261947
Train loss (w/o reg) on all data: 0.207379
Test loss (w/o reg) on all data: 0.871408
Train acc on all data:  0.931834662799
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 2.06415e-05
Norm of the params: 33.0356
              Random: fixed  64 labels. Loss 0.87141. Accuracy 0.756.
### Flips: 1030, rs: 8, checks: 412
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182472
Train loss (w/o reg) on all data: 0.139863
Test loss (w/o reg) on all data: 0.632439
Train acc on all data:  0.955765047136
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 2.97479e-05
Norm of the params: 29.192
     Influence (LOO): fixed 285 labels. Loss 0.63244. Accuracy 0.821.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140605
Train loss (w/o reg) on all data: 0.0848972
Test loss (w/o reg) on all data: 0.829599
Train acc on all data:  0.989364273628
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 1.68355e-05
Norm of the params: 33.3791
                Loss: fixed 305 labels. Loss 0.82960. Accuracy 0.781.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256258
Train loss (w/o reg) on all data: 0.202631
Test loss (w/o reg) on all data: 0.811676
Train acc on all data:  0.934493594392
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 3.96872e-05
Norm of the params: 32.7497
              Random: fixed 114 labels. Loss 0.81168. Accuracy 0.776.
### Flips: 1030, rs: 8, checks: 618
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171039
Train loss (w/o reg) on all data: 0.132092
Test loss (w/o reg) on all data: 0.494623
Train acc on all data:  0.956973652405
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 2.93308e-05
Norm of the params: 27.9093
     Influence (LOO): fixed 375 labels. Loss 0.49462. Accuracy 0.850.
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117299
Train loss (w/o reg) on all data: 0.065829
Test loss (w/o reg) on all data: 0.743369
Train acc on all data:  0.995890742084
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 3.75861e-05
Norm of the params: 32.0843
                Loss: fixed 385 labels. Loss 0.74337. Accuracy 0.807.
Using normal model
LBFGS training took [477] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249413
Train loss (w/o reg) on all data: 0.196615
Test loss (w/o reg) on all data: 0.798013
Train acc on all data:  0.935218757554
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 5.80417e-05
Norm of the params: 32.4957
              Random: fixed 160 labels. Loss 0.79801. Accuracy 0.782.
### Flips: 1030, rs: 8, checks: 824
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161508
Train loss (w/o reg) on all data: 0.125204
Test loss (w/o reg) on all data: 0.463608
Train acc on all data:  0.958423978729
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 2.15886e-05
Norm of the params: 26.9461
     Influence (LOO): fixed 466 labels. Loss 0.46361. Accuracy 0.862.
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106649
Train loss (w/o reg) on all data: 0.0580928
Test loss (w/o reg) on all data: 0.690366
Train acc on all data:  0.997099347353
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 3.70778e-05
Norm of the params: 31.1628
                Loss: fixed 432 labels. Loss 0.69037. Accuracy 0.815.
Using normal model
LBFGS training took [549] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240357
Train loss (w/o reg) on all data: 0.188094
Test loss (w/o reg) on all data: 0.769195
Train acc on all data:  0.941020062847
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 6.40427e-05
Norm of the params: 32.3303
              Random: fixed 210 labels. Loss 0.76919. Accuracy 0.796.
### Flips: 1030, rs: 8, checks: 1030
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151975
Train loss (w/o reg) on all data: 0.118417
Test loss (w/o reg) on all data: 0.34274
Train acc on all data:  0.960116026106
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 5.06456e-06
Norm of the params: 25.9068
     Influence (LOO): fixed 536 labels. Loss 0.34274. Accuracy 0.889.
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0974628
Train loss (w/o reg) on all data: 0.0522276
Test loss (w/o reg) on all data: 0.613385
Train acc on all data:  0.997824510515
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 3.01558e-05
Norm of the params: 30.0783
                Loss: fixed 476 labels. Loss 0.61338. Accuracy 0.839.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230096
Train loss (w/o reg) on all data: 0.17915
Test loss (w/o reg) on all data: 0.672669
Train acc on all data:  0.942953831279
Test acc on all data:   0.8
Norm of the mean of gradients: 2.00137e-05
Norm of the params: 31.9206
              Random: fixed 268 labels. Loss 0.67267. Accuracy 0.800.
### Flips: 1030, rs: 8, checks: 1236
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143647
Train loss (w/o reg) on all data: 0.112617
Test loss (w/o reg) on all data: 0.31942
Train acc on all data:  0.965192168238
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 1.0734e-05
Norm of the params: 24.9118
     Influence (LOO): fixed 604 labels. Loss 0.31942. Accuracy 0.911.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0902509
Train loss (w/o reg) on all data: 0.0477539
Test loss (w/o reg) on all data: 0.58856
Train acc on all data:  0.997582789461
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 2.35135e-05
Norm of the params: 29.1537
                Loss: fixed 520 labels. Loss 0.58856. Accuracy 0.843.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219659
Train loss (w/o reg) on all data: 0.169818
Test loss (w/o reg) on all data: 0.647957
Train acc on all data:  0.948271694465
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 1.28177e-05
Norm of the params: 31.5724
              Random: fixed 319 labels. Loss 0.64796. Accuracy 0.816.
Using normal model
LBFGS training took [577] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271012
Train loss (w/o reg) on all data: 0.214956
Test loss (w/o reg) on all data: 0.70817
Train acc on all data:  0.928450568044
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 2.70812e-05
Norm of the params: 33.4831
Flipped loss: 0.70817. Accuracy: 0.770
### Flips: 1030, rs: 9, checks: 206
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211628
Train loss (w/o reg) on all data: 0.163125
Test loss (w/o reg) on all data: 0.639338
Train acc on all data:  0.94488759971
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 0.000125488
Norm of the params: 31.1457
     Influence (LOO): fixed 156 labels. Loss 0.63934. Accuracy 0.820.
Using normal model
LBFGS training took [521] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180903
Train loss (w/o reg) on all data: 0.121215
Test loss (w/o reg) on all data: 0.762366
Train acc on all data:  0.973652405124
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 2.59682e-05
Norm of the params: 34.5508
                Loss: fixed 190 labels. Loss 0.76237. Accuracy 0.778.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26205
Train loss (w/o reg) on all data: 0.206663
Test loss (w/o reg) on all data: 0.704306
Train acc on all data:  0.931109499637
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 1.73669e-05
Norm of the params: 33.2826
              Random: fixed  45 labels. Loss 0.70431. Accuracy 0.786.
### Flips: 1030, rs: 9, checks: 412
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190516
Train loss (w/o reg) on all data: 0.147277
Test loss (w/o reg) on all data: 0.545118
Train acc on all data:  0.950930626058
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 5.0041e-05
Norm of the params: 29.4073
     Influence (LOO): fixed 281 labels. Loss 0.54512. Accuracy 0.857.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146043
Train loss (w/o reg) on all data: 0.0888543
Test loss (w/o reg) on all data: 0.709236
Train acc on all data:  0.989122552574
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 4.59957e-05
Norm of the params: 33.8199
                Loss: fixed 303 labels. Loss 0.70924. Accuracy 0.811.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254164
Train loss (w/o reg) on all data: 0.200315
Test loss (w/o reg) on all data: 0.687535
Train acc on all data:  0.931834662799
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 3.04454e-05
Norm of the params: 32.8173
              Random: fixed 106 labels. Loss 0.68753. Accuracy 0.789.
### Flips: 1030, rs: 9, checks: 618
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176968
Train loss (w/o reg) on all data: 0.137173
Test loss (w/o reg) on all data: 0.527967
Train acc on all data:  0.952864394489
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 1.21033e-05
Norm of the params: 28.2118
     Influence (LOO): fixed 381 labels. Loss 0.52797. Accuracy 0.863.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124935
Train loss (w/o reg) on all data: 0.0721905
Test loss (w/o reg) on all data: 0.623962
Train acc on all data:  0.991539763113
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 3.97175e-05
Norm of the params: 32.479
                Loss: fixed 391 labels. Loss 0.62396. Accuracy 0.826.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247466
Train loss (w/o reg) on all data: 0.194385
Test loss (w/o reg) on all data: 0.646362
Train acc on all data:  0.936427362823
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 8.0261e-05
Norm of the params: 32.5826
              Random: fixed 150 labels. Loss 0.64636. Accuracy 0.802.
### Flips: 1030, rs: 9, checks: 824
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164021
Train loss (w/o reg) on all data: 0.127998
Test loss (w/o reg) on all data: 0.424659
Train acc on all data:  0.955039883974
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 1.6968e-05
Norm of the params: 26.8413
     Influence (LOO): fixed 475 labels. Loss 0.42466. Accuracy 0.893.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109771
Train loss (w/o reg) on all data: 0.0611402
Test loss (w/o reg) on all data: 0.625186
Train acc on all data:  0.992990089437
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 1.92016e-05
Norm of the params: 31.1867
                Loss: fixed 452 labels. Loss 0.62519. Accuracy 0.842.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241742
Train loss (w/o reg) on all data: 0.189655
Test loss (w/o reg) on all data: 0.636607
Train acc on all data:  0.937877689147
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 6.10061e-05
Norm of the params: 32.2759
              Random: fixed 200 labels. Loss 0.63661. Accuracy 0.805.
### Flips: 1030, rs: 9, checks: 1030
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154525
Train loss (w/o reg) on all data: 0.120761
Test loss (w/o reg) on all data: 0.365174
Train acc on all data:  0.955765047136
Test acc on all data:   0.914009661836
Norm of the mean of gradients: 1.83113e-05
Norm of the params: 25.9864
     Influence (LOO): fixed 547 labels. Loss 0.36517. Accuracy 0.914.
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097405
Train loss (w/o reg) on all data: 0.0522975
Test loss (w/o reg) on all data: 0.543859
Train acc on all data:  0.996857626299
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 5.61584e-05
Norm of the params: 30.0358
                Loss: fixed 516 labels. Loss 0.54386. Accuracy 0.858.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232411
Train loss (w/o reg) on all data: 0.181626
Test loss (w/o reg) on all data: 0.588945
Train acc on all data:  0.941020062847
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 7.35473e-05
Norm of the params: 31.8703
              Random: fixed 258 labels. Loss 0.58894. Accuracy 0.816.
### Flips: 1030, rs: 9, checks: 1236
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144632
Train loss (w/o reg) on all data: 0.112986
Test loss (w/o reg) on all data: 0.339571
Train acc on all data:  0.959874305052
Test acc on all data:   0.914975845411
Norm of the mean of gradients: 3.09592e-05
Norm of the params: 25.158
     Influence (LOO): fixed 616 labels. Loss 0.33957. Accuracy 0.915.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0904913
Train loss (w/o reg) on all data: 0.0477469
Test loss (w/o reg) on all data: 0.50382
Train acc on all data:  0.997824510515
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 3.03761e-05
Norm of the params: 29.2385
                Loss: fixed 557 labels. Loss 0.50382. Accuracy 0.870.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222365
Train loss (w/o reg) on all data: 0.172729
Test loss (w/o reg) on all data: 0.611614
Train acc on all data:  0.943920715494
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 2.02141e-05
Norm of the params: 31.5075
              Random: fixed 306 labels. Loss 0.61161. Accuracy 0.819.
Using normal model
LBFGS training took [623] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279007
Train loss (w/o reg) on all data: 0.223357
Test loss (w/o reg) on all data: 0.936579
Train acc on all data:  0.923374425912
Test acc on all data:   0.757487922705
Norm of the mean of gradients: 4.69064e-05
Norm of the params: 33.3616
Flipped loss: 0.93658. Accuracy: 0.757
### Flips: 1030, rs: 10, checks: 206
Using normal model
LBFGS training took [390] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214464
Train loss (w/o reg) on all data: 0.166682
Test loss (w/o reg) on all data: 0.819364
Train acc on all data:  0.942228668117
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 1.52159e-05
Norm of the params: 30.9135
     Influence (LOO): fixed 169 labels. Loss 0.81936. Accuracy 0.776.
Using normal model
LBFGS training took [564] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19232
Train loss (w/o reg) on all data: 0.131182
Test loss (w/o reg) on all data: 0.991194
Train acc on all data:  0.964950447184
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 5.73126e-05
Norm of the params: 34.968
                Loss: fixed 191 labels. Loss 0.99119. Accuracy 0.752.
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273628
Train loss (w/o reg) on all data: 0.219198
Test loss (w/o reg) on all data: 0.877206
Train acc on all data:  0.921682378535
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 0.000129416
Norm of the params: 32.9938
              Random: fixed  51 labels. Loss 0.87721. Accuracy 0.768.
### Flips: 1030, rs: 10, checks: 412
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192927
Train loss (w/o reg) on all data: 0.149968
Test loss (w/o reg) on all data: 0.620128
Train acc on all data:  0.948029973411
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 3.92238e-05
Norm of the params: 29.3116
     Influence (LOO): fixed 291 labels. Loss 0.62013. Accuracy 0.812.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151635
Train loss (w/o reg) on all data: 0.093059
Test loss (w/o reg) on all data: 0.842169
Train acc on all data:  0.988397389413
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 2.71754e-05
Norm of the params: 34.2274
                Loss: fixed 312 labels. Loss 0.84217. Accuracy 0.783.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266256
Train loss (w/o reg) on all data: 0.212631
Test loss (w/o reg) on all data: 0.815105
Train acc on all data:  0.926516799613
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 6.36338e-05
Norm of the params: 32.7491
              Random: fixed 103 labels. Loss 0.81510. Accuracy 0.771.
### Flips: 1030, rs: 10, checks: 618
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179141
Train loss (w/o reg) on all data: 0.139591
Test loss (w/o reg) on all data: 0.532495
Train acc on all data:  0.951655789219
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 7.55068e-06
Norm of the params: 28.125
     Influence (LOO): fixed 382 labels. Loss 0.53250. Accuracy 0.833.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129964
Train loss (w/o reg) on all data: 0.0753993
Test loss (w/o reg) on all data: 0.78725
Train acc on all data:  0.992990089437
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 3.74388e-05
Norm of the params: 33.0346
                Loss: fixed 385 labels. Loss 0.78725. Accuracy 0.792.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259115
Train loss (w/o reg) on all data: 0.205849
Test loss (w/o reg) on all data: 0.809428
Train acc on all data:  0.930142615422
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 7.46513e-05
Norm of the params: 32.639
              Random: fixed 148 labels. Loss 0.80943. Accuracy 0.788.
### Flips: 1030, rs: 10, checks: 824
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.165746
Train loss (w/o reg) on all data: 0.129901
Test loss (w/o reg) on all data: 0.438069
Train acc on all data:  0.95600676819
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.57158e-05
Norm of the params: 26.7752
     Influence (LOO): fixed 478 labels. Loss 0.43807. Accuracy 0.865.
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116251
Train loss (w/o reg) on all data: 0.0650093
Test loss (w/o reg) on all data: 0.713458
Train acc on all data:  0.993956973652
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 4.11204e-05
Norm of the params: 32.0129
                Loss: fixed 444 labels. Loss 0.71346. Accuracy 0.815.
Using normal model
LBFGS training took [536] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249196
Train loss (w/o reg) on all data: 0.196822
Test loss (w/o reg) on all data: 0.695741
Train acc on all data:  0.931351220691
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 2.19274e-05
Norm of the params: 32.3647
              Random: fixed 200 labels. Loss 0.69574. Accuracy 0.808.
### Flips: 1030, rs: 10, checks: 1030
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155024
Train loss (w/o reg) on all data: 0.122089
Test loss (w/o reg) on all data: 0.382056
Train acc on all data:  0.956731931351
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 1.6614e-05
Norm of the params: 25.6648
     Influence (LOO): fixed 548 labels. Loss 0.38206. Accuracy 0.884.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106838
Train loss (w/o reg) on all data: 0.0586305
Test loss (w/o reg) on all data: 0.628547
Train acc on all data:  0.994198694706
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 3.76311e-05
Norm of the params: 31.0507
                Loss: fixed 491 labels. Loss 0.62855. Accuracy 0.840.
Using normal model
LBFGS training took [537] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243127
Train loss (w/o reg) on all data: 0.191503
Test loss (w/o reg) on all data: 0.640526
Train acc on all data:  0.933284989123
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 3.94258e-05
Norm of the params: 32.1325
              Random: fixed 247 labels. Loss 0.64053. Accuracy 0.812.
### Flips: 1030, rs: 10, checks: 1236
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147788
Train loss (w/o reg) on all data: 0.116587
Test loss (w/o reg) on all data: 0.332675
Train acc on all data:  0.958423978729
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 4.13551e-05
Norm of the params: 24.9805
     Influence (LOO): fixed 599 labels. Loss 0.33268. Accuracy 0.899.
Using normal model
LBFGS training took [484] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0972313
Train loss (w/o reg) on all data: 0.0525754
Test loss (w/o reg) on all data: 0.636386
Train acc on all data:  0.994198694706
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 1.92806e-05
Norm of the params: 29.8851
                Loss: fixed 543 labels. Loss 0.63639. Accuracy 0.854.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.233822
Train loss (w/o reg) on all data: 0.183348
Test loss (w/o reg) on all data: 0.60353
Train acc on all data:  0.936427362823
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 6.89506e-05
Norm of the params: 31.7723
              Random: fixed 296 labels. Loss 0.60353. Accuracy 0.815.
Using normal model
LBFGS training took [695] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.280542
Train loss (w/o reg) on all data: 0.225676
Test loss (w/o reg) on all data: 0.836043
Train acc on all data:  0.920473773266
Test acc on all data:   0.743961352657
Norm of the mean of gradients: 5.79345e-05
Norm of the params: 33.1258
Flipped loss: 0.83604. Accuracy: 0.744
### Flips: 1030, rs: 11, checks: 206
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220744
Train loss (w/o reg) on all data: 0.171706
Test loss (w/o reg) on all data: 0.709
Train acc on all data:  0.941261783901
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 2.15755e-05
Norm of the params: 31.3173
     Influence (LOO): fixed 151 labels. Loss 0.70900. Accuracy 0.791.
Using normal model
LBFGS training took [596] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188451
Train loss (w/o reg) on all data: 0.128538
Test loss (w/o reg) on all data: 0.818543
Train acc on all data:  0.969784868262
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 9.03149e-05
Norm of the params: 34.6158
                Loss: fixed 191 labels. Loss 0.81854. Accuracy 0.756.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271112
Train loss (w/o reg) on all data: 0.216939
Test loss (w/o reg) on all data: 0.782459
Train acc on all data:  0.924824752236
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 5.82682e-05
Norm of the params: 32.9161
              Random: fixed  48 labels. Loss 0.78246. Accuracy 0.761.
### Flips: 1030, rs: 11, checks: 412
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196636
Train loss (w/o reg) on all data: 0.152313
Test loss (w/o reg) on all data: 0.56859
Train acc on all data:  0.948513415518
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 1.78877e-05
Norm of the params: 29.7734
     Influence (LOO): fixed 277 labels. Loss 0.56859. Accuracy 0.829.
Using normal model
LBFGS training took [617] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148536
Train loss (w/o reg) on all data: 0.0918881
Test loss (w/o reg) on all data: 0.797895
Train acc on all data:  0.987188784143
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 4.41793e-05
Norm of the params: 33.6595
                Loss: fixed 301 labels. Loss 0.79790. Accuracy 0.790.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262314
Train loss (w/o reg) on all data: 0.20915
Test loss (w/o reg) on all data: 0.802129
Train acc on all data:  0.928692289098
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 8.15872e-05
Norm of the params: 32.6083
              Random: fixed  99 labels. Loss 0.80213. Accuracy 0.768.
### Flips: 1030, rs: 11, checks: 618
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181604
Train loss (w/o reg) on all data: 0.141007
Test loss (w/o reg) on all data: 0.502075
Train acc on all data:  0.954072999758
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 1.614e-05
Norm of the params: 28.4945
     Influence (LOO): fixed 372 labels. Loss 0.50208. Accuracy 0.856.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127237
Train loss (w/o reg) on all data: 0.0729814
Test loss (w/o reg) on all data: 0.733495
Train acc on all data:  0.99444041576
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 6.38071e-05
Norm of the params: 32.9412
                Loss: fixed 370 labels. Loss 0.73349. Accuracy 0.812.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254033
Train loss (w/o reg) on all data: 0.201478
Test loss (w/o reg) on all data: 0.791501
Train acc on all data:  0.931351220691
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 5.05449e-05
Norm of the params: 32.4207
              Random: fixed 150 labels. Loss 0.79150. Accuracy 0.783.
### Flips: 1030, rs: 11, checks: 824
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.165392
Train loss (w/o reg) on all data: 0.128725
Test loss (w/o reg) on all data: 0.452789
Train acc on all data:  0.957698815567
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 2.62218e-05
Norm of the params: 27.0804
     Influence (LOO): fixed 462 labels. Loss 0.45279. Accuracy 0.876.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111019
Train loss (w/o reg) on all data: 0.0611251
Test loss (w/o reg) on all data: 0.647251
Train acc on all data:  0.995890742084
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 4.17299e-05
Norm of the params: 31.5893
                Loss: fixed 437 labels. Loss 0.64725. Accuracy 0.829.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248115
Train loss (w/o reg) on all data: 0.196604
Test loss (w/o reg) on all data: 0.756902
Train acc on all data:  0.934251873338
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 2.63817e-05
Norm of the params: 32.0971
              Random: fixed 210 labels. Loss 0.75690. Accuracy 0.791.
### Flips: 1030, rs: 11, checks: 1030
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156285
Train loss (w/o reg) on all data: 0.122056
Test loss (w/o reg) on all data: 0.391506
Train acc on all data:  0.960116026106
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 6.817e-06
Norm of the params: 26.1644
     Influence (LOO): fixed 535 labels. Loss 0.39151. Accuracy 0.888.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0980215
Train loss (w/o reg) on all data: 0.0522098
Test loss (w/o reg) on all data: 0.568267
Train acc on all data:  0.997582789461
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 1.0755e-05
Norm of the params: 30.2693
                Loss: fixed 500 labels. Loss 0.56827. Accuracy 0.843.
Using normal model
LBFGS training took [544] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241133
Train loss (w/o reg) on all data: 0.190459
Test loss (w/o reg) on all data: 0.754351
Train acc on all data:  0.935943920715
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.93377e-05
Norm of the params: 31.8352
              Random: fixed 258 labels. Loss 0.75435. Accuracy 0.803.
### Flips: 1030, rs: 11, checks: 1236
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147037
Train loss (w/o reg) on all data: 0.115115
Test loss (w/o reg) on all data: 0.366905
Train acc on all data:  0.962774957699
Test acc on all data:   0.903381642512
Norm of the mean of gradients: 1.26338e-05
Norm of the params: 25.2673
     Influence (LOO): fixed 599 labels. Loss 0.36690. Accuracy 0.903.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0912019
Train loss (w/o reg) on all data: 0.0480219
Test loss (w/o reg) on all data: 0.544738
Train acc on all data:  0.998549673677
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.06961e-05
Norm of the params: 29.3871
                Loss: fixed 543 labels. Loss 0.54474. Accuracy 0.860.
Using normal model
LBFGS training took [532] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234058
Train loss (w/o reg) on all data: 0.183968
Test loss (w/o reg) on all data: 0.719715
Train acc on all data:  0.941745226009
Test acc on all data:   0.8
Norm of the mean of gradients: 3.23124e-05
Norm of the params: 31.6512
              Random: fixed 303 labels. Loss 0.71971. Accuracy 0.800.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.281314
Train loss (w/o reg) on all data: 0.227111
Test loss (w/o reg) on all data: 0.862015
Train acc on all data:  0.918540004834
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 6.75683e-05
Norm of the params: 32.925
Flipped loss: 0.86202. Accuracy: 0.753
### Flips: 1030, rs: 12, checks: 206
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222475
Train loss (w/o reg) on all data: 0.17429
Test loss (w/o reg) on all data: 0.823415
Train acc on all data:  0.938361131255
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 1.50391e-05
Norm of the params: 31.0433
     Influence (LOO): fixed 154 labels. Loss 0.82341. Accuracy 0.782.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19136
Train loss (w/o reg) on all data: 0.13248
Test loss (w/o reg) on all data: 0.919745
Train acc on all data:  0.966884215615
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 7.00141e-05
Norm of the params: 34.3162
                Loss: fixed 187 labels. Loss 0.91974. Accuracy 0.767.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273688
Train loss (w/o reg) on all data: 0.220144
Test loss (w/o reg) on all data: 0.809426
Train acc on all data:  0.922407541697
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 7.4391e-05
Norm of the params: 32.7242
              Random: fixed  56 labels. Loss 0.80943. Accuracy 0.776.
### Flips: 1030, rs: 12, checks: 412
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194405
Train loss (w/o reg) on all data: 0.151435
Test loss (w/o reg) on all data: 0.703581
Train acc on all data:  0.95044718395
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 6.11221e-05
Norm of the params: 29.3158
     Influence (LOO): fixed 292 labels. Loss 0.70358. Accuracy 0.809.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153081
Train loss (w/o reg) on all data: 0.0958951
Test loss (w/o reg) on all data: 0.836341
Train acc on all data:  0.985255015712
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 6.09813e-05
Norm of the params: 33.8189
                Loss: fixed 305 labels. Loss 0.83634. Accuracy 0.791.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265751
Train loss (w/o reg) on all data: 0.213393
Test loss (w/o reg) on all data: 0.72282
Train acc on all data:  0.925549915398
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 7.23037e-05
Norm of the params: 32.3599
              Random: fixed 109 labels. Loss 0.72282. Accuracy 0.786.
### Flips: 1030, rs: 12, checks: 618
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180909
Train loss (w/o reg) on all data: 0.14053
Test loss (w/o reg) on all data: 0.574448
Train acc on all data:  0.952380952381
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 2.08044e-05
Norm of the params: 28.4181
     Influence (LOO): fixed 378 labels. Loss 0.57445. Accuracy 0.828.
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131299
Train loss (w/o reg) on all data: 0.0771526
Test loss (w/o reg) on all data: 0.828534
Train acc on all data:  0.990331157844
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 3.15133e-05
Norm of the params: 32.9079
                Loss: fixed 382 labels. Loss 0.82853. Accuracy 0.799.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254142
Train loss (w/o reg) on all data: 0.202421
Test loss (w/o reg) on all data: 0.660988
Train acc on all data:  0.930142615422
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 1.66229e-05
Norm of the params: 32.1623
              Random: fixed 165 labels. Loss 0.66099. Accuracy 0.801.
### Flips: 1030, rs: 12, checks: 824
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174087
Train loss (w/o reg) on all data: 0.135563
Test loss (w/o reg) on all data: 0.523517
Train acc on all data:  0.954314720812
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 6.61657e-06
Norm of the params: 27.7575
     Influence (LOO): fixed 451 labels. Loss 0.52352. Accuracy 0.850.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115338
Train loss (w/o reg) on all data: 0.0650492
Test loss (w/o reg) on all data: 0.721855
Train acc on all data:  0.993231810491
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 1.02147e-05
Norm of the params: 31.7139
                Loss: fixed 452 labels. Loss 0.72186. Accuracy 0.813.
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244919
Train loss (w/o reg) on all data: 0.194058
Test loss (w/o reg) on all data: 0.632903
Train acc on all data:  0.93376843123
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 1.79502e-05
Norm of the params: 31.894
              Random: fixed 221 labels. Loss 0.63290. Accuracy 0.813.
### Flips: 1030, rs: 12, checks: 1030
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161595
Train loss (w/o reg) on all data: 0.125822
Test loss (w/o reg) on all data: 0.431692
Train acc on all data:  0.958423978729
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 7.59147e-05
Norm of the params: 26.7479
     Influence (LOO): fixed 522 labels. Loss 0.43169. Accuracy 0.877.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105688
Train loss (w/o reg) on all data: 0.0582112
Test loss (w/o reg) on all data: 0.657974
Train acc on all data:  0.993956973652
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 9.07354e-05
Norm of the params: 30.8147
                Loss: fixed 495 labels. Loss 0.65797. Accuracy 0.822.
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235472
Train loss (w/o reg) on all data: 0.185812
Test loss (w/o reg) on all data: 0.593697
Train acc on all data:  0.938602852308
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 1.74953e-05
Norm of the params: 31.5149
              Random: fixed 272 labels. Loss 0.59370. Accuracy 0.816.
### Flips: 1030, rs: 12, checks: 1236
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151516
Train loss (w/o reg) on all data: 0.118426
Test loss (w/o reg) on all data: 0.379281
Train acc on all data:  0.96035774716
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 8.20048e-06
Norm of the params: 25.7258
     Influence (LOO): fixed 595 labels. Loss 0.37928. Accuracy 0.892.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0961401
Train loss (w/o reg) on all data: 0.0516883
Test loss (w/o reg) on all data: 0.691078
Train acc on all data:  0.993956973652
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 1.2725e-05
Norm of the params: 29.8167
                Loss: fixed 542 labels. Loss 0.69108. Accuracy 0.837.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22931
Train loss (w/o reg) on all data: 0.180568
Test loss (w/o reg) on all data: 0.588782
Train acc on all data:  0.941261783901
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 6.42903e-05
Norm of the params: 31.2224
              Random: fixed 316 labels. Loss 0.58878. Accuracy 0.814.
Using normal model
LBFGS training took [643] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.277697
Train loss (w/o reg) on all data: 0.220328
Test loss (w/o reg) on all data: 0.77671
Train acc on all data:  0.928450568044
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 2.44747e-05
Norm of the params: 33.873
Flipped loss: 0.77671. Accuracy: 0.769
### Flips: 1030, rs: 13, checks: 206
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213391
Train loss (w/o reg) on all data: 0.163364
Test loss (w/o reg) on all data: 0.705872
Train acc on all data:  0.948029973411
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 4.89239e-05
Norm of the params: 31.6314
     Influence (LOO): fixed 157 labels. Loss 0.70587. Accuracy 0.806.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185357
Train loss (w/o reg) on all data: 0.124206
Test loss (w/o reg) on all data: 0.790787
Train acc on all data:  0.972927241963
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 1.33471e-05
Norm of the params: 34.9717
                Loss: fixed 186 labels. Loss 0.79079. Accuracy 0.784.
Using normal model
LBFGS training took [540] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271551
Train loss (w/o reg) on all data: 0.215529
Test loss (w/o reg) on all data: 0.721178
Train acc on all data:  0.92941745226
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 0.000111784
Norm of the params: 33.4731
              Random: fixed  53 labels. Loss 0.72118. Accuracy 0.788.
### Flips: 1030, rs: 13, checks: 412
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189888
Train loss (w/o reg) on all data: 0.145729
Test loss (w/o reg) on all data: 0.65352
Train acc on all data:  0.95358955765
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 2.5116e-05
Norm of the params: 29.7184
     Influence (LOO): fixed 278 labels. Loss 0.65352. Accuracy 0.835.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150766
Train loss (w/o reg) on all data: 0.0918609
Test loss (w/o reg) on all data: 0.784646
Train acc on all data:  0.987188784143
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 7.07829e-05
Norm of the params: 34.3234
                Loss: fixed 302 labels. Loss 0.78465. Accuracy 0.792.
Using normal model
LBFGS training took [528] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.264015
Train loss (w/o reg) on all data: 0.208998
Test loss (w/o reg) on all data: 0.63707
Train acc on all data:  0.932801547015
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 3.04854e-05
Norm of the params: 33.1712
              Random: fixed 116 labels. Loss 0.63707. Accuracy 0.803.
### Flips: 1030, rs: 13, checks: 618
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173449
Train loss (w/o reg) on all data: 0.132801
Test loss (w/o reg) on all data: 0.577092
Train acc on all data:  0.957940536621
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 1.39224e-05
Norm of the params: 28.5125
     Influence (LOO): fixed 383 labels. Loss 0.57709. Accuracy 0.865.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127135
Train loss (w/o reg) on all data: 0.0723401
Test loss (w/o reg) on all data: 0.741925
Train acc on all data:  0.992506647329
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 1.10308e-05
Norm of the params: 33.1045
                Loss: fixed 385 labels. Loss 0.74192. Accuracy 0.819.
Using normal model
LBFGS training took [555] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251575
Train loss (w/o reg) on all data: 0.197658
Test loss (w/o reg) on all data: 0.578515
Train acc on all data:  0.936910804931
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 3.74669e-05
Norm of the params: 32.8382
              Random: fixed 182 labels. Loss 0.57852. Accuracy 0.811.
### Flips: 1030, rs: 13, checks: 824
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162892
Train loss (w/o reg) on all data: 0.125149
Test loss (w/o reg) on all data: 0.492408
Train acc on all data:  0.958907420836
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 1.82266e-05
Norm of the params: 27.4748
     Influence (LOO): fixed 463 labels. Loss 0.49241. Accuracy 0.886.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116451
Train loss (w/o reg) on all data: 0.0642191
Test loss (w/o reg) on all data: 0.718163
Train acc on all data:  0.997341068407
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 8.75629e-06
Norm of the params: 32.3208
                Loss: fixed 428 labels. Loss 0.71816. Accuracy 0.831.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246123
Train loss (w/o reg) on all data: 0.193049
Test loss (w/o reg) on all data: 0.578706
Train acc on all data:  0.937394247039
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 9.65246e-05
Norm of the params: 32.5802
              Random: fixed 223 labels. Loss 0.57871. Accuracy 0.812.
### Flips: 1030, rs: 13, checks: 1030
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151825
Train loss (w/o reg) on all data: 0.117098
Test loss (w/o reg) on all data: 0.457658
Train acc on all data:  0.96035774716
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.23716e-05
Norm of the params: 26.3543
     Influence (LOO): fixed 543 labels. Loss 0.45766. Accuracy 0.906.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107012
Train loss (w/o reg) on all data: 0.0582001
Test loss (w/o reg) on all data: 0.665089
Train acc on all data:  0.997582789461
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 1.00668e-05
Norm of the params: 31.2448
                Loss: fixed 483 labels. Loss 0.66509. Accuracy 0.848.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23856
Train loss (w/o reg) on all data: 0.186492
Test loss (w/o reg) on all data: 0.57117
Train acc on all data:  0.941986947063
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 6.24558e-05
Norm of the params: 32.2701
              Random: fixed 267 labels. Loss 0.57117. Accuracy 0.820.
### Flips: 1030, rs: 13, checks: 1236
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141628
Train loss (w/o reg) on all data: 0.109473
Test loss (w/o reg) on all data: 0.442869
Train acc on all data:  0.963258399807
Test acc on all data:   0.915942028986
Norm of the mean of gradients: 1.49654e-05
Norm of the params: 25.3592
     Influence (LOO): fixed 609 labels. Loss 0.44287. Accuracy 0.916.
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0982848
Train loss (w/o reg) on all data: 0.0521937
Test loss (w/o reg) on all data: 0.630519
Train acc on all data:  0.997824510515
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 2.47943e-05
Norm of the params: 30.3615
                Loss: fixed 529 labels. Loss 0.63052. Accuracy 0.861.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231762
Train loss (w/o reg) on all data: 0.181173
Test loss (w/o reg) on all data: 0.593614
Train acc on all data:  0.942470389171
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 4.71723e-05
Norm of the params: 31.8084
              Random: fixed 318 labels. Loss 0.59361. Accuracy 0.833.
Using normal model
LBFGS training took [725] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282035
Train loss (w/o reg) on all data: 0.224657
Test loss (w/o reg) on all data: 0.775734
Train acc on all data:  0.92071549432
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 0.00018841
Norm of the params: 33.8757
Flipped loss: 0.77573. Accuracy: 0.757
### Flips: 1030, rs: 14, checks: 206
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221349
Train loss (w/o reg) on all data: 0.170914
Test loss (w/o reg) on all data: 0.709818
Train acc on all data:  0.942712110225
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 9.89052e-05
Norm of the params: 31.76
     Influence (LOO): fixed 166 labels. Loss 0.70982. Accuracy 0.819.
Using normal model
LBFGS training took [653] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200012
Train loss (w/o reg) on all data: 0.137739
Test loss (w/o reg) on all data: 0.751191
Train acc on all data:  0.967367657723
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 2.23944e-05
Norm of the params: 35.291
                Loss: fixed 189 labels. Loss 0.75119. Accuracy 0.780.
Using normal model
LBFGS training took [627] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273369
Train loss (w/o reg) on all data: 0.216776
Test loss (w/o reg) on all data: 0.809679
Train acc on all data:  0.927241962775
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 2.67537e-05
Norm of the params: 33.6431
              Random: fixed  52 labels. Loss 0.80968. Accuracy 0.757.
### Flips: 1030, rs: 14, checks: 412
Using normal model
LBFGS training took [576] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195173
Train loss (w/o reg) on all data: 0.149724
Test loss (w/o reg) on all data: 0.536001
Train acc on all data:  0.950205462896
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 4.08388e-05
Norm of the params: 30.1491
     Influence (LOO): fixed 286 labels. Loss 0.53600. Accuracy 0.841.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154944
Train loss (w/o reg) on all data: 0.0954304
Test loss (w/o reg) on all data: 0.645116
Train acc on all data:  0.988397389413
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 5.5242e-05
Norm of the params: 34.5004
                Loss: fixed 312 labels. Loss 0.64512. Accuracy 0.803.
Using normal model
LBFGS training took [652] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265757
Train loss (w/o reg) on all data: 0.209876
Test loss (w/o reg) on all data: 0.749449
Train acc on all data:  0.932076383853
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 4.51956e-05
Norm of the params: 33.4307
              Random: fixed 103 labels. Loss 0.74945. Accuracy 0.780.
### Flips: 1030, rs: 14, checks: 618
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182355
Train loss (w/o reg) on all data: 0.140463
Test loss (w/o reg) on all data: 0.480264
Train acc on all data:  0.952380952381
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 2.02987e-05
Norm of the params: 28.9455
     Influence (LOO): fixed 372 labels. Loss 0.48026. Accuracy 0.857.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134443
Train loss (w/o reg) on all data: 0.0785165
Test loss (w/o reg) on all data: 0.623833
Train acc on all data:  0.993473531545
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 3.10608e-05
Norm of the params: 33.4445
                Loss: fixed 386 labels. Loss 0.62383. Accuracy 0.824.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.258283
Train loss (w/o reg) on all data: 0.202726
Test loss (w/o reg) on all data: 0.685385
Train acc on all data:  0.933526710176
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 0.000105568
Norm of the params: 33.3339
              Random: fixed 152 labels. Loss 0.68538. Accuracy 0.789.
### Flips: 1030, rs: 14, checks: 824
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169186
Train loss (w/o reg) on all data: 0.131362
Test loss (w/o reg) on all data: 0.397676
Train acc on all data:  0.953106115543
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 0.000114776
Norm of the params: 27.5044
     Influence (LOO): fixed 454 labels. Loss 0.39768. Accuracy 0.877.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118543
Train loss (w/o reg) on all data: 0.0668889
Test loss (w/o reg) on all data: 0.545474
Train acc on all data:  0.994198694706
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 3.24538e-05
Norm of the params: 32.1417
                Loss: fixed 444 labels. Loss 0.54547. Accuracy 0.842.
Using normal model
LBFGS training took [592] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.250655
Train loss (w/o reg) on all data: 0.195991
Test loss (w/o reg) on all data: 0.646144
Train acc on all data:  0.937877689147
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 6.33407e-05
Norm of the params: 33.0647
              Random: fixed 204 labels. Loss 0.64614. Accuracy 0.803.
### Flips: 1030, rs: 14, checks: 1030
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159009
Train loss (w/o reg) on all data: 0.123839
Test loss (w/o reg) on all data: 0.364236
Train acc on all data:  0.955039883974
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 1.26095e-05
Norm of the params: 26.5216
     Influence (LOO): fixed 528 labels. Loss 0.36424. Accuracy 0.885.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10587
Train loss (w/o reg) on all data: 0.0577102
Test loss (w/o reg) on all data: 0.585124
Train acc on all data:  0.998307952623
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 4.03134e-05
Norm of the params: 31.0354
                Loss: fixed 502 labels. Loss 0.58512. Accuracy 0.845.
Using normal model
LBFGS training took [583] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242508
Train loss (w/o reg) on all data: 0.188853
Test loss (w/o reg) on all data: 0.588347
Train acc on all data:  0.940778341794
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 0.000215232
Norm of the params: 32.758
              Random: fixed 258 labels. Loss 0.58835. Accuracy 0.822.
### Flips: 1030, rs: 14, checks: 1236
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148434
Train loss (w/o reg) on all data: 0.11605
Test loss (w/o reg) on all data: 0.335382
Train acc on all data:  0.95914914189
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 4.98972e-05
Norm of the params: 25.4496
     Influence (LOO): fixed 596 labels. Loss 0.33538. Accuracy 0.900.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0949189
Train loss (w/o reg) on all data: 0.0505489
Test loss (w/o reg) on all data: 0.528669
Train acc on all data:  0.99879139473
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 1.73371e-05
Norm of the params: 29.7893
                Loss: fixed 546 labels. Loss 0.52867. Accuracy 0.871.
Using normal model
LBFGS training took [678] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234019
Train loss (w/o reg) on all data: 0.181961
Test loss (w/o reg) on all data: 0.538786
Train acc on all data:  0.942470389171
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 1.79048e-05
Norm of the params: 32.2669
              Random: fixed 312 labels. Loss 0.53879. Accuracy 0.837.
Using normal model
LBFGS training took [547] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271438
Train loss (w/o reg) on all data: 0.215561
Test loss (w/o reg) on all data: 1.026
Train acc on all data:  0.923374425912
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 0.000202584
Norm of the params: 33.4294
Flipped loss: 1.02600. Accuracy: 0.756
### Flips: 1030, rs: 15, checks: 206
Using normal model
LBFGS training took [425] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211346
Train loss (w/o reg) on all data: 0.163414
Test loss (w/o reg) on all data: 1.05069
Train acc on all data:  0.944645878656
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 4.56731e-05
Norm of the params: 30.9619
     Influence (LOO): fixed 162 labels. Loss 1.05069. Accuracy 0.795.
Using normal model
LBFGS training took [508] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181409
Train loss (w/o reg) on all data: 0.121321
Test loss (w/o reg) on all data: 1.06606
Train acc on all data:  0.971960357747
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 1.55965e-05
Norm of the params: 34.6666
                Loss: fixed 187 labels. Loss 1.06606. Accuracy 0.776.
Using normal model
LBFGS training took [502] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263225
Train loss (w/o reg) on all data: 0.20878
Test loss (w/o reg) on all data: 1.0159
Train acc on all data:  0.925791636452
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 3.71772e-05
Norm of the params: 32.9984
              Random: fixed  48 labels. Loss 1.01590. Accuracy 0.775.
### Flips: 1030, rs: 15, checks: 412
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189504
Train loss (w/o reg) on all data: 0.146685
Test loss (w/o reg) on all data: 0.926002
Train acc on all data:  0.948513415518
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 5.60939e-05
Norm of the params: 29.2639
     Influence (LOO): fixed 282 labels. Loss 0.92600. Accuracy 0.835.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147447
Train loss (w/o reg) on all data: 0.0887936
Test loss (w/o reg) on all data: 1.08813
Train acc on all data:  0.988397389413
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 1.78272e-05
Norm of the params: 34.2502
                Loss: fixed 290 labels. Loss 1.08813. Accuracy 0.772.
Using normal model
LBFGS training took [502] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.257803
Train loss (w/o reg) on all data: 0.204101
Test loss (w/o reg) on all data: 1.00889
Train acc on all data:  0.927241962775
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 7.12164e-05
Norm of the params: 32.7724
              Random: fixed 103 labels. Loss 1.00889. Accuracy 0.789.
### Flips: 1030, rs: 15, checks: 618
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171437
Train loss (w/o reg) on all data: 0.131805
Test loss (w/o reg) on all data: 0.855638
Train acc on all data:  0.955281605028
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 8.57057e-06
Norm of the params: 28.1539
     Influence (LOO): fixed 393 labels. Loss 0.85564. Accuracy 0.852.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127804
Train loss (w/o reg) on all data: 0.0727472
Test loss (w/o reg) on all data: 1.04368
Train acc on all data:  0.992023205221
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 2.04827e-05
Norm of the params: 33.1834
                Loss: fixed 368 labels. Loss 1.04368. Accuracy 0.804.
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.250564
Train loss (w/o reg) on all data: 0.197725
Test loss (w/o reg) on all data: 0.930551
Train acc on all data:  0.931351220691
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 0.000214513
Norm of the params: 32.5079
              Random: fixed 153 labels. Loss 0.93055. Accuracy 0.791.
### Flips: 1030, rs: 15, checks: 824
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161166
Train loss (w/o reg) on all data: 0.124665
Test loss (w/o reg) on all data: 0.712566
Train acc on all data:  0.959632583998
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 5.04486e-05
Norm of the params: 27.0191
     Influence (LOO): fixed 480 labels. Loss 0.71257. Accuracy 0.873.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116572
Train loss (w/o reg) on all data: 0.0651492
Test loss (w/o reg) on all data: 1.06627
Train acc on all data:  0.993473531545
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 9.74077e-06
Norm of the params: 32.0694
                Loss: fixed 419 labels. Loss 1.06627. Accuracy 0.808.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244289
Train loss (w/o reg) on all data: 0.192329
Test loss (w/o reg) on all data: 0.87805
Train acc on all data:  0.934493594392
Test acc on all data:   0.8
Norm of the mean of gradients: 1.7366e-05
Norm of the params: 32.2368
              Random: fixed 199 labels. Loss 0.87805. Accuracy 0.800.
### Flips: 1030, rs: 15, checks: 1030
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150045
Train loss (w/o reg) on all data: 0.116831
Test loss (w/o reg) on all data: 0.636851
Train acc on all data:  0.962533236645
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 6.07969e-06
Norm of the params: 25.7737
     Influence (LOO): fixed 558 labels. Loss 0.63685. Accuracy 0.888.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107368
Train loss (w/o reg) on all data: 0.0590758
Test loss (w/o reg) on all data: 0.966707
Train acc on all data:  0.997341068407
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 1.18869e-05
Norm of the params: 31.078
                Loss: fixed 471 labels. Loss 0.96671. Accuracy 0.833.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.234208
Train loss (w/o reg) on all data: 0.183832
Test loss (w/o reg) on all data: 0.757407
Train acc on all data:  0.936669083877
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.45219e-05
Norm of the params: 31.7414
              Random: fixed 253 labels. Loss 0.75741. Accuracy 0.824.
### Flips: 1030, rs: 15, checks: 1236
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143951
Train loss (w/o reg) on all data: 0.112576
Test loss (w/o reg) on all data: 0.62758
Train acc on all data:  0.963741841914
Test acc on all data:   0.905314009662
Norm of the mean of gradients: 1.05525e-05
Norm of the params: 25.0501
     Influence (LOO): fixed 614 labels. Loss 0.62758. Accuracy 0.905.
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0963999
Train loss (w/o reg) on all data: 0.0516533
Test loss (w/o reg) on all data: 0.895395
Train acc on all data:  0.998307952623
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 8.78334e-06
Norm of the params: 29.9154
                Loss: fixed 522 labels. Loss 0.89540. Accuracy 0.853.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224302
Train loss (w/o reg) on all data: 0.175562
Test loss (w/o reg) on all data: 0.669566
Train acc on all data:  0.939086294416
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 2.68882e-05
Norm of the params: 31.2218
              Random: fixed 301 labels. Loss 0.66957. Accuracy 0.831.
Using normal model
LBFGS training took [758] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.274617
Train loss (w/o reg) on all data: 0.21767
Test loss (w/o reg) on all data: 0.862446
Train acc on all data:  0.92385786802
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 7.41181e-05
Norm of the params: 33.7482
Flipped loss: 0.86245. Accuracy: 0.761
### Flips: 1030, rs: 16, checks: 206
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211909
Train loss (w/o reg) on all data: 0.162896
Test loss (w/o reg) on all data: 0.768017
Train acc on all data:  0.943920715494
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 4.13412e-05
Norm of the params: 31.3092
     Influence (LOO): fixed 163 labels. Loss 0.76802. Accuracy 0.816.
Using normal model
LBFGS training took [548] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194351
Train loss (w/o reg) on all data: 0.133695
Test loss (w/o reg) on all data: 0.89374
Train acc on all data:  0.966884215615
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 3.73325e-05
Norm of the params: 34.8298
                Loss: fixed 181 labels. Loss 0.89374. Accuracy 0.772.
Using normal model
LBFGS training took [571] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266842
Train loss (w/o reg) on all data: 0.210615
Test loss (w/o reg) on all data: 0.834402
Train acc on all data:  0.929175731206
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 6.51804e-05
Norm of the params: 33.5342
              Random: fixed  42 labels. Loss 0.83440. Accuracy 0.769.
### Flips: 1030, rs: 16, checks: 412
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191125
Train loss (w/o reg) on all data: 0.147529
Test loss (w/o reg) on all data: 0.59894
Train acc on all data:  0.949480299734
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 2.16398e-05
Norm of the params: 29.5282
     Influence (LOO): fixed 275 labels. Loss 0.59894. Accuracy 0.840.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158607
Train loss (w/o reg) on all data: 0.0999524
Test loss (w/o reg) on all data: 0.838535
Train acc on all data:  0.983562968335
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 5.18719e-05
Norm of the params: 34.2503
                Loss: fixed 293 labels. Loss 0.83853. Accuracy 0.799.
Using normal model
LBFGS training took [619] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260007
Train loss (w/o reg) on all data: 0.204376
Test loss (w/o reg) on all data: 0.789597
Train acc on all data:  0.93062605753
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 6.64517e-05
Norm of the params: 33.356
              Random: fixed  99 labels. Loss 0.78960. Accuracy 0.779.
### Flips: 1030, rs: 16, checks: 618
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177754
Train loss (w/o reg) on all data: 0.137636
Test loss (w/o reg) on all data: 0.49217
Train acc on all data:  0.955765047136
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 4.82108e-05
Norm of the params: 28.3259
     Influence (LOO): fixed 373 labels. Loss 0.49217. Accuracy 0.866.
Using normal model
LBFGS training took [591] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133664
Train loss (w/o reg) on all data: 0.0787507
Test loss (w/o reg) on all data: 0.77216
Train acc on all data:  0.990331157844
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 4.25134e-05
Norm of the params: 33.1402
                Loss: fixed 377 labels. Loss 0.77216. Accuracy 0.817.
Using normal model
LBFGS training took [617] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248846
Train loss (w/o reg) on all data: 0.194137
Test loss (w/o reg) on all data: 0.746834
Train acc on all data:  0.936669083877
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 9.10518e-05
Norm of the params: 33.0783
              Random: fixed 154 labels. Loss 0.74683. Accuracy 0.792.
### Flips: 1030, rs: 16, checks: 824
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16477
Train loss (w/o reg) on all data: 0.127994
Test loss (w/o reg) on all data: 0.429824
Train acc on all data:  0.958907420836
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 1.19298e-05
Norm of the params: 27.1204
     Influence (LOO): fixed 459 labels. Loss 0.42982. Accuracy 0.884.
Using normal model
LBFGS training took [555] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120053
Train loss (w/o reg) on all data: 0.0680885
Test loss (w/o reg) on all data: 0.737746
Train acc on all data:  0.992748368383
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 1.35513e-05
Norm of the params: 32.238
                Loss: fixed 432 labels. Loss 0.73775. Accuracy 0.831.
Using normal model
LBFGS training took [615] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246096
Train loss (w/o reg) on all data: 0.192171
Test loss (w/o reg) on all data: 0.702858
Train acc on all data:  0.936427362823
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 6.01581e-05
Norm of the params: 32.8403
              Random: fixed 199 labels. Loss 0.70286. Accuracy 0.813.
### Flips: 1030, rs: 16, checks: 1030
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156742
Train loss (w/o reg) on all data: 0.122044
Test loss (w/o reg) on all data: 0.378705
Train acc on all data:  0.959632583998
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 3.30865e-05
Norm of the params: 26.3428
     Influence (LOO): fixed 525 labels. Loss 0.37870. Accuracy 0.897.
Using normal model
LBFGS training took [512] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108771
Train loss (w/o reg) on all data: 0.0602846
Test loss (w/o reg) on all data: 0.720929
Train acc on all data:  0.992264926275
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 4.64377e-05
Norm of the params: 31.1404
                Loss: fixed 480 labels. Loss 0.72093. Accuracy 0.825.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240112
Train loss (w/o reg) on all data: 0.187654
Test loss (w/o reg) on all data: 0.648467
Train acc on all data:  0.937635968093
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 3.72693e-05
Norm of the params: 32.3909
              Random: fixed 248 labels. Loss 0.64847. Accuracy 0.811.
### Flips: 1030, rs: 16, checks: 1236
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148379
Train loss (w/o reg) on all data: 0.116091
Test loss (w/o reg) on all data: 0.34525
Train acc on all data:  0.962291515591
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 1.06969e-05
Norm of the params: 25.4119
     Influence (LOO): fixed 588 labels. Loss 0.34525. Accuracy 0.904.
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0994377
Train loss (w/o reg) on all data: 0.0539103
Test loss (w/o reg) on all data: 0.67068
Train acc on all data:  0.998307952623
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 2.19366e-05
Norm of the params: 30.1753
                Loss: fixed 535 labels. Loss 0.67068. Accuracy 0.849.
Using normal model
LBFGS training took [608] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228634
Train loss (w/o reg) on all data: 0.17741
Test loss (w/o reg) on all data: 0.585723
Train acc on all data:  0.941745226009
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 3.53226e-05
Norm of the params: 32.0073
              Random: fixed 306 labels. Loss 0.58572. Accuracy 0.826.
Using normal model
LBFGS training took [580] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.270546
Train loss (w/o reg) on all data: 0.212861
Test loss (w/o reg) on all data: 0.921033
Train acc on all data:  0.927000241721
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 0.000180156
Norm of the params: 33.9664
Flipped loss: 0.92103. Accuracy: 0.783
### Flips: 1030, rs: 17, checks: 206
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210416
Train loss (w/o reg) on all data: 0.160587
Test loss (w/o reg) on all data: 0.7209
Train acc on all data:  0.947304810249
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 1.86307e-05
Norm of the params: 31.5685
     Influence (LOO): fixed 162 labels. Loss 0.72090. Accuracy 0.823.
Using normal model
LBFGS training took [581] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182696
Train loss (w/o reg) on all data: 0.122031
Test loss (w/o reg) on all data: 0.920792
Train acc on all data:  0.971476915639
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 2.45211e-05
Norm of the params: 34.8324
                Loss: fixed 188 labels. Loss 0.92079. Accuracy 0.788.
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259666
Train loss (w/o reg) on all data: 0.203338
Test loss (w/o reg) on all data: 0.8969
Train acc on all data:  0.930867778584
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 9.54949e-05
Norm of the params: 33.5643
              Random: fixed  62 labels. Loss 0.89690. Accuracy 0.790.
### Flips: 1030, rs: 17, checks: 412
Using normal model
LBFGS training took [471] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186599
Train loss (w/o reg) on all data: 0.142645
Test loss (w/o reg) on all data: 0.678009
Train acc on all data:  0.95358955765
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 1.63617e-05
Norm of the params: 29.6494
     Influence (LOO): fixed 287 labels. Loss 0.67801. Accuracy 0.843.
Using normal model
LBFGS training took [625] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145713
Train loss (w/o reg) on all data: 0.0879615
Test loss (w/o reg) on all data: 0.812885
Train acc on all data:  0.987913947305
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 1.75692e-05
Norm of the params: 33.9857
                Loss: fixed 304 labels. Loss 0.81289. Accuracy 0.801.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253139
Train loss (w/o reg) on all data: 0.197341
Test loss (w/o reg) on all data: 0.8455
Train acc on all data:  0.933284989123
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 4.52522e-05
Norm of the params: 33.4061
              Random: fixed 107 labels. Loss 0.84550. Accuracy 0.805.
### Flips: 1030, rs: 17, checks: 618
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175156
Train loss (w/o reg) on all data: 0.134167
Test loss (w/o reg) on all data: 0.602388
Train acc on all data:  0.954556441866
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 9.90569e-05
Norm of the params: 28.6319
     Influence (LOO): fixed 381 labels. Loss 0.60239. Accuracy 0.870.
Using normal model
LBFGS training took [595] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128518
Train loss (w/o reg) on all data: 0.0738617
Test loss (w/o reg) on all data: 0.800952
Train acc on all data:  0.992264926275
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 1.14022e-05
Norm of the params: 33.0625
                Loss: fixed 366 labels. Loss 0.80095. Accuracy 0.817.
Using normal model
LBFGS training took [562] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245374
Train loss (w/o reg) on all data: 0.190637
Test loss (w/o reg) on all data: 0.768482
Train acc on all data:  0.936669083877
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 0.000122762
Norm of the params: 33.0869
              Random: fixed 163 labels. Loss 0.76848. Accuracy 0.816.
### Flips: 1030, rs: 17, checks: 824
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163169
Train loss (w/o reg) on all data: 0.12552
Test loss (w/o reg) on all data: 0.5732
Train acc on all data:  0.958665699782
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 1.47258e-05
Norm of the params: 27.4405
     Influence (LOO): fixed 461 labels. Loss 0.57320. Accuracy 0.897.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119065
Train loss (w/o reg) on all data: 0.0670494
Test loss (w/o reg) on all data: 0.770852
Train acc on all data:  0.992748368383
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 2.83417e-05
Norm of the params: 32.2538
                Loss: fixed 402 labels. Loss 0.77085. Accuracy 0.823.
Using normal model
LBFGS training took [600] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239593
Train loss (w/o reg) on all data: 0.186132
Test loss (w/o reg) on all data: 0.698132
Train acc on all data:  0.938361131255
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 8.28531e-05
Norm of the params: 32.6989
              Random: fixed 210 labels. Loss 0.69813. Accuracy 0.829.
### Flips: 1030, rs: 17, checks: 1030
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15118
Train loss (w/o reg) on all data: 0.117206
Test loss (w/o reg) on all data: 0.504419
Train acc on all data:  0.960116026106
Test acc on all data:   0.912077294686
Norm of the mean of gradients: 6.30014e-06
Norm of the params: 26.0668
     Influence (LOO): fixed 548 labels. Loss 0.50442. Accuracy 0.912.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109785
Train loss (w/o reg) on all data: 0.0601433
Test loss (w/o reg) on all data: 0.711461
Train acc on all data:  0.996857626299
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 9.80713e-06
Norm of the params: 31.5093
                Loss: fixed 442 labels. Loss 0.71146. Accuracy 0.830.
Using normal model
LBFGS training took [595] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228611
Train loss (w/o reg) on all data: 0.176003
Test loss (w/o reg) on all data: 0.707381
Train acc on all data:  0.944162436548
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 2.68094e-05
Norm of the params: 32.4369
              Random: fixed 275 labels. Loss 0.70738. Accuracy 0.846.
### Flips: 1030, rs: 17, checks: 1236
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143099
Train loss (w/o reg) on all data: 0.111554
Test loss (w/o reg) on all data: 0.490555
Train acc on all data:  0.962533236645
Test acc on all data:   0.921739130435
Norm of the mean of gradients: 6.14256e-06
Norm of the params: 25.1176
     Influence (LOO): fixed 601 labels. Loss 0.49056. Accuracy 0.922.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0989508
Train loss (w/o reg) on all data: 0.0530046
Test loss (w/o reg) on all data: 0.647471
Train acc on all data:  0.997824510515
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.06733e-05
Norm of the params: 30.3138
                Loss: fixed 499 labels. Loss 0.64747. Accuracy 0.851.
Using normal model
LBFGS training took [545] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219643
Train loss (w/o reg) on all data: 0.168164
Test loss (w/o reg) on all data: 0.648279
Train acc on all data:  0.947546531303
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 6.14467e-05
Norm of the params: 32.0869
              Random: fixed 315 labels. Loss 0.64828. Accuracy 0.858.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276052
Train loss (w/o reg) on all data: 0.221252
Test loss (w/o reg) on all data: 1.04552
Train acc on all data:  0.92506647329
Test acc on all data:   0.736231884058
Norm of the mean of gradients: 0.000180142
Norm of the params: 33.106
Flipped loss: 1.04552. Accuracy: 0.736
### Flips: 1030, rs: 18, checks: 206
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21435
Train loss (w/o reg) on all data: 0.166114
Test loss (w/o reg) on all data: 0.825057
Train acc on all data:  0.943437273387
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 3.42731e-05
Norm of the params: 31.0601
     Influence (LOO): fixed 158 labels. Loss 0.82506. Accuracy 0.788.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1904
Train loss (w/o reg) on all data: 0.129996
Test loss (w/o reg) on all data: 1.13758
Train acc on all data:  0.969301426154
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 1.12733e-05
Norm of the params: 34.7574
                Loss: fixed 190 labels. Loss 1.13758. Accuracy 0.752.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265681
Train loss (w/o reg) on all data: 0.212007
Test loss (w/o reg) on all data: 1.04328
Train acc on all data:  0.927241962775
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 1.74466e-05
Norm of the params: 32.7641
              Random: fixed  55 labels. Loss 1.04327. Accuracy 0.750.
### Flips: 1030, rs: 18, checks: 412
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191619
Train loss (w/o reg) on all data: 0.148765
Test loss (w/o reg) on all data: 0.745347
Train acc on all data:  0.948996857626
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 8.40035e-06
Norm of the params: 29.2761
     Influence (LOO): fixed 279 labels. Loss 0.74535. Accuracy 0.810.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153118
Train loss (w/o reg) on all data: 0.0947466
Test loss (w/o reg) on all data: 1.02558
Train acc on all data:  0.986463620981
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 2.60007e-05
Norm of the params: 34.1676
                Loss: fixed 296 labels. Loss 1.02558. Accuracy 0.763.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259945
Train loss (w/o reg) on all data: 0.207189
Test loss (w/o reg) on all data: 0.975634
Train acc on all data:  0.929900894368
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 4.20214e-05
Norm of the params: 32.4824
              Random: fixed 104 labels. Loss 0.97563. Accuracy 0.755.
### Flips: 1030, rs: 18, checks: 618
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178506
Train loss (w/o reg) on all data: 0.138424
Test loss (w/o reg) on all data: 0.712482
Train acc on all data:  0.954072999758
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 1.62472e-05
Norm of the params: 28.3132
     Influence (LOO): fixed 373 labels. Loss 0.71248. Accuracy 0.841.
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130728
Train loss (w/o reg) on all data: 0.076108
Test loss (w/o reg) on all data: 0.915665
Train acc on all data:  0.990814599952
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 3.32297e-05
Norm of the params: 33.0515
                Loss: fixed 386 labels. Loss 0.91566. Accuracy 0.782.
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251359
Train loss (w/o reg) on all data: 0.199615
Test loss (w/o reg) on all data: 0.959936
Train acc on all data:  0.93376843123
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 4.2845e-05
Norm of the params: 32.1695
              Random: fixed 158 labels. Loss 0.95994. Accuracy 0.753.
### Flips: 1030, rs: 18, checks: 824
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.168398
Train loss (w/o reg) on all data: 0.131485
Test loss (w/o reg) on all data: 0.604087
Train acc on all data:  0.955039883974
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 2.02306e-05
Norm of the params: 27.171
     Influence (LOO): fixed 467 labels. Loss 0.60409. Accuracy 0.861.
Using normal model
LBFGS training took [338] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115317
Train loss (w/o reg) on all data: 0.0647351
Test loss (w/o reg) on all data: 0.848642
Train acc on all data:  0.992990089437
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 9.71831e-06
Norm of the params: 31.8061
                Loss: fixed 445 labels. Loss 0.84864. Accuracy 0.792.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241101
Train loss (w/o reg) on all data: 0.190875
Test loss (w/o reg) on all data: 0.844353
Train acc on all data:  0.936427362823
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 1.2469e-05
Norm of the params: 31.6939
              Random: fixed 209 labels. Loss 0.84435. Accuracy 0.766.
### Flips: 1030, rs: 18, checks: 1030
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158171
Train loss (w/o reg) on all data: 0.124433
Test loss (w/o reg) on all data: 0.563003
Train acc on all data:  0.955765047136
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.9722e-05
Norm of the params: 25.9765
     Influence (LOO): fixed 532 labels. Loss 0.56300. Accuracy 0.882.
Using normal model
LBFGS training took [370] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104151
Train loss (w/o reg) on all data: 0.0572285
Test loss (w/o reg) on all data: 0.819906
Train acc on all data:  0.993473531545
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 8.39832e-06
Norm of the params: 30.6342
                Loss: fixed 497 labels. Loss 0.81991. Accuracy 0.819.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23394
Train loss (w/o reg) on all data: 0.184493
Test loss (w/o reg) on all data: 0.776917
Train acc on all data:  0.937877689147
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 1.79128e-05
Norm of the params: 31.4476
              Random: fixed 258 labels. Loss 0.77692. Accuracy 0.784.
### Flips: 1030, rs: 18, checks: 1236
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147735
Train loss (w/o reg) on all data: 0.116392
Test loss (w/o reg) on all data: 0.517222
Train acc on all data:  0.958665699782
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.40337e-05
Norm of the params: 25.0373
     Influence (LOO): fixed 605 labels. Loss 0.51722. Accuracy 0.894.
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0958681
Train loss (w/o reg) on all data: 0.0515418
Test loss (w/o reg) on all data: 0.826417
Train acc on all data:  0.994198694706
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 1.88184e-05
Norm of the params: 29.7746
                Loss: fixed 540 labels. Loss 0.82642. Accuracy 0.838.
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224467
Train loss (w/o reg) on all data: 0.175571
Test loss (w/o reg) on all data: 0.699044
Train acc on all data:  0.941986947063
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 4.39884e-05
Norm of the params: 31.2716
              Random: fixed 310 labels. Loss 0.69904. Accuracy 0.793.
Using normal model
LBFGS training took [597] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.274452
Train loss (w/o reg) on all data: 0.219585
Test loss (w/o reg) on all data: 1.00066
Train acc on all data:  0.927241962775
Test acc on all data:   0.744927536232
Norm of the mean of gradients: 6.73547e-05
Norm of the params: 33.1261
Flipped loss: 1.00066. Accuracy: 0.745
### Flips: 1030, rs: 19, checks: 206
Using normal model
LBFGS training took [411] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215134
Train loss (w/o reg) on all data: 0.168323
Test loss (w/o reg) on all data: 0.811474
Train acc on all data:  0.941745226009
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 3.18469e-05
Norm of the params: 30.5975
     Influence (LOO): fixed 160 labels. Loss 0.81147. Accuracy 0.786.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187751
Train loss (w/o reg) on all data: 0.127532
Test loss (w/o reg) on all data: 1.00741
Train acc on all data:  0.9690597051
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 1.27612e-05
Norm of the params: 34.7041
                Loss: fixed 186 labels. Loss 1.00741. Accuracy 0.757.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267455
Train loss (w/o reg) on all data: 0.213881
Test loss (w/o reg) on all data: 1.00583
Train acc on all data:  0.930142615422
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 0.000142994
Norm of the params: 32.7334
              Random: fixed  52 labels. Loss 1.00583. Accuracy 0.750.
### Flips: 1030, rs: 19, checks: 412
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192754
Train loss (w/o reg) on all data: 0.150686
Test loss (w/o reg) on all data: 0.739007
Train acc on all data:  0.948029973411
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 1.91607e-05
Norm of the params: 29.0064
     Influence (LOO): fixed 278 labels. Loss 0.73901. Accuracy 0.822.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147286
Train loss (w/o reg) on all data: 0.0892707
Test loss (w/o reg) on all data: 0.992798
Train acc on all data:  0.990814599952
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 2.57768e-05
Norm of the params: 34.0633
                Loss: fixed 302 labels. Loss 0.99280. Accuracy 0.781.
Using normal model
LBFGS training took [554] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261129
Train loss (w/o reg) on all data: 0.208299
Test loss (w/o reg) on all data: 0.934909
Train acc on all data:  0.933284989123
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 9.84355e-05
Norm of the params: 32.5055
              Random: fixed  99 labels. Loss 0.93491. Accuracy 0.759.
### Flips: 1030, rs: 19, checks: 618
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178427
Train loss (w/o reg) on all data: 0.139316
Test loss (w/o reg) on all data: 0.660489
Train acc on all data:  0.952622673435
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 2.1529e-05
Norm of the params: 27.9683
     Influence (LOO): fixed 378 labels. Loss 0.66049. Accuracy 0.840.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129607
Train loss (w/o reg) on all data: 0.0749427
Test loss (w/o reg) on all data: 0.872605
Train acc on all data:  0.993956973652
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 2.13466e-05
Norm of the params: 33.0648
                Loss: fixed 370 labels. Loss 0.87261. Accuracy 0.798.
Using normal model
LBFGS training took [547] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254896
Train loss (w/o reg) on all data: 0.202864
Test loss (w/o reg) on all data: 0.887207
Train acc on all data:  0.933043268069
Test acc on all data:   0.773913043478
Norm of the mean of gradients: 2.42615e-05
Norm of the params: 32.2589
              Random: fixed 146 labels. Loss 0.88721. Accuracy 0.774.
### Flips: 1030, rs: 19, checks: 824
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163975
Train loss (w/o reg) on all data: 0.127829
Test loss (w/o reg) on all data: 0.498218
Train acc on all data:  0.956973652405
Test acc on all data:   0.865700483092
Norm of the mean of gradients: 4.23509e-05
Norm of the params: 26.887
     Influence (LOO): fixed 461 labels. Loss 0.49822. Accuracy 0.866.
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117424
Train loss (w/o reg) on all data: 0.0663227
Test loss (w/o reg) on all data: 0.814814
Train acc on all data:  0.995890742084
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 8.00735e-06
Norm of the params: 31.9691
                Loss: fixed 419 labels. Loss 0.81481. Accuracy 0.805.
Using normal model
LBFGS training took [537] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24469
Train loss (w/o reg) on all data: 0.194232
Test loss (w/o reg) on all data: 0.806079
Train acc on all data:  0.935460478608
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 8.31869e-05
Norm of the params: 31.7673
              Random: fixed 203 labels. Loss 0.80608. Accuracy 0.795.
### Flips: 1030, rs: 19, checks: 1030
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153888
Train loss (w/o reg) on all data: 0.120264
Test loss (w/o reg) on all data: 0.441109
Train acc on all data:  0.958182257675
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 8.09631e-06
Norm of the params: 25.9323
     Influence (LOO): fixed 529 labels. Loss 0.44111. Accuracy 0.873.
Using normal model
LBFGS training took [358] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104549
Train loss (w/o reg) on all data: 0.0574236
Test loss (w/o reg) on all data: 0.736954
Train acc on all data:  0.997099347353
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 6.17392e-06
Norm of the params: 30.7003
                Loss: fixed 471 labels. Loss 0.73695. Accuracy 0.825.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238104
Train loss (w/o reg) on all data: 0.188283
Test loss (w/o reg) on all data: 0.784049
Train acc on all data:  0.940778341794
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 5.5506e-05
Norm of the params: 31.5663
              Random: fixed 251 labels. Loss 0.78405. Accuracy 0.797.
### Flips: 1030, rs: 19, checks: 1236
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146254
Train loss (w/o reg) on all data: 0.114546
Test loss (w/o reg) on all data: 0.401523
Train acc on all data:  0.95914914189
Test acc on all data:   0.888888888889
Norm of the mean of gradients: 1.64555e-05
Norm of the params: 25.1824
     Influence (LOO): fixed 593 labels. Loss 0.40152. Accuracy 0.889.
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0920686
Train loss (w/o reg) on all data: 0.0491405
Test loss (w/o reg) on all data: 0.684592
Train acc on all data:  0.997824510515
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 2.05958e-05
Norm of the params: 29.3012
                Loss: fixed 534 labels. Loss 0.68459. Accuracy 0.860.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229606
Train loss (w/o reg) on all data: 0.181564
Test loss (w/o reg) on all data: 0.829145
Train acc on all data:  0.941745226009
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 7.25076e-05
Norm of the params: 30.9974
              Random: fixed 310 labels. Loss 0.82915. Accuracy 0.811.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.274907
Train loss (w/o reg) on all data: 0.220637
Test loss (w/o reg) on all data: 0.924627
Train acc on all data:  0.919748610104
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 3.79235e-05
Norm of the params: 32.9455
Flipped loss: 0.92463. Accuracy: 0.776
### Flips: 1030, rs: 20, checks: 206
Using normal model
LBFGS training took [369] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21464
Train loss (w/o reg) on all data: 0.166526
Test loss (w/o reg) on all data: 0.688078
Train acc on all data:  0.943195552333
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.12272e-05
Norm of the params: 31.0204
     Influence (LOO): fixed 156 labels. Loss 0.68808. Accuracy 0.824.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189011
Train loss (w/o reg) on all data: 0.129781
Test loss (w/o reg) on all data: 1.01185
Train acc on all data:  0.970510031424
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 4.97754e-05
Norm of the params: 34.4181
                Loss: fixed 189 labels. Loss 1.01185. Accuracy 0.783.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272263
Train loss (w/o reg) on all data: 0.218072
Test loss (w/o reg) on all data: 0.906475
Train acc on all data:  0.921198936427
Test acc on all data:   0.765217391304
Norm of the mean of gradients: 7.18991e-05
Norm of the params: 32.9216
              Random: fixed  31 labels. Loss 0.90647. Accuracy 0.765.
### Flips: 1030, rs: 20, checks: 412
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191089
Train loss (w/o reg) on all data: 0.148162
Test loss (w/o reg) on all data: 0.708266
Train acc on all data:  0.950205462896
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.90109e-05
Norm of the params: 29.301
     Influence (LOO): fixed 288 labels. Loss 0.70827. Accuracy 0.840.
Using normal model
LBFGS training took [415] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152015
Train loss (w/o reg) on all data: 0.0944772
Test loss (w/o reg) on all data: 0.989667
Train acc on all data:  0.986221899927
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 2.20944e-05
Norm of the params: 33.9228
                Loss: fixed 303 labels. Loss 0.98967. Accuracy 0.796.
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265154
Train loss (w/o reg) on all data: 0.211876
Test loss (w/o reg) on all data: 0.862437
Train acc on all data:  0.925308194344
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 8.15404e-05
Norm of the params: 32.6429
              Random: fixed  88 labels. Loss 0.86244. Accuracy 0.786.
### Flips: 1030, rs: 20, checks: 618
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175666
Train loss (w/o reg) on all data: 0.13644
Test loss (w/o reg) on all data: 0.592416
Train acc on all data:  0.953347836597
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 2.85755e-05
Norm of the params: 28.0096
     Influence (LOO): fixed 393 labels. Loss 0.59242. Accuracy 0.858.
Using normal model
LBFGS training took [358] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126504
Train loss (w/o reg) on all data: 0.0735568
Test loss (w/o reg) on all data: 0.840367
Train acc on all data:  0.990814599952
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 9.46785e-06
Norm of the params: 32.5414
                Loss: fixed 395 labels. Loss 0.84037. Accuracy 0.813.
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.258719
Train loss (w/o reg) on all data: 0.206103
Test loss (w/o reg) on all data: 0.80076
Train acc on all data:  0.927967125937
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 4.23678e-05
Norm of the params: 32.4397
              Random: fixed 139 labels. Loss 0.80076. Accuracy 0.791.
### Flips: 1030, rs: 20, checks: 824
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1649
Train loss (w/o reg) on all data: 0.128281
Test loss (w/o reg) on all data: 0.530918
Train acc on all data:  0.955523326082
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 5.17165e-06
Norm of the params: 27.0625
     Influence (LOO): fixed 478 labels. Loss 0.53092. Accuracy 0.883.
Using normal model
LBFGS training took [346] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11571
Train loss (w/o reg) on all data: 0.0653524
Test loss (w/o reg) on all data: 0.835214
Train acc on all data:  0.992990089437
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 8.20772e-06
Norm of the params: 31.7355
                Loss: fixed 445 labels. Loss 0.83521. Accuracy 0.821.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248522
Train loss (w/o reg) on all data: 0.19706
Test loss (w/o reg) on all data: 0.747902
Train acc on all data:  0.931592941745
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 6.22417e-05
Norm of the params: 32.0818
              Random: fixed 193 labels. Loss 0.74790. Accuracy 0.802.
### Flips: 1030, rs: 20, checks: 1030
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154951
Train loss (w/o reg) on all data: 0.120949
Test loss (w/o reg) on all data: 0.57658
Train acc on all data:  0.957940536621
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 1.38974e-05
Norm of the params: 26.0773
     Influence (LOO): fixed 549 labels. Loss 0.57658. Accuracy 0.895.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104553
Train loss (w/o reg) on all data: 0.0572491
Test loss (w/o reg) on all data: 0.767004
Train acc on all data:  0.994198694706
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 8.16868e-06
Norm of the params: 30.7585
                Loss: fixed 497 labels. Loss 0.76700. Accuracy 0.834.
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240461
Train loss (w/o reg) on all data: 0.189735
Test loss (w/o reg) on all data: 0.689884
Train acc on all data:  0.935460478608
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 6.1011e-05
Norm of the params: 31.8515
              Random: fixed 238 labels. Loss 0.68988. Accuracy 0.816.
### Flips: 1030, rs: 20, checks: 1236
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146848
Train loss (w/o reg) on all data: 0.115164
Test loss (w/o reg) on all data: 0.60516
Train acc on all data:  0.95914914189
Test acc on all data:   0.894685990338
Norm of the mean of gradients: 3.63132e-06
Norm of the params: 25.1731
     Influence (LOO): fixed 616 labels. Loss 0.60516. Accuracy 0.895.
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0952072
Train loss (w/o reg) on all data: 0.05066
Test loss (w/o reg) on all data: 0.71298
Train acc on all data:  0.999033115784
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 3.99208e-05
Norm of the params: 29.8487
                Loss: fixed 542 labels. Loss 0.71298. Accuracy 0.850.
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23033
Train loss (w/o reg) on all data: 0.180037
Test loss (w/o reg) on all data: 0.706943
Train acc on all data:  0.939811457578
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 1.72311e-05
Norm of the params: 31.7154
              Random: fixed 287 labels. Loss 0.70694. Accuracy 0.818.
Using normal model
LBFGS training took [562] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272341
Train loss (w/o reg) on all data: 0.216858
Test loss (w/o reg) on all data: 0.909667
Train acc on all data:  0.922890983805
Test acc on all data:   0.744927536232
Norm of the mean of gradients: 0.00012431
Norm of the params: 33.3116
Flipped loss: 0.90967. Accuracy: 0.745
### Flips: 1030, rs: 21, checks: 206
Using normal model
LBFGS training took [415] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211167
Train loss (w/o reg) on all data: 0.162966
Test loss (w/o reg) on all data: 0.857912
Train acc on all data:  0.945854483926
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 2.82117e-05
Norm of the params: 31.0485
     Influence (LOO): fixed 158 labels. Loss 0.85791. Accuracy 0.779.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185456
Train loss (w/o reg) on all data: 0.124958
Test loss (w/o reg) on all data: 0.92232
Train acc on all data:  0.971718636693
Test acc on all data:   0.753623188406
Norm of the mean of gradients: 3.92643e-05
Norm of the params: 34.7843
                Loss: fixed 189 labels. Loss 0.92232. Accuracy 0.754.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266929
Train loss (w/o reg) on all data: 0.212438
Test loss (w/o reg) on all data: 0.915914
Train acc on all data:  0.927967125937
Test acc on all data:   0.753623188406
Norm of the mean of gradients: 3.17415e-05
Norm of the params: 33.0124
              Random: fixed  52 labels. Loss 0.91591. Accuracy 0.754.
### Flips: 1030, rs: 21, checks: 412
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189237
Train loss (w/o reg) on all data: 0.145612
Test loss (w/o reg) on all data: 0.691822
Train acc on all data:  0.953347836597
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 9.60507e-06
Norm of the params: 29.5381
     Influence (LOO): fixed 279 labels. Loss 0.69182. Accuracy 0.807.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148698
Train loss (w/o reg) on all data: 0.0908897
Test loss (w/o reg) on all data: 0.914619
Train acc on all data:  0.988397389413
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 0.000109229
Norm of the params: 34.0024
                Loss: fixed 293 labels. Loss 0.91462. Accuracy 0.769.
Using normal model
LBFGS training took [498] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262542
Train loss (w/o reg) on all data: 0.20924
Test loss (w/o reg) on all data: 0.857583
Train acc on all data:  0.928692289098
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 3.56405e-05
Norm of the params: 32.6504
              Random: fixed  93 labels. Loss 0.85758. Accuracy 0.767.
### Flips: 1030, rs: 21, checks: 618
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176284
Train loss (w/o reg) on all data: 0.136067
Test loss (w/o reg) on all data: 0.580991
Train acc on all data:  0.95600676819
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.20771e-05
Norm of the params: 28.361
     Influence (LOO): fixed 377 labels. Loss 0.58099. Accuracy 0.830.
Using normal model
LBFGS training took [462] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129234
Train loss (w/o reg) on all data: 0.0745783
Test loss (w/o reg) on all data: 0.837458
Train acc on all data:  0.992023205221
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 1.46399e-05
Norm of the params: 33.0624
                Loss: fixed 369 labels. Loss 0.83746. Accuracy 0.784.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.255445
Train loss (w/o reg) on all data: 0.202582
Test loss (w/o reg) on all data: 0.820016
Train acc on all data:  0.931351220691
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 3.27165e-05
Norm of the params: 32.5156
              Random: fixed 143 labels. Loss 0.82002. Accuracy 0.778.
### Flips: 1030, rs: 21, checks: 824
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164158
Train loss (w/o reg) on all data: 0.126981
Test loss (w/o reg) on all data: 0.544804
Train acc on all data:  0.957698815567
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 3.13233e-05
Norm of the params: 27.2677
     Influence (LOO): fixed 455 labels. Loss 0.54480. Accuracy 0.854.
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11512
Train loss (w/o reg) on all data: 0.0642919
Test loss (w/o reg) on all data: 0.796238
Train acc on all data:  0.992506647329
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 1.00412e-05
Norm of the params: 31.8834
                Loss: fixed 440 labels. Loss 0.79624. Accuracy 0.810.
Using normal model
LBFGS training took [482] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247066
Train loss (w/o reg) on all data: 0.194786
Test loss (w/o reg) on all data: 0.763966
Train acc on all data:  0.934735315446
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 8.46841e-05
Norm of the params: 32.3357
              Random: fixed 196 labels. Loss 0.76397. Accuracy 0.789.
### Flips: 1030, rs: 21, checks: 1030
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15558
Train loss (w/o reg) on all data: 0.120585
Test loss (w/o reg) on all data: 0.487397
Train acc on all data:  0.959632583998
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.13729e-05
Norm of the params: 26.4557
     Influence (LOO): fixed 525 labels. Loss 0.48740. Accuracy 0.864.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104201
Train loss (w/o reg) on all data: 0.056972
Test loss (w/o reg) on all data: 0.769989
Train acc on all data:  0.996374184191
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.04252e-05
Norm of the params: 30.7339
                Loss: fixed 491 labels. Loss 0.76999. Accuracy 0.824.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235429
Train loss (w/o reg) on all data: 0.1841
Test loss (w/o reg) on all data: 0.727557
Train acc on all data:  0.939811457578
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 2.94496e-05
Norm of the params: 32.0402
              Random: fixed 261 labels. Loss 0.72756. Accuracy 0.797.
### Flips: 1030, rs: 21, checks: 1236
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145763
Train loss (w/o reg) on all data: 0.113717
Test loss (w/o reg) on all data: 0.422566
Train acc on all data:  0.961808073483
Test acc on all data:   0.884057971014
Norm of the mean of gradients: 3.91256e-06
Norm of the params: 25.3165
     Influence (LOO): fixed 594 labels. Loss 0.42257. Accuracy 0.884.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096465
Train loss (w/o reg) on all data: 0.0520806
Test loss (w/o reg) on all data: 0.719303
Train acc on all data:  0.996857626299
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 6.03062e-06
Norm of the params: 29.7941
                Loss: fixed 537 labels. Loss 0.71930. Accuracy 0.839.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222452
Train loss (w/o reg) on all data: 0.172092
Test loss (w/o reg) on all data: 0.68065
Train acc on all data:  0.942712110225
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 1.69952e-05
Norm of the params: 31.7364
              Random: fixed 314 labels. Loss 0.68065. Accuracy 0.816.
Using normal model
LBFGS training took [702] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271519
Train loss (w/o reg) on all data: 0.21598
Test loss (w/o reg) on all data: 0.704107
Train acc on all data:  0.92385786802
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 3.0021e-05
Norm of the params: 33.3285
Flipped loss: 0.70411. Accuracy: 0.779
### Flips: 1030, rs: 22, checks: 206
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.212986
Train loss (w/o reg) on all data: 0.164626
Test loss (w/o reg) on all data: 0.609108
Train acc on all data:  0.941745226009
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 2.57206e-05
Norm of the params: 31.1
     Influence (LOO): fixed 154 labels. Loss 0.60911. Accuracy 0.828.
Using normal model
LBFGS training took [613] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18425
Train loss (w/o reg) on all data: 0.124539
Test loss (w/o reg) on all data: 0.717772
Train acc on all data:  0.970510031424
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 0.000119901
Norm of the params: 34.5573
                Loss: fixed 194 labels. Loss 0.71777. Accuracy 0.784.
Using normal model
LBFGS training took [592] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26456
Train loss (w/o reg) on all data: 0.209803
Test loss (w/o reg) on all data: 0.683642
Train acc on all data:  0.928692289098
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 0.000111987
Norm of the params: 33.0929
              Random: fixed  53 labels. Loss 0.68364. Accuracy 0.783.
### Flips: 1030, rs: 22, checks: 412
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190066
Train loss (w/o reg) on all data: 0.146497
Test loss (w/o reg) on all data: 0.517195
Train acc on all data:  0.948755136572
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 2.2629e-05
Norm of the params: 29.5192
     Influence (LOO): fixed 282 labels. Loss 0.51720. Accuracy 0.851.
Using normal model
LBFGS training took [626] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143768
Train loss (w/o reg) on all data: 0.0864718
Test loss (w/o reg) on all data: 0.665744
Train acc on all data:  0.988397389413
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 1.44387e-05
Norm of the params: 33.8515
                Loss: fixed 307 labels. Loss 0.66574. Accuracy 0.819.
Using normal model
LBFGS training took [658] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256725
Train loss (w/o reg) on all data: 0.203234
Test loss (w/o reg) on all data: 0.636928
Train acc on all data:  0.930867778584
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 2.22842e-05
Norm of the params: 32.7082
              Random: fixed 107 labels. Loss 0.63693. Accuracy 0.807.
### Flips: 1030, rs: 22, checks: 618
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178363
Train loss (w/o reg) on all data: 0.137563
Test loss (w/o reg) on all data: 0.42363
Train acc on all data:  0.950205462896
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 4.13312e-05
Norm of the params: 28.5658
     Influence (LOO): fixed 372 labels. Loss 0.42363. Accuracy 0.858.
Using normal model
LBFGS training took [603] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126795
Train loss (w/o reg) on all data: 0.0722675
Test loss (w/o reg) on all data: 0.66854
Train acc on all data:  0.992023205221
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 5.56876e-05
Norm of the params: 33.0234
                Loss: fixed 374 labels. Loss 0.66854. Accuracy 0.814.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247841
Train loss (w/o reg) on all data: 0.195014
Test loss (w/o reg) on all data: 0.600629
Train acc on all data:  0.9349770365
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 5.9973e-05
Norm of the params: 32.5044
              Random: fixed 156 labels. Loss 0.60063. Accuracy 0.811.
### Flips: 1030, rs: 22, checks: 824
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166128
Train loss (w/o reg) on all data: 0.127714
Test loss (w/o reg) on all data: 0.435744
Train acc on all data:  0.955523326082
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 9.54435e-06
Norm of the params: 27.7178
     Influence (LOO): fixed 447 labels. Loss 0.43574. Accuracy 0.874.
Using normal model
LBFGS training took [570] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117783
Train loss (w/o reg) on all data: 0.0655311
Test loss (w/o reg) on all data: 0.663032
Train acc on all data:  0.99444041576
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 3.28539e-05
Norm of the params: 32.3271
                Loss: fixed 416 labels. Loss 0.66303. Accuracy 0.814.
Using normal model
LBFGS training took [576] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239795
Train loss (w/o reg) on all data: 0.18823
Test loss (w/o reg) on all data: 0.571435
Train acc on all data:  0.937152525985
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 9.59713e-05
Norm of the params: 32.1137
              Random: fixed 207 labels. Loss 0.57144. Accuracy 0.820.
### Flips: 1030, rs: 22, checks: 1030
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156624
Train loss (w/o reg) on all data: 0.120752
Test loss (w/o reg) on all data: 0.407625
Train acc on all data:  0.96035774716
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 4.18117e-05
Norm of the params: 26.7854
     Influence (LOO): fixed 514 labels. Loss 0.40763. Accuracy 0.890.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103967
Train loss (w/o reg) on all data: 0.0562142
Test loss (w/o reg) on all data: 0.595395
Train acc on all data:  0.99444041576
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 1.16803e-05
Norm of the params: 30.904
                Loss: fixed 486 labels. Loss 0.59540. Accuracy 0.844.
Using normal model
LBFGS training took [596] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23165
Train loss (w/o reg) on all data: 0.181377
Test loss (w/o reg) on all data: 0.537794
Train acc on all data:  0.94053662074
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 9.98522e-05
Norm of the params: 31.7089
              Random: fixed 257 labels. Loss 0.53779. Accuracy 0.837.
### Flips: 1030, rs: 22, checks: 1236
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145674
Train loss (w/o reg) on all data: 0.113288
Test loss (w/o reg) on all data: 0.314177
Train acc on all data:  0.961566352429
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.76828e-05
Norm of the params: 25.4502
     Influence (LOO): fixed 586 labels. Loss 0.31418. Accuracy 0.907.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0955693
Train loss (w/o reg) on all data: 0.050545
Test loss (w/o reg) on all data: 0.55383
Train acc on all data:  0.994923857868
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 1.34333e-05
Norm of the params: 30.0081
                Loss: fixed 540 labels. Loss 0.55383. Accuracy 0.859.
Using normal model
LBFGS training took [640] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221637
Train loss (w/o reg) on all data: 0.172494
Test loss (w/o reg) on all data: 0.52744
Train acc on all data:  0.94488759971
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 3.02777e-05
Norm of the params: 31.3506
              Random: fixed 316 labels. Loss 0.52744. Accuracy 0.840.
Using normal model
LBFGS training took [645] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27173
Train loss (w/o reg) on all data: 0.214562
Test loss (w/o reg) on all data: 0.787655
Train acc on all data:  0.928450568044
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 5.06675e-05
Norm of the params: 33.8136
Flipped loss: 0.78765. Accuracy: 0.741
### Flips: 1030, rs: 23, checks: 206
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20949
Train loss (w/o reg) on all data: 0.161667
Test loss (w/o reg) on all data: 0.693284
Train acc on all data:  0.948271694465
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 1.70569e-05
Norm of the params: 30.9269
     Influence (LOO): fixed 172 labels. Loss 0.69328. Accuracy 0.797.
Using normal model
LBFGS training took [543] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184605
Train loss (w/o reg) on all data: 0.123608
Test loss (w/o reg) on all data: 0.70958
Train acc on all data:  0.971235194585
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 4.43858e-05
Norm of the params: 34.9276
                Loss: fixed 180 labels. Loss 0.70958. Accuracy 0.759.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265294
Train loss (w/o reg) on all data: 0.209433
Test loss (w/o reg) on all data: 0.723617
Train acc on all data:  0.929900894368
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 1.7425e-05
Norm of the params: 33.4248
              Random: fixed  49 labels. Loss 0.72362. Accuracy 0.764.
### Flips: 1030, rs: 23, checks: 412
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188938
Train loss (w/o reg) on all data: 0.145798
Test loss (w/o reg) on all data: 0.559221
Train acc on all data:  0.951172347111
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 5.74876e-05
Norm of the params: 29.3734
     Influence (LOO): fixed 291 labels. Loss 0.55922. Accuracy 0.830.
Using normal model
LBFGS training took [528] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150079
Train loss (w/o reg) on all data: 0.0920559
Test loss (w/o reg) on all data: 0.705623
Train acc on all data:  0.986947063089
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 4.24291e-05
Norm of the params: 34.0657
                Loss: fixed 297 labels. Loss 0.70562. Accuracy 0.775.
Using normal model
LBFGS training took [562] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256651
Train loss (w/o reg) on all data: 0.202107
Test loss (w/o reg) on all data: 0.652472
Train acc on all data:  0.934251873338
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 9.91113e-05
Norm of the params: 33.0286
              Random: fixed 113 labels. Loss 0.65247. Accuracy 0.785.
### Flips: 1030, rs: 23, checks: 618
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172479
Train loss (w/o reg) on all data: 0.133246
Test loss (w/o reg) on all data: 0.428838
Train acc on all data:  0.957698815567
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 1.05062e-05
Norm of the params: 28.0119
     Influence (LOO): fixed 400 labels. Loss 0.42884. Accuracy 0.853.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127335
Train loss (w/o reg) on all data: 0.0737105
Test loss (w/o reg) on all data: 0.688605
Train acc on all data:  0.991539763113
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 2.76076e-05
Norm of the params: 32.7488
                Loss: fixed 371 labels. Loss 0.68860. Accuracy 0.803.
Using normal model
LBFGS training took [537] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247865
Train loss (w/o reg) on all data: 0.194288
Test loss (w/o reg) on all data: 0.647816
Train acc on all data:  0.939569736524
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 0.000107457
Norm of the params: 32.7343
              Random: fixed 164 labels. Loss 0.64782. Accuracy 0.787.
### Flips: 1030, rs: 23, checks: 824
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160364
Train loss (w/o reg) on all data: 0.123646
Test loss (w/o reg) on all data: 0.33747
Train acc on all data:  0.960841189268
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.03392e-05
Norm of the params: 27.0991
     Influence (LOO): fixed 480 labels. Loss 0.33747. Accuracy 0.894.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113612
Train loss (w/o reg) on all data: 0.0638494
Test loss (w/o reg) on all data: 0.64043
Train acc on all data:  0.993956973652
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 1.23068e-05
Norm of the params: 31.5477
                Loss: fixed 433 labels. Loss 0.64043. Accuracy 0.825.
Using normal model
LBFGS training took [588] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242834
Train loss (w/o reg) on all data: 0.189793
Test loss (w/o reg) on all data: 0.664658
Train acc on all data:  0.939811457578
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 1.9392e-05
Norm of the params: 32.5704
              Random: fixed 202 labels. Loss 0.66466. Accuracy 0.788.
### Flips: 1030, rs: 23, checks: 1030
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150965
Train loss (w/o reg) on all data: 0.116837
Test loss (w/o reg) on all data: 0.311148
Train acc on all data:  0.963741841914
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 3.07785e-05
Norm of the params: 26.126
     Influence (LOO): fixed 556 labels. Loss 0.31115. Accuracy 0.900.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100785
Train loss (w/o reg) on all data: 0.0552868
Test loss (w/o reg) on all data: 0.533919
Train acc on all data:  0.993956973652
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 1.30533e-05
Norm of the params: 30.1656
                Loss: fixed 492 labels. Loss 0.53392. Accuracy 0.845.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232758
Train loss (w/o reg) on all data: 0.180862
Test loss (w/o reg) on all data: 0.638793
Train acc on all data:  0.943195552333
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 3.17237e-05
Norm of the params: 32.2167
              Random: fixed 261 labels. Loss 0.63879. Accuracy 0.810.
### Flips: 1030, rs: 23, checks: 1236
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141307
Train loss (w/o reg) on all data: 0.108911
Test loss (w/o reg) on all data: 0.325601
Train acc on all data:  0.9659173314
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 1.00756e-05
Norm of the params: 25.4541
     Influence (LOO): fixed 610 labels. Loss 0.32560. Accuracy 0.904.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0926177
Train loss (w/o reg) on all data: 0.0498226
Test loss (w/o reg) on all data: 0.530482
Train acc on all data:  0.99444041576
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 3.75523e-05
Norm of the params: 29.2558
                Loss: fixed 536 labels. Loss 0.53048. Accuracy 0.849.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22427
Train loss (w/o reg) on all data: 0.17374
Test loss (w/o reg) on all data: 0.630785
Train acc on all data:  0.948271694465
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 3.53747e-05
Norm of the params: 31.7899
              Random: fixed 314 labels. Loss 0.63078. Accuracy 0.814.
Using normal model
LBFGS training took [565] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.278398
Train loss (w/o reg) on all data: 0.223256
Test loss (w/o reg) on all data: 0.992505
Train acc on all data:  0.921682378535
Test acc on all data:   0.772946859903
Norm of the mean of gradients: 5.39458e-05
Norm of the params: 33.2089
Flipped loss: 0.99250. Accuracy: 0.773
### Flips: 1030, rs: 24, checks: 206
Using normal model
LBFGS training took [435] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213412
Train loss (w/o reg) on all data: 0.164143
Test loss (w/o reg) on all data: 0.741186
Train acc on all data:  0.946337926033
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 3.94831e-05
Norm of the params: 31.3908
     Influence (LOO): fixed 161 labels. Loss 0.74119. Accuracy 0.809.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191538
Train loss (w/o reg) on all data: 0.131476
Test loss (w/o reg) on all data: 0.961654
Train acc on all data:  0.9690597051
Test acc on all data:   0.773913043478
Norm of the mean of gradients: 0.000100894
Norm of the params: 34.659
                Loss: fixed 188 labels. Loss 0.96165. Accuracy 0.774.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272616
Train loss (w/o reg) on all data: 0.21786
Test loss (w/o reg) on all data: 0.976701
Train acc on all data:  0.924099589074
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 7.42968e-05
Norm of the params: 33.0927
              Random: fixed  47 labels. Loss 0.97670. Accuracy 0.782.
### Flips: 1030, rs: 24, checks: 412
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18766
Train loss (w/o reg) on all data: 0.144943
Test loss (w/o reg) on all data: 0.624941
Train acc on all data:  0.949480299734
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 5.07441e-05
Norm of the params: 29.2288
     Influence (LOO): fixed 289 labels. Loss 0.62494. Accuracy 0.838.
Using normal model
LBFGS training took [450] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152085
Train loss (w/o reg) on all data: 0.0940938
Test loss (w/o reg) on all data: 0.990297
Train acc on all data:  0.983562968335
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 2.74245e-05
Norm of the params: 34.0561
                Loss: fixed 306 labels. Loss 0.99030. Accuracy 0.802.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26356
Train loss (w/o reg) on all data: 0.209152
Test loss (w/o reg) on all data: 0.940784
Train acc on all data:  0.929659173314
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 0.000145898
Norm of the params: 32.987
              Random: fixed 104 labels. Loss 0.94078. Accuracy 0.781.
### Flips: 1030, rs: 24, checks: 618
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175436
Train loss (w/o reg) on all data: 0.135176
Test loss (w/o reg) on all data: 0.536243
Train acc on all data:  0.954072999758
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 5.18137e-05
Norm of the params: 28.3761
     Influence (LOO): fixed 375 labels. Loss 0.53624. Accuracy 0.860.
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12964
Train loss (w/o reg) on all data: 0.0754859
Test loss (w/o reg) on all data: 0.854851
Train acc on all data:  0.992990089437
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 5.0629e-05
Norm of the params: 32.9102
                Loss: fixed 376 labels. Loss 0.85485. Accuracy 0.813.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252904
Train loss (w/o reg) on all data: 0.199671
Test loss (w/o reg) on all data: 0.947232
Train acc on all data:  0.933284989123
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 6.71477e-05
Norm of the params: 32.629
              Random: fixed 161 labels. Loss 0.94723. Accuracy 0.778.
### Flips: 1030, rs: 24, checks: 824
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.165632
Train loss (w/o reg) on all data: 0.128137
Test loss (w/o reg) on all data: 0.449294
Train acc on all data:  0.956490210297
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 2.49075e-05
Norm of the params: 27.3845
     Influence (LOO): fixed 448 labels. Loss 0.44929. Accuracy 0.872.
Using normal model
LBFGS training took [397] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113006
Train loss (w/o reg) on all data: 0.062734
Test loss (w/o reg) on all data: 0.797715
Train acc on all data:  0.996615905245
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 6.97446e-05
Norm of the params: 31.7088
                Loss: fixed 442 labels. Loss 0.79772. Accuracy 0.825.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244269
Train loss (w/o reg) on all data: 0.191655
Test loss (w/o reg) on all data: 0.896645
Train acc on all data:  0.937394247039
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 0.000143235
Norm of the params: 32.439
              Random: fixed 211 labels. Loss 0.89665. Accuracy 0.790.
### Flips: 1030, rs: 24, checks: 1030
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.156144
Train loss (w/o reg) on all data: 0.121914
Test loss (w/o reg) on all data: 0.413524
Train acc on all data:  0.957698815567
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 4.13612e-06
Norm of the params: 26.1646
     Influence (LOO): fixed 526 labels. Loss 0.41352. Accuracy 0.890.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101866
Train loss (w/o reg) on all data: 0.0550478
Test loss (w/o reg) on all data: 0.735333
Train acc on all data:  0.997099347353
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 3.32288e-05
Norm of the params: 30.6
                Loss: fixed 496 labels. Loss 0.73533. Accuracy 0.830.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23938
Train loss (w/o reg) on all data: 0.187557
Test loss (w/o reg) on all data: 0.82213
Train acc on all data:  0.939086294416
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 4.57981e-05
Norm of the params: 32.194
              Random: fixed 249 labels. Loss 0.82213. Accuracy 0.805.
### Flips: 1030, rs: 24, checks: 1236
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147483
Train loss (w/o reg) on all data: 0.115623
Test loss (w/o reg) on all data: 0.349182
Train acc on all data:  0.957940536621
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 3.90043e-06
Norm of the params: 25.2431
     Influence (LOO): fixed 589 labels. Loss 0.34918. Accuracy 0.900.
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0918062
Train loss (w/o reg) on all data: 0.0482337
Test loss (w/o reg) on all data: 0.717438
Train acc on all data:  0.997582789461
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 1.28844e-05
Norm of the params: 29.5203
                Loss: fixed 547 labels. Loss 0.71744. Accuracy 0.843.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22901
Train loss (w/o reg) on all data: 0.178887
Test loss (w/o reg) on all data: 0.686945
Train acc on all data:  0.944404157602
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 3.53422e-05
Norm of the params: 31.6616
              Random: fixed 299 labels. Loss 0.68695. Accuracy 0.826.
Using normal model
LBFGS training took [568] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272651
Train loss (w/o reg) on all data: 0.217935
Test loss (w/o reg) on all data: 0.925059
Train acc on all data:  0.926033357505
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 5.66083e-05
Norm of the params: 33.0805
Flipped loss: 0.92506. Accuracy: 0.750
### Flips: 1030, rs: 25, checks: 206
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208771
Train loss (w/o reg) on all data: 0.162257
Test loss (w/o reg) on all data: 0.811226
Train acc on all data:  0.94488759971
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 1.0353e-05
Norm of the params: 30.5005
     Influence (LOO): fixed 165 labels. Loss 0.81123. Accuracy 0.801.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182388
Train loss (w/o reg) on all data: 0.124561
Test loss (w/o reg) on all data: 0.96348
Train acc on all data:  0.972685520909
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 7.42026e-05
Norm of the params: 34.0078
                Loss: fixed 189 labels. Loss 0.96348. Accuracy 0.779.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261157
Train loss (w/o reg) on all data: 0.206944
Test loss (w/o reg) on all data: 0.828517
Train acc on all data:  0.929175731206
Test acc on all data:   0.76231884058
Norm of the mean of gradients: 3.29418e-05
Norm of the params: 32.9281
              Random: fixed  54 labels. Loss 0.82852. Accuracy 0.762.
### Flips: 1030, rs: 25, checks: 412
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187019
Train loss (w/o reg) on all data: 0.14542
Test loss (w/o reg) on all data: 0.806737
Train acc on all data:  0.949480299734
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 6.07763e-05
Norm of the params: 28.8439
     Influence (LOO): fixed 288 labels. Loss 0.80674. Accuracy 0.826.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141421
Train loss (w/o reg) on all data: 0.0860768
Test loss (w/o reg) on all data: 0.919335
Train acc on all data:  0.987430505197
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 1.41428e-05
Norm of the params: 33.27
                Loss: fixed 306 labels. Loss 0.91933. Accuracy 0.787.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.253712
Train loss (w/o reg) on all data: 0.200263
Test loss (w/o reg) on all data: 0.807086
Train acc on all data:  0.932318104907
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 5.21553e-05
Norm of the params: 32.6951
              Random: fixed 102 labels. Loss 0.80709. Accuracy 0.785.
### Flips: 1030, rs: 25, checks: 618
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17278
Train loss (w/o reg) on all data: 0.134648
Test loss (w/o reg) on all data: 0.726141
Train acc on all data:  0.951897510273
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 3.49718e-05
Norm of the params: 27.6162
     Influence (LOO): fixed 388 labels. Loss 0.72614. Accuracy 0.843.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121423
Train loss (w/o reg) on all data: 0.0696135
Test loss (w/o reg) on all data: 0.819376
Train acc on all data:  0.992023205221
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 1.0501e-05
Norm of the params: 32.1898
                Loss: fixed 384 labels. Loss 0.81938. Accuracy 0.822.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246661
Train loss (w/o reg) on all data: 0.193814
Test loss (w/o reg) on all data: 0.845018
Train acc on all data:  0.936669083877
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 2.93367e-05
Norm of the params: 32.5106
              Random: fixed 153 labels. Loss 0.84502. Accuracy 0.788.
### Flips: 1030, rs: 25, checks: 824
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161769
Train loss (w/o reg) on all data: 0.126289
Test loss (w/o reg) on all data: 0.707233
Train acc on all data:  0.954314720812
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 4.85225e-06
Norm of the params: 26.6381
     Influence (LOO): fixed 470 labels. Loss 0.70723. Accuracy 0.869.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107421
Train loss (w/o reg) on all data: 0.0595395
Test loss (w/o reg) on all data: 0.764377
Train acc on all data:  0.993231810491
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 1.25932e-05
Norm of the params: 30.9457
                Loss: fixed 448 labels. Loss 0.76438. Accuracy 0.835.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23868
Train loss (w/o reg) on all data: 0.187155
Test loss (w/o reg) on all data: 0.833468
Train acc on all data:  0.93932801547
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 2.60261e-05
Norm of the params: 32.1015
              Random: fixed 206 labels. Loss 0.83347. Accuracy 0.795.
### Flips: 1030, rs: 25, checks: 1030
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151676
Train loss (w/o reg) on all data: 0.118383
Test loss (w/o reg) on all data: 0.575525
Train acc on all data:  0.960599468214
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 1.16788e-05
Norm of the params: 25.8042
     Influence (LOO): fixed 536 labels. Loss 0.57552. Accuracy 0.887.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0988023
Train loss (w/o reg) on all data: 0.0533625
Test loss (w/o reg) on all data: 0.604811
Train acc on all data:  0.997341068407
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 1.60306e-05
Norm of the params: 30.1462
                Loss: fixed 487 labels. Loss 0.60481. Accuracy 0.854.
Using normal model
LBFGS training took [521] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229013
Train loss (w/o reg) on all data: 0.178674
Test loss (w/o reg) on all data: 0.833468
Train acc on all data:  0.941986947063
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 5.49118e-05
Norm of the params: 31.7296
              Random: fixed 262 labels. Loss 0.83347. Accuracy 0.813.
### Flips: 1030, rs: 25, checks: 1236
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144692
Train loss (w/o reg) on all data: 0.113633
Test loss (w/o reg) on all data: 0.47719
Train acc on all data:  0.962049794537
Test acc on all data:   0.911111111111
Norm of the mean of gradients: 1.73885e-05
Norm of the params: 24.9235
     Influence (LOO): fixed 593 labels. Loss 0.47719. Accuracy 0.911.
Using normal model
LBFGS training took [351] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0914
Train loss (w/o reg) on all data: 0.0485994
Test loss (w/o reg) on all data: 0.61341
Train acc on all data:  0.997582789461
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 2.28059e-05
Norm of the params: 29.2577
                Loss: fixed 528 labels. Loss 0.61341. Accuracy 0.855.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.218465
Train loss (w/o reg) on all data: 0.169307
Test loss (w/o reg) on all data: 0.800004
Train acc on all data:  0.945612762872
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 1.93789e-05
Norm of the params: 31.3552
              Random: fixed 315 labels. Loss 0.80000. Accuracy 0.815.
Using normal model
LBFGS training took [731] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276516
Train loss (w/o reg) on all data: 0.219913
Test loss (w/o reg) on all data: 0.878268
Train acc on all data:  0.926275078559
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 4.8214e-05
Norm of the params: 33.6463
Flipped loss: 0.87827. Accuracy: 0.768
### Flips: 1030, rs: 26, checks: 206
Using normal model
LBFGS training took [484] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213431
Train loss (w/o reg) on all data: 0.163358
Test loss (w/o reg) on all data: 0.64976
Train acc on all data:  0.948996857626
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 4.74556e-05
Norm of the params: 31.6461
     Influence (LOO): fixed 163 labels. Loss 0.64976. Accuracy 0.814.
Using normal model
LBFGS training took [623] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18619
Train loss (w/o reg) on all data: 0.124782
Test loss (w/o reg) on all data: 0.934046
Train acc on all data:  0.971235194585
Test acc on all data:   0.76231884058
Norm of the mean of gradients: 4.4451e-05
Norm of the params: 35.0449
                Loss: fixed 194 labels. Loss 0.93405. Accuracy 0.762.
Using normal model
LBFGS training took [613] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.269883
Train loss (w/o reg) on all data: 0.213874
Test loss (w/o reg) on all data: 0.848789
Train acc on all data:  0.931592941745
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 3.70782e-05
Norm of the params: 33.4691
              Random: fixed  51 labels. Loss 0.84879. Accuracy 0.786.
### Flips: 1030, rs: 26, checks: 412
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19076
Train loss (w/o reg) on all data: 0.145819
Test loss (w/o reg) on all data: 0.59635
Train acc on all data:  0.952864394489
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 3.7232e-05
Norm of the params: 29.9801
     Influence (LOO): fixed 277 labels. Loss 0.59635. Accuracy 0.845.
Using normal model
LBFGS training took [597] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151039
Train loss (w/o reg) on all data: 0.0926672
Test loss (w/o reg) on all data: 0.85583
Train acc on all data:  0.986221899927
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 0.000133223
Norm of the params: 34.1679
                Loss: fixed 311 labels. Loss 0.85583. Accuracy 0.794.
Using normal model
LBFGS training took [640] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262719
Train loss (w/o reg) on all data: 0.207761
Test loss (w/o reg) on all data: 0.841241
Train acc on all data:  0.934493594392
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 9.6391e-05
Norm of the params: 33.1537
              Random: fixed  98 labels. Loss 0.84124. Accuracy 0.794.
### Flips: 1030, rs: 26, checks: 618
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179599
Train loss (w/o reg) on all data: 0.138023
Test loss (w/o reg) on all data: 0.579425
Train acc on all data:  0.954314720812
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.75132e-05
Norm of the params: 28.8362
     Influence (LOO): fixed 374 labels. Loss 0.57942. Accuracy 0.864.
Using normal model
LBFGS training took [523] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135587
Train loss (w/o reg) on all data: 0.0795251
Test loss (w/o reg) on all data: 0.759569
Train acc on all data:  0.990572878898
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 1.38024e-05
Norm of the params: 33.4848
                Loss: fixed 369 labels. Loss 0.75957. Accuracy 0.801.
Using normal model
LBFGS training took [568] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25379
Train loss (w/o reg) on all data: 0.199527
Test loss (w/o reg) on all data: 0.784958
Train acc on all data:  0.935702199662
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 0.000121578
Norm of the params: 32.9433
              Random: fixed 158 labels. Loss 0.78496. Accuracy 0.810.
### Flips: 1030, rs: 26, checks: 824
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.168368
Train loss (w/o reg) on all data: 0.129976
Test loss (w/o reg) on all data: 0.608382
Train acc on all data:  0.957698815567
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 1.59067e-05
Norm of the params: 27.71
     Influence (LOO): fixed 461 labels. Loss 0.60838. Accuracy 0.871.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119867
Train loss (w/o reg) on all data: 0.0678739
Test loss (w/o reg) on all data: 0.684613
Train acc on all data:  0.996374184191
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 8.82701e-06
Norm of the params: 32.2468
                Loss: fixed 430 labels. Loss 0.68461. Accuracy 0.825.
Using normal model
LBFGS training took [617] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245294
Train loss (w/o reg) on all data: 0.191982
Test loss (w/o reg) on all data: 0.698015
Train acc on all data:  0.938119410201
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 4.82718e-05
Norm of the params: 32.6536
              Random: fixed 207 labels. Loss 0.69802. Accuracy 0.827.
### Flips: 1030, rs: 26, checks: 1030
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160552
Train loss (w/o reg) on all data: 0.124888
Test loss (w/o reg) on all data: 0.53586
Train acc on all data:  0.958182257675
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 6.80898e-05
Norm of the params: 26.7072
     Influence (LOO): fixed 529 labels. Loss 0.53586. Accuracy 0.887.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110319
Train loss (w/o reg) on all data: 0.06099
Test loss (w/o reg) on all data: 0.632566
Train acc on all data:  0.997582789461
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 3.12663e-05
Norm of the params: 31.4098
                Loss: fixed 480 labels. Loss 0.63257. Accuracy 0.840.
Using normal model
LBFGS training took [617] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237646
Train loss (w/o reg) on all data: 0.185665
Test loss (w/o reg) on all data: 0.606349
Train acc on all data:  0.942228668117
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 5.64817e-05
Norm of the params: 32.243
              Random: fixed 253 labels. Loss 0.60635. Accuracy 0.839.
### Flips: 1030, rs: 26, checks: 1236
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147846
Train loss (w/o reg) on all data: 0.115689
Test loss (w/o reg) on all data: 0.466097
Train acc on all data:  0.960599468214
Test acc on all data:   0.904347826087
Norm of the mean of gradients: 7.39376e-06
Norm of the params: 25.3603
     Influence (LOO): fixed 593 labels. Loss 0.46610. Accuracy 0.904.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0998371
Train loss (w/o reg) on all data: 0.054053
Test loss (w/o reg) on all data: 0.58424
Train acc on all data:  0.997824510515
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 2.24983e-05
Norm of the params: 30.2603
                Loss: fixed 528 labels. Loss 0.58424. Accuracy 0.857.
Using normal model
LBFGS training took [633] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230471
Train loss (w/o reg) on all data: 0.178977
Test loss (w/o reg) on all data: 0.641573
Train acc on all data:  0.943920715494
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 5.05806e-05
Norm of the params: 32.0919
              Random: fixed 300 labels. Loss 0.64157. Accuracy 0.838.
Using normal model
LBFGS training took [506] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282406
Train loss (w/o reg) on all data: 0.226019
Test loss (w/o reg) on all data: 0.7527
Train acc on all data:  0.922165820643
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 2.5885e-05
Norm of the params: 33.5819
Flipped loss: 0.75270. Accuracy: 0.775
### Flips: 1030, rs: 27, checks: 206
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.220528
Train loss (w/o reg) on all data: 0.171215
Test loss (w/o reg) on all data: 0.670438
Train acc on all data:  0.939569736524
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 9.81945e-06
Norm of the params: 31.4047
     Influence (LOO): fixed 162 labels. Loss 0.67044. Accuracy 0.812.
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195331
Train loss (w/o reg) on all data: 0.13428
Test loss (w/o reg) on all data: 0.692247
Train acc on all data:  0.966884215615
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 4.29751e-05
Norm of the params: 34.9431
                Loss: fixed 190 labels. Loss 0.69225. Accuracy 0.786.
Using normal model
LBFGS training took [464] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275067
Train loss (w/o reg) on all data: 0.219318
Test loss (w/o reg) on all data: 0.739318
Train acc on all data:  0.925308194344
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 3.79085e-05
Norm of the params: 33.3914
              Random: fixed  51 labels. Loss 0.73932. Accuracy 0.791.
### Flips: 1030, rs: 27, checks: 412
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194539
Train loss (w/o reg) on all data: 0.150594
Test loss (w/o reg) on all data: 0.57779
Train acc on all data:  0.94923857868
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 1.16998e-05
Norm of the params: 29.6463
     Influence (LOO): fixed 288 labels. Loss 0.57779. Accuracy 0.841.
Using normal model
LBFGS training took [414] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150602
Train loss (w/o reg) on all data: 0.0927058
Test loss (w/o reg) on all data: 0.646971
Train acc on all data:  0.986705342035
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 3.17456e-05
Norm of the params: 34.0284
                Loss: fixed 314 labels. Loss 0.64697. Accuracy 0.806.
Using normal model
LBFGS training took [462] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268144
Train loss (w/o reg) on all data: 0.212698
Test loss (w/o reg) on all data: 0.716165
Train acc on all data:  0.929659173314
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 6.91019e-05
Norm of the params: 33.3004
              Random: fixed  94 labels. Loss 0.71616. Accuracy 0.794.
### Flips: 1030, rs: 27, checks: 618
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180834
Train loss (w/o reg) on all data: 0.14023
Test loss (w/o reg) on all data: 0.552988
Train acc on all data:  0.95358955765
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 2.02681e-05
Norm of the params: 28.4973
     Influence (LOO): fixed 391 labels. Loss 0.55299. Accuracy 0.859.
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129291
Train loss (w/o reg) on all data: 0.0748617
Test loss (w/o reg) on all data: 0.618495
Train acc on all data:  0.991781484167
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 1.71356e-05
Norm of the params: 32.9937
                Loss: fixed 386 labels. Loss 0.61849. Accuracy 0.811.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260267
Train loss (w/o reg) on all data: 0.205588
Test loss (w/o reg) on all data: 0.729134
Train acc on all data:  0.93062605753
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 1.14056e-05
Norm of the params: 33.0693
              Random: fixed 142 labels. Loss 0.72913. Accuracy 0.805.
### Flips: 1030, rs: 27, checks: 824
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169952
Train loss (w/o reg) on all data: 0.132005
Test loss (w/o reg) on all data: 0.451905
Train acc on all data:  0.954556441866
Test acc on all data:   0.871497584541
Norm of the mean of gradients: 3.27933e-05
Norm of the params: 27.5487
     Influence (LOO): fixed 472 labels. Loss 0.45190. Accuracy 0.871.
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115848
Train loss (w/o reg) on all data: 0.0646084
Test loss (w/o reg) on all data: 0.584571
Train acc on all data:  0.993715252599
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 2.77716e-05
Norm of the params: 32.0123
                Loss: fixed 451 labels. Loss 0.58457. Accuracy 0.830.
Using normal model
LBFGS training took [434] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249057
Train loss (w/o reg) on all data: 0.195739
Test loss (w/o reg) on all data: 0.726553
Train acc on all data:  0.937152525985
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 3.17719e-05
Norm of the params: 32.655
              Random: fixed 207 labels. Loss 0.72655. Accuracy 0.818.
### Flips: 1030, rs: 27, checks: 1030
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160813
Train loss (w/o reg) on all data: 0.125367
Test loss (w/o reg) on all data: 0.377994
Train acc on all data:  0.956731931351
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 1.23493e-05
Norm of the params: 26.6255
     Influence (LOO): fixed 541 labels. Loss 0.37799. Accuracy 0.890.
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105213
Train loss (w/o reg) on all data: 0.0573664
Test loss (w/o reg) on all data: 0.523309
Train acc on all data:  0.997824510515
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 1.80079e-05
Norm of the params: 30.9345
                Loss: fixed 504 labels. Loss 0.52331. Accuracy 0.847.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240081
Train loss (w/o reg) on all data: 0.188477
Test loss (w/o reg) on all data: 0.70763
Train acc on all data:  0.940294899686
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 1.94205e-05
Norm of the params: 32.1261
              Random: fixed 262 labels. Loss 0.70763. Accuracy 0.838.
### Flips: 1030, rs: 27, checks: 1236
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148248
Train loss (w/o reg) on all data: 0.115798
Test loss (w/o reg) on all data: 0.395133
Train acc on all data:  0.96035774716
Test acc on all data:   0.893719806763
Norm of the mean of gradients: 1.14549e-05
Norm of the params: 25.4756
     Influence (LOO): fixed 608 labels. Loss 0.39513. Accuracy 0.894.
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0974305
Train loss (w/o reg) on all data: 0.0520823
Test loss (w/o reg) on all data: 0.536296
Train acc on all data:  0.998549673677
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 9.37725e-06
Norm of the params: 30.1158
                Loss: fixed 546 labels. Loss 0.53630. Accuracy 0.852.
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.233681
Train loss (w/o reg) on all data: 0.182896
Test loss (w/o reg) on all data: 0.703385
Train acc on all data:  0.941020062847
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 1.4404e-05
Norm of the params: 31.8699
              Random: fixed 309 labels. Loss 0.70338. Accuracy 0.845.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.270246
Train loss (w/o reg) on all data: 0.213357
Test loss (w/o reg) on all data: 0.848656
Train acc on all data:  0.93062605753
Test acc on all data:   0.744927536232
Norm of the mean of gradients: 2.66138e-05
Norm of the params: 33.7307
Flipped loss: 0.84866. Accuracy: 0.745
### Flips: 1030, rs: 28, checks: 206
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21023
Train loss (w/o reg) on all data: 0.159815
Test loss (w/o reg) on all data: 0.741398
Train acc on all data:  0.95044718395
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 1.24062e-05
Norm of the params: 31.7537
     Influence (LOO): fixed 156 labels. Loss 0.74140. Accuracy 0.797.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180791
Train loss (w/o reg) on all data: 0.120236
Test loss (w/o reg) on all data: 0.815677
Train acc on all data:  0.97461928934
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 1.78872e-05
Norm of the params: 34.8007
                Loss: fixed 183 labels. Loss 0.81568. Accuracy 0.770.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265274
Train loss (w/o reg) on all data: 0.208757
Test loss (w/o reg) on all data: 0.813833
Train acc on all data:  0.933526710176
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 2.5359e-05
Norm of the params: 33.6206
              Random: fixed  46 labels. Loss 0.81383. Accuracy 0.760.
### Flips: 1030, rs: 28, checks: 412
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187196
Train loss (w/o reg) on all data: 0.142521
Test loss (w/o reg) on all data: 0.64114
Train acc on all data:  0.95600676819
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 9.9094e-06
Norm of the params: 29.8915
     Influence (LOO): fixed 291 labels. Loss 0.64114. Accuracy 0.841.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144568
Train loss (w/o reg) on all data: 0.0866194
Test loss (w/o reg) on all data: 0.790932
Train acc on all data:  0.988639110467
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 3.39191e-05
Norm of the params: 34.0438
                Loss: fixed 294 labels. Loss 0.79093. Accuracy 0.792.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.255946
Train loss (w/o reg) on all data: 0.200467
Test loss (w/o reg) on all data: 0.735685
Train acc on all data:  0.936669083877
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 4.86126e-05
Norm of the params: 33.3101
              Random: fixed  99 labels. Loss 0.73569. Accuracy 0.778.
### Flips: 1030, rs: 28, checks: 618
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173937
Train loss (w/o reg) on all data: 0.131821
Test loss (w/o reg) on all data: 0.526498
Train acc on all data:  0.957457094513
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 1.62695e-05
Norm of the params: 29.0228
     Influence (LOO): fixed 379 labels. Loss 0.52650. Accuracy 0.852.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126757
Train loss (w/o reg) on all data: 0.0720741
Test loss (w/o reg) on all data: 0.756821
Train acc on all data:  0.992023205221
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 1.1631e-05
Norm of the params: 33.0704
                Loss: fixed 361 labels. Loss 0.75682. Accuracy 0.806.
Using normal model
LBFGS training took [492] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25069
Train loss (w/o reg) on all data: 0.196253
Test loss (w/o reg) on all data: 0.672014
Train acc on all data:  0.938844573362
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 2.83673e-05
Norm of the params: 32.9961
              Random: fixed 150 labels. Loss 0.67201. Accuracy 0.793.
### Flips: 1030, rs: 28, checks: 824
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161229
Train loss (w/o reg) on all data: 0.121505
Test loss (w/o reg) on all data: 0.450916
Train acc on all data:  0.961324631375
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 2.22494e-05
Norm of the params: 28.1865
     Influence (LOO): fixed 460 labels. Loss 0.45092. Accuracy 0.864.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113363
Train loss (w/o reg) on all data: 0.0626144
Test loss (w/o reg) on all data: 0.694036
Train acc on all data:  0.993473531545
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 5.25518e-05
Norm of the params: 31.8585
                Loss: fixed 419 labels. Loss 0.69404. Accuracy 0.813.
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242175
Train loss (w/o reg) on all data: 0.189124
Test loss (w/o reg) on all data: 0.659826
Train acc on all data:  0.942953831279
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 6.26465e-05
Norm of the params: 32.5733
              Random: fixed 210 labels. Loss 0.65983. Accuracy 0.811.
### Flips: 1030, rs: 28, checks: 1030
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151442
Train loss (w/o reg) on all data: 0.114334
Test loss (w/o reg) on all data: 0.388003
Train acc on all data:  0.96470872613
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 5.97727e-06
Norm of the params: 27.2426
     Influence (LOO): fixed 545 labels. Loss 0.38800. Accuracy 0.886.
Using normal model
LBFGS training took [430] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103703
Train loss (w/o reg) on all data: 0.0562533
Test loss (w/o reg) on all data: 0.649976
Train acc on all data:  0.996615905245
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 1.009e-05
Norm of the params: 30.8056
                Loss: fixed 477 labels. Loss 0.64998. Accuracy 0.835.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237063
Train loss (w/o reg) on all data: 0.184754
Test loss (w/o reg) on all data: 0.594495
Train acc on all data:  0.947546531303
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 5.38019e-05
Norm of the params: 32.3449
              Random: fixed 257 labels. Loss 0.59449. Accuracy 0.819.
### Flips: 1030, rs: 28, checks: 1236
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143708
Train loss (w/o reg) on all data: 0.108635
Test loss (w/o reg) on all data: 0.342271
Train acc on all data:  0.966642494561
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 3.11865e-05
Norm of the params: 26.4849
     Influence (LOO): fixed 602 labels. Loss 0.34227. Accuracy 0.897.
Using normal model
LBFGS training took [374] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0983784
Train loss (w/o reg) on all data: 0.0529918
Test loss (w/o reg) on all data: 0.620018
Train acc on all data:  0.993956973652
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.06136e-05
Norm of the params: 30.1286
                Loss: fixed 519 labels. Loss 0.62002. Accuracy 0.840.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227202
Train loss (w/o reg) on all data: 0.176199
Test loss (w/o reg) on all data: 0.536716
Train acc on all data:  0.948271694465
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 8.82282e-05
Norm of the params: 31.9383
              Random: fixed 314 labels. Loss 0.53672. Accuracy 0.830.
Using normal model
LBFGS training took [657] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.277284
Train loss (w/o reg) on all data: 0.22139
Test loss (w/o reg) on all data: 1.0951
Train acc on all data:  0.925549915398
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 2.63038e-05
Norm of the params: 33.4344
Flipped loss: 1.09510. Accuracy: 0.749
### Flips: 1030, rs: 29, checks: 206
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211157
Train loss (w/o reg) on all data: 0.162016
Test loss (w/o reg) on all data: 1.0248
Train acc on all data:  0.948271694465
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 7.43111e-05
Norm of the params: 31.35
     Influence (LOO): fixed 160 labels. Loss 1.02480. Accuracy 0.786.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182483
Train loss (w/o reg) on all data: 0.122584
Test loss (w/o reg) on all data: 1.14857
Train acc on all data:  0.973168963017
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 5.91596e-05
Norm of the params: 34.6117
                Loss: fixed 188 labels. Loss 1.14857. Accuracy 0.767.
Using normal model
LBFGS training took [565] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.269233
Train loss (w/o reg) on all data: 0.214719
Test loss (w/o reg) on all data: 1.0795
Train acc on all data:  0.928692289098
Test acc on all data:   0.75845410628
Norm of the mean of gradients: 2.49265e-05
Norm of the params: 33.0193
              Random: fixed  56 labels. Loss 1.07950. Accuracy 0.758.
### Flips: 1030, rs: 29, checks: 412
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186904
Train loss (w/o reg) on all data: 0.143932
Test loss (w/o reg) on all data: 0.846096
Train acc on all data:  0.951655789219
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 1.80143e-05
Norm of the params: 29.3163
     Influence (LOO): fixed 295 labels. Loss 0.84610. Accuracy 0.815.
Using normal model
LBFGS training took [551] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144675
Train loss (w/o reg) on all data: 0.0873292
Test loss (w/o reg) on all data: 1.24219
Train acc on all data:  0.989122552574
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 2.00247e-05
Norm of the params: 33.8661
                Loss: fixed 308 labels. Loss 1.24219. Accuracy 0.776.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260109
Train loss (w/o reg) on all data: 0.206504
Test loss (w/o reg) on all data: 1.06643
Train acc on all data:  0.933043268069
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 0.000128449
Norm of the params: 32.743
              Random: fixed 105 labels. Loss 1.06643. Accuracy 0.771.
### Flips: 1030, rs: 29, checks: 618
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172359
Train loss (w/o reg) on all data: 0.133389
Test loss (w/o reg) on all data: 0.738773
Train acc on all data:  0.955281605028
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 2.3659e-05
Norm of the params: 27.918
     Influence (LOO): fixed 391 labels. Loss 0.73877. Accuracy 0.845.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1209
Train loss (w/o reg) on all data: 0.0685769
Test loss (w/o reg) on all data: 1.07301
Train acc on all data:  0.996615905245
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 8.0135e-05
Norm of the params: 32.3491
                Loss: fixed 395 labels. Loss 1.07301. Accuracy 0.799.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251065
Train loss (w/o reg) on all data: 0.198069
Test loss (w/o reg) on all data: 1.05365
Train acc on all data:  0.936910804931
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 3.29779e-05
Norm of the params: 32.5563
              Random: fixed 157 labels. Loss 1.05365. Accuracy 0.784.
### Flips: 1030, rs: 29, checks: 824
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161749
Train loss (w/o reg) on all data: 0.125795
Test loss (w/o reg) on all data: 0.673158
Train acc on all data:  0.956248489243
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 2.17603e-05
Norm of the params: 26.8159
     Influence (LOO): fixed 489 labels. Loss 0.67316. Accuracy 0.872.
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109426
Train loss (w/o reg) on all data: 0.0605923
Test loss (w/o reg) on all data: 0.966233
Train acc on all data:  0.996374184191
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 2.3912e-05
Norm of the params: 31.2519
                Loss: fixed 451 labels. Loss 0.96623. Accuracy 0.816.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244161
Train loss (w/o reg) on all data: 0.192414
Test loss (w/o reg) on all data: 1.03361
Train acc on all data:  0.939086294416
Test acc on all data:   0.8
Norm of the mean of gradients: 4.58099e-05
Norm of the params: 32.1704
              Random: fixed 209 labels. Loss 1.03361. Accuracy 0.800.
### Flips: 1030, rs: 29, checks: 1030
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153061
Train loss (w/o reg) on all data: 0.119396
Test loss (w/o reg) on all data: 0.656532
Train acc on all data:  0.957940536621
Test acc on all data:   0.885024154589
Norm of the mean of gradients: 6.86041e-06
Norm of the params: 25.9481
     Influence (LOO): fixed 556 labels. Loss 0.65653. Accuracy 0.885.
Using normal model
LBFGS training took [409] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100931
Train loss (w/o reg) on all data: 0.0547886
Test loss (w/o reg) on all data: 0.899282
Train acc on all data:  0.996132463138
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 3.8711e-05
Norm of the params: 30.3783
                Loss: fixed 495 labels. Loss 0.89928. Accuracy 0.827.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235808
Train loss (w/o reg) on all data: 0.184464
Test loss (w/o reg) on all data: 0.933945
Train acc on all data:  0.941745226009
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 2.21964e-05
Norm of the params: 32.0451
              Random: fixed 256 labels. Loss 0.93394. Accuracy 0.807.
### Flips: 1030, rs: 29, checks: 1236
Using normal model
LBFGS training took [340] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146265
Train loss (w/o reg) on all data: 0.114297
Test loss (w/o reg) on all data: 0.556521
Train acc on all data:  0.960841189268
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.7088e-05
Norm of the params: 25.2854
     Influence (LOO): fixed 610 labels. Loss 0.55652. Accuracy 0.900.
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092166
Train loss (w/o reg) on all data: 0.048905
Test loss (w/o reg) on all data: 0.783048
Train acc on all data:  0.998066231569
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 1.13983e-05
Norm of the params: 29.4146
                Loss: fixed 541 labels. Loss 0.78305. Accuracy 0.844.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223961
Train loss (w/o reg) on all data: 0.173978
Test loss (w/o reg) on all data: 0.926684
Train acc on all data:  0.944404157602
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 2.18233e-05
Norm of the params: 31.6175
              Random: fixed 315 labels. Loss 0.92668. Accuracy 0.825.
Using normal model
LBFGS training took [662] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272154
Train loss (w/o reg) on all data: 0.21638
Test loss (w/o reg) on all data: 0.859395
Train acc on all data:  0.927241962775
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 3.12297e-05
Norm of the params: 33.3987
Flipped loss: 0.85939. Accuracy: 0.759
### Flips: 1030, rs: 30, checks: 206
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209281
Train loss (w/o reg) on all data: 0.159578
Test loss (w/o reg) on all data: 0.797173
Train acc on all data:  0.94923857868
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 3.08308e-05
Norm of the params: 31.5289
     Influence (LOO): fixed 156 labels. Loss 0.79717. Accuracy 0.790.
Using normal model
LBFGS training took [597] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187855
Train loss (w/o reg) on all data: 0.126027
Test loss (w/o reg) on all data: 0.887923
Train acc on all data:  0.971960357747
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 3.42562e-05
Norm of the params: 35.1648
                Loss: fixed 189 labels. Loss 0.88792. Accuracy 0.760.
Using normal model
LBFGS training took [553] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.264618
Train loss (w/o reg) on all data: 0.209647
Test loss (w/o reg) on all data: 0.869251
Train acc on all data:  0.929900894368
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 0.000100905
Norm of the params: 33.1574
              Random: fixed  60 labels. Loss 0.86925. Accuracy 0.771.
### Flips: 1030, rs: 30, checks: 412
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188465
Train loss (w/o reg) on all data: 0.143819
Test loss (w/o reg) on all data: 0.642959
Train acc on all data:  0.952622673435
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 1.69291e-05
Norm of the params: 29.8818
     Influence (LOO): fixed 276 labels. Loss 0.64296. Accuracy 0.816.
Using normal model
LBFGS training took [532] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14852
Train loss (w/o reg) on all data: 0.0899684
Test loss (w/o reg) on all data: 0.796672
Train acc on all data:  0.988397389413
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 2.96754e-05
Norm of the params: 34.2203
                Loss: fixed 297 labels. Loss 0.79667. Accuracy 0.786.
Using normal model
LBFGS training took [551] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.257313
Train loss (w/o reg) on all data: 0.203147
Test loss (w/o reg) on all data: 0.859731
Train acc on all data:  0.935218757554
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 3.14418e-05
Norm of the params: 32.9136
              Random: fixed 110 labels. Loss 0.85973. Accuracy 0.786.
### Flips: 1030, rs: 30, checks: 618
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175
Train loss (w/o reg) on all data: 0.133907
Test loss (w/o reg) on all data: 0.620248
Train acc on all data:  0.95600676819
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 2.79603e-05
Norm of the params: 28.6683
     Influence (LOO): fixed 376 labels. Loss 0.62025. Accuracy 0.845.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130072
Train loss (w/o reg) on all data: 0.0758042
Test loss (w/o reg) on all data: 0.778012
Train acc on all data:  0.993715252599
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 7.52247e-06
Norm of the params: 32.9446
                Loss: fixed 361 labels. Loss 0.77801. Accuracy 0.797.
Using normal model
LBFGS training took [545] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247532
Train loss (w/o reg) on all data: 0.194189
Test loss (w/o reg) on all data: 0.863065
Train acc on all data:  0.940778341794
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 4.66672e-05
Norm of the params: 32.6629
              Random: fixed 175 labels. Loss 0.86307. Accuracy 0.781.
### Flips: 1030, rs: 30, checks: 824
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164606
Train loss (w/o reg) on all data: 0.126804
Test loss (w/o reg) on all data: 0.52481
Train acc on all data:  0.959390862944
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 3.72849e-05
Norm of the params: 27.4961
     Influence (LOO): fixed 463 labels. Loss 0.52481. Accuracy 0.872.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117089
Train loss (w/o reg) on all data: 0.0660093
Test loss (w/o reg) on all data: 0.766787
Train acc on all data:  0.995890742084
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 1.61754e-05
Norm of the params: 31.9624
                Loss: fixed 414 labels. Loss 0.76679. Accuracy 0.813.
Using normal model
LBFGS training took [532] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238347
Train loss (w/o reg) on all data: 0.186075
Test loss (w/o reg) on all data: 0.904039
Train acc on all data:  0.941020062847
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 2.62696e-05
Norm of the params: 32.3332
              Random: fixed 225 labels. Loss 0.90404. Accuracy 0.790.
### Flips: 1030, rs: 30, checks: 1030
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152779
Train loss (w/o reg) on all data: 0.118235
Test loss (w/o reg) on all data: 0.452737
Train acc on all data:  0.960841189268
Test acc on all data:   0.887922705314
Norm of the mean of gradients: 5.12827e-06
Norm of the params: 26.2848
     Influence (LOO): fixed 528 labels. Loss 0.45274. Accuracy 0.888.
Using normal model
LBFGS training took [390] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107439
Train loss (w/o reg) on all data: 0.0588707
Test loss (w/o reg) on all data: 0.70288
Train acc on all data:  0.996132463138
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 9.91536e-06
Norm of the params: 31.1666
                Loss: fixed 455 labels. Loss 0.70288. Accuracy 0.830.
Using normal model
LBFGS training took [536] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22874
Train loss (w/o reg) on all data: 0.177868
Test loss (w/o reg) on all data: 0.898658
Train acc on all data:  0.945371041818
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 3.16477e-05
Norm of the params: 31.8975
              Random: fixed 275 labels. Loss 0.89866. Accuracy 0.794.
### Flips: 1030, rs: 30, checks: 1236
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143784
Train loss (w/o reg) on all data: 0.11207
Test loss (w/o reg) on all data: 0.447357
Train acc on all data:  0.960841189268
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 9.97803e-06
Norm of the params: 25.1848
     Influence (LOO): fixed 593 labels. Loss 0.44736. Accuracy 0.898.
Using normal model
LBFGS training took [361] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0942537
Train loss (w/o reg) on all data: 0.0500892
Test loss (w/o reg) on all data: 0.634977
Train acc on all data:  0.997582789461
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 4.28413e-05
Norm of the params: 29.7202
                Loss: fixed 512 labels. Loss 0.63498. Accuracy 0.845.
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219229
Train loss (w/o reg) on all data: 0.169931
Test loss (w/o reg) on all data: 0.870183
Train acc on all data:  0.949963741842
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 2.12108e-05
Norm of the params: 31.3999
              Random: fixed 320 labels. Loss 0.87018. Accuracy 0.805.
Using normal model
LBFGS training took [656] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273476
Train loss (w/o reg) on all data: 0.216388
Test loss (w/o reg) on all data: 0.925082
Train acc on all data:  0.927241962775
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 9.68693e-05
Norm of the params: 33.7902
Flipped loss: 0.92508. Accuracy: 0.766
### Flips: 1030, rs: 31, checks: 206
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.208964
Train loss (w/o reg) on all data: 0.159382
Test loss (w/o reg) on all data: 0.792015
Train acc on all data:  0.950930626058
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 4.67165e-05
Norm of the params: 31.4904
     Influence (LOO): fixed 160 labels. Loss 0.79202. Accuracy 0.797.
Using normal model
LBFGS training took [580] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190664
Train loss (w/o reg) on all data: 0.129094
Test loss (w/o reg) on all data: 0.998931
Train acc on all data:  0.9690597051
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 5.35044e-05
Norm of the params: 35.0913
                Loss: fixed 185 labels. Loss 0.99893. Accuracy 0.752.
Using normal model
LBFGS training took [606] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267142
Train loss (w/o reg) on all data: 0.210787
Test loss (w/o reg) on all data: 0.854413
Train acc on all data:  0.927483683829
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 8.14389e-05
Norm of the params: 33.5721
              Random: fixed  53 labels. Loss 0.85441. Accuracy 0.771.
### Flips: 1030, rs: 31, checks: 412
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184718
Train loss (w/o reg) on all data: 0.140968
Test loss (w/o reg) on all data: 0.739591
Train acc on all data:  0.955039883974
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 1.17668e-05
Norm of the params: 29.5803
     Influence (LOO): fixed 291 labels. Loss 0.73959. Accuracy 0.832.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154262
Train loss (w/o reg) on all data: 0.0956651
Test loss (w/o reg) on all data: 0.999223
Train acc on all data:  0.986705342035
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 0.000104129
Norm of the params: 34.2336
                Loss: fixed 299 labels. Loss 0.99922. Accuracy 0.782.
Using normal model
LBFGS training took [594] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259547
Train loss (w/o reg) on all data: 0.203979
Test loss (w/o reg) on all data: 0.80746
Train acc on all data:  0.930384336476
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 7.17319e-05
Norm of the params: 33.337
              Random: fixed 101 labels. Loss 0.80746. Accuracy 0.785.
### Flips: 1030, rs: 31, checks: 618
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17185
Train loss (w/o reg) on all data: 0.131717
Test loss (w/o reg) on all data: 0.651401
Train acc on all data:  0.958423978729
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 4.71197e-05
Norm of the params: 28.3315
     Influence (LOO): fixed 382 labels. Loss 0.65140. Accuracy 0.843.
Using normal model
LBFGS training took [485] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128142
Train loss (w/o reg) on all data: 0.073587
Test loss (w/o reg) on all data: 0.873413
Train acc on all data:  0.992748368383
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 2.69667e-05
Norm of the params: 33.0319
                Loss: fixed 391 labels. Loss 0.87341. Accuracy 0.814.
Using normal model
LBFGS training took [619] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251343
Train loss (w/o reg) on all data: 0.19657
Test loss (w/o reg) on all data: 0.764071
Train acc on all data:  0.938119410201
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 2.65269e-05
Norm of the params: 33.0976
              Random: fixed 150 labels. Loss 0.76407. Accuracy 0.790.
### Flips: 1030, rs: 31, checks: 824
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161949
Train loss (w/o reg) on all data: 0.124614
Test loss (w/o reg) on all data: 0.566442
Train acc on all data:  0.957457094513
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 7.89482e-06
Norm of the params: 27.3259
     Influence (LOO): fixed 468 labels. Loss 0.56644. Accuracy 0.862.
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111516
Train loss (w/o reg) on all data: 0.0618382
Test loss (w/o reg) on all data: 0.844373
Train acc on all data:  0.996374184191
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 3.66061e-05
Norm of the params: 31.5206
                Loss: fixed 451 labels. Loss 0.84437. Accuracy 0.821.
Using normal model
LBFGS training took [618] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242752
Train loss (w/o reg) on all data: 0.188997
Test loss (w/o reg) on all data: 0.697113
Train acc on all data:  0.940053178632
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 4.41679e-05
Norm of the params: 32.7886
              Random: fixed 205 labels. Loss 0.69711. Accuracy 0.816.
### Flips: 1030, rs: 31, checks: 1030
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152619
Train loss (w/o reg) on all data: 0.118043
Test loss (w/o reg) on all data: 0.544996
Train acc on all data:  0.957940536621
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 6.88868e-06
Norm of the params: 26.2966
     Influence (LOO): fixed 547 labels. Loss 0.54500. Accuracy 0.881.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10261
Train loss (w/o reg) on all data: 0.0557803
Test loss (w/o reg) on all data: 0.764756
Train acc on all data:  0.997341068407
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 4.75651e-05
Norm of the params: 30.6039
                Loss: fixed 490 labels. Loss 0.76476. Accuracy 0.841.
Using normal model
LBFGS training took [572] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23424
Train loss (w/o reg) on all data: 0.181136
Test loss (w/o reg) on all data: 0.677581
Train acc on all data:  0.943437273387
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 0.000111488
Norm of the params: 32.5896
              Random: fixed 260 labels. Loss 0.67758. Accuracy 0.814.
### Flips: 1030, rs: 31, checks: 1236
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142109
Train loss (w/o reg) on all data: 0.110087
Test loss (w/o reg) on all data: 0.478583
Train acc on all data:  0.959874305052
Test acc on all data:   0.896618357488
Norm of the mean of gradients: 4.05556e-05
Norm of the params: 25.3068
     Influence (LOO): fixed 615 labels. Loss 0.47858. Accuracy 0.897.
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0934365
Train loss (w/o reg) on all data: 0.0497791
Test loss (w/o reg) on all data: 0.654601
Train acc on all data:  0.997582789461
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 1.25644e-05
Norm of the params: 29.5491
                Loss: fixed 545 labels. Loss 0.65460. Accuracy 0.851.
Using normal model
LBFGS training took [619] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226153
Train loss (w/o reg) on all data: 0.173991
Test loss (w/o reg) on all data: 0.684174
Train acc on all data:  0.945129320764
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 8.67687e-05
Norm of the params: 32.2994
              Random: fixed 297 labels. Loss 0.68417. Accuracy 0.813.
Using normal model
LBFGS training took [621] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267147
Train loss (w/o reg) on all data: 0.210186
Test loss (w/o reg) on all data: 0.814788
Train acc on all data:  0.934735315446
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 7.82596e-05
Norm of the params: 33.7524
Flipped loss: 0.81479. Accuracy: 0.767
### Flips: 1030, rs: 32, checks: 206
Using normal model
LBFGS training took [449] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210186
Train loss (w/o reg) on all data: 0.158972
Test loss (w/o reg) on all data: 0.776358
Train acc on all data:  0.950205462896
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 2.15655e-05
Norm of the params: 32.0045
     Influence (LOO): fixed 152 labels. Loss 0.77636. Accuracy 0.795.
Using normal model
LBFGS training took [570] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18019
Train loss (w/o reg) on all data: 0.118142
Test loss (w/o reg) on all data: 0.779755
Train acc on all data:  0.978245105149
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 6.61356e-05
Norm of the params: 35.2274
                Loss: fixed 177 labels. Loss 0.77975. Accuracy 0.787.
Using normal model
LBFGS training took [535] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262461
Train loss (w/o reg) on all data: 0.206581
Test loss (w/o reg) on all data: 0.746852
Train acc on all data:  0.935702199662
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 4.94743e-05
Norm of the params: 33.4305
              Random: fixed  56 labels. Loss 0.74685. Accuracy 0.777.
### Flips: 1030, rs: 32, checks: 412
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190436
Train loss (w/o reg) on all data: 0.144837
Test loss (w/o reg) on all data: 0.653951
Train acc on all data:  0.954072999758
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 1.97284e-05
Norm of the params: 30.199
     Influence (LOO): fixed 279 labels. Loss 0.65395. Accuracy 0.825.
Using normal model
LBFGS training took [523] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149536
Train loss (w/o reg) on all data: 0.0906346
Test loss (w/o reg) on all data: 0.711779
Train acc on all data:  0.986947063089
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 7.35338e-05
Norm of the params: 34.3223
                Loss: fixed 282 labels. Loss 0.71178. Accuracy 0.808.
Using normal model
LBFGS training took [554] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254316
Train loss (w/o reg) on all data: 0.199651
Test loss (w/o reg) on all data: 0.740322
Train acc on all data:  0.936669083877
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 3.72386e-05
Norm of the params: 33.065
              Random: fixed 108 labels. Loss 0.74032. Accuracy 0.793.
### Flips: 1030, rs: 32, checks: 618
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174986
Train loss (w/o reg) on all data: 0.132993
Test loss (w/o reg) on all data: 0.539822
Train acc on all data:  0.958907420836
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 1.69211e-05
Norm of the params: 28.9805
     Influence (LOO): fixed 382 labels. Loss 0.53982. Accuracy 0.855.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13089
Train loss (w/o reg) on all data: 0.0753106
Test loss (w/o reg) on all data: 0.65559
Train acc on all data:  0.992264926275
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 1.70788e-05
Norm of the params: 33.3404
                Loss: fixed 356 labels. Loss 0.65559. Accuracy 0.809.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245837
Train loss (w/o reg) on all data: 0.192842
Test loss (w/o reg) on all data: 0.716007
Train acc on all data:  0.938602852308
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 5.11026e-05
Norm of the params: 32.556
              Random: fixed 158 labels. Loss 0.71601. Accuracy 0.799.
### Flips: 1030, rs: 32, checks: 824
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164754
Train loss (w/o reg) on all data: 0.125826
Test loss (w/o reg) on all data: 0.4932
Train acc on all data:  0.962774957699
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 2.44198e-05
Norm of the params: 27.9026
     Influence (LOO): fixed 465 labels. Loss 0.49320. Accuracy 0.877.
Using normal model
LBFGS training took [585] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116027
Train loss (w/o reg) on all data: 0.0645331
Test loss (w/o reg) on all data: 0.661521
Train acc on all data:  0.997099347353
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.32983e-05
Norm of the params: 32.0916
                Loss: fixed 421 labels. Loss 0.66152. Accuracy 0.830.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239264
Train loss (w/o reg) on all data: 0.187355
Test loss (w/o reg) on all data: 0.69067
Train acc on all data:  0.941261783901
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 2.68882e-05
Norm of the params: 32.2211
              Random: fixed 205 labels. Loss 0.69067. Accuracy 0.820.
### Flips: 1030, rs: 32, checks: 1030
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151186
Train loss (w/o reg) on all data: 0.115818
Test loss (w/o reg) on all data: 0.438493
Train acc on all data:  0.9659173314
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 3.00201e-05
Norm of the params: 26.5961
     Influence (LOO): fixed 542 labels. Loss 0.43849. Accuracy 0.893.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103976
Train loss (w/o reg) on all data: 0.0562155
Test loss (w/o reg) on all data: 0.631245
Train acc on all data:  0.997582789461
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 1.67493e-05
Norm of the params: 30.9065
                Loss: fixed 478 labels. Loss 0.63124. Accuracy 0.853.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.229136
Train loss (w/o reg) on all data: 0.177783
Test loss (w/o reg) on all data: 0.696582
Train acc on all data:  0.94488759971
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 6.2934e-05
Norm of the params: 32.0479
              Random: fixed 248 labels. Loss 0.69658. Accuracy 0.829.
### Flips: 1030, rs: 32, checks: 1236
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142634
Train loss (w/o reg) on all data: 0.109057
Test loss (w/o reg) on all data: 0.344372
Train acc on all data:  0.9690597051
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 6.96362e-06
Norm of the params: 25.9139
     Influence (LOO): fixed 612 labels. Loss 0.34437. Accuracy 0.906.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0978944
Train loss (w/o reg) on all data: 0.0522716
Test loss (w/o reg) on all data: 0.64249
Train acc on all data:  0.998066231569
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.13692e-05
Norm of the params: 30.2069
                Loss: fixed 518 labels. Loss 0.64249. Accuracy 0.858.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222592
Train loss (w/o reg) on all data: 0.172126
Test loss (w/o reg) on all data: 0.582242
Train acc on all data:  0.946579647087
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 0.000117269
Norm of the params: 31.7698
              Random: fixed 297 labels. Loss 0.58224. Accuracy 0.839.
Using normal model
LBFGS training took [614] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.285904
Train loss (w/o reg) on all data: 0.228424
Test loss (w/o reg) on all data: 0.796869
Train acc on all data:  0.919265167996
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 4.3414e-05
Norm of the params: 33.9058
Flipped loss: 0.79687. Accuracy: 0.768
### Flips: 1030, rs: 33, checks: 206
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217755
Train loss (w/o reg) on all data: 0.16689
Test loss (w/o reg) on all data: 0.71221
Train acc on all data:  0.94367899444
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 1.86431e-05
Norm of the params: 31.8953
     Influence (LOO): fixed 161 labels. Loss 0.71221. Accuracy 0.795.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195024
Train loss (w/o reg) on all data: 0.133468
Test loss (w/o reg) on all data: 0.762405
Train acc on all data:  0.968092820885
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 2.57938e-05
Norm of the params: 35.0872
                Loss: fixed 192 labels. Loss 0.76241. Accuracy 0.783.
Using normal model
LBFGS training took [557] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27929
Train loss (w/o reg) on all data: 0.22289
Test loss (w/o reg) on all data: 0.768788
Train acc on all data:  0.922649262751
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 3.31718e-05
Norm of the params: 33.586
              Random: fixed  48 labels. Loss 0.76879. Accuracy 0.786.
### Flips: 1030, rs: 33, checks: 412
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194514
Train loss (w/o reg) on all data: 0.149633
Test loss (w/o reg) on all data: 0.586414
Train acc on all data:  0.950205462896
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 5.04774e-05
Norm of the params: 29.9603
     Influence (LOO): fixed 289 labels. Loss 0.58641. Accuracy 0.827.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154191
Train loss (w/o reg) on all data: 0.095114
Test loss (w/o reg) on all data: 0.762706
Train acc on all data:  0.986221899927
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 3.25892e-05
Norm of the params: 34.3736
                Loss: fixed 313 labels. Loss 0.76271. Accuracy 0.794.
Using normal model
LBFGS training took [557] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271948
Train loss (w/o reg) on all data: 0.215926
Test loss (w/o reg) on all data: 0.720215
Train acc on all data:  0.927241962775
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 2.23349e-05
Norm of the params: 33.4729
              Random: fixed 106 labels. Loss 0.72022. Accuracy 0.791.
### Flips: 1030, rs: 33, checks: 618
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181377
Train loss (w/o reg) on all data: 0.140383
Test loss (w/o reg) on all data: 0.511385
Train acc on all data:  0.952380952381
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 1.11101e-05
Norm of the params: 28.6338
     Influence (LOO): fixed 387 labels. Loss 0.51138. Accuracy 0.859.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132009
Train loss (w/o reg) on all data: 0.0766008
Test loss (w/o reg) on all data: 0.659881
Train acc on all data:  0.990814599952
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 3.02013e-05
Norm of the params: 33.2892
                Loss: fixed 394 labels. Loss 0.65988. Accuracy 0.814.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263298
Train loss (w/o reg) on all data: 0.207946
Test loss (w/o reg) on all data: 0.681757
Train acc on all data:  0.933526710176
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 8.39543e-05
Norm of the params: 33.2723
              Random: fixed 151 labels. Loss 0.68176. Accuracy 0.794.
### Flips: 1030, rs: 33, checks: 824
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.168696
Train loss (w/o reg) on all data: 0.130611
Test loss (w/o reg) on all data: 0.448929
Train acc on all data:  0.95358955765
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 6.76777e-06
Norm of the params: 27.5986
     Influence (LOO): fixed 472 labels. Loss 0.44893. Accuracy 0.870.
Using normal model
LBFGS training took [490] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117332
Train loss (w/o reg) on all data: 0.065921
Test loss (w/o reg) on all data: 0.617753
Train acc on all data:  0.992264926275
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 1.69528e-05
Norm of the params: 32.066
                Loss: fixed 457 labels. Loss 0.61775. Accuracy 0.835.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256242
Train loss (w/o reg) on all data: 0.201238
Test loss (w/o reg) on all data: 0.693525
Train acc on all data:  0.935218757554
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 6.75675e-05
Norm of the params: 33.1677
              Random: fixed 200 labels. Loss 0.69352. Accuracy 0.796.
### Flips: 1030, rs: 33, checks: 1030
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.157361
Train loss (w/o reg) on all data: 0.121997
Test loss (w/o reg) on all data: 0.387872
Train acc on all data:  0.956973652405
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 1.06167e-05
Norm of the params: 26.5947
     Influence (LOO): fixed 540 labels. Loss 0.38787. Accuracy 0.893.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107135
Train loss (w/o reg) on all data: 0.059007
Test loss (w/o reg) on all data: 0.606017
Train acc on all data:  0.993473531545
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 1.6954e-05
Norm of the params: 31.0251
                Loss: fixed 511 labels. Loss 0.60602. Accuracy 0.838.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24898
Train loss (w/o reg) on all data: 0.194802
Test loss (w/o reg) on all data: 0.623676
Train acc on all data:  0.937877689147
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 7.32288e-05
Norm of the params: 32.9176
              Random: fixed 244 labels. Loss 0.62368. Accuracy 0.810.
### Flips: 1030, rs: 33, checks: 1236
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149227
Train loss (w/o reg) on all data: 0.116463
Test loss (w/o reg) on all data: 0.365289
Train acc on all data:  0.958182257675
Test acc on all data:   0.902415458937
Norm of the mean of gradients: 1.37206e-05
Norm of the params: 25.5986
     Influence (LOO): fixed 602 labels. Loss 0.36529. Accuracy 0.902.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0980124
Train loss (w/o reg) on all data: 0.0526874
Test loss (w/o reg) on all data: 0.584678
Train acc on all data:  0.99444041576
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 3.57889e-05
Norm of the params: 30.1081
                Loss: fixed 555 labels. Loss 0.58468. Accuracy 0.852.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238771
Train loss (w/o reg) on all data: 0.185582
Test loss (w/o reg) on all data: 0.594088
Train acc on all data:  0.941986947063
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 1.95037e-05
Norm of the params: 32.6158
              Random: fixed 294 labels. Loss 0.59409. Accuracy 0.807.
Using normal model
LBFGS training took [719] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268336
Train loss (w/o reg) on all data: 0.211982
Test loss (w/o reg) on all data: 1.23402
Train acc on all data:  0.927241962775
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 2.409e-05
Norm of the params: 33.5718
Flipped loss: 1.23402. Accuracy: 0.770
### Flips: 1030, rs: 34, checks: 206
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206026
Train loss (w/o reg) on all data: 0.158062
Test loss (w/o reg) on all data: 1.01149
Train acc on all data:  0.948271694465
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 3.51636e-05
Norm of the params: 30.9725
     Influence (LOO): fixed 166 labels. Loss 1.01149. Accuracy 0.795.
Using normal model
LBFGS training took [593] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180531
Train loss (w/o reg) on all data: 0.120883
Test loss (w/o reg) on all data: 1.19225
Train acc on all data:  0.973410684071
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 6.26668e-05
Norm of the params: 34.5394
                Loss: fixed 192 labels. Loss 1.19225. Accuracy 0.775.
Using normal model
LBFGS training took [597] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262395
Train loss (w/o reg) on all data: 0.206704
Test loss (w/o reg) on all data: 1.21212
Train acc on all data:  0.929900894368
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 0.000133617
Norm of the params: 33.3737
              Random: fixed  42 labels. Loss 1.21212. Accuracy 0.768.
### Flips: 1030, rs: 34, checks: 412
Using normal model
LBFGS training took [333] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18588
Train loss (w/o reg) on all data: 0.142973
Test loss (w/o reg) on all data: 0.920808
Train acc on all data:  0.953106115543
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 3.45666e-05
Norm of the params: 29.2943
     Influence (LOO): fixed 280 labels. Loss 0.92081. Accuracy 0.821.
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14436
Train loss (w/o reg) on all data: 0.086866
Test loss (w/o reg) on all data: 1.16243
Train acc on all data:  0.98888083152
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 6.09432e-05
Norm of the params: 33.9099
                Loss: fixed 306 labels. Loss 1.16243. Accuracy 0.791.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.255022
Train loss (w/o reg) on all data: 0.200732
Test loss (w/o reg) on all data: 1.15486
Train acc on all data:  0.931592941745
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 5.97949e-05
Norm of the params: 32.9515
              Random: fixed  94 labels. Loss 1.15486. Accuracy 0.784.
### Flips: 1030, rs: 34, checks: 618
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173266
Train loss (w/o reg) on all data: 0.133684
Test loss (w/o reg) on all data: 0.796939
Train acc on all data:  0.955765047136
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 2.642e-05
Norm of the params: 28.1361
     Influence (LOO): fixed 383 labels. Loss 0.79694. Accuracy 0.843.
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125531
Train loss (w/o reg) on all data: 0.0716681
Test loss (w/o reg) on all data: 0.949617
Train acc on all data:  0.99444041576
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 6.56432e-05
Norm of the params: 32.8216
                Loss: fixed 376 labels. Loss 0.94962. Accuracy 0.812.
Using normal model
LBFGS training took [579] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248872
Train loss (w/o reg) on all data: 0.195282
Test loss (w/o reg) on all data: 1.10629
Train acc on all data:  0.935943920715
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 8.89846e-05
Norm of the params: 32.7384
              Random: fixed 149 labels. Loss 1.10629. Accuracy 0.795.
### Flips: 1030, rs: 34, checks: 824
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161846
Train loss (w/o reg) on all data: 0.125686
Test loss (w/o reg) on all data: 0.621908
Train acc on all data:  0.957215373459
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 1.07639e-05
Norm of the params: 26.8925
     Influence (LOO): fixed 465 labels. Loss 0.62191. Accuracy 0.861.
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111159
Train loss (w/o reg) on all data: 0.0608488
Test loss (w/o reg) on all data: 0.929953
Train acc on all data:  0.997824510515
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 9.26847e-05
Norm of the params: 31.7208
                Loss: fixed 432 labels. Loss 0.92995. Accuracy 0.823.
Using normal model
LBFGS training took [553] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243697
Train loss (w/o reg) on all data: 0.191128
Test loss (w/o reg) on all data: 1.00651
Train acc on all data:  0.938361131255
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 6.56653e-05
Norm of the params: 32.4249
              Random: fixed 192 labels. Loss 1.00651. Accuracy 0.818.
### Flips: 1030, rs: 34, checks: 1030
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153325
Train loss (w/o reg) on all data: 0.119451
Test loss (w/o reg) on all data: 0.559017
Train acc on all data:  0.96035774716
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.21501e-05
Norm of the params: 26.0285
     Influence (LOO): fixed 531 labels. Loss 0.55902. Accuracy 0.877.
Using normal model
LBFGS training took [377] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0981566
Train loss (w/o reg) on all data: 0.0522923
Test loss (w/o reg) on all data: 0.855603
Train acc on all data:  0.998066231569
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 3.41382e-05
Norm of the params: 30.2867
                Loss: fixed 493 labels. Loss 0.85560. Accuracy 0.832.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.236209
Train loss (w/o reg) on all data: 0.184743
Test loss (w/o reg) on all data: 0.908793
Train acc on all data:  0.942712110225
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 0.000145037
Norm of the params: 32.083
              Random: fixed 239 labels. Loss 0.90879. Accuracy 0.814.
### Flips: 1030, rs: 34, checks: 1236
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143761
Train loss (w/o reg) on all data: 0.112147
Test loss (w/o reg) on all data: 0.484452
Train acc on all data:  0.961566352429
Test acc on all data:   0.898550724638
Norm of the mean of gradients: 1.10432e-05
Norm of the params: 25.1451
     Influence (LOO): fixed 597 labels. Loss 0.48445. Accuracy 0.899.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0881341
Train loss (w/o reg) on all data: 0.0461352
Test loss (w/o reg) on all data: 0.795837
Train acc on all data:  0.998307952623
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 1.92432e-05
Norm of the params: 28.9824
                Loss: fixed 554 labels. Loss 0.79584. Accuracy 0.853.
Using normal model
LBFGS training took [435] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227254
Train loss (w/o reg) on all data: 0.176934
Test loss (w/o reg) on all data: 0.805397
Train acc on all data:  0.944645878656
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 1.55598e-05
Norm of the params: 31.724
              Random: fixed 290 labels. Loss 0.80540. Accuracy 0.828.
Using normal model
LBFGS training took [664] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276374
Train loss (w/o reg) on all data: 0.221275
Test loss (w/o reg) on all data: 0.979116
Train acc on all data:  0.928450568044
Test acc on all data:   0.740096618357
Norm of the mean of gradients: 0.000108398
Norm of the params: 33.1963
Flipped loss: 0.97912. Accuracy: 0.740
### Flips: 1030, rs: 35, checks: 206
Using normal model
LBFGS training took [416] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214841
Train loss (w/o reg) on all data: 0.166425
Test loss (w/o reg) on all data: 0.910168
Train acc on all data:  0.948755136572
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 1.65852e-05
Norm of the params: 31.1181
     Influence (LOO): fixed 158 labels. Loss 0.91017. Accuracy 0.789.
Using normal model
LBFGS training took [604] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186402
Train loss (w/o reg) on all data: 0.126967
Test loss (w/o reg) on all data: 1.08148
Train acc on all data:  0.97026831037
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 2.4786e-05
Norm of the params: 34.4775
                Loss: fixed 189 labels. Loss 1.08148. Accuracy 0.741.
Using normal model
LBFGS training took [607] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26908
Train loss (w/o reg) on all data: 0.214529
Test loss (w/o reg) on all data: 0.895251
Train acc on all data:  0.929900894368
Test acc on all data:   0.757487922705
Norm of the mean of gradients: 2.83351e-05
Norm of the params: 33.0305
              Random: fixed  52 labels. Loss 0.89525. Accuracy 0.757.
### Flips: 1030, rs: 35, checks: 412
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187684
Train loss (w/o reg) on all data: 0.14537
Test loss (w/o reg) on all data: 0.787952
Train acc on all data:  0.953831278704
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 1.4487e-05
Norm of the params: 29.0908
     Influence (LOO): fixed 278 labels. Loss 0.78795. Accuracy 0.825.
Using normal model
LBFGS training took [646] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151101
Train loss (w/o reg) on all data: 0.0934279
Test loss (w/o reg) on all data: 1.05909
Train acc on all data:  0.986463620981
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 1.61559e-05
Norm of the params: 33.9627
                Loss: fixed 298 labels. Loss 1.05909. Accuracy 0.763.
Using normal model
LBFGS training took [612] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261817
Train loss (w/o reg) on all data: 0.208046
Test loss (w/o reg) on all data: 0.884502
Train acc on all data:  0.932318104907
Test acc on all data:   0.765217391304
Norm of the mean of gradients: 0.000153462
Norm of the params: 32.7937
              Random: fixed  99 labels. Loss 0.88450. Accuracy 0.765.
### Flips: 1030, rs: 35, checks: 618
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174003
Train loss (w/o reg) on all data: 0.134791
Test loss (w/o reg) on all data: 0.731284
Train acc on all data:  0.956248489243
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 6.91172e-06
Norm of the params: 28.0044
     Influence (LOO): fixed 384 labels. Loss 0.73128. Accuracy 0.854.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123982
Train loss (w/o reg) on all data: 0.0712667
Test loss (w/o reg) on all data: 0.9518
Train acc on all data:  0.99564902103
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.77026e-05
Norm of the params: 32.4701
                Loss: fixed 387 labels. Loss 0.95180. Accuracy 0.803.
Using normal model
LBFGS training took [620] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254238
Train loss (w/o reg) on all data: 0.201265
Test loss (w/o reg) on all data: 0.87587
Train acc on all data:  0.932801547015
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 9.2079e-05
Norm of the params: 32.5495
              Random: fixed 147 labels. Loss 0.87587. Accuracy 0.776.
### Flips: 1030, rs: 35, checks: 824
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162761
Train loss (w/o reg) on all data: 0.126835
Test loss (w/o reg) on all data: 0.697685
Train acc on all data:  0.958665699782
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 3.00392e-05
Norm of the params: 26.8053
     Influence (LOO): fixed 473 labels. Loss 0.69768. Accuracy 0.877.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109335
Train loss (w/o reg) on all data: 0.0605472
Test loss (w/o reg) on all data: 0.899641
Train acc on all data:  0.997341068407
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 1.27989e-05
Norm of the params: 31.237
                Loss: fixed 453 labels. Loss 0.89964. Accuracy 0.815.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248269
Train loss (w/o reg) on all data: 0.195563
Test loss (w/o reg) on all data: 0.84554
Train acc on all data:  0.937152525985
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 2.79516e-05
Norm of the params: 32.467
              Random: fixed 206 labels. Loss 0.84554. Accuracy 0.790.
### Flips: 1030, rs: 35, checks: 1030
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152199
Train loss (w/o reg) on all data: 0.118583
Test loss (w/o reg) on all data: 0.59657
Train acc on all data:  0.96035774716
Test acc on all data:   0.891787439614
Norm of the mean of gradients: 1.98722e-05
Norm of the params: 25.929
     Influence (LOO): fixed 546 labels. Loss 0.59657. Accuracy 0.892.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100585
Train loss (w/o reg) on all data: 0.0546299
Test loss (w/o reg) on all data: 0.81161
Train acc on all data:  0.998066231569
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 1.76105e-05
Norm of the params: 30.3168
                Loss: fixed 492 labels. Loss 0.81161. Accuracy 0.832.
Using normal model
LBFGS training took [610] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238936
Train loss (w/o reg) on all data: 0.187489
Test loss (w/o reg) on all data: 0.869297
Train acc on all data:  0.93932801547
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 8.1248e-05
Norm of the params: 32.077
              Random: fixed 259 labels. Loss 0.86930. Accuracy 0.804.
### Flips: 1030, rs: 35, checks: 1236
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146747
Train loss (w/o reg) on all data: 0.114756
Test loss (w/o reg) on all data: 0.535293
Train acc on all data:  0.961566352429
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 1.6961e-05
Norm of the params: 25.2946
     Influence (LOO): fixed 593 labels. Loss 0.53529. Accuracy 0.907.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0930315
Train loss (w/o reg) on all data: 0.0496899
Test loss (w/o reg) on all data: 0.799551
Train acc on all data:  0.998066231569
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.62963e-05
Norm of the params: 29.442
                Loss: fixed 531 labels. Loss 0.79955. Accuracy 0.840.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230321
Train loss (w/o reg) on all data: 0.179795
Test loss (w/o reg) on all data: 0.794779
Train acc on all data:  0.943195552333
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 5.38462e-05
Norm of the params: 31.7887
              Random: fixed 304 labels. Loss 0.79478. Accuracy 0.813.
Using normal model
LBFGS training took [658] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271515
Train loss (w/o reg) on all data: 0.215227
Test loss (w/o reg) on all data: 0.844906
Train acc on all data:  0.926033357505
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 3.81729e-05
Norm of the params: 33.5523
Flipped loss: 0.84491. Accuracy: 0.766
### Flips: 1030, rs: 36, checks: 206
Using normal model
LBFGS training took [379] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209084
Train loss (w/o reg) on all data: 0.16045
Test loss (w/o reg) on all data: 0.705648
Train acc on all data:  0.945854483926
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 1.70319e-05
Norm of the params: 31.1877
     Influence (LOO): fixed 156 labels. Loss 0.70565. Accuracy 0.796.
Using normal model
LBFGS training took [586] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178543
Train loss (w/o reg) on all data: 0.118392
Test loss (w/o reg) on all data: 0.841769
Train acc on all data:  0.973168963017
Test acc on all data:   0.772946859903
Norm of the mean of gradients: 4.00434e-05
Norm of the params: 34.6847
                Loss: fixed 184 labels. Loss 0.84177. Accuracy 0.773.
Using normal model
LBFGS training took [595] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.264819
Train loss (w/o reg) on all data: 0.209107
Test loss (w/o reg) on all data: 0.807117
Train acc on all data:  0.927725404883
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 9.76426e-05
Norm of the params: 33.3801
              Random: fixed  52 labels. Loss 0.80712. Accuracy 0.776.
### Flips: 1030, rs: 36, checks: 412
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183779
Train loss (w/o reg) on all data: 0.141029
Test loss (w/o reg) on all data: 0.612601
Train acc on all data:  0.957457094513
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 1.57808e-05
Norm of the params: 29.2403
     Influence (LOO): fixed 283 labels. Loss 0.61260. Accuracy 0.831.
Using normal model
LBFGS training took [521] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143576
Train loss (w/o reg) on all data: 0.0866295
Test loss (w/o reg) on all data: 0.782072
Train acc on all data:  0.987672226251
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 3.1005e-05
Norm of the params: 33.748
                Loss: fixed 295 labels. Loss 0.78207. Accuracy 0.795.
Using normal model
LBFGS training took [606] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.258219
Train loss (w/o reg) on all data: 0.203422
Test loss (w/o reg) on all data: 0.792586
Train acc on all data:  0.930867778584
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 2.6419e-05
Norm of the params: 33.1048
              Random: fixed 103 labels. Loss 0.79259. Accuracy 0.778.
### Flips: 1030, rs: 36, checks: 618
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169601
Train loss (w/o reg) on all data: 0.130611
Test loss (w/o reg) on all data: 0.548513
Train acc on all data:  0.960116026106
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 1.05841e-05
Norm of the params: 27.9252
     Influence (LOO): fixed 383 labels. Loss 0.54851. Accuracy 0.847.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125693
Train loss (w/o reg) on all data: 0.0723465
Test loss (w/o reg) on all data: 0.740206
Train acc on all data:  0.99008943679
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 8.21512e-06
Norm of the params: 32.664
                Loss: fixed 370 labels. Loss 0.74021. Accuracy 0.807.
Using normal model
LBFGS training took [591] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.250288
Train loss (w/o reg) on all data: 0.195885
Test loss (w/o reg) on all data: 0.744685
Train acc on all data:  0.935702199662
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 3.10946e-05
Norm of the params: 32.9856
              Random: fixed 151 labels. Loss 0.74469. Accuracy 0.776.
### Flips: 1030, rs: 36, checks: 824
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160614
Train loss (w/o reg) on all data: 0.124435
Test loss (w/o reg) on all data: 0.513772
Train acc on all data:  0.961808073483
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 2.53141e-05
Norm of the params: 26.8992
     Influence (LOO): fixed 456 labels. Loss 0.51377. Accuracy 0.875.
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114058
Train loss (w/o reg) on all data: 0.0635759
Test loss (w/o reg) on all data: 0.684859
Train acc on all data:  0.993473531545
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 2.00708e-05
Norm of the params: 31.775
                Loss: fixed 421 labels. Loss 0.68486. Accuracy 0.823.
Using normal model
LBFGS training took [622] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242173
Train loss (w/o reg) on all data: 0.189257
Test loss (w/o reg) on all data: 0.669592
Train acc on all data:  0.937394247039
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 7.35035e-05
Norm of the params: 32.5316
              Random: fixed 197 labels. Loss 0.66959. Accuracy 0.796.
### Flips: 1030, rs: 36, checks: 1030
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150163
Train loss (w/o reg) on all data: 0.116738
Test loss (w/o reg) on all data: 0.522169
Train acc on all data:  0.964467005076
Test acc on all data:   0.892753623188
Norm of the mean of gradients: 1.02016e-05
Norm of the params: 25.8552
     Influence (LOO): fixed 532 labels. Loss 0.52217. Accuracy 0.893.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10409
Train loss (w/o reg) on all data: 0.0568631
Test loss (w/o reg) on all data: 0.671198
Train acc on all data:  0.99444041576
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.937e-05
Norm of the params: 30.7335
                Loss: fixed 470 labels. Loss 0.67120. Accuracy 0.830.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23606
Train loss (w/o reg) on all data: 0.183852
Test loss (w/o reg) on all data: 0.627618
Train acc on all data:  0.940778341794
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 0.000108249
Norm of the params: 32.3134
              Random: fixed 239 labels. Loss 0.62762. Accuracy 0.811.
### Flips: 1030, rs: 36, checks: 1236
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141016
Train loss (w/o reg) on all data: 0.109562
Test loss (w/o reg) on all data: 0.468676
Train acc on all data:  0.9659173314
Test acc on all data:   0.906280193237
Norm of the mean of gradients: 1.12107e-05
Norm of the params: 25.0814
     Influence (LOO): fixed 592 labels. Loss 0.46868. Accuracy 0.906.
Using normal model
LBFGS training took [368] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0969285
Train loss (w/o reg) on all data: 0.0520918
Test loss (w/o reg) on all data: 0.664615
Train acc on all data:  0.998307952623
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 9.50482e-06
Norm of the params: 29.9455
                Loss: fixed 509 labels. Loss 0.66462. Accuracy 0.849.
Using normal model
LBFGS training took [612] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226939
Train loss (w/o reg) on all data: 0.175664
Test loss (w/o reg) on all data: 0.585799
Train acc on all data:  0.945371041818
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 4.31165e-05
Norm of the params: 32.0235
              Random: fixed 294 labels. Loss 0.58580. Accuracy 0.820.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275024
Train loss (w/o reg) on all data: 0.219715
Test loss (w/o reg) on all data: 0.873586
Train acc on all data:  0.922407541697
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 2.53219e-05
Norm of the params: 33.2593
Flipped loss: 0.87359. Accuracy: 0.755
### Flips: 1030, rs: 37, checks: 206
Using normal model
LBFGS training took [363] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217069
Train loss (w/o reg) on all data: 0.167927
Test loss (w/o reg) on all data: 0.810939
Train acc on all data:  0.945129320764
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 8.12637e-05
Norm of the params: 31.3503
     Influence (LOO): fixed 148 labels. Loss 0.81094. Accuracy 0.802.
Using normal model
LBFGS training took [500] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186451
Train loss (w/o reg) on all data: 0.127305
Test loss (w/o reg) on all data: 0.908657
Train acc on all data:  0.969543147208
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 2.87155e-05
Norm of the params: 34.3935
                Loss: fixed 190 labels. Loss 0.90866. Accuracy 0.784.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266855
Train loss (w/o reg) on all data: 0.212068
Test loss (w/o reg) on all data: 0.843223
Train acc on all data:  0.927725404883
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 8.25241e-05
Norm of the params: 33.102
              Random: fixed  43 labels. Loss 0.84322. Accuracy 0.766.
### Flips: 1030, rs: 37, checks: 412
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193435
Train loss (w/o reg) on all data: 0.150184
Test loss (w/o reg) on all data: 0.659826
Train acc on all data:  0.948029973411
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 1.06557e-05
Norm of the params: 29.4113
     Influence (LOO): fixed 276 labels. Loss 0.65983. Accuracy 0.833.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149388
Train loss (w/o reg) on all data: 0.091527
Test loss (w/o reg) on all data: 0.906065
Train acc on all data:  0.98888083152
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 1.28159e-05
Norm of the params: 34.0179
                Loss: fixed 302 labels. Loss 0.90606. Accuracy 0.794.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260986
Train loss (w/o reg) on all data: 0.206462
Test loss (w/o reg) on all data: 0.81698
Train acc on all data:  0.929659173314
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 6.1724e-05
Norm of the params: 33.0224
              Random: fixed  86 labels. Loss 0.81698. Accuracy 0.783.
### Flips: 1030, rs: 37, checks: 618
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17727
Train loss (w/o reg) on all data: 0.137407
Test loss (w/o reg) on all data: 0.498079
Train acc on all data:  0.952622673435
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 6.65318e-05
Norm of the params: 28.2358
     Influence (LOO): fixed 383 labels. Loss 0.49808. Accuracy 0.861.
Using normal model
LBFGS training took [400] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127478
Train loss (w/o reg) on all data: 0.0734859
Test loss (w/o reg) on all data: 0.823696
Train acc on all data:  0.992264926275
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 1.08101e-05
Norm of the params: 32.8609
                Loss: fixed 385 labels. Loss 0.82370. Accuracy 0.814.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251738
Train loss (w/o reg) on all data: 0.197754
Test loss (w/o reg) on all data: 0.803093
Train acc on all data:  0.93376843123
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 3.60282e-05
Norm of the params: 32.8585
              Random: fixed 137 labels. Loss 0.80309. Accuracy 0.794.
### Flips: 1030, rs: 37, checks: 824
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164916
Train loss (w/o reg) on all data: 0.128624
Test loss (w/o reg) on all data: 0.456411
Train acc on all data:  0.954072999758
Test acc on all data:   0.88115942029
Norm of the mean of gradients: 6.54197e-06
Norm of the params: 26.9413
     Influence (LOO): fixed 474 labels. Loss 0.45641. Accuracy 0.881.
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114292
Train loss (w/o reg) on all data: 0.0643003
Test loss (w/o reg) on all data: 0.763468
Train acc on all data:  0.993956973652
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 1.31373e-05
Norm of the params: 31.6202
                Loss: fixed 438 labels. Loss 0.76347. Accuracy 0.822.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24343
Train loss (w/o reg) on all data: 0.190387
Test loss (w/o reg) on all data: 0.739478
Train acc on all data:  0.938119410201
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 2.06891e-05
Norm of the params: 32.5709
              Random: fixed 199 labels. Loss 0.73948. Accuracy 0.801.
### Flips: 1030, rs: 37, checks: 1030
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154873
Train loss (w/o reg) on all data: 0.121606
Test loss (w/o reg) on all data: 0.390514
Train acc on all data:  0.957457094513
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 2.73783e-05
Norm of the params: 25.7945
     Influence (LOO): fixed 547 labels. Loss 0.39051. Accuracy 0.887.
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107416
Train loss (w/o reg) on all data: 0.0599039
Test loss (w/o reg) on all data: 0.695194
Train acc on all data:  0.993715252599
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 2.45427e-05
Norm of the params: 30.8261
                Loss: fixed 482 labels. Loss 0.69519. Accuracy 0.839.
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232659
Train loss (w/o reg) on all data: 0.181017
Test loss (w/o reg) on all data: 0.701753
Train acc on all data:  0.940778341794
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 6.22937e-05
Norm of the params: 32.138
              Random: fixed 260 labels. Loss 0.70175. Accuracy 0.803.
### Flips: 1030, rs: 37, checks: 1236
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144928
Train loss (w/o reg) on all data: 0.113936
Test loss (w/o reg) on all data: 0.335291
Train acc on all data:  0.958907420836
Test acc on all data:   0.907246376812
Norm of the mean of gradients: 2.01825e-05
Norm of the params: 24.8966
     Influence (LOO): fixed 614 labels. Loss 0.33529. Accuracy 0.907.
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0959766
Train loss (w/o reg) on all data: 0.0519307
Test loss (w/o reg) on all data: 0.660868
Train acc on all data:  0.998307952623
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 5.46105e-05
Norm of the params: 29.6802
                Loss: fixed 543 labels. Loss 0.66087. Accuracy 0.855.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223735
Train loss (w/o reg) on all data: 0.172723
Test loss (w/o reg) on all data: 0.69149
Train acc on all data:  0.947304810249
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 1.21326e-05
Norm of the params: 31.9413
              Random: fixed 313 labels. Loss 0.69149. Accuracy 0.813.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276495
Train loss (w/o reg) on all data: 0.220577
Test loss (w/o reg) on all data: 0.875105
Train acc on all data:  0.926033357505
Test acc on all data:   0.731400966184
Norm of the mean of gradients: 2.05555e-05
Norm of the params: 33.442
Flipped loss: 0.87510. Accuracy: 0.731
### Flips: 1030, rs: 38, checks: 206
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215365
Train loss (w/o reg) on all data: 0.167179
Test loss (w/o reg) on all data: 0.817598
Train acc on all data:  0.94367899444
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 5.43878e-05
Norm of the params: 31.0437
     Influence (LOO): fixed 162 labels. Loss 0.81760. Accuracy 0.784.
Using normal model
LBFGS training took [484] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187133
Train loss (w/o reg) on all data: 0.127447
Test loss (w/o reg) on all data: 0.932367
Train acc on all data:  0.971476915639
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 0.000104566
Norm of the params: 34.5503
                Loss: fixed 196 labels. Loss 0.93237. Accuracy 0.749.
Using normal model
LBFGS training took [484] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268705
Train loss (w/o reg) on all data: 0.213865
Test loss (w/o reg) on all data: 0.887138
Train acc on all data:  0.928692289098
Test acc on all data:   0.744927536232
Norm of the mean of gradients: 3.26627e-05
Norm of the params: 33.1179
              Random: fixed  52 labels. Loss 0.88714. Accuracy 0.745.
### Flips: 1030, rs: 38, checks: 412
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190442
Train loss (w/o reg) on all data: 0.147442
Test loss (w/o reg) on all data: 0.613328
Train acc on all data:  0.950930626058
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 1.18603e-05
Norm of the params: 29.3258
     Influence (LOO): fixed 282 labels. Loss 0.61333. Accuracy 0.828.
Using normal model
LBFGS training took [462] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143825
Train loss (w/o reg) on all data: 0.087238
Test loss (w/o reg) on all data: 0.827769
Train acc on all data:  0.989605994682
Test acc on all data:   0.772946859903
Norm of the mean of gradients: 1.0972e-05
Norm of the params: 33.6412
                Loss: fixed 309 labels. Loss 0.82777. Accuracy 0.773.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2616
Train loss (w/o reg) on all data: 0.207197
Test loss (w/o reg) on all data: 0.876967
Train acc on all data:  0.931109499637
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 1.69997e-05
Norm of the params: 32.9858
              Random: fixed 107 labels. Loss 0.87697. Accuracy 0.761.
### Flips: 1030, rs: 38, checks: 618
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178254
Train loss (w/o reg) on all data: 0.13819
Test loss (w/o reg) on all data: 0.576234
Train acc on all data:  0.955281605028
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 2.38427e-05
Norm of the params: 28.3067
     Influence (LOO): fixed 369 labels. Loss 0.57623. Accuracy 0.843.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125565
Train loss (w/o reg) on all data: 0.0726805
Test loss (w/o reg) on all data: 0.761138
Train acc on all data:  0.991781484167
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 4.51794e-05
Norm of the params: 32.5221
                Loss: fixed 395 labels. Loss 0.76114. Accuracy 0.799.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25111
Train loss (w/o reg) on all data: 0.197407
Test loss (w/o reg) on all data: 0.843744
Train acc on all data:  0.936185641769
Test acc on all data:   0.76231884058
Norm of the mean of gradients: 4.5861e-05
Norm of the params: 32.773
              Random: fixed 150 labels. Loss 0.84374. Accuracy 0.762.
### Flips: 1030, rs: 38, checks: 824
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167108
Train loss (w/o reg) on all data: 0.12952
Test loss (w/o reg) on all data: 0.489161
Train acc on all data:  0.957698815567
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 8.54983e-06
Norm of the params: 27.4181
     Influence (LOO): fixed 456 labels. Loss 0.48916. Accuracy 0.864.
Using normal model
LBFGS training took [393] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110805
Train loss (w/o reg) on all data: 0.061909
Test loss (w/o reg) on all data: 0.733246
Train acc on all data:  0.993715252599
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 1.16751e-05
Norm of the params: 31.2718
                Loss: fixed 457 labels. Loss 0.73325. Accuracy 0.805.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244349
Train loss (w/o reg) on all data: 0.191068
Test loss (w/o reg) on all data: 0.770474
Train acc on all data:  0.937152525985
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 3.38499e-05
Norm of the params: 32.644
              Random: fixed 206 labels. Loss 0.77047. Accuracy 0.775.
### Flips: 1030, rs: 38, checks: 1030
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155358
Train loss (w/o reg) on all data: 0.120919
Test loss (w/o reg) on all data: 0.407921
Train acc on all data:  0.959390862944
Test acc on all data:   0.88309178744
Norm of the mean of gradients: 1.07943e-05
Norm of the params: 26.2447
     Influence (LOO): fixed 533 labels. Loss 0.40792. Accuracy 0.883.
Using normal model
LBFGS training took [401] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101532
Train loss (w/o reg) on all data: 0.0557058
Test loss (w/o reg) on all data: 0.640978
Train acc on all data:  0.994198694706
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 2.3587e-05
Norm of the params: 30.2741
                Loss: fixed 500 labels. Loss 0.64098. Accuracy 0.830.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.236301
Train loss (w/o reg) on all data: 0.18466
Test loss (w/o reg) on all data: 0.736085
Train acc on all data:  0.941503504955
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 0.000103777
Norm of the params: 32.1376
              Random: fixed 261 labels. Loss 0.73608. Accuracy 0.790.
### Flips: 1030, rs: 38, checks: 1236
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148128
Train loss (w/o reg) on all data: 0.116076
Test loss (w/o reg) on all data: 0.363668
Train acc on all data:  0.960841189268
Test acc on all data:   0.899516908213
Norm of the mean of gradients: 1.73685e-05
Norm of the params: 25.3187
     Influence (LOO): fixed 588 labels. Loss 0.36367. Accuracy 0.900.
Using normal model
LBFGS training took [385] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0938061
Train loss (w/o reg) on all data: 0.050797
Test loss (w/o reg) on all data: 0.567632
Train acc on all data:  0.997824510515
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 1.82079e-05
Norm of the params: 29.3289
                Loss: fixed 551 labels. Loss 0.56763. Accuracy 0.846.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224966
Train loss (w/o reg) on all data: 0.174826
Test loss (w/o reg) on all data: 0.717956
Train acc on all data:  0.944404157602
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 4.80263e-05
Norm of the params: 31.6672
              Random: fixed 319 labels. Loss 0.71796. Accuracy 0.802.
Using normal model
LBFGS training took [711] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268872
Train loss (w/o reg) on all data: 0.213393
Test loss (w/o reg) on all data: 0.737812
Train acc on all data:  0.924583031182
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 2.08626e-05
Norm of the params: 33.3103
Flipped loss: 0.73781. Accuracy: 0.791
### Flips: 1030, rs: 39, checks: 206
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207278
Train loss (w/o reg) on all data: 0.15912
Test loss (w/o reg) on all data: 0.710369
Train acc on all data:  0.945612762872
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 1.70808e-05
Norm of the params: 31.035
     Influence (LOO): fixed 163 labels. Loss 0.71037. Accuracy 0.816.
Using normal model
LBFGS training took [545] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181573
Train loss (w/o reg) on all data: 0.122711
Test loss (w/o reg) on all data: 0.725734
Train acc on all data:  0.970751752478
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 3.08943e-05
Norm of the params: 34.3107
                Loss: fixed 192 labels. Loss 0.72573. Accuracy 0.814.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261937
Train loss (w/o reg) on all data: 0.207036
Test loss (w/o reg) on all data: 0.722553
Train acc on all data:  0.925791636452
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 6.27512e-05
Norm of the params: 33.1363
              Random: fixed  48 labels. Loss 0.72255. Accuracy 0.802.
### Flips: 1030, rs: 39, checks: 412
Using normal model
LBFGS training took [445] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185519
Train loss (w/o reg) on all data: 0.142362
Test loss (w/o reg) on all data: 0.583107
Train acc on all data:  0.953347836597
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 4.52975e-05
Norm of the params: 29.3793
     Influence (LOO): fixed 281 labels. Loss 0.58311. Accuracy 0.852.
Using normal model
LBFGS training took [596] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142094
Train loss (w/o reg) on all data: 0.0857093
Test loss (w/o reg) on all data: 0.755327
Train acc on all data:  0.987430505197
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 8.38325e-05
Norm of the params: 33.5812
                Loss: fixed 310 labels. Loss 0.75533. Accuracy 0.819.
Using normal model
LBFGS training took [593] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256359
Train loss (w/o reg) on all data: 0.202593
Test loss (w/o reg) on all data: 0.690797
Train acc on all data:  0.928692289098
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 6.9832e-05
Norm of the params: 32.792
              Random: fixed 104 labels. Loss 0.69080. Accuracy 0.811.
### Flips: 1030, rs: 39, checks: 618
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174076
Train loss (w/o reg) on all data: 0.134827
Test loss (w/o reg) on all data: 0.5227
Train acc on all data:  0.95479816292
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 7.59982e-05
Norm of the params: 28.0173
     Influence (LOO): fixed 382 labels. Loss 0.52270. Accuracy 0.873.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122334
Train loss (w/o reg) on all data: 0.0698147
Test loss (w/o reg) on all data: 0.726446
Train acc on all data:  0.993473531545
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 2.06294e-05
Norm of the params: 32.4097
                Loss: fixed 383 labels. Loss 0.72645. Accuracy 0.832.
Using normal model
LBFGS training took [579] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24828
Train loss (w/o reg) on all data: 0.195661
Test loss (w/o reg) on all data: 0.684523
Train acc on all data:  0.932076383853
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 8.90111e-05
Norm of the params: 32.4402
              Random: fixed 156 labels. Loss 0.68452. Accuracy 0.816.
### Flips: 1030, rs: 39, checks: 824
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162694
Train loss (w/o reg) on all data: 0.126485
Test loss (w/o reg) on all data: 0.563331
Train acc on all data:  0.956731931351
Test acc on all data:   0.889855072464
Norm of the mean of gradients: 2.11597e-05
Norm of the params: 26.9106
     Influence (LOO): fixed 468 labels. Loss 0.56333. Accuracy 0.890.
Using normal model
LBFGS training took [420] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110274
Train loss (w/o reg) on all data: 0.0613511
Test loss (w/o reg) on all data: 0.794338
Train acc on all data:  0.994198694706
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 3.13807e-05
Norm of the params: 31.2802
                Loss: fixed 434 labels. Loss 0.79434. Accuracy 0.840.
Using normal model
LBFGS training took [607] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245436
Train loss (w/o reg) on all data: 0.193588
Test loss (w/o reg) on all data: 0.620706
Train acc on all data:  0.931592941745
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 6.35451e-05
Norm of the params: 32.2019
              Random: fixed 196 labels. Loss 0.62071. Accuracy 0.828.
### Flips: 1030, rs: 39, checks: 1030
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154347
Train loss (w/o reg) on all data: 0.12025
Test loss (w/o reg) on all data: 0.579835
Train acc on all data:  0.958907420836
Test acc on all data:   0.897584541063
Norm of the mean of gradients: 7.60362e-06
Norm of the params: 26.1139
     Influence (LOO): fixed 526 labels. Loss 0.57983. Accuracy 0.898.
Using normal model
LBFGS training took [419] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100928
Train loss (w/o reg) on all data: 0.0546131
Test loss (w/o reg) on all data: 0.740443
Train acc on all data:  0.998066231569
Test acc on all data:   0.844444444444
Norm of the mean of gradients: 9.36125e-06
Norm of the params: 30.4352
                Loss: fixed 482 labels. Loss 0.74044. Accuracy 0.844.
Using normal model
LBFGS training took [616] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23339
Train loss (w/o reg) on all data: 0.182689
Test loss (w/o reg) on all data: 0.598195
Train acc on all data:  0.936910804931
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 3.93142e-05
Norm of the params: 31.8439
              Random: fixed 250 labels. Loss 0.59819. Accuracy 0.826.
### Flips: 1030, rs: 39, checks: 1236
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144969
Train loss (w/o reg) on all data: 0.113354
Test loss (w/o reg) on all data: 0.553445
Train acc on all data:  0.959632583998
Test acc on all data:   0.913043478261
Norm of the mean of gradients: 5.83702e-06
Norm of the params: 25.1456
     Influence (LOO): fixed 595 labels. Loss 0.55344. Accuracy 0.913.
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0910589
Train loss (w/o reg) on all data: 0.0479797
Test loss (w/o reg) on all data: 0.643729
Train acc on all data:  0.997824510515
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.2619e-05
Norm of the params: 29.3527
                Loss: fixed 531 labels. Loss 0.64373. Accuracy 0.858.
Using normal model
LBFGS training took [594] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222125
Train loss (w/o reg) on all data: 0.17227
Test loss (w/o reg) on all data: 0.547808
Train acc on all data:  0.942953831279
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 3.09691e-05
Norm of the params: 31.577
              Random: fixed 303 labels. Loss 0.54781. Accuracy 0.835.
Using normal model
LBFGS training took [783] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.297701
Train loss (w/o reg) on all data: 0.239484
Test loss (w/o reg) on all data: 1.03413
Train acc on all data:  0.913463862702
Test acc on all data:   0.710144927536
Norm of the mean of gradients: 2.14181e-05
Norm of the params: 34.1222
Flipped loss: 1.03413. Accuracy: 0.710
### Flips: 1236, rs: 0, checks: 206
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242976
Train loss (w/o reg) on all data: 0.19102
Test loss (w/o reg) on all data: 0.9542
Train acc on all data:  0.9349770365
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 3.71994e-05
Norm of the params: 32.2355
     Influence (LOO): fixed 148 labels. Loss 0.95420. Accuracy 0.760.
Using normal model
LBFGS training took [622] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21851
Train loss (w/o reg) on all data: 0.154984
Test loss (w/o reg) on all data: 1.03167
Train acc on all data:  0.95600676819
Test acc on all data:   0.725603864734
Norm of the mean of gradients: 8.24736e-05
Norm of the params: 35.6443
                Loss: fixed 174 labels. Loss 1.03167. Accuracy 0.726.
Using normal model
LBFGS training took [584] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.289722
Train loss (w/o reg) on all data: 0.232375
Test loss (w/o reg) on all data: 0.963226
Train acc on all data:  0.919265167996
Test acc on all data:   0.739130434783
Norm of the mean of gradients: 3.86658e-05
Norm of the params: 33.8664
              Random: fixed  63 labels. Loss 0.96323. Accuracy 0.739.
### Flips: 1236, rs: 0, checks: 412
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221265
Train loss (w/o reg) on all data: 0.173172
Test loss (w/o reg) on all data: 0.864138
Train acc on all data:  0.939086294416
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 0.000105985
Norm of the params: 31.014
     Influence (LOO): fixed 270 labels. Loss 0.86414. Accuracy 0.772.
Using normal model
LBFGS training took [663] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178847
Train loss (w/o reg) on all data: 0.11611
Test loss (w/o reg) on all data: 1.02272
Train acc on all data:  0.980420594634
Test acc on all data:   0.744927536232
Norm of the mean of gradients: 2.26985e-05
Norm of the params: 35.4222
                Loss: fixed 290 labels. Loss 1.02272. Accuracy 0.745.
Using normal model
LBFGS training took [612] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.281519
Train loss (w/o reg) on all data: 0.224677
Test loss (w/o reg) on all data: 0.945704
Train acc on all data:  0.92506647329
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 0.000122639
Norm of the params: 33.7169
              Random: fixed 127 labels. Loss 0.94570. Accuracy 0.752.
### Flips: 1236, rs: 0, checks: 618
Using normal model
LBFGS training took [455] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210281
Train loss (w/o reg) on all data: 0.164708
Test loss (w/o reg) on all data: 0.735658
Train acc on all data:  0.941986947063
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 3.49838e-05
Norm of the params: 30.1905
     Influence (LOO): fixed 373 labels. Loss 0.73566. Accuracy 0.797.
Using normal model
LBFGS training took [555] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154342
Train loss (w/o reg) on all data: 0.0940022
Test loss (w/o reg) on all data: 0.987093
Train acc on all data:  0.990814599952
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 4.37068e-05
Norm of the params: 34.7389
                Loss: fixed 389 labels. Loss 0.98709. Accuracy 0.767.
Using normal model
LBFGS training took [624] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.274167
Train loss (w/o reg) on all data: 0.218046
Test loss (w/o reg) on all data: 0.919135
Train acc on all data:  0.928934010152
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 2.71716e-05
Norm of the params: 33.5026
              Random: fixed 189 labels. Loss 0.91914. Accuracy 0.767.
### Flips: 1236, rs: 0, checks: 824
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197526
Train loss (w/o reg) on all data: 0.155344
Test loss (w/o reg) on all data: 0.701811
Train acc on all data:  0.944645878656
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 1.35723e-05
Norm of the params: 29.0453
     Influence (LOO): fixed 465 labels. Loss 0.70181. Accuracy 0.825.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137612
Train loss (w/o reg) on all data: 0.0803064
Test loss (w/o reg) on all data: 0.914729
Train acc on all data:  0.995165578922
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 4.27153e-05
Norm of the params: 33.8542
                Loss: fixed 465 labels. Loss 0.91473. Accuracy 0.769.
Using normal model
LBFGS training took [632] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265374
Train loss (w/o reg) on all data: 0.210552
Test loss (w/o reg) on all data: 0.88968
Train acc on all data:  0.92941745226
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 2.00118e-05
Norm of the params: 33.1127
              Random: fixed 249 labels. Loss 0.88968. Accuracy 0.768.
### Flips: 1236, rs: 0, checks: 1030
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18494
Train loss (w/o reg) on all data: 0.145181
Test loss (w/o reg) on all data: 0.60926
Train acc on all data:  0.950205462896
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 4.33198e-05
Norm of the params: 28.1989
     Influence (LOO): fixed 559 labels. Loss 0.60926. Accuracy 0.840.
Using normal model
LBFGS training took [546] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127807
Train loss (w/o reg) on all data: 0.0723321
Test loss (w/o reg) on all data: 0.84869
Train acc on all data:  0.996857626299
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 3.99451e-05
Norm of the params: 33.3092
                Loss: fixed 518 labels. Loss 0.84869. Accuracy 0.796.
Using normal model
LBFGS training took [592] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.255892
Train loss (w/o reg) on all data: 0.20227
Test loss (w/o reg) on all data: 0.853325
Train acc on all data:  0.930867778584
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 0.000153588
Norm of the params: 32.7482
              Random: fixed 307 labels. Loss 0.85333. Accuracy 0.782.
### Flips: 1236, rs: 0, checks: 1236
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17443
Train loss (w/o reg) on all data: 0.136761
Test loss (w/o reg) on all data: 0.502396
Train acc on all data:  0.953106115543
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 2.03213e-05
Norm of the params: 27.4478
     Influence (LOO): fixed 631 labels. Loss 0.50240. Accuracy 0.865.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118193
Train loss (w/o reg) on all data: 0.0658397
Test loss (w/o reg) on all data: 0.82885
Train acc on all data:  0.996374184191
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 1.71616e-05
Norm of the params: 32.3585
                Loss: fixed 562 labels. Loss 0.82885. Accuracy 0.798.
Using normal model
LBFGS training took [595] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246433
Train loss (w/o reg) on all data: 0.19362
Test loss (w/o reg) on all data: 0.834079
Train acc on all data:  0.936427362823
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 9.99046e-05
Norm of the params: 32.4999
              Random: fixed 369 labels. Loss 0.83408. Accuracy 0.786.
Using normal model
LBFGS training took [591] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299476
Train loss (w/o reg) on all data: 0.242979
Test loss (w/o reg) on all data: 0.986926
Train acc on all data:  0.911046652163
Test acc on all data:   0.746859903382
Norm of the mean of gradients: 9.34539e-05
Norm of the params: 33.6147
Flipped loss: 0.98693. Accuracy: 0.747
### Flips: 1236, rs: 1, checks: 206
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242431
Train loss (w/o reg) on all data: 0.18947
Test loss (w/o reg) on all data: 0.985409
Train acc on all data:  0.934735315446
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 1.65171e-05
Norm of the params: 32.5457
     Influence (LOO): fixed 150 labels. Loss 0.98541. Accuracy 0.768.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221802
Train loss (w/o reg) on all data: 0.159195
Test loss (w/o reg) on all data: 1.05585
Train acc on all data:  0.955281605028
Test acc on all data:   0.724637681159
Norm of the mean of gradients: 4.31679e-05
Norm of the params: 35.3858
                Loss: fixed 187 labels. Loss 1.05585. Accuracy 0.725.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291478
Train loss (w/o reg) on all data: 0.234907
Test loss (w/o reg) on all data: 0.96802
Train acc on all data:  0.914672467972
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 0.000230411
Norm of the params: 33.6368
              Random: fixed  59 labels. Loss 0.96802. Accuracy 0.753.
### Flips: 1236, rs: 1, checks: 412
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22065
Train loss (w/o reg) on all data: 0.172401
Test loss (w/o reg) on all data: 0.91663
Train acc on all data:  0.938844573362
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 5.49153e-05
Norm of the params: 31.0643
     Influence (LOO): fixed 270 labels. Loss 0.91663. Accuracy 0.791.
Using normal model
LBFGS training took [554] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177155
Train loss (w/o reg) on all data: 0.114148
Test loss (w/o reg) on all data: 1.01054
Train acc on all data:  0.978003384095
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 1.63976e-05
Norm of the params: 35.4984
                Loss: fixed 319 labels. Loss 1.01054. Accuracy 0.757.
Using normal model
LBFGS training took [556] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282465
Train loss (w/o reg) on all data: 0.227166
Test loss (w/o reg) on all data: 0.944321
Train acc on all data:  0.919748610104
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 0.000116311
Norm of the params: 33.2564
              Random: fixed 131 labels. Loss 0.94432. Accuracy 0.771.
### Flips: 1236, rs: 1, checks: 618
Using normal model
LBFGS training took [416] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205299
Train loss (w/o reg) on all data: 0.160403
Test loss (w/o reg) on all data: 0.76574
Train acc on all data:  0.942470389171
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 5.3947e-05
Norm of the params: 29.9656
     Influence (LOO): fixed 374 labels. Loss 0.76574. Accuracy 0.819.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152822
Train loss (w/o reg) on all data: 0.0920458
Test loss (w/o reg) on all data: 0.970241
Train acc on all data:  0.988155668359
Test acc on all data:   0.773913043478
Norm of the mean of gradients: 3.12535e-05
Norm of the params: 34.8643
                Loss: fixed 399 labels. Loss 0.97024. Accuracy 0.774.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273363
Train loss (w/o reg) on all data: 0.218666
Test loss (w/o reg) on all data: 0.897049
Train acc on all data:  0.92385786802
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 2.64239e-05
Norm of the params: 33.0748
              Random: fixed 196 labels. Loss 0.89705. Accuracy 0.784.
### Flips: 1236, rs: 1, checks: 824
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191759
Train loss (w/o reg) on all data: 0.14953
Test loss (w/o reg) on all data: 0.681404
Train acc on all data:  0.944162436548
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 4.00532e-05
Norm of the params: 29.0615
     Influence (LOO): fixed 472 labels. Loss 0.68140. Accuracy 0.823.
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140181
Train loss (w/o reg) on all data: 0.0815185
Test loss (w/o reg) on all data: 0.880018
Train acc on all data:  0.989847715736
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 6.38833e-05
Norm of the params: 34.2528
                Loss: fixed 446 labels. Loss 0.88002. Accuracy 0.781.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267533
Train loss (w/o reg) on all data: 0.212881
Test loss (w/o reg) on all data: 0.859829
Train acc on all data:  0.926275078559
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 4.62854e-05
Norm of the params: 33.0611
              Random: fixed 242 labels. Loss 0.85983. Accuracy 0.788.
### Flips: 1236, rs: 1, checks: 1030
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180241
Train loss (w/o reg) on all data: 0.140163
Test loss (w/o reg) on all data: 0.623837
Train acc on all data:  0.947304810249
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 7.59107e-06
Norm of the params: 28.3118
     Influence (LOO): fixed 546 labels. Loss 0.62384. Accuracy 0.859.
Using normal model
LBFGS training took [535] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129421
Train loss (w/o reg) on all data: 0.0733316
Test loss (w/o reg) on all data: 0.865188
Train acc on all data:  0.991781484167
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 3.56228e-05
Norm of the params: 33.4932
                Loss: fixed 503 labels. Loss 0.86519. Accuracy 0.794.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251875
Train loss (w/o reg) on all data: 0.198167
Test loss (w/o reg) on all data: 0.80624
Train acc on all data:  0.93376843123
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 2.8398e-05
Norm of the params: 32.7742
              Random: fixed 314 labels. Loss 0.80624. Accuracy 0.807.
### Flips: 1236, rs: 1, checks: 1236
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171373
Train loss (w/o reg) on all data: 0.133308
Test loss (w/o reg) on all data: 0.630444
Train acc on all data:  0.950930626058
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 1.19766e-05
Norm of the params: 27.5914
     Influence (LOO): fixed 627 labels. Loss 0.63044. Accuracy 0.872.
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116971
Train loss (w/o reg) on all data: 0.0646506
Test loss (w/o reg) on all data: 0.842429
Train acc on all data:  0.993473531545
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 1.67137e-05
Norm of the params: 32.3483
                Loss: fixed 569 labels. Loss 0.84243. Accuracy 0.806.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242536
Train loss (w/o reg) on all data: 0.19071
Test loss (w/o reg) on all data: 0.731672
Train acc on all data:  0.935218757554
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 2.58043e-05
Norm of the params: 32.1952
              Random: fixed 373 labels. Loss 0.73167. Accuracy 0.814.
Using normal model
LBFGS training took [679] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299513
Train loss (w/o reg) on all data: 0.240324
Test loss (w/o reg) on all data: 1.2027
Train acc on all data:  0.914430746918
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 0.000137625
Norm of the params: 34.4061
Flipped loss: 1.20270. Accuracy: 0.738
### Flips: 1236, rs: 2, checks: 206
Using normal model
LBFGS training took [517] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239108
Train loss (w/o reg) on all data: 0.186076
Test loss (w/o reg) on all data: 1.06979
Train acc on all data:  0.935943920715
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 7.74214e-05
Norm of the params: 32.5673
     Influence (LOO): fixed 151 labels. Loss 1.06979. Accuracy 0.792.
Using normal model
LBFGS training took [571] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217048
Train loss (w/o reg) on all data: 0.153223
Test loss (w/o reg) on all data: 1.19451
Train acc on all data:  0.959874305052
Test acc on all data:   0.747826086957
Norm of the mean of gradients: 5.04177e-05
Norm of the params: 35.7283
                Loss: fixed 192 labels. Loss 1.19451. Accuracy 0.748.
Using normal model
LBFGS training took [568] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293077
Train loss (w/o reg) on all data: 0.23478
Test loss (w/o reg) on all data: 1.14873
Train acc on all data:  0.916606236403
Test acc on all data:   0.753623188406
Norm of the mean of gradients: 0.000187289
Norm of the params: 34.1461
              Random: fixed  51 labels. Loss 1.14873. Accuracy 0.754.
### Flips: 1236, rs: 2, checks: 412
Using normal model
LBFGS training took [564] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215557
Train loss (w/o reg) on all data: 0.16743
Test loss (w/o reg) on all data: 0.846929
Train acc on all data:  0.941503504955
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 2.48597e-05
Norm of the params: 31.0246
     Influence (LOO): fixed 276 labels. Loss 0.84693. Accuracy 0.813.
Using normal model
LBFGS training took [592] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171263
Train loss (w/o reg) on all data: 0.108548
Test loss (w/o reg) on all data: 1.22185
Train acc on all data:  0.982596084119
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 2.5821e-05
Norm of the params: 35.4161
                Loss: fixed 322 labels. Loss 1.22185. Accuracy 0.764.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28543
Train loss (w/o reg) on all data: 0.228285
Test loss (w/o reg) on all data: 1.1151
Train acc on all data:  0.919023446942
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 4.40129e-05
Norm of the params: 33.8067
              Random: fixed 111 labels. Loss 1.11510. Accuracy 0.760.
### Flips: 1236, rs: 2, checks: 618
Using normal model
LBFGS training took [555] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200536
Train loss (w/o reg) on all data: 0.156007
Test loss (w/o reg) on all data: 0.694259
Train acc on all data:  0.946821368141
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 4.82668e-05
Norm of the params: 29.8427
     Influence (LOO): fixed 383 labels. Loss 0.69426. Accuracy 0.826.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148709
Train loss (w/o reg) on all data: 0.0884357
Test loss (w/o reg) on all data: 1.07518
Train acc on all data:  0.99008943679
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 1.54346e-05
Norm of the params: 34.7198
                Loss: fixed 402 labels. Loss 1.07518. Accuracy 0.802.
Using normal model
LBFGS training took [573] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275889
Train loss (w/o reg) on all data: 0.220148
Test loss (w/o reg) on all data: 1.05007
Train acc on all data:  0.922165820643
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 3.20328e-05
Norm of the params: 33.3889
              Random: fixed 172 labels. Loss 1.05007. Accuracy 0.783.
### Flips: 1236, rs: 2, checks: 824
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184972
Train loss (w/o reg) on all data: 0.143804
Test loss (w/o reg) on all data: 0.58175
Train acc on all data:  0.949722020788
Test acc on all data:   0.856038647343
Norm of the mean of gradients: 1.5354e-05
Norm of the params: 28.6941
     Influence (LOO): fixed 484 labels. Loss 0.58175. Accuracy 0.856.
Using normal model
LBFGS training took [576] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137103
Train loss (w/o reg) on all data: 0.078853
Test loss (w/o reg) on all data: 1.11548
Train acc on all data:  0.99564902103
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 2.00116e-05
Norm of the params: 34.132
                Loss: fixed 454 labels. Loss 1.11548. Accuracy 0.816.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266574
Train loss (w/o reg) on all data: 0.211876
Test loss (w/o reg) on all data: 1.02691
Train acc on all data:  0.925549915398
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 3.46948e-05
Norm of the params: 33.075
              Random: fixed 234 labels. Loss 1.02691. Accuracy 0.801.
### Flips: 1236, rs: 2, checks: 1030
Using normal model
LBFGS training took [454] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176667
Train loss (w/o reg) on all data: 0.137362
Test loss (w/o reg) on all data: 0.490357
Train acc on all data:  0.95044718395
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 4.98104e-05
Norm of the params: 28.0376
     Influence (LOO): fixed 559 labels. Loss 0.49036. Accuracy 0.872.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126211
Train loss (w/o reg) on all data: 0.0706431
Test loss (w/o reg) on all data: 1.13493
Train acc on all data:  0.996615905245
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 3.32018e-05
Norm of the params: 33.3369
                Loss: fixed 510 labels. Loss 1.13493. Accuracy 0.820.
Using normal model
LBFGS training took [584] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.257977
Train loss (w/o reg) on all data: 0.204448
Test loss (w/o reg) on all data: 0.985765
Train acc on all data:  0.92941745226
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 6.91175e-05
Norm of the params: 32.7199
              Random: fixed 293 labels. Loss 0.98577. Accuracy 0.820.
### Flips: 1236, rs: 2, checks: 1236
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17033
Train loss (w/o reg) on all data: 0.133067
Test loss (w/o reg) on all data: 0.41857
Train acc on all data:  0.950688905004
Test acc on all data:   0.882125603865
Norm of the mean of gradients: 1.68215e-05
Norm of the params: 27.2992
     Influence (LOO): fixed 631 labels. Loss 0.41857. Accuracy 0.882.
Using normal model
LBFGS training took [464] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115399
Train loss (w/o reg) on all data: 0.0630719
Test loss (w/o reg) on all data: 1.05287
Train acc on all data:  0.996374184191
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 3.61685e-05
Norm of the params: 32.3503
                Loss: fixed 563 labels. Loss 1.05287. Accuracy 0.831.
Using normal model
LBFGS training took [573] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249823
Train loss (w/o reg) on all data: 0.19774
Test loss (w/o reg) on all data: 0.957338
Train acc on all data:  0.934010152284
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 0.000161191
Norm of the params: 32.2747
              Random: fixed 348 labels. Loss 0.95734. Accuracy 0.832.
Using normal model
LBFGS training took [704] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.296866
Train loss (w/o reg) on all data: 0.239948
Test loss (w/o reg) on all data: 1.01267
Train acc on all data:  0.910079767948
Test acc on all data:   0.730434782609
Norm of the mean of gradients: 9.86884e-05
Norm of the params: 33.7398
Flipped loss: 1.01267. Accuracy: 0.730
### Flips: 1236, rs: 3, checks: 206
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237252
Train loss (w/o reg) on all data: 0.185053
Test loss (w/o reg) on all data: 0.983419
Train acc on all data:  0.939569736524
Test acc on all data:   0.747826086957
Norm of the mean of gradients: 2.49872e-05
Norm of the params: 32.3107
     Influence (LOO): fixed 146 labels. Loss 0.98342. Accuracy 0.748.
Using normal model
LBFGS training took [575] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213756
Train loss (w/o reg) on all data: 0.150558
Test loss (w/o reg) on all data: 1.13425
Train acc on all data:  0.963258399807
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 2.43008e-05
Norm of the params: 35.5524
                Loss: fixed 191 labels. Loss 1.13425. Accuracy 0.750.
Using normal model
LBFGS training took [590] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.288171
Train loss (w/o reg) on all data: 0.231693
Test loss (w/o reg) on all data: 0.945664
Train acc on all data:  0.918298283781
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 3.30386e-05
Norm of the params: 33.6089
              Random: fixed  77 labels. Loss 0.94566. Accuracy 0.757.
### Flips: 1236, rs: 3, checks: 412
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21368
Train loss (w/o reg) on all data: 0.165514
Test loss (w/o reg) on all data: 0.805089
Train acc on all data:  0.94367899444
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 3.21546e-05
Norm of the params: 31.0375
     Influence (LOO): fixed 268 labels. Loss 0.80509. Accuracy 0.792.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166269
Train loss (w/o reg) on all data: 0.105263
Test loss (w/o reg) on all data: 1.11242
Train acc on all data:  0.983804689388
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 4.13046e-05
Norm of the params: 34.9303
                Loss: fixed 317 labels. Loss 1.11242. Accuracy 0.756.
Using normal model
LBFGS training took [634] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.280093
Train loss (w/o reg) on all data: 0.224363
Test loss (w/o reg) on all data: 0.866948
Train acc on all data:  0.922407541697
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 7.7847e-05
Norm of the params: 33.3854
              Random: fixed 141 labels. Loss 0.86695. Accuracy 0.769.
### Flips: 1236, rs: 3, checks: 618
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199976
Train loss (w/o reg) on all data: 0.15475
Test loss (w/o reg) on all data: 0.752557
Train acc on all data:  0.947304810249
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 1.26082e-05
Norm of the params: 30.0752
     Influence (LOO): fixed 377 labels. Loss 0.75256. Accuracy 0.812.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147814
Train loss (w/o reg) on all data: 0.089272
Test loss (w/o reg) on all data: 1.06523
Train acc on all data:  0.988155668359
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 2.08147e-05
Norm of the params: 34.2175
                Loss: fixed 394 labels. Loss 1.06523. Accuracy 0.760.
Using normal model
LBFGS training took [548] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276348
Train loss (w/o reg) on all data: 0.221584
Test loss (w/o reg) on all data: 0.814702
Train acc on all data:  0.921682378535
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 3.40131e-05
Norm of the params: 33.0952
              Random: fixed 199 labels. Loss 0.81470. Accuracy 0.776.
### Flips: 1236, rs: 3, checks: 824
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188876
Train loss (w/o reg) on all data: 0.146423
Test loss (w/o reg) on all data: 0.704334
Train acc on all data:  0.950205462896
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 1.15115e-05
Norm of the params: 29.1387
     Influence (LOO): fixed 477 labels. Loss 0.70433. Accuracy 0.834.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134088
Train loss (w/o reg) on all data: 0.0775736
Test loss (w/o reg) on all data: 1.04719
Train acc on all data:  0.992264926275
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 2.86363e-05
Norm of the params: 33.6198
                Loss: fixed 448 labels. Loss 1.04719. Accuracy 0.777.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267124
Train loss (w/o reg) on all data: 0.213369
Test loss (w/o reg) on all data: 0.795355
Train acc on all data:  0.926033357505
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 8.60186e-05
Norm of the params: 32.7886
              Random: fixed 257 labels. Loss 0.79535. Accuracy 0.775.
### Flips: 1236, rs: 3, checks: 1030
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178778
Train loss (w/o reg) on all data: 0.139058
Test loss (w/o reg) on all data: 0.645428
Train acc on all data:  0.953106115543
Test acc on all data:   0.869565217391
Norm of the mean of gradients: 7.10161e-06
Norm of the params: 28.185
     Influence (LOO): fixed 562 labels. Loss 0.64543. Accuracy 0.870.
Using normal model
LBFGS training took [457] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122192
Train loss (w/o reg) on all data: 0.0683376
Test loss (w/o reg) on all data: 0.929966
Train acc on all data:  0.994198694706
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 2.95108e-05
Norm of the params: 32.8191
                Loss: fixed 515 labels. Loss 0.92997. Accuracy 0.804.
Using normal model
LBFGS training took [522] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.257022
Train loss (w/o reg) on all data: 0.204073
Test loss (w/o reg) on all data: 0.768964
Train acc on all data:  0.92941745226
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 6.49091e-05
Norm of the params: 32.5419
              Random: fixed 320 labels. Loss 0.76896. Accuracy 0.787.
### Flips: 1236, rs: 3, checks: 1236
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171284
Train loss (w/o reg) on all data: 0.134082
Test loss (w/o reg) on all data: 0.594793
Train acc on all data:  0.952380952381
Test acc on all data:   0.873429951691
Norm of the mean of gradients: 1.99517e-05
Norm of the params: 27.2774
     Influence (LOO): fixed 635 labels. Loss 0.59479. Accuracy 0.873.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109301
Train loss (w/o reg) on all data: 0.0595958
Test loss (w/o reg) on all data: 0.787101
Train acc on all data:  0.997824510515
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 1.11516e-05
Norm of the params: 31.5296
                Loss: fixed 589 labels. Loss 0.78710. Accuracy 0.836.
Using normal model
LBFGS training took [508] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24533
Train loss (w/o reg) on all data: 0.193318
Test loss (w/o reg) on all data: 0.737995
Train acc on all data:  0.935218757554
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 2.16413e-05
Norm of the params: 32.253
              Random: fixed 389 labels. Loss 0.73799. Accuracy 0.806.
Using normal model
LBFGS training took [560] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.302379
Train loss (w/o reg) on all data: 0.244352
Test loss (w/o reg) on all data: 1.04665
Train acc on all data:  0.909112883732
Test acc on all data:   0.732367149758
Norm of the mean of gradients: 0.000151405
Norm of the params: 34.0667
Flipped loss: 1.04665. Accuracy: 0.732
### Flips: 1236, rs: 4, checks: 206
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245504
Train loss (w/o reg) on all data: 0.193571
Test loss (w/o reg) on all data: 0.976566
Train acc on all data:  0.929175731206
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 7.67099e-05
Norm of the params: 32.2281
     Influence (LOO): fixed 149 labels. Loss 0.97657. Accuracy 0.787.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225818
Train loss (w/o reg) on all data: 0.163568
Test loss (w/o reg) on all data: 1.15527
Train acc on all data:  0.95044718395
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 6.12413e-05
Norm of the params: 35.2846
                Loss: fixed 187 labels. Loss 1.15527. Accuracy 0.750.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.296535
Train loss (w/o reg) on all data: 0.238557
Test loss (w/o reg) on all data: 0.997551
Train acc on all data:  0.910321489002
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 6.84928e-05
Norm of the params: 34.0521
              Random: fixed  57 labels. Loss 0.99755. Accuracy 0.738.
### Flips: 1236, rs: 4, checks: 412
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.225009
Train loss (w/o reg) on all data: 0.177519
Test loss (w/o reg) on all data: 0.773141
Train acc on all data:  0.934251873338
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 3.07184e-05
Norm of the params: 30.8188
     Influence (LOO): fixed 262 labels. Loss 0.77314. Accuracy 0.801.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179267
Train loss (w/o reg) on all data: 0.117016
Test loss (w/o reg) on all data: 1.15986
Train acc on all data:  0.977036499879
Test acc on all data:   0.750724637681
Norm of the mean of gradients: 5.00856e-05
Norm of the params: 35.2847
                Loss: fixed 319 labels. Loss 1.15986. Accuracy 0.751.
Using normal model
LBFGS training took [529] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.289812
Train loss (w/o reg) on all data: 0.233018
Test loss (w/o reg) on all data: 0.932519
Train acc on all data:  0.913222141649
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 0.000175045
Norm of the params: 33.7027
              Random: fixed 123 labels. Loss 0.93252. Accuracy 0.759.
### Flips: 1236, rs: 4, checks: 618
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211444
Train loss (w/o reg) on all data: 0.166642
Test loss (w/o reg) on all data: 0.669064
Train acc on all data:  0.937635968093
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 3.98204e-05
Norm of the params: 29.934
     Influence (LOO): fixed 374 labels. Loss 0.66906. Accuracy 0.825.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154894
Train loss (w/o reg) on all data: 0.094799
Test loss (w/o reg) on all data: 1.01167
Train acc on all data:  0.98573845782
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 1.06505e-05
Norm of the params: 34.6686
                Loss: fixed 400 labels. Loss 1.01167. Accuracy 0.768.
Using normal model
LBFGS training took [565] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282747
Train loss (w/o reg) on all data: 0.226977
Test loss (w/o reg) on all data: 0.852645
Train acc on all data:  0.917573120619
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 4.05142e-05
Norm of the params: 33.3978
              Random: fixed 189 labels. Loss 0.85265. Accuracy 0.778.
### Flips: 1236, rs: 4, checks: 824
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200627
Train loss (w/o reg) on all data: 0.158492
Test loss (w/o reg) on all data: 0.584523
Train acc on all data:  0.940053178632
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 1.00058e-05
Norm of the params: 29.0292
     Influence (LOO): fixed 452 labels. Loss 0.58452. Accuracy 0.843.
Using normal model
LBFGS training took [462] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.142379
Train loss (w/o reg) on all data: 0.0842904
Test loss (w/o reg) on all data: 1.02517
Train acc on all data:  0.989122552574
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 6.18718e-05
Norm of the params: 34.0848
                Loss: fixed 450 labels. Loss 1.02517. Accuracy 0.772.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276701
Train loss (w/o reg) on all data: 0.221391
Test loss (w/o reg) on all data: 0.808343
Train acc on all data:  0.919265167996
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 9.69734e-05
Norm of the params: 33.2593
              Random: fixed 242 labels. Loss 0.80834. Accuracy 0.794.
### Flips: 1236, rs: 4, checks: 1030
Using normal model
LBFGS training took [356] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190723
Train loss (w/o reg) on all data: 0.151176
Test loss (w/o reg) on all data: 0.542833
Train acc on all data:  0.941020062847
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 4.29997e-05
Norm of the params: 28.124
     Influence (LOO): fixed 533 labels. Loss 0.54283. Accuracy 0.861.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131482
Train loss (w/o reg) on all data: 0.075498
Test loss (w/o reg) on all data: 1.02024
Train acc on all data:  0.990331157844
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 1.37977e-05
Norm of the params: 33.4616
                Loss: fixed 496 labels. Loss 1.02024. Accuracy 0.786.
Using normal model
LBFGS training took [638] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27041
Train loss (w/o reg) on all data: 0.215173
Test loss (w/o reg) on all data: 0.816474
Train acc on all data:  0.92385786802
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 4.20617e-05
Norm of the params: 33.2375
              Random: fixed 286 labels. Loss 0.81647. Accuracy 0.801.
### Flips: 1236, rs: 4, checks: 1236
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182027
Train loss (w/o reg) on all data: 0.14501
Test loss (w/o reg) on all data: 0.530079
Train acc on all data:  0.942228668117
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 7.30196e-05
Norm of the params: 27.2094
     Influence (LOO): fixed 613 labels. Loss 0.53008. Accuracy 0.869.
Using normal model
LBFGS training took [443] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120929
Train loss (w/o reg) on all data: 0.0675374
Test loss (w/o reg) on all data: 1.05048
Train acc on all data:  0.993715252599
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 3.69087e-05
Norm of the params: 32.6778
                Loss: fixed 562 labels. Loss 1.05048. Accuracy 0.802.
Using normal model
LBFGS training took [590] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262902
Train loss (w/o reg) on all data: 0.208537
Test loss (w/o reg) on all data: 0.699761
Train acc on all data:  0.926758520667
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 3.24822e-05
Norm of the params: 32.9741
              Random: fixed 343 labels. Loss 0.69976. Accuracy 0.808.
Using normal model
LBFGS training took [680] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.300327
Train loss (w/o reg) on all data: 0.240847
Test loss (w/o reg) on all data: 1.01026
Train acc on all data:  0.914914189026
Test acc on all data:   0.713043478261
Norm of the mean of gradients: 0.000114434
Norm of the params: 34.4906
Flipped loss: 1.01026. Accuracy: 0.713
### Flips: 1236, rs: 5, checks: 206
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239058
Train loss (w/o reg) on all data: 0.185996
Test loss (w/o reg) on all data: 0.995482
Train acc on all data:  0.937394247039
Test acc on all data:   0.750724637681
Norm of the mean of gradients: 2.1433e-05
Norm of the params: 32.5766
     Influence (LOO): fixed 156 labels. Loss 0.99548. Accuracy 0.751.
Using normal model
LBFGS training took [609] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216875
Train loss (w/o reg) on all data: 0.152263
Test loss (w/o reg) on all data: 1.05007
Train acc on all data:  0.963741841914
Test acc on all data:   0.728502415459
Norm of the mean of gradients: 8.91341e-05
Norm of the params: 35.9475
                Loss: fixed 189 labels. Loss 1.05007. Accuracy 0.729.
Using normal model
LBFGS training took [588] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291964
Train loss (w/o reg) on all data: 0.23306
Test loss (w/o reg) on all data: 0.919826
Train acc on all data:  0.918781725888
Test acc on all data:   0.723671497585
Norm of the mean of gradients: 7.06101e-05
Norm of the params: 34.323
              Random: fixed  59 labels. Loss 0.91983. Accuracy 0.724.
### Flips: 1236, rs: 5, checks: 412
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214436
Train loss (w/o reg) on all data: 0.165838
Test loss (w/o reg) on all data: 0.832232
Train acc on all data:  0.944162436548
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 7.6287e-05
Norm of the params: 31.1766
     Influence (LOO): fixed 278 labels. Loss 0.83223. Accuracy 0.789.
Using normal model
LBFGS training took [590] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173013
Train loss (w/o reg) on all data: 0.109101
Test loss (w/o reg) on all data: 0.916175
Train acc on all data:  0.984771573604
Test acc on all data:   0.747826086957
Norm of the mean of gradients: 1.65542e-05
Norm of the params: 35.7525
                Loss: fixed 308 labels. Loss 0.91617. Accuracy 0.748.
Using normal model
LBFGS training took [603] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28273
Train loss (w/o reg) on all data: 0.224669
Test loss (w/o reg) on all data: 0.881166
Train acc on all data:  0.924583031182
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 1.99572e-05
Norm of the params: 34.0767
              Random: fixed 122 labels. Loss 0.88117. Accuracy 0.741.
### Flips: 1236, rs: 5, checks: 618
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20266
Train loss (w/o reg) on all data: 0.156526
Test loss (w/o reg) on all data: 0.720405
Train acc on all data:  0.944645878656
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 3.81079e-05
Norm of the params: 30.3757
     Influence (LOO): fixed 369 labels. Loss 0.72040. Accuracy 0.804.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150887
Train loss (w/o reg) on all data: 0.0899272
Test loss (w/o reg) on all data: 0.828827
Train acc on all data:  0.99008943679
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 3.30565e-05
Norm of the params: 34.9169
                Loss: fixed 388 labels. Loss 0.82883. Accuracy 0.756.
Using normal model
LBFGS training took [615] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.274486
Train loss (w/o reg) on all data: 0.21805
Test loss (w/o reg) on all data: 0.80958
Train acc on all data:  0.928934010152
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 6.90068e-05
Norm of the params: 33.5964
              Random: fixed 199 labels. Loss 0.80958. Accuracy 0.760.
### Flips: 1236, rs: 5, checks: 824
Using normal model
LBFGS training took [416] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191949
Train loss (w/o reg) on all data: 0.148231
Test loss (w/o reg) on all data: 0.588625
Train acc on all data:  0.948513415518
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 1.43379e-05
Norm of the params: 29.5696
     Influence (LOO): fixed 467 labels. Loss 0.58863. Accuracy 0.827.
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13666
Train loss (w/o reg) on all data: 0.0786972
Test loss (w/o reg) on all data: 0.818297
Train acc on all data:  0.991539763113
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 1.64347e-05
Norm of the params: 34.0478
                Loss: fixed 452 labels. Loss 0.81830. Accuracy 0.778.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265797
Train loss (w/o reg) on all data: 0.210487
Test loss (w/o reg) on all data: 0.733282
Train acc on all data:  0.930384336476
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 3.53952e-05
Norm of the params: 33.2594
              Random: fixed 259 labels. Loss 0.73328. Accuracy 0.779.
### Flips: 1236, rs: 5, checks: 1030
Using normal model
LBFGS training took [349] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185856
Train loss (w/o reg) on all data: 0.144187
Test loss (w/o reg) on all data: 0.548525
Train acc on all data:  0.949963741842
Test acc on all data:   0.83768115942
Norm of the mean of gradients: 3.5706e-05
Norm of the params: 28.8681
     Influence (LOO): fixed 546 labels. Loss 0.54852. Accuracy 0.838.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12902
Train loss (w/o reg) on all data: 0.0729337
Test loss (w/o reg) on all data: 0.823168
Train acc on all data:  0.995890742084
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 1.06881e-05
Norm of the params: 33.4923
                Loss: fixed 489 labels. Loss 0.82317. Accuracy 0.793.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256978
Train loss (w/o reg) on all data: 0.202459
Test loss (w/o reg) on all data: 0.722004
Train acc on all data:  0.933526710176
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 5.30704e-05
Norm of the params: 33.0207
              Random: fixed 314 labels. Loss 0.72200. Accuracy 0.785.
### Flips: 1236, rs: 5, checks: 1236
Using normal model
LBFGS training took [339] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174979
Train loss (w/o reg) on all data: 0.135732
Test loss (w/o reg) on all data: 0.511078
Train acc on all data:  0.951414068165
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 1.30193e-05
Norm of the params: 28.0168
     Influence (LOO): fixed 620 labels. Loss 0.51108. Accuracy 0.840.
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118554
Train loss (w/o reg) on all data: 0.0653887
Test loss (w/o reg) on all data: 0.782016
Train acc on all data:  0.996857626299
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 8.62139e-05
Norm of the params: 32.6083
                Loss: fixed 541 labels. Loss 0.78202. Accuracy 0.808.
Using normal model
LBFGS training took [523] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247192
Train loss (w/o reg) on all data: 0.194158
Test loss (w/o reg) on all data: 0.692163
Train acc on all data:  0.936910804931
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 6.2494e-05
Norm of the params: 32.5678
              Random: fixed 377 labels. Loss 0.69216. Accuracy 0.807.
Using normal model
LBFGS training took [522] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.300556
Train loss (w/o reg) on all data: 0.242274
Test loss (w/o reg) on all data: 0.900855
Train acc on all data:  0.91394730481
Test acc on all data:   0.707246376812
Norm of the mean of gradients: 3.94921e-05
Norm of the params: 34.1415
Flipped loss: 0.90086. Accuracy: 0.707
### Flips: 1236, rs: 6, checks: 206
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241412
Train loss (w/o reg) on all data: 0.188879
Test loss (w/o reg) on all data: 0.841522
Train acc on all data:  0.935702199662
Test acc on all data:   0.75845410628
Norm of the mean of gradients: 1.10568e-05
Norm of the params: 32.414
     Influence (LOO): fixed 147 labels. Loss 0.84152. Accuracy 0.758.
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217476
Train loss (w/o reg) on all data: 0.15512
Test loss (w/o reg) on all data: 0.946248
Train acc on all data:  0.959390862944
Test acc on all data:   0.734299516908
Norm of the mean of gradients: 3.89948e-05
Norm of the params: 35.3146
                Loss: fixed 185 labels. Loss 0.94625. Accuracy 0.734.
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298357
Train loss (w/o reg) on all data: 0.240973
Test loss (w/o reg) on all data: 0.855947
Train acc on all data:  0.912980420595
Test acc on all data:   0.726570048309
Norm of the mean of gradients: 3.49527e-05
Norm of the params: 33.8774
              Random: fixed  61 labels. Loss 0.85595. Accuracy 0.727.
### Flips: 1236, rs: 6, checks: 412
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21716
Train loss (w/o reg) on all data: 0.169312
Test loss (w/o reg) on all data: 0.733197
Train acc on all data:  0.939811457578
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 1.44499e-05
Norm of the params: 30.9349
     Influence (LOO): fixed 267 labels. Loss 0.73320. Accuracy 0.777.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176341
Train loss (w/o reg) on all data: 0.11431
Test loss (w/o reg) on all data: 0.949369
Train acc on all data:  0.981629199903
Test acc on all data:   0.746859903382
Norm of the mean of gradients: 3.0774e-05
Norm of the params: 35.2224
                Loss: fixed 305 labels. Loss 0.94937. Accuracy 0.747.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291128
Train loss (w/o reg) on all data: 0.234555
Test loss (w/o reg) on all data: 0.815876
Train acc on all data:  0.916606236403
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 1.27215e-05
Norm of the params: 33.6372
              Random: fixed 127 labels. Loss 0.81588. Accuracy 0.733.
### Flips: 1236, rs: 6, checks: 618
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199995
Train loss (w/o reg) on all data: 0.154909
Test loss (w/o reg) on all data: 0.65195
Train acc on all data:  0.94367899444
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 1.06452e-05
Norm of the params: 30.0285
     Influence (LOO): fixed 374 labels. Loss 0.65195. Accuracy 0.798.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155148
Train loss (w/o reg) on all data: 0.0958018
Test loss (w/o reg) on all data: 0.877853
Train acc on all data:  0.987913947305
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 3.85719e-05
Norm of the params: 34.4518
                Loss: fixed 384 labels. Loss 0.87785. Accuracy 0.763.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279518
Train loss (w/o reg) on all data: 0.224491
Test loss (w/o reg) on all data: 0.768074
Train acc on all data:  0.920232052212
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 6.46448e-05
Norm of the params: 33.1743
              Random: fixed 196 labels. Loss 0.76807. Accuracy 0.768.
### Flips: 1236, rs: 6, checks: 824
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191419
Train loss (w/o reg) on all data: 0.14907
Test loss (w/o reg) on all data: 0.613608
Train acc on all data:  0.945854483926
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 1.59684e-05
Norm of the params: 29.1027
     Influence (LOO): fixed 459 labels. Loss 0.61361. Accuracy 0.833.
Using normal model
LBFGS training took [447] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138821
Train loss (w/o reg) on all data: 0.0826845
Test loss (w/o reg) on all data: 0.794993
Train acc on all data:  0.990814599952
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 1.41439e-05
Norm of the params: 33.507
                Loss: fixed 451 labels. Loss 0.79499. Accuracy 0.784.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.269689
Train loss (w/o reg) on all data: 0.215411
Test loss (w/o reg) on all data: 0.759388
Train acc on all data:  0.925791636452
Test acc on all data:   0.76231884058
Norm of the mean of gradients: 4.67912e-05
Norm of the params: 32.9478
              Random: fixed 259 labels. Loss 0.75939. Accuracy 0.762.
### Flips: 1236, rs: 6, checks: 1030
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183245
Train loss (w/o reg) on all data: 0.143304
Test loss (w/o reg) on all data: 0.586528
Train acc on all data:  0.947304810249
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 1.56181e-05
Norm of the params: 28.2634
     Influence (LOO): fixed 538 labels. Loss 0.58653. Accuracy 0.839.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128667
Train loss (w/o reg) on all data: 0.0746773
Test loss (w/o reg) on all data: 0.780447
Train acc on all data:  0.991056321006
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 1.58012e-05
Norm of the params: 32.8601
                Loss: fixed 496 labels. Loss 0.78045. Accuracy 0.807.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260163
Train loss (w/o reg) on all data: 0.20695
Test loss (w/o reg) on all data: 0.663868
Train acc on all data:  0.93062605753
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 1.52436e-05
Norm of the params: 32.623
              Random: fixed 331 labels. Loss 0.66387. Accuracy 0.783.
### Flips: 1236, rs: 6, checks: 1236
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174862
Train loss (w/o reg) on all data: 0.136683
Test loss (w/o reg) on all data: 0.533773
Train acc on all data:  0.951172347111
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.65134e-05
Norm of the params: 27.633
     Influence (LOO): fixed 615 labels. Loss 0.53377. Accuracy 0.864.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114715
Train loss (w/o reg) on all data: 0.0642933
Test loss (w/o reg) on all data: 0.722873
Train acc on all data:  0.992506647329
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 9.73654e-06
Norm of the params: 31.7557
                Loss: fixed 561 labels. Loss 0.72287. Accuracy 0.826.
Using normal model
LBFGS training took [493] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249669
Train loss (w/o reg) on all data: 0.197165
Test loss (w/o reg) on all data: 0.660575
Train acc on all data:  0.936669083877
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 1.83501e-05
Norm of the params: 32.4051
              Random: fixed 386 labels. Loss 0.66057. Accuracy 0.791.
Using normal model
LBFGS training took [689] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30771
Train loss (w/o reg) on all data: 0.24961
Test loss (w/o reg) on all data: 1.2185
Train acc on all data:  0.911288373217
Test acc on all data:   0.729468599034
Norm of the mean of gradients: 6.38322e-05
Norm of the params: 34.0883
Flipped loss: 1.21850. Accuracy: 0.729
### Flips: 1236, rs: 7, checks: 206
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.250667
Train loss (w/o reg) on all data: 0.19684
Test loss (w/o reg) on all data: 1.13766
Train acc on all data:  0.931592941745
Test acc on all data:   0.76231884058
Norm of the mean of gradients: 2.46893e-05
Norm of the params: 32.8108
     Influence (LOO): fixed 140 labels. Loss 1.13766. Accuracy 0.762.
Using normal model
LBFGS training took [627] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22517
Train loss (w/o reg) on all data: 0.161774
Test loss (w/o reg) on all data: 1.27164
Train acc on all data:  0.956973652405
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 6.46624e-05
Norm of the params: 35.6079
                Loss: fixed 190 labels. Loss 1.27164. Accuracy 0.733.
Using normal model
LBFGS training took [605] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.301106
Train loss (w/o reg) on all data: 0.243928
Test loss (w/o reg) on all data: 1.11447
Train acc on all data:  0.91515591008
Test acc on all data:   0.745893719807
Norm of the mean of gradients: 2.98285e-05
Norm of the params: 33.8165
              Random: fixed  66 labels. Loss 1.11447. Accuracy 0.746.
### Flips: 1236, rs: 7, checks: 412
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.226072
Train loss (w/o reg) on all data: 0.176743
Test loss (w/o reg) on all data: 0.888137
Train acc on all data:  0.936669083877
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 5.67996e-05
Norm of the params: 31.4099
     Influence (LOO): fixed 267 labels. Loss 0.88814. Accuracy 0.786.
Using normal model
LBFGS training took [598] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17962
Train loss (w/o reg) on all data: 0.116033
Test loss (w/o reg) on all data: 1.20518
Train acc on all data:  0.982596084119
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 3.59949e-05
Norm of the params: 35.6615
                Loss: fixed 315 labels. Loss 1.20518. Accuracy 0.741.
Using normal model
LBFGS training took [611] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291109
Train loss (w/o reg) on all data: 0.234446
Test loss (w/o reg) on all data: 1.03634
Train acc on all data:  0.918540004834
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 4.66553e-05
Norm of the params: 33.6638
              Random: fixed 128 labels. Loss 1.03634. Accuracy 0.753.
### Flips: 1236, rs: 7, checks: 618
Using normal model
LBFGS training took [391] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211652
Train loss (w/o reg) on all data: 0.164922
Test loss (w/o reg) on all data: 0.790693
Train acc on all data:  0.939811457578
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 3.93542e-05
Norm of the params: 30.5714
     Influence (LOO): fixed 370 labels. Loss 0.79069. Accuracy 0.818.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155196
Train loss (w/o reg) on all data: 0.0939478
Test loss (w/o reg) on all data: 1.156
Train acc on all data:  0.98888083152
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 4.1286e-05
Norm of the params: 34.9994
                Loss: fixed 404 labels. Loss 1.15600. Accuracy 0.771.
Using normal model
LBFGS training took [645] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.281538
Train loss (w/o reg) on all data: 0.22536
Test loss (w/o reg) on all data: 1.04515
Train acc on all data:  0.923616146966
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 0.000108926
Norm of the params: 33.5196
              Random: fixed 194 labels. Loss 1.04515. Accuracy 0.766.
### Flips: 1236, rs: 7, checks: 824
Using normal model
LBFGS training took [380] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197925
Train loss (w/o reg) on all data: 0.153671
Test loss (w/o reg) on all data: 0.667834
Train acc on all data:  0.94367899444
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 3.00202e-05
Norm of the params: 29.7505
     Influence (LOO): fixed 454 labels. Loss 0.66783. Accuracy 0.825.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140734
Train loss (w/o reg) on all data: 0.0822658
Test loss (w/o reg) on all data: 1.07161
Train acc on all data:  0.992264926275
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 4.17723e-05
Norm of the params: 34.1959
                Loss: fixed 474 labels. Loss 1.07161. Accuracy 0.790.
Using normal model
LBFGS training took [624] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272564
Train loss (w/o reg) on all data: 0.217714
Test loss (w/o reg) on all data: 0.948697
Train acc on all data:  0.927725404883
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 4.12177e-05
Norm of the params: 33.121
              Random: fixed 255 labels. Loss 0.94870. Accuracy 0.783.
### Flips: 1236, rs: 7, checks: 1030
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186576
Train loss (w/o reg) on all data: 0.144984
Test loss (w/o reg) on all data: 0.645609
Train acc on all data:  0.946579647087
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 3.37574e-05
Norm of the params: 28.8414
     Influence (LOO): fixed 534 labels. Loss 0.64561. Accuracy 0.847.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133019
Train loss (w/o reg) on all data: 0.0768704
Test loss (w/o reg) on all data: 1.00215
Train acc on all data:  0.990814599952
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 1.55269e-05
Norm of the params: 33.5108
                Loss: fixed 521 labels. Loss 1.00215. Accuracy 0.789.
Using normal model
LBFGS training took [607] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265135
Train loss (w/o reg) on all data: 0.211095
Test loss (w/o reg) on all data: 0.871551
Train acc on all data:  0.930867778584
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 0.000155455
Norm of the params: 32.8758
              Random: fixed 305 labels. Loss 0.87155. Accuracy 0.786.
### Flips: 1236, rs: 7, checks: 1236
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177648
Train loss (w/o reg) on all data: 0.13863
Test loss (w/o reg) on all data: 0.526533
Train acc on all data:  0.94923857868
Test acc on all data:   0.868599033816
Norm of the mean of gradients: 1.13651e-05
Norm of the params: 27.9349
     Influence (LOO): fixed 617 labels. Loss 0.52653. Accuracy 0.869.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120612
Train loss (w/o reg) on all data: 0.0676914
Test loss (w/o reg) on all data: 0.985041
Train acc on all data:  0.992023205221
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 4.18464e-05
Norm of the params: 32.5331
                Loss: fixed 578 labels. Loss 0.98504. Accuracy 0.801.
Using normal model
LBFGS training took [613] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254301
Train loss (w/o reg) on all data: 0.201247
Test loss (w/o reg) on all data: 0.807818
Train acc on all data:  0.932318104907
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 2.98755e-05
Norm of the params: 32.574
              Random: fixed 357 labels. Loss 0.80782. Accuracy 0.783.
Using normal model
LBFGS training took [571] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298299
Train loss (w/o reg) on all data: 0.239947
Test loss (w/o reg) on all data: 0.98646
Train acc on all data:  0.909354604786
Test acc on all data:   0.72270531401
Norm of the mean of gradients: 3.14655e-05
Norm of the params: 34.162
Flipped loss: 0.98646. Accuracy: 0.723
### Flips: 1236, rs: 8, checks: 206
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242426
Train loss (w/o reg) on all data: 0.190431
Test loss (w/o reg) on all data: 0.910557
Train acc on all data:  0.929659173314
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 0.000149531
Norm of the params: 32.2476
     Influence (LOO): fixed 153 labels. Loss 0.91056. Accuracy 0.759.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228076
Train loss (w/o reg) on all data: 0.164502
Test loss (w/o reg) on all data: 0.967587
Train acc on all data:  0.951655789219
Test acc on all data:   0.739130434783
Norm of the mean of gradients: 1.63224e-05
Norm of the params: 35.6577
                Loss: fixed 173 labels. Loss 0.96759. Accuracy 0.739.
Using normal model
LBFGS training took [523] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29525
Train loss (w/o reg) on all data: 0.2377
Test loss (w/o reg) on all data: 0.947743
Train acc on all data:  0.910804931109
Test acc on all data:   0.727536231884
Norm of the mean of gradients: 4.35588e-05
Norm of the params: 33.9263
              Random: fixed  49 labels. Loss 0.94774. Accuracy 0.728.
### Flips: 1236, rs: 8, checks: 412
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221296
Train loss (w/o reg) on all data: 0.173526
Test loss (w/o reg) on all data: 0.783362
Train acc on all data:  0.936427362823
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 3.83262e-05
Norm of the params: 30.9093
     Influence (LOO): fixed 266 labels. Loss 0.78336. Accuracy 0.778.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188211
Train loss (w/o reg) on all data: 0.123804
Test loss (w/o reg) on all data: 0.930615
Train acc on all data:  0.976069615664
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 9.64031e-05
Norm of the params: 35.8908
                Loss: fixed 299 labels. Loss 0.93061. Accuracy 0.756.
Using normal model
LBFGS training took [551] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.284689
Train loss (w/o reg) on all data: 0.228106
Test loss (w/o reg) on all data: 0.839537
Train acc on all data:  0.915397631134
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 5.80201e-05
Norm of the params: 33.6401
              Random: fixed 121 labels. Loss 0.83954. Accuracy 0.749.
### Flips: 1236, rs: 8, checks: 618
Using normal model
LBFGS training took [367] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205798
Train loss (w/o reg) on all data: 0.16186
Test loss (w/o reg) on all data: 0.678343
Train acc on all data:  0.940053178632
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 1.39054e-05
Norm of the params: 29.6438
     Influence (LOO): fixed 373 labels. Loss 0.67834. Accuracy 0.813.
Using normal model
LBFGS training took [532] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.161095
Train loss (w/o reg) on all data: 0.0992589
Test loss (w/o reg) on all data: 0.861153
Train acc on all data:  0.985255015712
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 1.26048e-05
Norm of the params: 35.1672
                Loss: fixed 381 labels. Loss 0.86115. Accuracy 0.780.
Using normal model
LBFGS training took [567] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.277322
Train loss (w/o reg) on all data: 0.221081
Test loss (w/o reg) on all data: 0.808905
Train acc on all data:  0.91950688905
Test acc on all data:   0.76231884058
Norm of the mean of gradients: 3.34334e-05
Norm of the params: 33.5383
              Random: fixed 178 labels. Loss 0.80891. Accuracy 0.762.
### Flips: 1236, rs: 8, checks: 824
Using normal model
LBFGS training took [345] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19546
Train loss (w/o reg) on all data: 0.154826
Test loss (w/o reg) on all data: 0.596638
Train acc on all data:  0.941745226009
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 8.28164e-05
Norm of the params: 28.5075
     Influence (LOO): fixed 465 labels. Loss 0.59664. Accuracy 0.824.
Using normal model
LBFGS training took [518] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146199
Train loss (w/o reg) on all data: 0.0865607
Test loss (w/o reg) on all data: 0.84082
Train acc on all data:  0.987913947305
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 1.52315e-05
Norm of the params: 34.5363
                Loss: fixed 447 labels. Loss 0.84082. Accuracy 0.786.
Using normal model
LBFGS training took [544] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267251
Train loss (w/o reg) on all data: 0.211642
Test loss (w/o reg) on all data: 0.792193
Train acc on all data:  0.92506647329
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 5.60022e-05
Norm of the params: 33.3494
              Random: fixed 247 labels. Loss 0.79219. Accuracy 0.782.
### Flips: 1236, rs: 8, checks: 1030
Using normal model
LBFGS training took [332] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184556
Train loss (w/o reg) on all data: 0.146459
Test loss (w/o reg) on all data: 0.573759
Train acc on all data:  0.94367899444
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.90757e-05
Norm of the params: 27.6034
     Influence (LOO): fixed 545 labels. Loss 0.57376. Accuracy 0.843.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133064
Train loss (w/o reg) on all data: 0.0762374
Test loss (w/o reg) on all data: 0.830435
Train acc on all data:  0.991298042059
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 4.95983e-05
Norm of the params: 33.7124
                Loss: fixed 505 labels. Loss 0.83044. Accuracy 0.798.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.255538
Train loss (w/o reg) on all data: 0.201876
Test loss (w/o reg) on all data: 0.696751
Train acc on all data:  0.929900894368
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 2.81646e-05
Norm of the params: 32.7604
              Random: fixed 319 labels. Loss 0.69675. Accuracy 0.797.
### Flips: 1236, rs: 8, checks: 1236
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171929
Train loss (w/o reg) on all data: 0.136178
Test loss (w/o reg) on all data: 0.516146
Train acc on all data:  0.946096204979
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 4.70242e-05
Norm of the params: 26.7395
     Influence (LOO): fixed 627 labels. Loss 0.51615. Accuracy 0.863.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120265
Train loss (w/o reg) on all data: 0.0668445
Test loss (w/o reg) on all data: 0.781109
Train acc on all data:  0.996132463138
Test acc on all data:   0.821256038647
Norm of the mean of gradients: 9.72258e-06
Norm of the params: 32.6865
                Loss: fixed 573 labels. Loss 0.78111. Accuracy 0.821.
Using normal model
LBFGS training took [507] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24793
Train loss (w/o reg) on all data: 0.195218
Test loss (w/o reg) on all data: 0.620483
Train acc on all data:  0.936910804931
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 4.1338e-05
Norm of the params: 32.469
              Random: fixed 372 labels. Loss 0.62048. Accuracy 0.820.
Using normal model
LBFGS training took [750] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.302109
Train loss (w/o reg) on all data: 0.243212
Test loss (w/o reg) on all data: 0.911676
Train acc on all data:  0.912496978487
Test acc on all data:   0.72077294686
Norm of the mean of gradients: 4.34827e-05
Norm of the params: 34.3213
Flipped loss: 0.91168. Accuracy: 0.721
### Flips: 1236, rs: 9, checks: 206
Using normal model
LBFGS training took [555] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241618
Train loss (w/o reg) on all data: 0.187899
Test loss (w/o reg) on all data: 0.833466
Train acc on all data:  0.936427362823
Test acc on all data:   0.750724637681
Norm of the mean of gradients: 2.78559e-05
Norm of the params: 32.7777
     Influence (LOO): fixed 155 labels. Loss 0.83347. Accuracy 0.751.
Using normal model
LBFGS training took [663] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222546
Train loss (w/o reg) on all data: 0.157328
Test loss (w/o reg) on all data: 0.9574
Train acc on all data:  0.958182257675
Test acc on all data:   0.731400966184
Norm of the mean of gradients: 6.73561e-05
Norm of the params: 36.1158
                Loss: fixed 190 labels. Loss 0.95740. Accuracy 0.731.
Using normal model
LBFGS training took [686] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.296832
Train loss (w/o reg) on all data: 0.238469
Test loss (w/o reg) on all data: 0.877771
Train acc on all data:  0.914430746918
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 6.52293e-05
Norm of the params: 34.1653
              Random: fixed  61 labels. Loss 0.87777. Accuracy 0.733.
### Flips: 1236, rs: 9, checks: 412
Using normal model
LBFGS training took [502] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219757
Train loss (w/o reg) on all data: 0.170341
Test loss (w/o reg) on all data: 0.702622
Train acc on all data:  0.942228668117
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 2.2327e-05
Norm of the params: 31.4374
     Influence (LOO): fixed 270 labels. Loss 0.70262. Accuracy 0.786.
Using normal model
LBFGS training took [633] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180499
Train loss (w/o reg) on all data: 0.11492
Test loss (w/o reg) on all data: 0.985749
Train acc on all data:  0.979453710418
Test acc on all data:   0.740096618357
Norm of the mean of gradients: 3.71061e-05
Norm of the params: 36.2157
                Loss: fixed 311 labels. Loss 0.98575. Accuracy 0.740.
Using normal model
LBFGS training took [654] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291129
Train loss (w/o reg) on all data: 0.233412
Test loss (w/o reg) on all data: 0.841135
Train acc on all data:  0.917573120619
Test acc on all data:   0.744927536232
Norm of the mean of gradients: 9.90706e-05
Norm of the params: 33.9755
              Random: fixed 123 labels. Loss 0.84113. Accuracy 0.745.
### Flips: 1236, rs: 9, checks: 618
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206541
Train loss (w/o reg) on all data: 0.160485
Test loss (w/o reg) on all data: 0.579658
Train acc on all data:  0.945129320764
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 6.69987e-05
Norm of the params: 30.3498
     Influence (LOO): fixed 379 labels. Loss 0.57966. Accuracy 0.819.
Using normal model
LBFGS training took [582] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15387
Train loss (w/o reg) on all data: 0.091195
Test loss (w/o reg) on all data: 0.905091
Train acc on all data:  0.989122552574
Test acc on all data:   0.747826086957
Norm of the mean of gradients: 5.60556e-05
Norm of the params: 35.4049
                Loss: fixed 399 labels. Loss 0.90509. Accuracy 0.748.
Using normal model
LBFGS training took [628] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282831
Train loss (w/o reg) on all data: 0.225906
Test loss (w/o reg) on all data: 0.755187
Train acc on all data:  0.921198936427
Test acc on all data:   0.753623188406
Norm of the mean of gradients: 9.90751e-05
Norm of the params: 33.7418
              Random: fixed 186 labels. Loss 0.75519. Accuracy 0.754.
### Flips: 1236, rs: 9, checks: 824
Using normal model
LBFGS training took [471] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194482
Train loss (w/o reg) on all data: 0.151125
Test loss (w/o reg) on all data: 0.531763
Train acc on all data:  0.948996857626
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 0.000119384
Norm of the params: 29.4472
     Influence (LOO): fixed 477 labels. Loss 0.53176. Accuracy 0.828.
Using normal model
LBFGS training took [610] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143431
Train loss (w/o reg) on all data: 0.0830019
Test loss (w/o reg) on all data: 0.82764
Train acc on all data:  0.990572878898
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 2.08845e-05
Norm of the params: 34.7647
                Loss: fixed 450 labels. Loss 0.82764. Accuracy 0.767.
Using normal model
LBFGS training took [602] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275252
Train loss (w/o reg) on all data: 0.219568
Test loss (w/o reg) on all data: 0.736095
Train acc on all data:  0.922407541697
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 5.5588e-05
Norm of the params: 33.3718
              Random: fixed 240 labels. Loss 0.73609. Accuracy 0.770.
### Flips: 1236, rs: 9, checks: 1030
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185726
Train loss (w/o reg) on all data: 0.14453
Test loss (w/o reg) on all data: 0.519272
Train acc on all data:  0.950930626058
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 1.83784e-05
Norm of the params: 28.7037
     Influence (LOO): fixed 556 labels. Loss 0.51927. Accuracy 0.839.
Using normal model
LBFGS training took [592] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130701
Train loss (w/o reg) on all data: 0.0733934
Test loss (w/o reg) on all data: 0.786218
Train acc on all data:  0.992506647329
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 3.40538e-05
Norm of the params: 33.8548
                Loss: fixed 497 labels. Loss 0.78622. Accuracy 0.775.
Using normal model
LBFGS training took [629] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267282
Train loss (w/o reg) on all data: 0.21244
Test loss (w/o reg) on all data: 0.711121
Train acc on all data:  0.926033357505
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 5.56418e-05
Norm of the params: 33.1187
              Random: fixed 288 labels. Loss 0.71112. Accuracy 0.778.
### Flips: 1236, rs: 9, checks: 1236
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175073
Train loss (w/o reg) on all data: 0.136991
Test loss (w/o reg) on all data: 0.461542
Train acc on all data:  0.950930626058
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 9.21155e-06
Norm of the params: 27.5979
     Influence (LOO): fixed 636 labels. Loss 0.46154. Accuracy 0.852.
Using normal model
LBFGS training took [588] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118164
Train loss (w/o reg) on all data: 0.064565
Test loss (w/o reg) on all data: 0.73916
Train acc on all data:  0.993956973652
Test acc on all data:   0.793236714976
Norm of the mean of gradients: 1.80934e-05
Norm of the params: 32.7411
                Loss: fixed 556 labels. Loss 0.73916. Accuracy 0.793.
Using normal model
LBFGS training took [638] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.257182
Train loss (w/o reg) on all data: 0.202788
Test loss (w/o reg) on all data: 0.694749
Train acc on all data:  0.934010152284
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 1.95077e-05
Norm of the params: 32.9829
              Random: fixed 350 labels. Loss 0.69475. Accuracy 0.788.
Using normal model
LBFGS training took [704] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.296548
Train loss (w/o reg) on all data: 0.237486
Test loss (w/o reg) on all data: 1.08813
Train acc on all data:  0.916606236403
Test acc on all data:   0.712077294686
Norm of the mean of gradients: 8.92899e-05
Norm of the params: 34.3693
Flipped loss: 1.08813. Accuracy: 0.712
### Flips: 1236, rs: 10, checks: 206
Using normal model
LBFGS training took [440] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243146
Train loss (w/o reg) on all data: 0.189186
Test loss (w/o reg) on all data: 1.09193
Train acc on all data:  0.934251873338
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 9.09751e-05
Norm of the params: 32.8511
     Influence (LOO): fixed 141 labels. Loss 1.09193. Accuracy 0.752.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216908
Train loss (w/o reg) on all data: 0.152859
Test loss (w/o reg) on all data: 1.03088
Train acc on all data:  0.960116026106
Test acc on all data:   0.724637681159
Norm of the mean of gradients: 4.68795e-05
Norm of the params: 35.7906
                Loss: fixed 183 labels. Loss 1.03088. Accuracy 0.725.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.292269
Train loss (w/o reg) on all data: 0.233251
Test loss (w/o reg) on all data: 1.12611
Train acc on all data:  0.918540004834
Test acc on all data:   0.719806763285
Norm of the mean of gradients: 0.000113022
Norm of the params: 34.3564
              Random: fixed  59 labels. Loss 1.12611. Accuracy 0.720.
### Flips: 1236, rs: 10, checks: 412
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219395
Train loss (w/o reg) on all data: 0.169617
Test loss (w/o reg) on all data: 0.895938
Train acc on all data:  0.940294899686
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 1.43647e-05
Norm of the params: 31.5527
     Influence (LOO): fixed 254 labels. Loss 0.89594. Accuracy 0.778.
Using normal model
LBFGS training took [593] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17037
Train loss (w/o reg) on all data: 0.107816
Test loss (w/o reg) on all data: 1.05284
Train acc on all data:  0.982112642011
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 2.01242e-05
Norm of the params: 35.3706
                Loss: fixed 316 labels. Loss 1.05284. Accuracy 0.738.
Using normal model
LBFGS training took [658] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.284541
Train loss (w/o reg) on all data: 0.225831
Test loss (w/o reg) on all data: 1.1238
Train acc on all data:  0.924099589074
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 6.38929e-05
Norm of the params: 34.2666
              Random: fixed 123 labels. Loss 1.12380. Accuracy 0.735.
### Flips: 1236, rs: 10, checks: 618
Using normal model
LBFGS training took [418] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204531
Train loss (w/o reg) on all data: 0.157952
Test loss (w/o reg) on all data: 0.766983
Train acc on all data:  0.94488759971
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.99643e-05
Norm of the params: 30.5217
     Influence (LOO): fixed 360 labels. Loss 0.76698. Accuracy 0.803.
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149284
Train loss (w/o reg) on all data: 0.0887487
Test loss (w/o reg) on all data: 1.03616
Train acc on all data:  0.99008943679
Test acc on all data:   0.76231884058
Norm of the mean of gradients: 0.000104816
Norm of the params: 34.7951
                Loss: fixed 400 labels. Loss 1.03616. Accuracy 0.762.
Using normal model
LBFGS training took [626] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276693
Train loss (w/o reg) on all data: 0.2187
Test loss (w/o reg) on all data: 1.10556
Train acc on all data:  0.928692289098
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 5.85212e-05
Norm of the params: 34.0566
              Random: fixed 184 labels. Loss 1.10556. Accuracy 0.750.
### Flips: 1236, rs: 10, checks: 824
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.194823
Train loss (w/o reg) on all data: 0.151179
Test loss (w/o reg) on all data: 0.743253
Train acc on all data:  0.947063089195
Test acc on all data:   0.826086956522
Norm of the mean of gradients: 1.50899e-05
Norm of the params: 29.5445
     Influence (LOO): fixed 454 labels. Loss 0.74325. Accuracy 0.826.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.138206
Train loss (w/o reg) on all data: 0.0792257
Test loss (w/o reg) on all data: 1.06507
Train acc on all data:  0.992748368383
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 3.80755e-05
Norm of the params: 34.3453
                Loss: fixed 452 labels. Loss 1.06507. Accuracy 0.777.
Using normal model
LBFGS training took [647] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268277
Train loss (w/o reg) on all data: 0.210652
Test loss (w/o reg) on all data: 1.09809
Train acc on all data:  0.932559825961
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 5.7498e-05
Norm of the params: 33.9486
              Random: fixed 245 labels. Loss 1.09809. Accuracy 0.759.
### Flips: 1236, rs: 10, checks: 1030
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185213
Train loss (w/o reg) on all data: 0.143651
Test loss (w/o reg) on all data: 0.634361
Train acc on all data:  0.948996857626
Test acc on all data:   0.838647342995
Norm of the mean of gradients: 9.10961e-05
Norm of the params: 28.8315
     Influence (LOO): fixed 535 labels. Loss 0.63436. Accuracy 0.839.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130336
Train loss (w/o reg) on all data: 0.0738066
Test loss (w/o reg) on all data: 0.9549
Train acc on all data:  0.992990089437
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 8.78328e-06
Norm of the params: 33.6242
                Loss: fixed 495 labels. Loss 0.95490. Accuracy 0.788.
Using normal model
LBFGS training took [625] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.258196
Train loss (w/o reg) on all data: 0.202279
Test loss (w/o reg) on all data: 1.03825
Train acc on all data:  0.936669083877
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 6.15578e-05
Norm of the params: 33.4416
              Random: fixed 321 labels. Loss 1.03825. Accuracy 0.786.
### Flips: 1236, rs: 10, checks: 1236
Using normal model
LBFGS training took [354] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17311
Train loss (w/o reg) on all data: 0.134455
Test loss (w/o reg) on all data: 0.57295
Train acc on all data:  0.952139231327
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 8.09695e-05
Norm of the params: 27.8048
     Influence (LOO): fixed 620 labels. Loss 0.57295. Accuracy 0.858.
Using normal model
LBFGS training took [406] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120088
Train loss (w/o reg) on all data: 0.0667333
Test loss (w/o reg) on all data: 0.843318
Train acc on all data:  0.993715252599
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 2.18544e-05
Norm of the params: 32.6663
                Loss: fixed 552 labels. Loss 0.84332. Accuracy 0.805.
Using normal model
LBFGS training took [643] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24758
Train loss (w/o reg) on all data: 0.193409
Test loss (w/o reg) on all data: 0.932972
Train acc on all data:  0.937635968093
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 2.66693e-05
Norm of the params: 32.9155
              Random: fixed 387 labels. Loss 0.93297. Accuracy 0.780.
Using normal model
LBFGS training took [765] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30321
Train loss (w/o reg) on all data: 0.24535
Test loss (w/o reg) on all data: 1.09371
Train acc on all data:  0.910563210056
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 0.000156688
Norm of the params: 34.0178
Flipped loss: 1.09371. Accuracy: 0.735
### Flips: 1236, rs: 11, checks: 206
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2455
Train loss (w/o reg) on all data: 0.193283
Test loss (w/o reg) on all data: 1.07494
Train acc on all data:  0.931592941745
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 4.23247e-05
Norm of the params: 32.3162
     Influence (LOO): fixed 140 labels. Loss 1.07494. Accuracy 0.770.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223326
Train loss (w/o reg) on all data: 0.160259
Test loss (w/o reg) on all data: 1.18514
Train acc on all data:  0.955281605028
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 0.000169387
Norm of the params: 35.5154
                Loss: fixed 182 labels. Loss 1.18514. Accuracy 0.738.
Using normal model
LBFGS training took [634] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.297045
Train loss (w/o reg) on all data: 0.239583
Test loss (w/o reg) on all data: 1.12477
Train acc on all data:  0.912013536379
Test acc on all data:   0.742995169082
Norm of the mean of gradients: 0.000132047
Norm of the params: 33.9005
              Random: fixed  62 labels. Loss 1.12477. Accuracy 0.743.
### Flips: 1236, rs: 11, checks: 412
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221438
Train loss (w/o reg) on all data: 0.173555
Test loss (w/o reg) on all data: 0.929715
Train acc on all data:  0.939086294416
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 2.56639e-05
Norm of the params: 30.9461
     Influence (LOO): fixed 259 labels. Loss 0.92971. Accuracy 0.807.
Using normal model
LBFGS training took [554] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177816
Train loss (w/o reg) on all data: 0.115828
Test loss (w/o reg) on all data: 1.19332
Train acc on all data:  0.978486826203
Test acc on all data:   0.745893719807
Norm of the mean of gradients: 1.56044e-05
Norm of the params: 35.2103
                Loss: fixed 308 labels. Loss 1.19332. Accuracy 0.746.
Using normal model
LBFGS training took [595] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291025
Train loss (w/o reg) on all data: 0.23374
Test loss (w/o reg) on all data: 1.06945
Train acc on all data:  0.916122794295
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 7.03312e-05
Norm of the params: 33.8481
              Random: fixed 120 labels. Loss 1.06945. Accuracy 0.756.
### Flips: 1236, rs: 11, checks: 618
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207784
Train loss (w/o reg) on all data: 0.163006
Test loss (w/o reg) on all data: 0.889057
Train acc on all data:  0.942470389171
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 5.72712e-05
Norm of the params: 29.9259
     Influence (LOO): fixed 356 labels. Loss 0.88906. Accuracy 0.824.
Using normal model
LBFGS training took [557] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16117
Train loss (w/o reg) on all data: 0.100999
Test loss (w/o reg) on all data: 1.23759
Train acc on all data:  0.984771573604
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 2.03164e-05
Norm of the params: 34.6902
                Loss: fixed 394 labels. Loss 1.23759. Accuracy 0.763.
Using normal model
LBFGS training took [612] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282669
Train loss (w/o reg) on all data: 0.225849
Test loss (w/o reg) on all data: 1.03195
Train acc on all data:  0.920232052212
Test acc on all data:   0.765217391304
Norm of the mean of gradients: 3.07926e-05
Norm of the params: 33.7107
              Random: fixed 182 labels. Loss 1.03195. Accuracy 0.765.
### Flips: 1236, rs: 11, checks: 824
Using normal model
LBFGS training took [348] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19961
Train loss (w/o reg) on all data: 0.157297
Test loss (w/o reg) on all data: 0.874028
Train acc on all data:  0.943920715494
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 1.34819e-05
Norm of the params: 29.0905
     Influence (LOO): fixed 435 labels. Loss 0.87403. Accuracy 0.848.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140496
Train loss (w/o reg) on all data: 0.0832802
Test loss (w/o reg) on all data: 1.20426
Train acc on all data:  0.989364273628
Test acc on all data:   0.772946859903
Norm of the mean of gradients: 1.75408e-05
Norm of the params: 33.8278
                Loss: fixed 462 labels. Loss 1.20426. Accuracy 0.773.
Using normal model
LBFGS training took [569] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273887
Train loss (w/o reg) on all data: 0.217978
Test loss (w/o reg) on all data: 0.988241
Train acc on all data:  0.927000241721
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 6.02599e-05
Norm of the params: 33.4392
              Random: fixed 237 labels. Loss 0.98824. Accuracy 0.781.
### Flips: 1236, rs: 11, checks: 1030
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185945
Train loss (w/o reg) on all data: 0.145729
Test loss (w/o reg) on all data: 0.860268
Train acc on all data:  0.948996857626
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 1.83045e-05
Norm of the params: 28.3606
     Influence (LOO): fixed 517 labels. Loss 0.86027. Accuracy 0.861.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128623
Train loss (w/o reg) on all data: 0.073682
Test loss (w/o reg) on all data: 1.14779
Train acc on all data:  0.992023205221
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 2.62676e-05
Norm of the params: 33.1485
                Loss: fixed 512 labels. Loss 1.14779. Accuracy 0.787.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.264586
Train loss (w/o reg) on all data: 0.209358
Test loss (w/o reg) on all data: 0.978752
Train acc on all data:  0.928934010152
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 2.24386e-05
Norm of the params: 33.235
              Random: fixed 302 labels. Loss 0.97875. Accuracy 0.786.
### Flips: 1236, rs: 11, checks: 1236
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176494
Train loss (w/o reg) on all data: 0.138495
Test loss (w/o reg) on all data: 0.792614
Train acc on all data:  0.948513415518
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 8.57053e-06
Norm of the params: 27.5678
     Influence (LOO): fixed 583 labels. Loss 0.79261. Accuracy 0.874.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114886
Train loss (w/o reg) on all data: 0.0636167
Test loss (w/o reg) on all data: 0.977089
Train acc on all data:  0.993473531545
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 2.75274e-05
Norm of the params: 32.0215
                Loss: fixed 578 labels. Loss 0.97709. Accuracy 0.805.
Using normal model
LBFGS training took [601] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254127
Train loss (w/o reg) on all data: 0.198714
Test loss (w/o reg) on all data: 0.947643
Train acc on all data:  0.933284989123
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 3.36493e-05
Norm of the params: 33.2906
              Random: fixed 364 labels. Loss 0.94764. Accuracy 0.791.
Using normal model
LBFGS training took [596] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299962
Train loss (w/o reg) on all data: 0.243057
Test loss (w/o reg) on all data: 0.947911
Train acc on all data:  0.910321489002
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 0.000135813
Norm of the params: 33.7359
Flipped loss: 0.94791. Accuracy: 0.733
### Flips: 1236, rs: 12, checks: 206
Using normal model
LBFGS training took [444] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.236972
Train loss (w/o reg) on all data: 0.185191
Test loss (w/o reg) on all data: 0.77399
Train acc on all data:  0.934735315446
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 5.94103e-05
Norm of the params: 32.1812
     Influence (LOO): fixed 155 labels. Loss 0.77399. Accuracy 0.783.
Using normal model
LBFGS training took [489] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216098
Train loss (w/o reg) on all data: 0.15363
Test loss (w/o reg) on all data: 0.923261
Train acc on all data:  0.959632583998
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 8.95954e-05
Norm of the params: 35.3462
                Loss: fixed 195 labels. Loss 0.92326. Accuracy 0.755.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.294599
Train loss (w/o reg) on all data: 0.237647
Test loss (w/o reg) on all data: 0.923707
Train acc on all data:  0.912013536379
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 2.85059e-05
Norm of the params: 33.7498
              Random: fixed  60 labels. Loss 0.92371. Accuracy 0.753.
### Flips: 1236, rs: 12, checks: 412
Using normal model
LBFGS training took [413] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215439
Train loss (w/o reg) on all data: 0.168589
Test loss (w/o reg) on all data: 0.646205
Train acc on all data:  0.939811457578
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 1.1286e-05
Norm of the params: 30.6105
     Influence (LOO): fixed 283 labels. Loss 0.64620. Accuracy 0.819.
Using normal model
LBFGS training took [505] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175894
Train loss (w/o reg) on all data: 0.113386
Test loss (w/o reg) on all data: 0.847422
Train acc on all data:  0.980904036742
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 1.61205e-05
Norm of the params: 35.3577
                Loss: fixed 306 labels. Loss 0.84742. Accuracy 0.775.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.284029
Train loss (w/o reg) on all data: 0.228721
Test loss (w/o reg) on all data: 0.828526
Train acc on all data:  0.916364515349
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 4.27026e-05
Norm of the params: 33.259
              Random: fixed 135 labels. Loss 0.82853. Accuracy 0.770.
### Flips: 1236, rs: 12, checks: 618
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197449
Train loss (w/o reg) on all data: 0.153616
Test loss (w/o reg) on all data: 0.581775
Train acc on all data:  0.945612762872
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.21165e-05
Norm of the params: 29.6082
     Influence (LOO): fixed 388 labels. Loss 0.58178. Accuracy 0.830.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.146485
Train loss (w/o reg) on all data: 0.0871202
Test loss (w/o reg) on all data: 0.78002
Train acc on all data:  0.989122552574
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 4.09573e-05
Norm of the params: 34.4573
                Loss: fixed 401 labels. Loss 0.78002. Accuracy 0.780.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276009
Train loss (w/o reg) on all data: 0.220965
Test loss (w/o reg) on all data: 0.82046
Train acc on all data:  0.921440657481
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 1.34598e-05
Norm of the params: 33.1796
              Random: fixed 186 labels. Loss 0.82046. Accuracy 0.776.
### Flips: 1236, rs: 12, checks: 824
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184999
Train loss (w/o reg) on all data: 0.144267
Test loss (w/o reg) on all data: 0.53081
Train acc on all data:  0.946579647087
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 8.81006e-06
Norm of the params: 28.542
     Influence (LOO): fixed 472 labels. Loss 0.53081. Accuracy 0.847.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13749
Train loss (w/o reg) on all data: 0.0803965
Test loss (w/o reg) on all data: 0.773342
Train acc on all data:  0.989605994682
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 2.06644e-05
Norm of the params: 33.7917
                Loss: fixed 450 labels. Loss 0.77334. Accuracy 0.803.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266052
Train loss (w/o reg) on all data: 0.211881
Test loss (w/o reg) on all data: 0.82893
Train acc on all data:  0.926275078559
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 3.74911e-05
Norm of the params: 32.9155
              Random: fixed 249 labels. Loss 0.82893. Accuracy 0.788.
### Flips: 1236, rs: 12, checks: 1030
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176756
Train loss (w/o reg) on all data: 0.138444
Test loss (w/o reg) on all data: 0.459081
Train acc on all data:  0.945612762872
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 1.1919e-05
Norm of the params: 27.681
     Influence (LOO): fixed 549 labels. Loss 0.45908. Accuracy 0.861.
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124624
Train loss (w/o reg) on all data: 0.0706107
Test loss (w/o reg) on all data: 0.736965
Train acc on all data:  0.991781484167
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 1.37692e-05
Norm of the params: 32.8675
                Loss: fixed 499 labels. Loss 0.73696. Accuracy 0.825.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256077
Train loss (w/o reg) on all data: 0.202545
Test loss (w/o reg) on all data: 0.765871
Train acc on all data:  0.930867778584
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 9.04464e-05
Norm of the params: 32.7207
              Random: fixed 305 labels. Loss 0.76587. Accuracy 0.796.
### Flips: 1236, rs: 12, checks: 1236
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.167918
Train loss (w/o reg) on all data: 0.131711
Test loss (w/o reg) on all data: 0.429105
Train acc on all data:  0.948271694465
Test acc on all data:   0.886956521739
Norm of the mean of gradients: 1.40573e-05
Norm of the params: 26.9099
     Influence (LOO): fixed 619 labels. Loss 0.42910. Accuracy 0.887.
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111935
Train loss (w/o reg) on all data: 0.0613894
Test loss (w/o reg) on all data: 0.664847
Train acc on all data:  0.996615905245
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 1.24678e-05
Norm of the params: 31.7948
                Loss: fixed 562 labels. Loss 0.66485. Accuracy 0.831.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247019
Train loss (w/o reg) on all data: 0.194412
Test loss (w/o reg) on all data: 0.710054
Train acc on all data:  0.935943920715
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 2.25171e-05
Norm of the params: 32.4366
              Random: fixed 365 labels. Loss 0.71005. Accuracy 0.797.
Using normal model
LBFGS training took [617] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298295
Train loss (w/o reg) on all data: 0.239725
Test loss (w/o reg) on all data: 0.868717
Train acc on all data:  0.913222141649
Test acc on all data:   0.727536231884
Norm of the mean of gradients: 3.16063e-05
Norm of the params: 34.2258
Flipped loss: 0.86872. Accuracy: 0.728
### Flips: 1236, rs: 13, checks: 206
Using normal model
LBFGS training took [450] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239013
Train loss (w/o reg) on all data: 0.186334
Test loss (w/o reg) on all data: 0.801764
Train acc on all data:  0.931351220691
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 5.16745e-05
Norm of the params: 32.4587
     Influence (LOO): fixed 152 labels. Loss 0.80176. Accuracy 0.753.
Using normal model
LBFGS training took [522] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.231224
Train loss (w/o reg) on all data: 0.167189
Test loss (w/o reg) on all data: 0.894653
Train acc on all data:  0.95479816292
Test acc on all data:   0.730434782609
Norm of the mean of gradients: 5.72589e-05
Norm of the params: 35.7869
                Loss: fixed 174 labels. Loss 0.89465. Accuracy 0.730.
Using normal model
LBFGS training took [503] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293912
Train loss (w/o reg) on all data: 0.235516
Test loss (w/o reg) on all data: 0.870153
Train acc on all data:  0.915639352188
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 0.000136982
Norm of the params: 34.175
              Random: fixed  60 labels. Loss 0.87015. Accuracy 0.733.
### Flips: 1236, rs: 13, checks: 412
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215896
Train loss (w/o reg) on all data: 0.1683
Test loss (w/o reg) on all data: 0.646916
Train acc on all data:  0.934493594392
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 3.9813e-05
Norm of the params: 30.8532
     Influence (LOO): fixed 277 labels. Loss 0.64692. Accuracy 0.796.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186365
Train loss (w/o reg) on all data: 0.122465
Test loss (w/o reg) on all data: 0.983057
Train acc on all data:  0.976069615664
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 8.73111e-05
Norm of the params: 35.7491
                Loss: fixed 299 labels. Loss 0.98306. Accuracy 0.759.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.285787
Train loss (w/o reg) on all data: 0.228217
Test loss (w/o reg) on all data: 0.826944
Train acc on all data:  0.919023446942
Test acc on all data:   0.742028985507
Norm of the mean of gradients: 2.9633e-05
Norm of the params: 33.9322
              Random: fixed 125 labels. Loss 0.82694. Accuracy 0.742.
### Flips: 1236, rs: 13, checks: 618
Using normal model
LBFGS training took [372] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201506
Train loss (w/o reg) on all data: 0.157439
Test loss (w/o reg) on all data: 0.515849
Train acc on all data:  0.939569736524
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 1.87163e-05
Norm of the params: 29.6876
     Influence (LOO): fixed 392 labels. Loss 0.51585. Accuracy 0.832.
Using normal model
LBFGS training took [490] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162416
Train loss (w/o reg) on all data: 0.101134
Test loss (w/o reg) on all data: 0.934671
Train acc on all data:  0.984046410442
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 1.98333e-05
Norm of the params: 35.0092
                Loss: fixed 384 labels. Loss 0.93467. Accuracy 0.775.
Using normal model
LBFGS training took [585] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279912
Train loss (w/o reg) on all data: 0.22373
Test loss (w/o reg) on all data: 0.789903
Train acc on all data:  0.918781725888
Test acc on all data:   0.753623188406
Norm of the mean of gradients: 6.5871e-05
Norm of the params: 33.5208
              Random: fixed 178 labels. Loss 0.78990. Accuracy 0.754.
### Flips: 1236, rs: 13, checks: 824
Using normal model
LBFGS training took [342] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18781
Train loss (w/o reg) on all data: 0.146365
Test loss (w/o reg) on all data: 0.481558
Train acc on all data:  0.944162436548
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 2.317e-05
Norm of the params: 28.7906
     Influence (LOO): fixed 479 labels. Loss 0.48156. Accuracy 0.846.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147026
Train loss (w/o reg) on all data: 0.0881045
Test loss (w/o reg) on all data: 0.81704
Train acc on all data:  0.988639110467
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 2.83228e-05
Norm of the params: 34.3282
                Loss: fixed 444 labels. Loss 0.81704. Accuracy 0.789.
Using normal model
LBFGS training took [562] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273469
Train loss (w/o reg) on all data: 0.217666
Test loss (w/o reg) on all data: 0.748804
Train acc on all data:  0.923616146966
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 2.41932e-05
Norm of the params: 33.4075
              Random: fixed 234 labels. Loss 0.74880. Accuracy 0.760.
### Flips: 1236, rs: 13, checks: 1030
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177235
Train loss (w/o reg) on all data: 0.138565
Test loss (w/o reg) on all data: 0.449053
Train acc on all data:  0.948755136572
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 2.20417e-05
Norm of the params: 27.8102
     Influence (LOO): fixed 559 labels. Loss 0.44905. Accuracy 0.863.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132111
Train loss (w/o reg) on all data: 0.0767589
Test loss (w/o reg) on all data: 0.794553
Train acc on all data:  0.991298042059
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 1.14243e-05
Norm of the params: 33.2722
                Loss: fixed 508 labels. Loss 0.79455. Accuracy 0.805.
Using normal model
LBFGS training took [526] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266794
Train loss (w/o reg) on all data: 0.211509
Test loss (w/o reg) on all data: 0.707628
Train acc on all data:  0.927000241721
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 4.05861e-05
Norm of the params: 33.2522
              Random: fixed 291 labels. Loss 0.70763. Accuracy 0.769.
### Flips: 1236, rs: 13, checks: 1236
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169341
Train loss (w/o reg) on all data: 0.132696
Test loss (w/o reg) on all data: 0.431931
Train acc on all data:  0.949722020788
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 3.27146e-05
Norm of the params: 27.072
     Influence (LOO): fixed 633 labels. Loss 0.43193. Accuracy 0.877.
Using normal model
LBFGS training took [435] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114921
Train loss (w/o reg) on all data: 0.0638789
Test loss (w/o reg) on all data: 0.708234
Train acc on all data:  0.995407299976
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 3.36622e-05
Norm of the params: 31.9507
                Loss: fixed 580 labels. Loss 0.70823. Accuracy 0.840.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262507
Train loss (w/o reg) on all data: 0.208311
Test loss (w/o reg) on all data: 0.693453
Train acc on all data:  0.927241962775
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 5.70819e-05
Norm of the params: 32.9229
              Random: fixed 353 labels. Loss 0.69345. Accuracy 0.786.
Using normal model
LBFGS training took [623] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298602
Train loss (w/o reg) on all data: 0.239276
Test loss (w/o reg) on all data: 1.20439
Train acc on all data:  0.912013536379
Test acc on all data:   0.702415458937
Norm of the mean of gradients: 4.24631e-05
Norm of the params: 34.4458
Flipped loss: 1.20439. Accuracy: 0.702
### Flips: 1236, rs: 14, checks: 206
Using normal model
LBFGS training took [474] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24215
Train loss (w/o reg) on all data: 0.188139
Test loss (w/o reg) on all data: 1.12956
Train acc on all data:  0.936185641769
Test acc on all data:   0.746859903382
Norm of the mean of gradients: 4.80564e-05
Norm of the params: 32.8665
     Influence (LOO): fixed 144 labels. Loss 1.12956. Accuracy 0.747.
Using normal model
LBFGS training took [568] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.218884
Train loss (w/o reg) on all data: 0.154439
Test loss (w/o reg) on all data: 1.14688
Train acc on all data:  0.958182257675
Test acc on all data:   0.721739130435
Norm of the mean of gradients: 3.40599e-05
Norm of the params: 35.9012
                Loss: fixed 182 labels. Loss 1.14688. Accuracy 0.722.
Using normal model
LBFGS training took [551] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293034
Train loss (w/o reg) on all data: 0.234733
Test loss (w/o reg) on all data: 1.12572
Train acc on all data:  0.918056562727
Test acc on all data:   0.724637681159
Norm of the mean of gradients: 4.31695e-05
Norm of the params: 34.1471
              Random: fixed  62 labels. Loss 1.12572. Accuracy 0.725.
### Flips: 1236, rs: 14, checks: 412
Using normal model
LBFGS training took [434] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217562
Train loss (w/o reg) on all data: 0.168382
Test loss (w/o reg) on all data: 0.954333
Train acc on all data:  0.942228668117
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 2.79283e-05
Norm of the params: 31.3624
     Influence (LOO): fixed 265 labels. Loss 0.95433. Accuracy 0.786.
Using normal model
LBFGS training took [565] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179319
Train loss (w/o reg) on all data: 0.115144
Test loss (w/o reg) on all data: 1.20057
Train acc on all data:  0.979937152526
Test acc on all data:   0.737198067633
Norm of the mean of gradients: 3.76843e-05
Norm of the params: 35.8259
                Loss: fixed 300 labels. Loss 1.20057. Accuracy 0.737.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.286268
Train loss (w/o reg) on all data: 0.228284
Test loss (w/o reg) on all data: 1.1286
Train acc on all data:  0.920473773266
Test acc on all data:   0.72270531401
Norm of the mean of gradients: 0.000154375
Norm of the params: 34.0543
              Random: fixed 112 labels. Loss 1.12860. Accuracy 0.723.
### Flips: 1236, rs: 14, checks: 618
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202948
Train loss (w/o reg) on all data: 0.157439
Test loss (w/o reg) on all data: 0.869197
Train acc on all data:  0.946579647087
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 3.6884e-05
Norm of the params: 30.1691
     Influence (LOO): fixed 375 labels. Loss 0.86920. Accuracy 0.808.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149988
Train loss (w/o reg) on all data: 0.090022
Test loss (w/o reg) on all data: 1.13035
Train acc on all data:  0.989122552574
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 4.0728e-05
Norm of the params: 34.6311
                Loss: fixed 386 labels. Loss 1.13035. Accuracy 0.753.
Using normal model
LBFGS training took [597] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279416
Train loss (w/o reg) on all data: 0.222091
Test loss (w/o reg) on all data: 1.11382
Train acc on all data:  0.926516799613
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 7.95727e-05
Norm of the params: 33.8601
              Random: fixed 175 labels. Loss 1.11382. Accuracy 0.735.
### Flips: 1236, rs: 14, checks: 824
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191244
Train loss (w/o reg) on all data: 0.14907
Test loss (w/o reg) on all data: 0.715654
Train acc on all data:  0.950205462896
Test acc on all data:   0.828019323671
Norm of the mean of gradients: 2.42797e-05
Norm of the params: 29.0428
     Influence (LOO): fixed 473 labels. Loss 0.71565. Accuracy 0.828.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135748
Train loss (w/o reg) on all data: 0.0787578
Test loss (w/o reg) on all data: 1.22111
Train acc on all data:  0.993956973652
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 1.39775e-05
Norm of the params: 33.7611
                Loss: fixed 452 labels. Loss 1.22111. Accuracy 0.775.
Using normal model
LBFGS training took [572] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267739
Train loss (w/o reg) on all data: 0.211164
Test loss (w/o reg) on all data: 1.09311
Train acc on all data:  0.928692289098
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 0.000124215
Norm of the params: 33.6378
              Random: fixed 241 labels. Loss 1.09311. Accuracy 0.756.
### Flips: 1236, rs: 14, checks: 1030
Using normal model
LBFGS training took [320] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181479
Train loss (w/o reg) on all data: 0.14149
Test loss (w/o reg) on all data: 0.704054
Train acc on all data:  0.953347836597
Test acc on all data:   0.848309178744
Norm of the mean of gradients: 4.49605e-05
Norm of the params: 28.2804
     Influence (LOO): fixed 543 labels. Loss 0.70405. Accuracy 0.848.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124735
Train loss (w/o reg) on all data: 0.0707858
Test loss (w/o reg) on all data: 1.19688
Train acc on all data:  0.994198694706
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 1.15252e-05
Norm of the params: 32.8478
                Loss: fixed 506 labels. Loss 1.19688. Accuracy 0.778.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256634
Train loss (w/o reg) on all data: 0.201223
Test loss (w/o reg) on all data: 0.942183
Train acc on all data:  0.933043268069
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 5.27761e-05
Norm of the params: 33.2897
              Random: fixed 298 labels. Loss 0.94218. Accuracy 0.776.
### Flips: 1236, rs: 14, checks: 1236
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172875
Train loss (w/o reg) on all data: 0.13493
Test loss (w/o reg) on all data: 0.635985
Train acc on all data:  0.953831278704
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 1.40351e-05
Norm of the params: 27.5483
     Influence (LOO): fixed 619 labels. Loss 0.63599. Accuracy 0.859.
Using normal model
LBFGS training took [536] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11324
Train loss (w/o reg) on all data: 0.0623978
Test loss (w/o reg) on all data: 1.09342
Train acc on all data:  0.996374184191
Test acc on all data:   0.80193236715
Norm of the mean of gradients: 3.97325e-05
Norm of the params: 31.888
                Loss: fixed 560 labels. Loss 1.09342. Accuracy 0.802.
Using normal model
LBFGS training took [510] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247802
Train loss (w/o reg) on all data: 0.193341
Test loss (w/o reg) on all data: 0.844966
Train acc on all data:  0.937877689147
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 3.84001e-05
Norm of the params: 33.0034
              Random: fixed 360 labels. Loss 0.84497. Accuracy 0.783.
Using normal model
LBFGS training took [685] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.300967
Train loss (w/o reg) on all data: 0.242904
Test loss (w/o reg) on all data: 0.93136
Train acc on all data:  0.912980420595
Test acc on all data:   0.723671497585
Norm of the mean of gradients: 2.75784e-05
Norm of the params: 34.0774
Flipped loss: 0.93136. Accuracy: 0.724
### Flips: 1236, rs: 15, checks: 206
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245764
Train loss (w/o reg) on all data: 0.193186
Test loss (w/o reg) on all data: 0.90443
Train acc on all data:  0.931592941745
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 3.22614e-05
Norm of the params: 32.4278
     Influence (LOO): fixed 148 labels. Loss 0.90443. Accuracy 0.770.
Using normal model
LBFGS training took [610] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.224093
Train loss (w/o reg) on all data: 0.159367
Test loss (w/o reg) on all data: 1.02379
Train acc on all data:  0.956248489243
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 0.00010668
Norm of the params: 35.9795
                Loss: fixed 188 labels. Loss 1.02379. Accuracy 0.733.
Using normal model
LBFGS training took [549] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293204
Train loss (w/o reg) on all data: 0.235592
Test loss (w/o reg) on all data: 0.920706
Train acc on all data:  0.916606236403
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 4.92893e-05
Norm of the params: 33.9448
              Random: fixed  62 labels. Loss 0.92071. Accuracy 0.741.
### Flips: 1236, rs: 15, checks: 412
Using normal model
LBFGS training took [464] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223293
Train loss (w/o reg) on all data: 0.175057
Test loss (w/o reg) on all data: 0.796863
Train acc on all data:  0.936185641769
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 2.45588e-05
Norm of the params: 31.0599
     Influence (LOO): fixed 262 labels. Loss 0.79686. Accuracy 0.810.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179027
Train loss (w/o reg) on all data: 0.114532
Test loss (w/o reg) on all data: 0.925462
Train acc on all data:  0.980904036742
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 3.19852e-05
Norm of the params: 35.9151
                Loss: fixed 316 labels. Loss 0.92546. Accuracy 0.759.
Using normal model
LBFGS training took [647] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.286977
Train loss (w/o reg) on all data: 0.230371
Test loss (w/o reg) on all data: 0.847691
Train acc on all data:  0.917089678511
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 3.54026e-05
Norm of the params: 33.6469
              Random: fixed 120 labels. Loss 0.84769. Accuracy 0.750.
### Flips: 1236, rs: 15, checks: 618
Using normal model
LBFGS training took [405] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206102
Train loss (w/o reg) on all data: 0.161304
Test loss (w/o reg) on all data: 0.759825
Train acc on all data:  0.941503504955
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 1.20651e-05
Norm of the params: 29.9326
     Influence (LOO): fixed 378 labels. Loss 0.75982. Accuracy 0.836.
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154852
Train loss (w/o reg) on all data: 0.0934062
Test loss (w/o reg) on all data: 0.930521
Train acc on all data:  0.992023205221
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 5.53422e-05
Norm of the params: 35.0559
                Loss: fixed 395 labels. Loss 0.93052. Accuracy 0.790.
Using normal model
LBFGS training took [645] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.280938
Train loss (w/o reg) on all data: 0.225191
Test loss (w/o reg) on all data: 0.870432
Train acc on all data:  0.920232052212
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 0.000169993
Norm of the params: 33.3907
              Random: fixed 177 labels. Loss 0.87043. Accuracy 0.770.
### Flips: 1236, rs: 15, checks: 824
Using normal model
LBFGS training took [375] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195079
Train loss (w/o reg) on all data: 0.1527
Test loss (w/o reg) on all data: 0.596755
Train acc on all data:  0.943920715494
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 1.08936e-05
Norm of the params: 29.1135
     Influence (LOO): fixed 467 labels. Loss 0.59675. Accuracy 0.859.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140757
Train loss (w/o reg) on all data: 0.0821511
Test loss (w/o reg) on all data: 0.914261
Train acc on all data:  0.990572878898
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.08186e-05
Norm of the params: 34.2363
                Loss: fixed 452 labels. Loss 0.91426. Accuracy 0.803.
Using normal model
LBFGS training took [627] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272322
Train loss (w/o reg) on all data: 0.217682
Test loss (w/o reg) on all data: 0.846868
Train acc on all data:  0.92071549432
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 0.000118474
Norm of the params: 33.0575
              Random: fixed 243 labels. Loss 0.84687. Accuracy 0.787.
### Flips: 1236, rs: 15, checks: 1030
Using normal model
LBFGS training took [373] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.184971
Train loss (w/o reg) on all data: 0.145376
Test loss (w/o reg) on all data: 0.627342
Train acc on all data:  0.946821368141
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 2.90434e-05
Norm of the params: 28.1405
     Influence (LOO): fixed 552 labels. Loss 0.62734. Accuracy 0.858.
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127793
Train loss (w/o reg) on all data: 0.0723706
Test loss (w/o reg) on all data: 0.828308
Train acc on all data:  0.996374184191
Test acc on all data:   0.828985507246
Norm of the mean of gradients: 3.38994e-05
Norm of the params: 33.2934
                Loss: fixed 504 labels. Loss 0.82831. Accuracy 0.829.
Using normal model
LBFGS training took [575] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262586
Train loss (w/o reg) on all data: 0.208545
Test loss (w/o reg) on all data: 0.865804
Train acc on all data:  0.926516799613
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 9.13467e-05
Norm of the params: 32.8757
              Random: fixed 298 labels. Loss 0.86580. Accuracy 0.778.
### Flips: 1236, rs: 15, checks: 1236
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177268
Train loss (w/o reg) on all data: 0.139999
Test loss (w/o reg) on all data: 0.535941
Train acc on all data:  0.947546531303
Test acc on all data:   0.885990338164
Norm of the mean of gradients: 2.38949e-05
Norm of the params: 27.302
     Influence (LOO): fixed 624 labels. Loss 0.53594. Accuracy 0.886.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114464
Train loss (w/o reg) on all data: 0.0630456
Test loss (w/o reg) on all data: 0.791705
Train acc on all data:  0.996374184191
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 2.92798e-05
Norm of the params: 32.0683
                Loss: fixed 573 labels. Loss 0.79170. Accuracy 0.840.
Using normal model
LBFGS training took [607] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254136
Train loss (w/o reg) on all data: 0.201038
Test loss (w/o reg) on all data: 0.802611
Train acc on all data:  0.931109499637
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 6.71103e-05
Norm of the params: 32.588
              Random: fixed 358 labels. Loss 0.80261. Accuracy 0.797.
Using normal model
LBFGS training took [711] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298672
Train loss (w/o reg) on all data: 0.240779
Test loss (w/o reg) on all data: 1.12438
Train acc on all data:  0.916364515349
Test acc on all data:   0.717874396135
Norm of the mean of gradients: 7.02298e-05
Norm of the params: 34.0274
Flipped loss: 1.12438. Accuracy: 0.718
### Flips: 1236, rs: 16, checks: 206
Using normal model
LBFGS training took [521] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237684
Train loss (w/o reg) on all data: 0.18485
Test loss (w/o reg) on all data: 1.1593
Train acc on all data:  0.939811457578
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 8.40962e-05
Norm of the params: 32.5067
     Influence (LOO): fixed 157 labels. Loss 1.15930. Accuracy 0.785.
Using normal model
LBFGS training took [652] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214155
Train loss (w/o reg) on all data: 0.150371
Test loss (w/o reg) on all data: 1.18751
Train acc on all data:  0.959874305052
Test acc on all data:   0.737198067633
Norm of the mean of gradients: 7.5032e-05
Norm of the params: 35.7167
                Loss: fixed 189 labels. Loss 1.18751. Accuracy 0.737.
Using normal model
LBFGS training took [642] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.294559
Train loss (w/o reg) on all data: 0.237691
Test loss (w/o reg) on all data: 1.0967
Train acc on all data:  0.916847957457
Test acc on all data:   0.731400966184
Norm of the mean of gradients: 2.87637e-05
Norm of the params: 33.7248
              Random: fixed  61 labels. Loss 1.09670. Accuracy 0.731.
### Flips: 1236, rs: 16, checks: 412
Using normal model
LBFGS training took [490] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216226
Train loss (w/o reg) on all data: 0.169514
Test loss (w/o reg) on all data: 1.01563
Train acc on all data:  0.944404157602
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 3.68194e-05
Norm of the params: 30.5656
     Influence (LOO): fixed 284 labels. Loss 1.01563. Accuracy 0.803.
Using normal model
LBFGS training took [663] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175757
Train loss (w/o reg) on all data: 0.112929
Test loss (w/o reg) on all data: 1.24912
Train acc on all data:  0.980420594634
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 9.02594e-05
Norm of the params: 35.4479
                Loss: fixed 306 labels. Loss 1.24912. Accuracy 0.755.
Using normal model
LBFGS training took [602] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.285
Train loss (w/o reg) on all data: 0.228868
Test loss (w/o reg) on all data: 1.08517
Train acc on all data:  0.92071549432
Test acc on all data:   0.747826086957
Norm of the mean of gradients: 8.87081e-05
Norm of the params: 33.5056
              Random: fixed 116 labels. Loss 1.08517. Accuracy 0.748.
### Flips: 1236, rs: 16, checks: 618
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202625
Train loss (w/o reg) on all data: 0.158283
Test loss (w/o reg) on all data: 0.895908
Train acc on all data:  0.947063089195
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 1.1127e-05
Norm of the params: 29.7799
     Influence (LOO): fixed 382 labels. Loss 0.89591. Accuracy 0.820.
Using normal model
LBFGS training took [711] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150737
Train loss (w/o reg) on all data: 0.0902493
Test loss (w/o reg) on all data: 1.21847
Train acc on all data:  0.988639110467
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 6.59164e-05
Norm of the params: 34.7815
                Loss: fixed 399 labels. Loss 1.21847. Accuracy 0.776.
Using normal model
LBFGS training took [709] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.278589
Train loss (w/o reg) on all data: 0.223008
Test loss (w/o reg) on all data: 0.956472
Train acc on all data:  0.924583031182
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 0.000125526
Norm of the params: 33.341
              Random: fixed 177 labels. Loss 0.95647. Accuracy 0.759.
### Flips: 1236, rs: 16, checks: 824
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190177
Train loss (w/o reg) on all data: 0.148718
Test loss (w/o reg) on all data: 0.759835
Train acc on all data:  0.949722020788
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 2.13128e-05
Norm of the params: 28.7956
     Influence (LOO): fixed 478 labels. Loss 0.75984. Accuracy 0.832.
Using normal model
LBFGS training took [679] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134895
Train loss (w/o reg) on all data: 0.0774714
Test loss (w/o reg) on all data: 1.22614
Train acc on all data:  0.992990089437
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 4.56025e-05
Norm of the params: 33.889
                Loss: fixed 452 labels. Loss 1.22614. Accuracy 0.781.
Using normal model
LBFGS training took [631] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268915
Train loss (w/o reg) on all data: 0.213802
Test loss (w/o reg) on all data: 0.889411
Train acc on all data:  0.92941745226
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 0.000128242
Norm of the params: 33.2003
              Random: fixed 240 labels. Loss 0.88941. Accuracy 0.764.
### Flips: 1236, rs: 16, checks: 1030
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182611
Train loss (w/o reg) on all data: 0.143092
Test loss (w/o reg) on all data: 0.791047
Train acc on all data:  0.950930626058
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 1.32339e-05
Norm of the params: 28.1138
     Influence (LOO): fixed 544 labels. Loss 0.79105. Accuracy 0.841.
Using normal model
LBFGS training took [565] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125603
Train loss (w/o reg) on all data: 0.0707048
Test loss (w/o reg) on all data: 1.18622
Train acc on all data:  0.992990089437
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 6.91507e-05
Norm of the params: 33.1356
                Loss: fixed 500 labels. Loss 1.18622. Accuracy 0.798.
Using normal model
LBFGS training took [644] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259012
Train loss (w/o reg) on all data: 0.204953
Test loss (w/o reg) on all data: 0.921847
Train acc on all data:  0.932801547015
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 4.2696e-05
Norm of the params: 32.8813
              Random: fixed 304 labels. Loss 0.92185. Accuracy 0.772.
### Flips: 1236, rs: 16, checks: 1236
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175064
Train loss (w/o reg) on all data: 0.137501
Test loss (w/o reg) on all data: 0.745276
Train acc on all data:  0.953347836597
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 1.92321e-05
Norm of the params: 27.4091
     Influence (LOO): fixed 617 labels. Loss 0.74528. Accuracy 0.858.
Using normal model
LBFGS training took [601] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115163
Train loss (w/o reg) on all data: 0.0635531
Test loss (w/o reg) on all data: 1.13843
Train acc on all data:  0.996857626299
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 1.03314e-05
Norm of the params: 32.1278
                Loss: fixed 557 labels. Loss 1.13843. Accuracy 0.827.
Using normal model
LBFGS training took [659] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248979
Train loss (w/o reg) on all data: 0.195768
Test loss (w/o reg) on all data: 0.867846
Train acc on all data:  0.936427362823
Test acc on all data:   0.8
Norm of the mean of gradients: 7.85128e-05
Norm of the params: 32.6225
              Random: fixed 363 labels. Loss 0.86785. Accuracy 0.800.
Using normal model
LBFGS training took [640] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29976
Train loss (w/o reg) on all data: 0.241384
Test loss (w/o reg) on all data: 1.20905
Train acc on all data:  0.907904278463
Test acc on all data:   0.72270531401
Norm of the mean of gradients: 0.000184251
Norm of the params: 34.169
Flipped loss: 1.20905. Accuracy: 0.723
### Flips: 1236, rs: 17, checks: 206
Using normal model
LBFGS training took [431] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.245755
Train loss (w/o reg) on all data: 0.192253
Test loss (w/o reg) on all data: 1.06003
Train acc on all data:  0.929175731206
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 6.45282e-05
Norm of the params: 32.7115
     Influence (LOO): fixed 141 labels. Loss 1.06003. Accuracy 0.755.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228974
Train loss (w/o reg) on all data: 0.165765
Test loss (w/o reg) on all data: 1.21855
Train acc on all data:  0.953347836597
Test acc on all data:   0.721739130435
Norm of the mean of gradients: 2.31606e-05
Norm of the params: 35.5554
                Loss: fixed 183 labels. Loss 1.21855. Accuracy 0.722.
Using normal model
LBFGS training took [540] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.292873
Train loss (w/o reg) on all data: 0.235231
Test loss (w/o reg) on all data: 1.16552
Train acc on all data:  0.913222141649
Test acc on all data:   0.721739130435
Norm of the mean of gradients: 5.20299e-05
Norm of the params: 33.9537
              Random: fixed  55 labels. Loss 1.16552. Accuracy 0.722.
### Flips: 1236, rs: 17, checks: 412
Using normal model
LBFGS training took [389] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222785
Train loss (w/o reg) on all data: 0.174508
Test loss (w/o reg) on all data: 0.94353
Train acc on all data:  0.9349770365
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 7.81304e-05
Norm of the params: 31.0731
     Influence (LOO): fixed 270 labels. Loss 0.94353. Accuracy 0.784.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182421
Train loss (w/o reg) on all data: 0.119598
Test loss (w/o reg) on all data: 1.29675
Train acc on all data:  0.978486826203
Test acc on all data:   0.743961352657
Norm of the mean of gradients: 2.81756e-05
Norm of the params: 35.4468
                Loss: fixed 314 labels. Loss 1.29675. Accuracy 0.744.
Using normal model
LBFGS training took [577] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.283398
Train loss (w/o reg) on all data: 0.227004
Test loss (w/o reg) on all data: 1.10324
Train acc on all data:  0.919748610104
Test acc on all data:   0.717874396135
Norm of the mean of gradients: 7.17836e-05
Norm of the params: 33.5841
              Random: fixed 126 labels. Loss 1.10324. Accuracy 0.718.
### Flips: 1236, rs: 17, checks: 618
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20968
Train loss (w/o reg) on all data: 0.164695
Test loss (w/o reg) on all data: 0.877844
Train acc on all data:  0.93932801547
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 3.77e-05
Norm of the params: 29.995
     Influence (LOO): fixed 367 labels. Loss 0.87784. Accuracy 0.820.
Using normal model
LBFGS training took [540] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.162343
Train loss (w/o reg) on all data: 0.101678
Test loss (w/o reg) on all data: 1.1999
Train acc on all data:  0.984046410442
Test acc on all data:   0.757487922705
Norm of the mean of gradients: 1.58811e-05
Norm of the params: 34.8325
                Loss: fixed 390 labels. Loss 1.19990. Accuracy 0.757.
Using normal model
LBFGS training took [581] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279116
Train loss (w/o reg) on all data: 0.223407
Test loss (w/o reg) on all data: 1.05146
Train acc on all data:  0.920473773266
Test acc on all data:   0.723671497585
Norm of the mean of gradients: 2.20777e-05
Norm of the params: 33.3792
              Random: fixed 179 labels. Loss 1.05146. Accuracy 0.724.
### Flips: 1236, rs: 17, checks: 824
Using normal model
LBFGS training took [399] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198651
Train loss (w/o reg) on all data: 0.156086
Test loss (w/o reg) on all data: 0.782813
Train acc on all data:  0.942712110225
Test acc on all data:   0.847342995169
Norm of the mean of gradients: 4.00464e-05
Norm of the params: 29.1771
     Influence (LOO): fixed 446 labels. Loss 0.78281. Accuracy 0.847.
Using normal model
LBFGS training took [537] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.144885
Train loss (w/o reg) on all data: 0.086212
Test loss (w/o reg) on all data: 1.0571
Train acc on all data:  0.98888083152
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 5.55263e-05
Norm of the params: 34.2558
                Loss: fixed 452 labels. Loss 1.05710. Accuracy 0.778.
Using normal model
LBFGS training took [568] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271517
Train loss (w/o reg) on all data: 0.216537
Test loss (w/o reg) on all data: 1.01907
Train acc on all data:  0.925549915398
Test acc on all data:   0.743961352657
Norm of the mean of gradients: 7.55478e-05
Norm of the params: 33.1602
              Random: fixed 241 labels. Loss 1.01907. Accuracy 0.744.
### Flips: 1236, rs: 17, checks: 1030
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190234
Train loss (w/o reg) on all data: 0.150065
Test loss (w/o reg) on all data: 0.684365
Train acc on all data:  0.942470389171
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 2.50765e-05
Norm of the params: 28.3439
     Influence (LOO): fixed 535 labels. Loss 0.68437. Accuracy 0.851.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13442
Train loss (w/o reg) on all data: 0.0784316
Test loss (w/o reg) on all data: 1.03494
Train acc on all data:  0.989605994682
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 2.1726e-05
Norm of the params: 33.4631
                Loss: fixed 506 labels. Loss 1.03494. Accuracy 0.789.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.257392
Train loss (w/o reg) on all data: 0.203234
Test loss (w/o reg) on all data: 0.938351
Train acc on all data:  0.933043268069
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 0.000126297
Norm of the params: 32.9114
              Random: fixed 309 labels. Loss 0.93835. Accuracy 0.761.
### Flips: 1236, rs: 17, checks: 1236
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181198
Train loss (w/o reg) on all data: 0.143704
Test loss (w/o reg) on all data: 0.61805
Train acc on all data:  0.943437273387
Test acc on all data:   0.870531400966
Norm of the mean of gradients: 1.7837e-05
Norm of the params: 27.3839
     Influence (LOO): fixed 605 labels. Loss 0.61805. Accuracy 0.871.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120195
Train loss (w/o reg) on all data: 0.0676131
Test loss (w/o reg) on all data: 0.978354
Train acc on all data:  0.992506647329
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 1.20289e-05
Norm of the params: 32.4291
                Loss: fixed 580 labels. Loss 0.97835. Accuracy 0.806.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.247578
Train loss (w/o reg) on all data: 0.194121
Test loss (w/o reg) on all data: 0.919784
Train acc on all data:  0.938119410201
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 0.000115025
Norm of the params: 32.6976
              Random: fixed 381 labels. Loss 0.91978. Accuracy 0.784.
Using normal model
LBFGS training took [666] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.300563
Train loss (w/o reg) on all data: 0.241082
Test loss (w/o reg) on all data: 0.943207
Train acc on all data:  0.916847957457
Test acc on all data:   0.709178743961
Norm of the mean of gradients: 3.39973e-05
Norm of the params: 34.4911
Flipped loss: 0.94321. Accuracy: 0.709
### Flips: 1236, rs: 18, checks: 206
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.238334
Train loss (w/o reg) on all data: 0.183746
Test loss (w/o reg) on all data: 0.786296
Train acc on all data:  0.940294899686
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 1.8962e-05
Norm of the params: 33.0416
     Influence (LOO): fixed 157 labels. Loss 0.78630. Accuracy 0.776.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215846
Train loss (w/o reg) on all data: 0.150167
Test loss (w/o reg) on all data: 0.924296
Train acc on all data:  0.9659173314
Test acc on all data:   0.732367149758
Norm of the mean of gradients: 8.25384e-05
Norm of the params: 36.2432
                Loss: fixed 183 labels. Loss 0.92430. Accuracy 0.732.
Using normal model
LBFGS training took [540] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29405
Train loss (w/o reg) on all data: 0.234836
Test loss (w/o reg) on all data: 0.901219
Train acc on all data:  0.919748610104
Test acc on all data:   0.717874396135
Norm of the mean of gradients: 4.68581e-05
Norm of the params: 34.4135
              Random: fixed  59 labels. Loss 0.90122. Accuracy 0.718.
### Flips: 1236, rs: 18, checks: 412
Using normal model
LBFGS training took [441] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217156
Train loss (w/o reg) on all data: 0.166421
Test loss (w/o reg) on all data: 0.68455
Train acc on all data:  0.947063089195
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 1.7281e-05
Norm of the params: 31.8545
     Influence (LOO): fixed 266 labels. Loss 0.68455. Accuracy 0.782.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179137
Train loss (w/o reg) on all data: 0.114458
Test loss (w/o reg) on all data: 0.914774
Train acc on all data:  0.982354363065
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 7.00409e-05
Norm of the params: 35.9662
                Loss: fixed 304 labels. Loss 0.91477. Accuracy 0.750.
Using normal model
LBFGS training took [563] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.289584
Train loss (w/o reg) on all data: 0.231064
Test loss (w/o reg) on all data: 0.868765
Train acc on all data:  0.921198936427
Test acc on all data:   0.72270531401
Norm of the mean of gradients: 0.000126613
Norm of the params: 34.2111
              Random: fixed 110 labels. Loss 0.86876. Accuracy 0.723.
### Flips: 1236, rs: 18, checks: 618
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205041
Train loss (w/o reg) on all data: 0.157529
Test loss (w/o reg) on all data: 0.631717
Train acc on all data:  0.951655789219
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 1.3817e-05
Norm of the params: 30.826
     Influence (LOO): fixed 357 labels. Loss 0.63172. Accuracy 0.817.
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154424
Train loss (w/o reg) on all data: 0.0933538
Test loss (w/o reg) on all data: 0.880936
Train acc on all data:  0.987913947305
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 1.96952e-05
Norm of the params: 34.9487
                Loss: fixed 392 labels. Loss 0.88094. Accuracy 0.776.
Using normal model
LBFGS training took [609] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279686
Train loss (w/o reg) on all data: 0.221463
Test loss (w/o reg) on all data: 0.878842
Train acc on all data:  0.927000241721
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 0.0002006
Norm of the params: 34.1241
              Random: fixed 178 labels. Loss 0.87884. Accuracy 0.735.
### Flips: 1236, rs: 18, checks: 824
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192325
Train loss (w/o reg) on all data: 0.14784
Test loss (w/o reg) on all data: 0.610991
Train acc on all data:  0.953106115543
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 2.58405e-05
Norm of the params: 29.8281
     Influence (LOO): fixed 451 labels. Loss 0.61099. Accuracy 0.825.
Using normal model
LBFGS training took [476] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137876
Train loss (w/o reg) on all data: 0.0798936
Test loss (w/o reg) on all data: 0.764797
Train acc on all data:  0.990331157844
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 4.13709e-05
Norm of the params: 34.0535
                Loss: fixed 461 labels. Loss 0.76480. Accuracy 0.803.
Using normal model
LBFGS training took [635] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.270482
Train loss (w/o reg) on all data: 0.213314
Test loss (w/o reg) on all data: 0.799105
Train acc on all data:  0.930384336476
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 3.58151e-05
Norm of the params: 33.8136
              Random: fixed 253 labels. Loss 0.79911. Accuracy 0.761.
### Flips: 1236, rs: 18, checks: 1030
Using normal model
LBFGS training took [383] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182783
Train loss (w/o reg) on all data: 0.14059
Test loss (w/o reg) on all data: 0.508144
Train acc on all data:  0.955523326082
Test acc on all data:   0.84347826087
Norm of the mean of gradients: 1.48263e-05
Norm of the params: 29.0492
     Influence (LOO): fixed 539 labels. Loss 0.50814. Accuracy 0.843.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127968
Train loss (w/o reg) on all data: 0.0723558
Test loss (w/o reg) on all data: 0.773181
Train acc on all data:  0.995165578922
Test acc on all data:   0.823188405797
Norm of the mean of gradients: 4.14254e-05
Norm of the params: 33.3502
                Loss: fixed 512 labels. Loss 0.77318. Accuracy 0.823.
Using normal model
LBFGS training took [600] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259847
Train loss (w/o reg) on all data: 0.20301
Test loss (w/o reg) on all data: 0.751883
Train acc on all data:  0.936185641769
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 2.49192e-05
Norm of the params: 33.7156
              Random: fixed 309 labels. Loss 0.75188. Accuracy 0.781.
### Flips: 1236, rs: 18, checks: 1236
Using normal model
LBFGS training took [343] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173278
Train loss (w/o reg) on all data: 0.133865
Test loss (w/o reg) on all data: 0.439074
Train acc on all data:  0.957215373459
Test acc on all data:   0.858937198068
Norm of the mean of gradients: 2.83648e-05
Norm of the params: 28.0759
     Influence (LOO): fixed 622 labels. Loss 0.43907. Accuracy 0.859.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120966
Train loss (w/o reg) on all data: 0.067402
Test loss (w/o reg) on all data: 0.741952
Train acc on all data:  0.995407299976
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 1.38335e-05
Norm of the params: 32.7303
                Loss: fixed 562 labels. Loss 0.74195. Accuracy 0.830.
Using normal model
LBFGS training took [610] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248609
Train loss (w/o reg) on all data: 0.192643
Test loss (w/o reg) on all data: 0.734129
Train acc on all data:  0.941986947063
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 5.16853e-05
Norm of the params: 33.4562
              Random: fixed 375 labels. Loss 0.73413. Accuracy 0.785.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.292744
Train loss (w/o reg) on all data: 0.232439
Test loss (w/o reg) on all data: 0.885501
Train acc on all data:  0.916847957457
Test acc on all data:   0.725603864734
Norm of the mean of gradients: 9.95137e-05
Norm of the params: 34.7291
Flipped loss: 0.88550. Accuracy: 0.726
### Flips: 1236, rs: 19, checks: 206
Using normal model
LBFGS training took [394] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.239523
Train loss (w/o reg) on all data: 0.185316
Test loss (w/o reg) on all data: 0.829615
Train acc on all data:  0.939569736524
Test acc on all data:   0.757487922705
Norm of the mean of gradients: 1.66224e-05
Norm of the params: 32.9262
     Influence (LOO): fixed 145 labels. Loss 0.82962. Accuracy 0.757.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213329
Train loss (w/o reg) on all data: 0.147836
Test loss (w/o reg) on all data: 0.880616
Train acc on all data:  0.963741841914
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 1.42413e-05
Norm of the params: 36.192
                Loss: fixed 186 labels. Loss 0.88062. Accuracy 0.738.
Using normal model
LBFGS training took [451] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.288217
Train loss (w/o reg) on all data: 0.228668
Test loss (w/o reg) on all data: 0.814652
Train acc on all data:  0.922890983805
Test acc on all data:   0.742995169082
Norm of the mean of gradients: 9.65983e-05
Norm of the params: 34.5106
              Random: fixed  72 labels. Loss 0.81465. Accuracy 0.743.
### Flips: 1236, rs: 19, checks: 412
Using normal model
LBFGS training took [364] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216533
Train loss (w/o reg) on all data: 0.167025
Test loss (w/o reg) on all data: 0.713583
Train acc on all data:  0.945371041818
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 1.35404e-05
Norm of the params: 31.4669
     Influence (LOO): fixed 264 labels. Loss 0.71358. Accuracy 0.789.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183221
Train loss (w/o reg) on all data: 0.118522
Test loss (w/o reg) on all data: 0.845368
Train acc on all data:  0.981629199903
Test acc on all data:   0.745893719807
Norm of the mean of gradients: 2.99189e-05
Norm of the params: 35.9719
                Loss: fixed 287 labels. Loss 0.84537. Accuracy 0.746.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.281762
Train loss (w/o reg) on all data: 0.222926
Test loss (w/o reg) on all data: 0.780231
Train acc on all data:  0.925791636452
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 2.83274e-05
Norm of the params: 34.3034
              Random: fixed 136 labels. Loss 0.78023. Accuracy 0.755.
### Flips: 1236, rs: 19, checks: 618
Using normal model
LBFGS training took [358] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.204963
Train loss (w/o reg) on all data: 0.158668
Test loss (w/o reg) on all data: 0.653765
Train acc on all data:  0.945371041818
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 4.69155e-05
Norm of the params: 30.4285
     Influence (LOO): fixed 360 labels. Loss 0.65377. Accuracy 0.808.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160889
Train loss (w/o reg) on all data: 0.0987953
Test loss (w/o reg) on all data: 0.818946
Train acc on all data:  0.989122552574
Test acc on all data:   0.75845410628
Norm of the mean of gradients: 4.44324e-05
Norm of the params: 35.2403
                Loss: fixed 369 labels. Loss 0.81895. Accuracy 0.758.
Using normal model
LBFGS training took [461] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273329
Train loss (w/o reg) on all data: 0.215763
Test loss (w/o reg) on all data: 0.770746
Train acc on all data:  0.930867778584
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 7.61141e-05
Norm of the params: 33.9311
              Random: fixed 201 labels. Loss 0.77075. Accuracy 0.766.
### Flips: 1236, rs: 19, checks: 824
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195426
Train loss (w/o reg) on all data: 0.151429
Test loss (w/o reg) on all data: 0.626822
Train acc on all data:  0.94923857868
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 2.04954e-05
Norm of the params: 29.6639
     Influence (LOO): fixed 439 labels. Loss 0.62682. Accuracy 0.820.
Using normal model
LBFGS training took [457] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148369
Train loss (w/o reg) on all data: 0.088637
Test loss (w/o reg) on all data: 0.777415
Train acc on all data:  0.990572878898
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 3.0555e-05
Norm of the params: 34.5637
                Loss: fixed 424 labels. Loss 0.77741. Accuracy 0.779.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263939
Train loss (w/o reg) on all data: 0.207567
Test loss (w/o reg) on all data: 0.726198
Train acc on all data:  0.936185641769
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 3.56913e-05
Norm of the params: 33.5773
              Random: fixed 257 labels. Loss 0.72620. Accuracy 0.772.
### Flips: 1236, rs: 19, checks: 1030
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187551
Train loss (w/o reg) on all data: 0.145649
Test loss (w/o reg) on all data: 0.543033
Train acc on all data:  0.948996857626
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 7.87608e-05
Norm of the params: 28.9487
     Influence (LOO): fixed 518 labels. Loss 0.54303. Accuracy 0.833.
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136421
Train loss (w/o reg) on all data: 0.078887
Test loss (w/o reg) on all data: 0.7324
Train acc on all data:  0.991539763113
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 4.63608e-05
Norm of the params: 33.9218
                Loss: fixed 475 labels. Loss 0.73240. Accuracy 0.794.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.258289
Train loss (w/o reg) on all data: 0.202764
Test loss (w/o reg) on all data: 0.706723
Train acc on all data:  0.935943920715
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 2.70267e-05
Norm of the params: 33.3242
              Random: fixed 317 labels. Loss 0.70672. Accuracy 0.778.
### Flips: 1236, rs: 19, checks: 1236
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175631
Train loss (w/o reg) on all data: 0.136715
Test loss (w/o reg) on all data: 0.490151
Train acc on all data:  0.952864394489
Test acc on all data:   0.851207729469
Norm of the mean of gradients: 2.56057e-05
Norm of the params: 27.8983
     Influence (LOO): fixed 603 labels. Loss 0.49015. Accuracy 0.851.
Using normal model
LBFGS training took [427] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123302
Train loss (w/o reg) on all data: 0.0689746
Test loss (w/o reg) on all data: 0.688859
Train acc on all data:  0.992264926275
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.00629e-05
Norm of the params: 32.9629
                Loss: fixed 545 labels. Loss 0.68886. Accuracy 0.803.
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249511
Train loss (w/o reg) on all data: 0.194903
Test loss (w/o reg) on all data: 0.686172
Train acc on all data:  0.936910804931
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 0.000109807
Norm of the params: 33.0478
              Random: fixed 372 labels. Loss 0.68617. Accuracy 0.797.
Using normal model
LBFGS training took [684] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29816
Train loss (w/o reg) on all data: 0.23957
Test loss (w/o reg) on all data: 1.12594
Train acc on all data:  0.915881073241
Test acc on all data:   0.693719806763
Norm of the mean of gradients: 7.3787e-05
Norm of the params: 34.2315
Flipped loss: 1.12594. Accuracy: 0.694
### Flips: 1236, rs: 20, checks: 206
Using normal model
LBFGS training took [594] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241376
Train loss (w/o reg) on all data: 0.188034
Test loss (w/o reg) on all data: 1.08177
Train acc on all data:  0.937394247039
Test acc on all data:   0.734299516908
Norm of the mean of gradients: 7.7905e-05
Norm of the params: 32.6625
     Influence (LOO): fixed 139 labels. Loss 1.08177. Accuracy 0.734.
Using normal model
LBFGS training took [605] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213934
Train loss (w/o reg) on all data: 0.152046
Test loss (w/o reg) on all data: 1.13848
Train acc on all data:  0.961566352429
Test acc on all data:   0.72077294686
Norm of the mean of gradients: 8.04519e-05
Norm of the params: 35.1819
                Loss: fixed 184 labels. Loss 1.13848. Accuracy 0.721.
Using normal model
LBFGS training took [548] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.292051
Train loss (w/o reg) on all data: 0.234646
Test loss (w/o reg) on all data: 1.12554
Train acc on all data:  0.915639352188
Test acc on all data:   0.700483091787
Norm of the mean of gradients: 0.000126086
Norm of the params: 33.8836
              Random: fixed  55 labels. Loss 1.12554. Accuracy 0.700.
### Flips: 1236, rs: 20, checks: 412
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.218246
Train loss (w/o reg) on all data: 0.169641
Test loss (w/o reg) on all data: 1.02218
Train acc on all data:  0.942470389171
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 3.28316e-05
Norm of the params: 31.1786
     Influence (LOO): fixed 264 labels. Loss 1.02218. Accuracy 0.778.
Using normal model
LBFGS training took [662] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179167
Train loss (w/o reg) on all data: 0.117225
Test loss (w/o reg) on all data: 1.14202
Train acc on all data:  0.979937152526
Test acc on all data:   0.725603864734
Norm of the mean of gradients: 4.34567e-05
Norm of the params: 35.197
                Loss: fixed 298 labels. Loss 1.14202. Accuracy 0.726.
Using normal model
LBFGS training took [635] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.285369
Train loss (w/o reg) on all data: 0.227993
Test loss (w/o reg) on all data: 1.11851
Train acc on all data:  0.920232052212
Test acc on all data:   0.714975845411
Norm of the mean of gradients: 3.01756e-05
Norm of the params: 33.875
              Random: fixed 117 labels. Loss 1.11851. Accuracy 0.715.
### Flips: 1236, rs: 20, checks: 618
Using normal model
LBFGS training took [401] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202758
Train loss (w/o reg) on all data: 0.157295
Test loss (w/o reg) on all data: 1.00974
Train acc on all data:  0.946096204979
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 1.6837e-05
Norm of the params: 30.1541
     Influence (LOO): fixed 369 labels. Loss 1.00974. Accuracy 0.789.
Using normal model
LBFGS training took [639] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155122
Train loss (w/o reg) on all data: 0.0952043
Test loss (w/o reg) on all data: 1.03971
Train acc on all data:  0.987430505197
Test acc on all data:   0.742995169082
Norm of the mean of gradients: 5.98876e-05
Norm of the params: 34.6171
                Loss: fixed 388 labels. Loss 1.03971. Accuracy 0.743.
Using normal model
LBFGS training took [641] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279586
Train loss (w/o reg) on all data: 0.222652
Test loss (w/o reg) on all data: 1.07923
Train acc on all data:  0.92071549432
Test acc on all data:   0.731400966184
Norm of the mean of gradients: 0.000247565
Norm of the params: 33.7444
              Random: fixed 172 labels. Loss 1.07923. Accuracy 0.731.
### Flips: 1236, rs: 20, checks: 824
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192986
Train loss (w/o reg) on all data: 0.1495
Test loss (w/o reg) on all data: 0.925885
Train acc on all data:  0.948271694465
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 1.30895e-05
Norm of the params: 29.4908
     Influence (LOO): fixed 451 labels. Loss 0.92588. Accuracy 0.813.
Using normal model
LBFGS training took [475] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140073
Train loss (w/o reg) on all data: 0.0832324
Test loss (w/o reg) on all data: 1.02655
Train acc on all data:  0.992748368383
Test acc on all data:   0.757487922705
Norm of the mean of gradients: 1.79738e-05
Norm of the params: 33.7166
                Loss: fixed 445 labels. Loss 1.02655. Accuracy 0.757.
Using normal model
LBFGS training took [669] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273203
Train loss (w/o reg) on all data: 0.216975
Test loss (w/o reg) on all data: 1.02983
Train acc on all data:  0.92385786802
Test acc on all data:   0.729468599034
Norm of the mean of gradients: 5.25776e-05
Norm of the params: 33.5342
              Random: fixed 228 labels. Loss 1.02983. Accuracy 0.729.
### Flips: 1236, rs: 20, checks: 1030
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179413
Train loss (w/o reg) on all data: 0.139317
Test loss (w/o reg) on all data: 0.879431
Train acc on all data:  0.952139231327
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 3.33954e-05
Norm of the params: 28.3181
     Influence (LOO): fixed 543 labels. Loss 0.87943. Accuracy 0.832.
Using normal model
LBFGS training took [501] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.127652
Train loss (w/o reg) on all data: 0.0733376
Test loss (w/o reg) on all data: 0.975038
Train acc on all data:  0.994923857868
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 1.39829e-05
Norm of the params: 32.9588
                Loss: fixed 509 labels. Loss 0.97504. Accuracy 0.778.
Using normal model
LBFGS training took [654] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263675
Train loss (w/o reg) on all data: 0.208805
Test loss (w/o reg) on all data: 0.934744
Train acc on all data:  0.926033357505
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 3.17763e-05
Norm of the params: 33.127
              Random: fixed 295 labels. Loss 0.93474. Accuracy 0.759.
### Flips: 1236, rs: 20, checks: 1236
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172329
Train loss (w/o reg) on all data: 0.134164
Test loss (w/o reg) on all data: 0.822515
Train acc on all data:  0.952622673435
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 4.60721e-05
Norm of the params: 27.6277
     Influence (LOO): fixed 614 labels. Loss 0.82251. Accuracy 0.849.
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116482
Train loss (w/o reg) on all data: 0.0652928
Test loss (w/o reg) on all data: 0.908625
Train acc on all data:  0.99564902103
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 1.24252e-05
Norm of the params: 31.9966
                Loss: fixed 562 labels. Loss 0.90862. Accuracy 0.801.
Using normal model
LBFGS training took [640] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25521
Train loss (w/o reg) on all data: 0.201106
Test loss (w/o reg) on all data: 0.852031
Train acc on all data:  0.92941745226
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 0.000117175
Norm of the params: 32.8952
              Random: fixed 350 labels. Loss 0.85203. Accuracy 0.769.
Using normal model
LBFGS training took [767] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293909
Train loss (w/o reg) on all data: 0.233554
Test loss (w/o reg) on all data: 1.09833
Train acc on all data:  0.923374425912
Test acc on all data:   0.71690821256
Norm of the mean of gradients: 4.03405e-05
Norm of the params: 34.7433
Flipped loss: 1.09833. Accuracy: 0.717
### Flips: 1236, rs: 21, checks: 206
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.236169
Train loss (w/o reg) on all data: 0.181501
Test loss (w/o reg) on all data: 1.0109
Train acc on all data:  0.942470389171
Test acc on all data:   0.745893719807
Norm of the mean of gradients: 6.89152e-05
Norm of the params: 33.066
     Influence (LOO): fixed 135 labels. Loss 1.01090. Accuracy 0.746.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213023
Train loss (w/o reg) on all data: 0.147752
Test loss (w/o reg) on all data: 1.22408
Train acc on all data:  0.964225284022
Test acc on all data:   0.730434782609
Norm of the mean of gradients: 0.000155394
Norm of the params: 36.1306
                Loss: fixed 174 labels. Loss 1.22408. Accuracy 0.730.
Using normal model
LBFGS training took [633] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.285288
Train loss (w/o reg) on all data: 0.225767
Test loss (w/o reg) on all data: 1.06097
Train acc on all data:  0.927483683829
Test acc on all data:   0.736231884058
Norm of the mean of gradients: 7.89882e-05
Norm of the params: 34.5025
              Random: fixed  64 labels. Loss 1.06097. Accuracy 0.736.
### Flips: 1236, rs: 21, checks: 412
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211343
Train loss (w/o reg) on all data: 0.16179
Test loss (w/o reg) on all data: 0.918393
Train acc on all data:  0.947788252357
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 3.53987e-05
Norm of the params: 31.4812
     Influence (LOO): fixed 261 labels. Loss 0.91839. Accuracy 0.782.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169228
Train loss (w/o reg) on all data: 0.106715
Test loss (w/o reg) on all data: 1.16036
Train acc on all data:  0.98573845782
Test acc on all data:   0.737198067633
Norm of the mean of gradients: 6.27078e-05
Norm of the params: 35.3591
                Loss: fixed 297 labels. Loss 1.16036. Accuracy 0.737.
Using normal model
LBFGS training took [648] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276606
Train loss (w/o reg) on all data: 0.217411
Test loss (w/o reg) on all data: 1.047
Train acc on all data:  0.930867778584
Test acc on all data:   0.742995169082
Norm of the mean of gradients: 0.000108695
Norm of the params: 34.4077
              Random: fixed 128 labels. Loss 1.04700. Accuracy 0.743.
### Flips: 1236, rs: 21, checks: 618
Using normal model
LBFGS training took [410] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199169
Train loss (w/o reg) on all data: 0.152624
Test loss (w/o reg) on all data: 0.853638
Train acc on all data:  0.951414068165
Test acc on all data:   0.8
Norm of the mean of gradients: 3.44598e-05
Norm of the params: 30.511
     Influence (LOO): fixed 361 labels. Loss 0.85364. Accuracy 0.800.
Using normal model
LBFGS training took [574] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.145167
Train loss (w/o reg) on all data: 0.0858772
Test loss (w/o reg) on all data: 1.12124
Train acc on all data:  0.989847715736
Test acc on all data:   0.750724637681
Norm of the mean of gradients: 3.77418e-05
Norm of the params: 34.4353
                Loss: fixed 384 labels. Loss 1.12124. Accuracy 0.751.
Using normal model
LBFGS training took [659] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.269729
Train loss (w/o reg) on all data: 0.211763
Test loss (w/o reg) on all data: 0.96644
Train acc on all data:  0.932559825961
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 9.74268e-05
Norm of the params: 34.0486
              Random: fixed 184 labels. Loss 0.96644. Accuracy 0.761.
### Flips: 1236, rs: 21, checks: 824
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18928
Train loss (w/o reg) on all data: 0.145514
Test loss (w/o reg) on all data: 0.739145
Train acc on all data:  0.952864394489
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 4.00056e-05
Norm of the params: 29.586
     Influence (LOO): fixed 446 labels. Loss 0.73915. Accuracy 0.814.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130912
Train loss (w/o reg) on all data: 0.075231
Test loss (w/o reg) on all data: 1.01253
Train acc on all data:  0.991781484167
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 2.41767e-05
Norm of the params: 33.3708
                Loss: fixed 455 labels. Loss 1.01253. Accuracy 0.785.
Using normal model
LBFGS training took [666] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260049
Train loss (w/o reg) on all data: 0.203602
Test loss (w/o reg) on all data: 0.940021
Train acc on all data:  0.935702199662
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 3.11611e-05
Norm of the params: 33.5999
              Random: fixed 248 labels. Loss 0.94002. Accuracy 0.778.
### Flips: 1236, rs: 21, checks: 1030
Using normal model
LBFGS training took [351] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178955
Train loss (w/o reg) on all data: 0.137496
Test loss (w/o reg) on all data: 0.657741
Train acc on all data:  0.955523326082
Test acc on all data:   0.857004830918
Norm of the mean of gradients: 1.83925e-05
Norm of the params: 28.7953
     Influence (LOO): fixed 536 labels. Loss 0.65774. Accuracy 0.857.
Using normal model
LBFGS training took [470] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115877
Train loss (w/o reg) on all data: 0.0640537
Test loss (w/o reg) on all data: 1.0017
Train acc on all data:  0.993231810491
Test acc on all data:   0.799033816425
Norm of the mean of gradients: 3.98673e-05
Norm of the params: 32.1941
                Loss: fixed 513 labels. Loss 1.00170. Accuracy 0.799.
Using normal model
LBFGS training took [615] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25055
Train loss (w/o reg) on all data: 0.195707
Test loss (w/o reg) on all data: 0.89454
Train acc on all data:  0.937877689147
Test acc on all data:   0.795169082126
Norm of the mean of gradients: 8.36381e-05
Norm of the params: 33.1188
              Random: fixed 305 labels. Loss 0.89454. Accuracy 0.795.
### Flips: 1236, rs: 21, checks: 1236
Using normal model
LBFGS training took [371] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.168592
Train loss (w/o reg) on all data: 0.12958
Test loss (w/o reg) on all data: 0.55514
Train acc on all data:  0.957215373459
Test acc on all data:   0.866666666667
Norm of the mean of gradients: 1.223e-05
Norm of the params: 27.9327
     Influence (LOO): fixed 622 labels. Loss 0.55514. Accuracy 0.867.
Using normal model
LBFGS training took [528] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109349
Train loss (w/o reg) on all data: 0.0595508
Test loss (w/o reg) on all data: 0.964787
Train acc on all data:  0.997341068407
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 9.30398e-06
Norm of the params: 31.5588
                Loss: fixed 562 labels. Loss 0.96479. Accuracy 0.827.
Using normal model
LBFGS training took [657] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241516
Train loss (w/o reg) on all data: 0.187292
Test loss (w/o reg) on all data: 0.841222
Train acc on all data:  0.941503504955
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 6.84598e-05
Norm of the params: 32.9317
              Random: fixed 364 labels. Loss 0.84122. Accuracy 0.803.
Using normal model
LBFGS training took [635] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.286112
Train loss (w/o reg) on all data: 0.226198
Test loss (w/o reg) on all data: 1.03878
Train acc on all data:  0.923132704859
Test acc on all data:   0.72077294686
Norm of the mean of gradients: 0.00014068
Norm of the params: 34.616
Flipped loss: 1.03878. Accuracy: 0.721
### Flips: 1236, rs: 22, checks: 206
Using normal model
LBFGS training took [435] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235556
Train loss (w/o reg) on all data: 0.181356
Test loss (w/o reg) on all data: 0.954235
Train acc on all data:  0.936910804931
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 0.000110637
Norm of the params: 32.9244
     Influence (LOO): fixed 142 labels. Loss 0.95423. Accuracy 0.759.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213596
Train loss (w/o reg) on all data: 0.149281
Test loss (w/o reg) on all data: 1.01333
Train acc on all data:  0.961082910321
Test acc on all data:   0.72270531401
Norm of the mean of gradients: 3.12399e-05
Norm of the params: 35.8651
                Loss: fixed 175 labels. Loss 1.01333. Accuracy 0.723.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.281236
Train loss (w/o reg) on all data: 0.222196
Test loss (w/o reg) on all data: 1.04848
Train acc on all data:  0.923616146966
Test acc on all data:   0.71690821256
Norm of the mean of gradients: 2.80434e-05
Norm of the params: 34.3628
              Random: fixed  58 labels. Loss 1.04848. Accuracy 0.717.
### Flips: 1236, rs: 22, checks: 412
Using normal model
LBFGS training took [448] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216517
Train loss (w/o reg) on all data: 0.167244
Test loss (w/o reg) on all data: 0.745357
Train acc on all data:  0.941745226009
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 3.35883e-05
Norm of the params: 31.3919
     Influence (LOO): fixed 267 labels. Loss 0.74536. Accuracy 0.788.
Using normal model
LBFGS training took [521] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176293
Train loss (w/o reg) on all data: 0.112951
Test loss (w/o reg) on all data: 0.929176
Train acc on all data:  0.982112642011
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 2.84108e-05
Norm of the params: 35.5926
                Loss: fixed 297 labels. Loss 0.92918. Accuracy 0.752.
Using normal model
LBFGS training took [572] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.276259
Train loss (w/o reg) on all data: 0.217932
Test loss (w/o reg) on all data: 0.974214
Train acc on all data:  0.925308194344
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 0.000131786
Norm of the params: 34.1545
              Random: fixed 118 labels. Loss 0.97421. Accuracy 0.738.
### Flips: 1236, rs: 22, checks: 618
Using normal model
LBFGS training took [365] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.206914
Train loss (w/o reg) on all data: 0.160112
Test loss (w/o reg) on all data: 0.68486
Train acc on all data:  0.94367899444
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 3.5947e-05
Norm of the params: 30.5948
     Influence (LOO): fixed 358 labels. Loss 0.68486. Accuracy 0.794.
Using normal model
LBFGS training took [524] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.154024
Train loss (w/o reg) on all data: 0.0936284
Test loss (w/o reg) on all data: 0.867666
Train acc on all data:  0.985980178874
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 3.18378e-05
Norm of the params: 34.755
                Loss: fixed 379 labels. Loss 0.86767. Accuracy 0.769.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266944
Train loss (w/o reg) on all data: 0.209942
Test loss (w/o reg) on all data: 0.890395
Train acc on all data:  0.928934010152
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 5.95984e-05
Norm of the params: 33.7645
              Random: fixed 187 labels. Loss 0.89040. Accuracy 0.757.
### Flips: 1236, rs: 22, checks: 824
Using normal model
LBFGS training took [404] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19338
Train loss (w/o reg) on all data: 0.149459
Test loss (w/o reg) on all data: 0.606516
Train acc on all data:  0.948029973411
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 3.65226e-05
Norm of the params: 29.638
     Influence (LOO): fixed 445 labels. Loss 0.60652. Accuracy 0.827.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1422
Train loss (w/o reg) on all data: 0.0839979
Test loss (w/o reg) on all data: 0.819866
Train acc on all data:  0.989605994682
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 1.68095e-05
Norm of the params: 34.118
                Loss: fixed 440 labels. Loss 0.81987. Accuracy 0.790.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262379
Train loss (w/o reg) on all data: 0.205745
Test loss (w/o reg) on all data: 0.874711
Train acc on all data:  0.933284989123
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 5.24736e-05
Norm of the params: 33.6553
              Random: fixed 241 labels. Loss 0.87471. Accuracy 0.749.
### Flips: 1236, rs: 22, checks: 1030
Using normal model
LBFGS training took [378] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179872
Train loss (w/o reg) on all data: 0.138831
Test loss (w/o reg) on all data: 0.497102
Train acc on all data:  0.948513415518
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 2.15794e-05
Norm of the params: 28.6502
     Influence (LOO): fixed 540 labels. Loss 0.49710. Accuracy 0.843.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.130472
Train loss (w/o reg) on all data: 0.0751256
Test loss (w/o reg) on all data: 0.76752
Train acc on all data:  0.991056321006
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 1.30377e-05
Norm of the params: 33.2704
                Loss: fixed 496 labels. Loss 0.76752. Accuracy 0.808.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252796
Train loss (w/o reg) on all data: 0.197435
Test loss (w/o reg) on all data: 0.771945
Train acc on all data:  0.934735315446
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 2.26026e-05
Norm of the params: 33.275
              Random: fixed 305 labels. Loss 0.77195. Accuracy 0.763.
### Flips: 1236, rs: 22, checks: 1236
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171634
Train loss (w/o reg) on all data: 0.13285
Test loss (w/o reg) on all data: 0.42609
Train acc on all data:  0.951655789219
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 1.57737e-05
Norm of the params: 27.8507
     Influence (LOO): fixed 615 labels. Loss 0.42609. Accuracy 0.877.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115987
Train loss (w/o reg) on all data: 0.0649786
Test loss (w/o reg) on all data: 0.761586
Train acc on all data:  0.993231810491
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 3.56978e-05
Norm of the params: 31.9399
                Loss: fixed 562 labels. Loss 0.76159. Accuracy 0.810.
Using normal model
LBFGS training took [551] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244424
Train loss (w/o reg) on all data: 0.190075
Test loss (w/o reg) on all data: 0.744385
Train acc on all data:  0.938602852308
Test acc on all data:   0.780676328502
Norm of the mean of gradients: 8.24969e-05
Norm of the params: 32.9693
              Random: fixed 363 labels. Loss 0.74438. Accuracy 0.781.
Using normal model
LBFGS training took [645] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.290525
Train loss (w/o reg) on all data: 0.23246
Test loss (w/o reg) on all data: 0.868788
Train acc on all data:  0.917331399565
Test acc on all data:   0.728502415459
Norm of the mean of gradients: 5.96763e-05
Norm of the params: 34.0776
Flipped loss: 0.86879. Accuracy: 0.729
### Flips: 1236, rs: 23, checks: 206
Using normal model
LBFGS training took [433] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.232965
Train loss (w/o reg) on all data: 0.180553
Test loss (w/o reg) on all data: 0.749479
Train acc on all data:  0.939811457578
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 0.000113903
Norm of the params: 32.3767
     Influence (LOO): fixed 146 labels. Loss 0.74948. Accuracy 0.775.
Using normal model
LBFGS training took [526] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.210597
Train loss (w/o reg) on all data: 0.14754
Test loss (w/o reg) on all data: 0.883353
Train acc on all data:  0.962291515591
Test acc on all data:   0.734299516908
Norm of the mean of gradients: 7.51611e-05
Norm of the params: 35.5127
                Loss: fixed 185 labels. Loss 0.88335. Accuracy 0.734.
Using normal model
LBFGS training took [564] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.284056
Train loss (w/o reg) on all data: 0.226671
Test loss (w/o reg) on all data: 0.816372
Train acc on all data:  0.922407541697
Test acc on all data:   0.742028985507
Norm of the mean of gradients: 8.65817e-05
Norm of the params: 33.8779
              Random: fixed  57 labels. Loss 0.81637. Accuracy 0.742.
### Flips: 1236, rs: 23, checks: 412
Using normal model
LBFGS training took [355] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209878
Train loss (w/o reg) on all data: 0.162687
Test loss (w/o reg) on all data: 0.625968
Train acc on all data:  0.946579647087
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 2.83056e-05
Norm of the params: 30.7216
     Influence (LOO): fixed 270 labels. Loss 0.62597. Accuracy 0.808.
Using normal model
LBFGS training took [546] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16844
Train loss (w/o reg) on all data: 0.106883
Test loss (w/o reg) on all data: 0.899423
Train acc on all data:  0.985496736766
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 2.55865e-05
Norm of the params: 35.0875
                Loss: fixed 316 labels. Loss 0.89942. Accuracy 0.753.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.280719
Train loss (w/o reg) on all data: 0.224722
Test loss (w/o reg) on all data: 0.829048
Train acc on all data:  0.922649262751
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 4.7066e-05
Norm of the params: 33.4656
              Random: fixed 112 labels. Loss 0.82905. Accuracy 0.755.
### Flips: 1236, rs: 23, checks: 618
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197977
Train loss (w/o reg) on all data: 0.153497
Test loss (w/o reg) on all data: 0.556074
Train acc on all data:  0.947788252357
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.23078e-05
Norm of the params: 29.826
     Influence (LOO): fixed 365 labels. Loss 0.55607. Accuracy 0.824.
Using normal model
LBFGS training took [531] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.151645
Train loss (w/o reg) on all data: 0.0920651
Test loss (w/o reg) on all data: 0.801191
Train acc on all data:  0.987430505197
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 2.32808e-05
Norm of the params: 34.5196
                Loss: fixed 381 labels. Loss 0.80119. Accuracy 0.764.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271221
Train loss (w/o reg) on all data: 0.215841
Test loss (w/o reg) on all data: 0.769698
Train acc on all data:  0.925791636452
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 4.0258e-05
Norm of the params: 33.2808
              Random: fixed 181 labels. Loss 0.76970. Accuracy 0.775.
### Flips: 1236, rs: 23, checks: 824
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18718
Train loss (w/o reg) on all data: 0.145833
Test loss (w/o reg) on all data: 0.51701
Train acc on all data:  0.950205462896
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 6.60414e-06
Norm of the params: 28.7563
     Influence (LOO): fixed 442 labels. Loss 0.51701. Accuracy 0.840.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139679
Train loss (w/o reg) on all data: 0.0820896
Test loss (w/o reg) on all data: 0.779271
Train acc on all data:  0.991298042059
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 1.40005e-05
Norm of the params: 33.938
                Loss: fixed 444 labels. Loss 0.77927. Accuracy 0.780.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262526
Train loss (w/o reg) on all data: 0.208487
Test loss (w/o reg) on all data: 0.717691
Train acc on all data:  0.929175731206
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 1.90431e-05
Norm of the params: 32.8751
              Random: fixed 251 labels. Loss 0.71769. Accuracy 0.787.
### Flips: 1236, rs: 23, checks: 1030
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176759
Train loss (w/o reg) on all data: 0.138201
Test loss (w/o reg) on all data: 0.435356
Train acc on all data:  0.950688905004
Test acc on all data:   0.860869565217
Norm of the mean of gradients: 7.46572e-06
Norm of the params: 27.7697
     Influence (LOO): fixed 529 labels. Loss 0.43536. Accuracy 0.861.
Using normal model
LBFGS training took [525] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126243
Train loss (w/o reg) on all data: 0.071809
Test loss (w/o reg) on all data: 0.709142
Train acc on all data:  0.992264926275
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 3.80456e-05
Norm of the params: 32.9953
                Loss: fixed 516 labels. Loss 0.70914. Accuracy 0.803.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254967
Train loss (w/o reg) on all data: 0.201385
Test loss (w/o reg) on all data: 0.658878
Train acc on all data:  0.935943920715
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 4.36192e-05
Norm of the params: 32.7357
              Random: fixed 317 labels. Loss 0.65888. Accuracy 0.804.
### Flips: 1236, rs: 23, checks: 1236
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.170541
Train loss (w/o reg) on all data: 0.133863
Test loss (w/o reg) on all data: 0.358606
Train acc on all data:  0.951897510273
Test acc on all data:   0.87729468599
Norm of the mean of gradients: 6.05145e-06
Norm of the params: 27.0841
     Influence (LOO): fixed 591 labels. Loss 0.35861. Accuracy 0.877.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111538
Train loss (w/o reg) on all data: 0.0615266
Test loss (w/o reg) on all data: 0.662632
Train acc on all data:  0.996374184191
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 2.85231e-05
Norm of the params: 31.6263
                Loss: fixed 586 labels. Loss 0.66263. Accuracy 0.814.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243325
Train loss (w/o reg) on all data: 0.190489
Test loss (w/o reg) on all data: 0.601252
Train acc on all data:  0.939811457578
Test acc on all data:   0.817391304348
Norm of the mean of gradients: 3.60237e-05
Norm of the params: 32.5074
              Random: fixed 382 labels. Loss 0.60125. Accuracy 0.817.
Using normal model
LBFGS training took [556] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.300592
Train loss (w/o reg) on all data: 0.241504
Test loss (w/o reg) on all data: 0.8942
Train acc on all data:  0.914189025864
Test acc on all data:   0.736231884058
Norm of the mean of gradients: 8.9816e-05
Norm of the params: 34.3768
Flipped loss: 0.89420. Accuracy: 0.736
### Flips: 1236, rs: 24, checks: 206
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243202
Train loss (w/o reg) on all data: 0.189938
Test loss (w/o reg) on all data: 0.85064
Train acc on all data:  0.934251873338
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 2.81301e-05
Norm of the params: 32.6386
     Influence (LOO): fixed 151 labels. Loss 0.85064. Accuracy 0.767.
Using normal model
LBFGS training took [506] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216192
Train loss (w/o reg) on all data: 0.152246
Test loss (w/o reg) on all data: 0.908114
Train acc on all data:  0.960841189268
Test acc on all data:   0.746859903382
Norm of the mean of gradients: 1.72929e-05
Norm of the params: 35.7621
                Loss: fixed 189 labels. Loss 0.90811. Accuracy 0.747.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.293219
Train loss (w/o reg) on all data: 0.235249
Test loss (w/o reg) on all data: 0.84558
Train acc on all data:  0.918781725888
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 5.64998e-05
Norm of the params: 34.0499
              Random: fixed  61 labels. Loss 0.84558. Accuracy 0.752.
### Flips: 1236, rs: 24, checks: 412
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223953
Train loss (w/o reg) on all data: 0.174758
Test loss (w/o reg) on all data: 0.726057
Train acc on all data:  0.940294899686
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 4.09546e-05
Norm of the params: 31.3672
     Influence (LOO): fixed 262 labels. Loss 0.72606. Accuracy 0.803.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17068
Train loss (w/o reg) on all data: 0.108016
Test loss (w/o reg) on all data: 0.904002
Train acc on all data:  0.984046410442
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 3.87742e-05
Norm of the params: 35.4018
                Loss: fixed 313 labels. Loss 0.90400. Accuracy 0.756.
Using normal model
LBFGS training took [582] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.284027
Train loss (w/o reg) on all data: 0.226243
Test loss (w/o reg) on all data: 0.85241
Train acc on all data:  0.921682378535
Test acc on all data:   0.75845410628
Norm of the mean of gradients: 2.21158e-05
Norm of the params: 33.9953
              Random: fixed 113 labels. Loss 0.85241. Accuracy 0.758.
### Flips: 1236, rs: 24, checks: 618
Using normal model
LBFGS training took [448] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.211993
Train loss (w/o reg) on all data: 0.165259
Test loss (w/o reg) on all data: 0.632886
Train acc on all data:  0.942953831279
Test acc on all data:   0.825120772947
Norm of the mean of gradients: 6.76393e-05
Norm of the params: 30.5726
     Influence (LOO): fixed 348 labels. Loss 0.63289. Accuracy 0.825.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150613
Train loss (w/o reg) on all data: 0.0904423
Test loss (w/o reg) on all data: 0.8662
Train acc on all data:  0.989605994682
Test acc on all data:   0.765217391304
Norm of the mean of gradients: 4.33942e-05
Norm of the params: 34.6903
                Loss: fixed 396 labels. Loss 0.86620. Accuracy 0.765.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273916
Train loss (w/o reg) on all data: 0.216877
Test loss (w/o reg) on all data: 0.843964
Train acc on all data:  0.927241962775
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 5.86771e-05
Norm of the params: 33.7755
              Random: fixed 169 labels. Loss 0.84396. Accuracy 0.761.
### Flips: 1236, rs: 24, checks: 824
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197881
Train loss (w/o reg) on all data: 0.154607
Test loss (w/o reg) on all data: 0.546935
Train acc on all data:  0.946337926033
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 6.45816e-06
Norm of the params: 29.419
     Influence (LOO): fixed 449 labels. Loss 0.54693. Accuracy 0.843.
Using normal model
LBFGS training took [490] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.140881
Train loss (w/o reg) on all data: 0.0825592
Test loss (w/o reg) on all data: 0.770023
Train acc on all data:  0.991298042059
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 1.86401e-05
Norm of the params: 34.153
                Loss: fixed 437 labels. Loss 0.77002. Accuracy 0.789.
Using normal model
LBFGS training took [561] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.270013
Train loss (w/o reg) on all data: 0.214003
Test loss (w/o reg) on all data: 0.805839
Train acc on all data:  0.928450568044
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 3.36354e-05
Norm of the params: 33.4694
              Random: fixed 224 labels. Loss 0.80584. Accuracy 0.771.
### Flips: 1236, rs: 24, checks: 1030
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.188438
Train loss (w/o reg) on all data: 0.14733
Test loss (w/o reg) on all data: 0.505055
Train acc on all data:  0.948029973411
Test acc on all data:   0.854106280193
Norm of the mean of gradients: 7.13249e-05
Norm of the params: 28.6732
     Influence (LOO): fixed 530 labels. Loss 0.50506. Accuracy 0.854.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128408
Train loss (w/o reg) on all data: 0.0732206
Test loss (w/o reg) on all data: 0.733463
Train acc on all data:  0.992990089437
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 1.73317e-05
Norm of the params: 33.2226
                Loss: fixed 489 labels. Loss 0.73346. Accuracy 0.804.
Using normal model
LBFGS training took [554] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261348
Train loss (w/o reg) on all data: 0.206213
Test loss (w/o reg) on all data: 0.755347
Train acc on all data:  0.935702199662
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 2.10421e-05
Norm of the params: 33.2071
              Random: fixed 294 labels. Loss 0.75535. Accuracy 0.783.
### Flips: 1236, rs: 24, checks: 1236
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177154
Train loss (w/o reg) on all data: 0.13846
Test loss (w/o reg) on all data: 0.446904
Train acc on all data:  0.952864394489
Test acc on all data:   0.874396135266
Norm of the mean of gradients: 5.638e-06
Norm of the params: 27.8188
     Influence (LOO): fixed 613 labels. Loss 0.44690. Accuracy 0.874.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118684
Train loss (w/o reg) on all data: 0.0661667
Test loss (w/o reg) on all data: 0.702097
Train acc on all data:  0.993715252599
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 1.31758e-05
Norm of the params: 32.409
                Loss: fixed 554 labels. Loss 0.70210. Accuracy 0.824.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.251014
Train loss (w/o reg) on all data: 0.197423
Test loss (w/o reg) on all data: 0.711435
Train acc on all data:  0.937877689147
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 1.76173e-05
Norm of the params: 32.7389
              Random: fixed 368 labels. Loss 0.71144. Accuracy 0.803.
Using normal model
LBFGS training took [695] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298127
Train loss (w/o reg) on all data: 0.240294
Test loss (w/o reg) on all data: 0.964675
Train acc on all data:  0.912496978487
Test acc on all data:   0.734299516908
Norm of the mean of gradients: 0.000159843
Norm of the params: 34.0095
Flipped loss: 0.96468. Accuracy: 0.734
### Flips: 1236, rs: 25, checks: 206
Using normal model
LBFGS training took [465] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237128
Train loss (w/o reg) on all data: 0.184098
Test loss (w/o reg) on all data: 0.842328
Train acc on all data:  0.934493594392
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 5.03286e-05
Norm of the params: 32.567
     Influence (LOO): fixed 153 labels. Loss 0.84233. Accuracy 0.753.
Using normal model
LBFGS training took [602] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213435
Train loss (w/o reg) on all data: 0.151226
Test loss (w/o reg) on all data: 1.01407
Train acc on all data:  0.961082910321
Test acc on all data:   0.737198067633
Norm of the mean of gradients: 7.47996e-05
Norm of the params: 35.2728
                Loss: fixed 189 labels. Loss 1.01407. Accuracy 0.737.
Using normal model
LBFGS training took [546] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291224
Train loss (w/o reg) on all data: 0.234348
Test loss (w/o reg) on all data: 0.953362
Train acc on all data:  0.914189025864
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 3.29648e-05
Norm of the params: 33.7271
              Random: fixed  59 labels. Loss 0.95336. Accuracy 0.738.
### Flips: 1236, rs: 25, checks: 412
Using normal model
LBFGS training took [387] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215788
Train loss (w/o reg) on all data: 0.167174
Test loss (w/o reg) on all data: 0.656172
Train acc on all data:  0.940053178632
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 1.2438e-05
Norm of the params: 31.1816
     Influence (LOO): fixed 266 labels. Loss 0.65617. Accuracy 0.786.
Using normal model
LBFGS training took [589] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17363
Train loss (w/o reg) on all data: 0.112243
Test loss (w/o reg) on all data: 0.958053
Train acc on all data:  0.981387478849
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 2.65294e-05
Norm of the params: 35.0389
                Loss: fixed 313 labels. Loss 0.95805. Accuracy 0.764.
Using normal model
LBFGS training took [574] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282147
Train loss (w/o reg) on all data: 0.226517
Test loss (w/o reg) on all data: 0.923269
Train acc on all data:  0.918540004834
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 5.16968e-05
Norm of the params: 33.3554
              Random: fixed 129 labels. Loss 0.92327. Accuracy 0.757.
### Flips: 1236, rs: 25, checks: 618
Using normal model
LBFGS training took [359] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202258
Train loss (w/o reg) on all data: 0.156559
Test loss (w/o reg) on all data: 0.583704
Train acc on all data:  0.94488759971
Test acc on all data:   0.805797101449
Norm of the mean of gradients: 8.80035e-06
Norm of the params: 30.2321
     Influence (LOO): fixed 370 labels. Loss 0.58370. Accuracy 0.806.
Using normal model
LBFGS training took [538] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153558
Train loss (w/o reg) on all data: 0.0939406
Test loss (w/o reg) on all data: 0.894583
Train acc on all data:  0.988155668359
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 5.64046e-05
Norm of the params: 34.5305
                Loss: fixed 392 labels. Loss 0.89458. Accuracy 0.779.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275179
Train loss (w/o reg) on all data: 0.220752
Test loss (w/o reg) on all data: 0.885247
Train acc on all data:  0.921440657481
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 0.000148186
Norm of the params: 32.993
              Random: fixed 194 labels. Loss 0.88525. Accuracy 0.757.
### Flips: 1236, rs: 25, checks: 824
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1898
Train loss (w/o reg) on all data: 0.146979
Test loss (w/o reg) on all data: 0.500471
Train acc on all data:  0.949480299734
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 2.27029e-05
Norm of the params: 29.2648
     Influence (LOO): fixed 475 labels. Loss 0.50047. Accuracy 0.827.
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134597
Train loss (w/o reg) on all data: 0.0782417
Test loss (w/o reg) on all data: 0.820093
Train acc on all data:  0.992023205221
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 9.78141e-05
Norm of the params: 33.5725
                Loss: fixed 460 labels. Loss 0.82009. Accuracy 0.798.
Using normal model
LBFGS training took [560] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.264748
Train loss (w/o reg) on all data: 0.211312
Test loss (w/o reg) on all data: 0.787897
Train acc on all data:  0.924824752236
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 2.92376e-05
Norm of the params: 32.6915
              Random: fixed 254 labels. Loss 0.78790. Accuracy 0.779.
### Flips: 1236, rs: 25, checks: 1030
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177577
Train loss (w/o reg) on all data: 0.137771
Test loss (w/o reg) on all data: 0.4307
Train acc on all data:  0.952622673435
Test acc on all data:   0.861835748792
Norm of the mean of gradients: 1.36326e-05
Norm of the params: 28.2157
     Influence (LOO): fixed 577 labels. Loss 0.43070. Accuracy 0.862.
Using normal model
LBFGS training took [442] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121127
Train loss (w/o reg) on all data: 0.0680373
Test loss (w/o reg) on all data: 0.772111
Train acc on all data:  0.993231810491
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 1.20883e-05
Norm of the params: 32.5852
                Loss: fixed 516 labels. Loss 0.77211. Accuracy 0.804.
Using normal model
LBFGS training took [584] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.255542
Train loss (w/o reg) on all data: 0.202434
Test loss (w/o reg) on all data: 0.72415
Train acc on all data:  0.930142615422
Test acc on all data:   0.773913043478
Norm of the mean of gradients: 8.23414e-05
Norm of the params: 32.5906
              Random: fixed 314 labels. Loss 0.72415. Accuracy 0.774.
### Flips: 1236, rs: 25, checks: 1236
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16625
Train loss (w/o reg) on all data: 0.12938
Test loss (w/o reg) on all data: 0.375513
Train acc on all data:  0.95358955765
Test acc on all data:   0.876328502415
Norm of the mean of gradients: 1.99913e-05
Norm of the params: 27.155
     Influence (LOO): fixed 645 labels. Loss 0.37551. Accuracy 0.876.
Using normal model
LBFGS training took [423] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11337
Train loss (w/o reg) on all data: 0.0623905
Test loss (w/o reg) on all data: 0.713038
Train acc on all data:  0.993956973652
Test acc on all data:   0.824154589372
Norm of the mean of gradients: 2.94383e-05
Norm of the params: 31.9309
                Loss: fixed 561 labels. Loss 0.71304. Accuracy 0.824.
Using normal model
LBFGS training took [579] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246272
Train loss (w/o reg) on all data: 0.194306
Test loss (w/o reg) on all data: 0.652148
Train acc on all data:  0.935460478608
Test acc on all data:   0.792270531401
Norm of the mean of gradients: 6.4675e-05
Norm of the params: 32.2383
              Random: fixed 380 labels. Loss 0.65215. Accuracy 0.792.
Using normal model
LBFGS training took [748] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299345
Train loss (w/o reg) on all data: 0.241308
Test loss (w/o reg) on all data: 1.04473
Train acc on all data:  0.91394730481
Test acc on all data:   0.688888888889
Norm of the mean of gradients: 0.00011683
Norm of the params: 34.0696
Flipped loss: 1.04473. Accuracy: 0.689
### Flips: 1236, rs: 26, checks: 206
Using normal model
LBFGS training took [456] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23936
Train loss (w/o reg) on all data: 0.186073
Test loss (w/o reg) on all data: 1.00909
Train acc on all data:  0.935702199662
Test acc on all data:   0.736231884058
Norm of the mean of gradients: 2.66703e-05
Norm of the params: 32.6458
     Influence (LOO): fixed 147 labels. Loss 1.00909. Accuracy 0.736.
Using normal model
LBFGS training took [579] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2225
Train loss (w/o reg) on all data: 0.159739
Test loss (w/o reg) on all data: 1.12087
Train acc on all data:  0.958423978729
Test acc on all data:   0.708212560386
Norm of the mean of gradients: 3.46825e-05
Norm of the params: 35.429
                Loss: fixed 185 labels. Loss 1.12087. Accuracy 0.708.
Using normal model
LBFGS training took [560] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291872
Train loss (w/o reg) on all data: 0.234729
Test loss (w/o reg) on all data: 0.987487
Train acc on all data:  0.918298283781
Test acc on all data:   0.714009661836
Norm of the mean of gradients: 3.6476e-05
Norm of the params: 33.8062
              Random: fixed  62 labels. Loss 0.98749. Accuracy 0.714.
### Flips: 1236, rs: 26, checks: 412
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217
Train loss (w/o reg) on all data: 0.168942
Test loss (w/o reg) on all data: 0.840377
Train acc on all data:  0.938844573362
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 5.40111e-05
Norm of the params: 31.0024
     Influence (LOO): fixed 264 labels. Loss 0.84038. Accuracy 0.782.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185873
Train loss (w/o reg) on all data: 0.122396
Test loss (w/o reg) on all data: 1.10216
Train acc on all data:  0.975586173556
Test acc on all data:   0.721739130435
Norm of the mean of gradients: 2.31932e-05
Norm of the params: 35.6305
                Loss: fixed 298 labels. Loss 1.10216. Accuracy 0.722.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.284304
Train loss (w/o reg) on all data: 0.227497
Test loss (w/o reg) on all data: 0.93773
Train acc on all data:  0.919748610104
Test acc on all data:   0.729468599034
Norm of the mean of gradients: 0.000124505
Norm of the params: 33.7068
              Random: fixed 129 labels. Loss 0.93773. Accuracy 0.729.
### Flips: 1236, rs: 26, checks: 618
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205087
Train loss (w/o reg) on all data: 0.159508
Test loss (w/o reg) on all data: 0.730005
Train acc on all data:  0.942712110225
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 5.21059e-05
Norm of the params: 30.1925
     Influence (LOO): fixed 366 labels. Loss 0.73001. Accuracy 0.796.
Using normal model
LBFGS training took [568] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160558
Train loss (w/o reg) on all data: 0.099613
Test loss (w/o reg) on all data: 0.967138
Train acc on all data:  0.986463620981
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 3.79633e-05
Norm of the params: 34.9127
                Loss: fixed 390 labels. Loss 0.96714. Accuracy 0.756.
Using normal model
LBFGS training took [550] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27626
Train loss (w/o reg) on all data: 0.22033
Test loss (w/o reg) on all data: 0.922732
Train acc on all data:  0.924099589074
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 0.000115637
Norm of the params: 33.4454
              Random: fixed 192 labels. Loss 0.92273. Accuracy 0.750.
### Flips: 1236, rs: 26, checks: 824
Using normal model
LBFGS training took [458] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190713
Train loss (w/o reg) on all data: 0.14836
Test loss (w/o reg) on all data: 0.597245
Train acc on all data:  0.948029973411
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 1.78005e-05
Norm of the params: 29.1043
     Influence (LOO): fixed 462 labels. Loss 0.59724. Accuracy 0.816.
Using normal model
LBFGS training took [496] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.147002
Train loss (w/o reg) on all data: 0.0883751
Test loss (w/o reg) on all data: 0.923546
Train acc on all data:  0.988639110467
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 2.05751e-05
Norm of the params: 34.2423
                Loss: fixed 444 labels. Loss 0.92355. Accuracy 0.772.
Using normal model
LBFGS training took [628] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268753
Train loss (w/o reg) on all data: 0.21358
Test loss (w/o reg) on all data: 0.863426
Train acc on all data:  0.928692289098
Test acc on all data:   0.755555555556
Norm of the mean of gradients: 2.84695e-05
Norm of the params: 33.2184
              Random: fixed 250 labels. Loss 0.86343. Accuracy 0.756.
### Flips: 1236, rs: 26, checks: 1030
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181396
Train loss (w/o reg) on all data: 0.141544
Test loss (w/o reg) on all data: 0.552869
Train acc on all data:  0.948755136572
Test acc on all data:   0.84154589372
Norm of the mean of gradients: 2.82538e-05
Norm of the params: 28.232
     Influence (LOO): fixed 538 labels. Loss 0.55287. Accuracy 0.842.
Using normal model
LBFGS training took [526] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.133523
Train loss (w/o reg) on all data: 0.078037
Test loss (w/o reg) on all data: 0.808656
Train acc on all data:  0.990331157844
Test acc on all data:   0.786473429952
Norm of the mean of gradients: 2.90002e-05
Norm of the params: 33.3123
                Loss: fixed 499 labels. Loss 0.80866. Accuracy 0.786.
Using normal model
LBFGS training took [596] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.258081
Train loss (w/o reg) on all data: 0.203756
Test loss (w/o reg) on all data: 0.825062
Train acc on all data:  0.935943920715
Test acc on all data:   0.766183574879
Norm of the mean of gradients: 7.6581e-05
Norm of the params: 32.9623
              Random: fixed 315 labels. Loss 0.82506. Accuracy 0.766.
### Flips: 1236, rs: 26, checks: 1236
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172241
Train loss (w/o reg) on all data: 0.134421
Test loss (w/o reg) on all data: 0.512073
Train acc on all data:  0.952139231327
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 2.97672e-05
Norm of the params: 27.5027
     Influence (LOO): fixed 619 labels. Loss 0.51207. Accuracy 0.865.
Using normal model
LBFGS training took [390] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119267
Train loss (w/o reg) on all data: 0.0677793
Test loss (w/o reg) on all data: 0.757103
Train acc on all data:  0.992990089437
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 1.28481e-05
Norm of the params: 32.0898
                Loss: fixed 572 labels. Loss 0.75710. Accuracy 0.814.
Using normal model
LBFGS training took [609] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.248834
Train loss (w/o reg) on all data: 0.195996
Test loss (w/o reg) on all data: 0.79265
Train acc on all data:  0.934493594392
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 1.87635e-05
Norm of the params: 32.5079
              Random: fixed 381 labels. Loss 0.79265. Accuracy 0.778.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.308967
Train loss (w/o reg) on all data: 0.252011
Test loss (w/o reg) on all data: 1.0403
Train acc on all data:  0.907662557409
Test acc on all data:   0.696618357488
Norm of the mean of gradients: 6.90624e-05
Norm of the params: 33.7508
Flipped loss: 1.04030. Accuracy: 0.697
### Flips: 1236, rs: 27, checks: 206
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249193
Train loss (w/o reg) on all data: 0.196302
Test loss (w/o reg) on all data: 1.03244
Train acc on all data:  0.925791636452
Test acc on all data:   0.739130434783
Norm of the mean of gradients: 1.72503e-05
Norm of the params: 32.5241
     Influence (LOO): fixed 152 labels. Loss 1.03244. Accuracy 0.739.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.228001
Train loss (w/o reg) on all data: 0.16447
Test loss (w/o reg) on all data: 1.09098
Train acc on all data:  0.954556441866
Test acc on all data:   0.713043478261
Norm of the mean of gradients: 2.13577e-05
Norm of the params: 35.6458
                Loss: fixed 189 labels. Loss 1.09098. Accuracy 0.713.
Using normal model
LBFGS training took [481] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298964
Train loss (w/o reg) on all data: 0.242548
Test loss (w/o reg) on all data: 1.02412
Train acc on all data:  0.914189025864
Test acc on all data:   0.713043478261
Norm of the mean of gradients: 3.64389e-05
Norm of the params: 33.5903
              Random: fixed  62 labels. Loss 1.02412. Accuracy 0.713.
### Flips: 1236, rs: 27, checks: 412
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223818
Train loss (w/o reg) on all data: 0.176172
Test loss (w/o reg) on all data: 0.846829
Train acc on all data:  0.930867778584
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 3.57172e-05
Norm of the params: 30.8695
     Influence (LOO): fixed 277 labels. Loss 0.84683. Accuracy 0.768.
Using normal model
LBFGS training took [509] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182281
Train loss (w/o reg) on all data: 0.119066
Test loss (w/o reg) on all data: 1.04856
Train acc on all data:  0.977761663041
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 2.28061e-05
Norm of the params: 35.5568
                Loss: fixed 311 labels. Loss 1.04856. Accuracy 0.741.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.291092
Train loss (w/o reg) on all data: 0.235577
Test loss (w/o reg) on all data: 0.996828
Train acc on all data:  0.916847957457
Test acc on all data:   0.726570048309
Norm of the mean of gradients: 2.3035e-05
Norm of the params: 33.3209
              Random: fixed 128 labels. Loss 0.99683. Accuracy 0.727.
### Flips: 1236, rs: 27, checks: 618
Using normal model
LBFGS training took [398] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209173
Train loss (w/o reg) on all data: 0.164548
Test loss (w/o reg) on all data: 0.715907
Train acc on all data:  0.937152525985
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 5.08423e-05
Norm of the params: 29.875
     Influence (LOO): fixed 384 labels. Loss 0.71591. Accuracy 0.796.
Using normal model
LBFGS training took [453] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.159744
Train loss (w/o reg) on all data: 0.0993785
Test loss (w/o reg) on all data: 0.968948
Train acc on all data:  0.984771573604
Test acc on all data:   0.745893719807
Norm of the mean of gradients: 2.68315e-05
Norm of the params: 34.7463
                Loss: fixed 393 labels. Loss 0.96895. Accuracy 0.746.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282896
Train loss (w/o reg) on all data: 0.227447
Test loss (w/o reg) on all data: 0.938027
Train acc on all data:  0.921440657481
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 1.46958e-05
Norm of the params: 33.3013
              Random: fixed 182 labels. Loss 0.93803. Accuracy 0.741.
### Flips: 1236, rs: 27, checks: 824
Using normal model
LBFGS training took [350] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.198848
Train loss (w/o reg) on all data: 0.156839
Test loss (w/o reg) on all data: 0.609589
Train acc on all data:  0.938844573362
Test acc on all data:   0.829951690821
Norm of the mean of gradients: 2.23967e-05
Norm of the params: 28.9857
     Influence (LOO): fixed 470 labels. Loss 0.60959. Accuracy 0.830.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143375
Train loss (w/o reg) on all data: 0.0854084
Test loss (w/o reg) on all data: 0.944723
Train acc on all data:  0.989364273628
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 2.14608e-05
Norm of the params: 34.049
                Loss: fixed 454 labels. Loss 0.94472. Accuracy 0.771.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279149
Train loss (w/o reg) on all data: 0.224339
Test loss (w/o reg) on all data: 0.899382
Train acc on all data:  0.926033357505
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 2.44025e-05
Norm of the params: 33.109
              Random: fixed 239 labels. Loss 0.89938. Accuracy 0.763.
### Flips: 1236, rs: 27, checks: 1030
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.189889
Train loss (w/o reg) on all data: 0.150109
Test loss (w/o reg) on all data: 0.558204
Train acc on all data:  0.941503504955
Test acc on all data:   0.83961352657
Norm of the mean of gradients: 4.95801e-05
Norm of the params: 28.2063
     Influence (LOO): fixed 536 labels. Loss 0.55820. Accuracy 0.840.
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13358
Train loss (w/o reg) on all data: 0.077905
Test loss (w/o reg) on all data: 0.878863
Train acc on all data:  0.994682136814
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 1.10913e-05
Norm of the params: 33.3691
                Loss: fixed 499 labels. Loss 0.87886. Accuracy 0.787.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.272344
Train loss (w/o reg) on all data: 0.21847
Test loss (w/o reg) on all data: 0.839624
Train acc on all data:  0.924341310128
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 2.2748e-05
Norm of the params: 32.8252
              Random: fixed 299 labels. Loss 0.83962. Accuracy 0.772.
### Flips: 1236, rs: 27, checks: 1236
Using normal model
LBFGS training took [327] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.181978
Train loss (w/o reg) on all data: 0.144571
Test loss (w/o reg) on all data: 0.486006
Train acc on all data:  0.943195552333
Test acc on all data:   0.863768115942
Norm of the mean of gradients: 1.33862e-05
Norm of the params: 27.3525
     Influence (LOO): fixed 613 labels. Loss 0.48601. Accuracy 0.864.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11999
Train loss (w/o reg) on all data: 0.0679462
Test loss (w/o reg) on all data: 0.845973
Train acc on all data:  0.996857626299
Test acc on all data:   0.812560386473
Norm of the mean of gradients: 2.08023e-05
Norm of the params: 32.2627
                Loss: fixed 562 labels. Loss 0.84597. Accuracy 0.813.
Using normal model
LBFGS training took [471] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261186
Train loss (w/o reg) on all data: 0.207801
Test loss (w/o reg) on all data: 0.722643
Train acc on all data:  0.932801547015
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 9.04783e-05
Norm of the params: 32.6756
              Random: fixed 369 labels. Loss 0.72264. Accuracy 0.789.
Using normal model
LBFGS training took [635] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304692
Train loss (w/o reg) on all data: 0.245957
Test loss (w/o reg) on all data: 1.31815
Train acc on all data:  0.90838772057
Test acc on all data:   0.711111111111
Norm of the mean of gradients: 3.31139e-05
Norm of the params: 34.2739
Flipped loss: 1.31815. Accuracy: 0.711
### Flips: 1236, rs: 28, checks: 206
Using normal model
LBFGS training took [392] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241942
Train loss (w/o reg) on all data: 0.188329
Test loss (w/o reg) on all data: 1.1681
Train acc on all data:  0.937635968093
Test acc on all data:   0.739130434783
Norm of the mean of gradients: 3.87744e-05
Norm of the params: 32.7452
     Influence (LOO): fixed 161 labels. Loss 1.16810. Accuracy 0.739.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21985
Train loss (w/o reg) on all data: 0.156227
Test loss (w/o reg) on all data: 1.37385
Train acc on all data:  0.957457094513
Test acc on all data:   0.715942028986
Norm of the mean of gradients: 2.51562e-05
Norm of the params: 35.6717
                Loss: fixed 189 labels. Loss 1.37385. Accuracy 0.716.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.296879
Train loss (w/o reg) on all data: 0.238441
Test loss (w/o reg) on all data: 1.30352
Train acc on all data:  0.91515591008
Test acc on all data:   0.728502415459
Norm of the mean of gradients: 8.86969e-05
Norm of the params: 34.1872
              Random: fixed  66 labels. Loss 1.30352. Accuracy 0.729.
### Flips: 1236, rs: 28, checks: 412
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.217947
Train loss (w/o reg) on all data: 0.168338
Test loss (w/o reg) on all data: 0.962828
Train acc on all data:  0.943920715494
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 8.47334e-05
Norm of the params: 31.4989
     Influence (LOO): fixed 281 labels. Loss 0.96283. Accuracy 0.780.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.178014
Train loss (w/o reg) on all data: 0.114709
Test loss (w/o reg) on all data: 1.29724
Train acc on all data:  0.98017887358
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 5.70552e-05
Norm of the params: 35.5824
                Loss: fixed 309 labels. Loss 1.29724. Accuracy 0.735.
Using normal model
LBFGS training took [523] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.289577
Train loss (w/o reg) on all data: 0.231625
Test loss (w/o reg) on all data: 1.22412
Train acc on all data:  0.919023446942
Test acc on all data:   0.742028985507
Norm of the mean of gradients: 7.30294e-05
Norm of the params: 34.0449
              Random: fixed 124 labels. Loss 1.22412. Accuracy 0.742.
### Flips: 1236, rs: 28, checks: 618
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203731
Train loss (w/o reg) on all data: 0.157166
Test loss (w/o reg) on all data: 0.840206
Train acc on all data:  0.948513415518
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 1.24371e-05
Norm of the params: 30.5173
     Influence (LOO): fixed 389 labels. Loss 0.84021. Accuracy 0.814.
Using normal model
LBFGS training took [499] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152246
Train loss (w/o reg) on all data: 0.0913706
Test loss (w/o reg) on all data: 1.24836
Train acc on all data:  0.989364273628
Test acc on all data:   0.751690821256
Norm of the mean of gradients: 1.85085e-05
Norm of the params: 34.8928
                Loss: fixed 395 labels. Loss 1.24836. Accuracy 0.752.
Using normal model
LBFGS training took [572] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282164
Train loss (w/o reg) on all data: 0.225614
Test loss (w/o reg) on all data: 1.16486
Train acc on all data:  0.921682378535
Test acc on all data:   0.736231884058
Norm of the mean of gradients: 3.74257e-05
Norm of the params: 33.6305
              Random: fixed 183 labels. Loss 1.16486. Accuracy 0.736.
### Flips: 1236, rs: 28, checks: 824
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192399
Train loss (w/o reg) on all data: 0.148197
Test loss (w/o reg) on all data: 0.669905
Train acc on all data:  0.949722020788
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 3.57441e-05
Norm of the params: 29.7328
     Influence (LOO): fixed 479 labels. Loss 0.66990. Accuracy 0.837.
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.137251
Train loss (w/o reg) on all data: 0.0795348
Test loss (w/o reg) on all data: 1.17885
Train acc on all data:  0.991298042059
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 2.33297e-05
Norm of the params: 33.9753
                Loss: fixed 460 labels. Loss 1.17885. Accuracy 0.769.
Using normal model
LBFGS training took [469] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275832
Train loss (w/o reg) on all data: 0.220008
Test loss (w/o reg) on all data: 1.09985
Train acc on all data:  0.923616146966
Test acc on all data:   0.753623188406
Norm of the mean of gradients: 0.000171011
Norm of the params: 33.414
              Random: fixed 248 labels. Loss 1.09985. Accuracy 0.754.
### Flips: 1236, rs: 28, checks: 1030
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183344
Train loss (w/o reg) on all data: 0.141438
Test loss (w/o reg) on all data: 0.568381
Train acc on all data:  0.952139231327
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 9.44943e-06
Norm of the params: 28.9504
     Influence (LOO): fixed 558 labels. Loss 0.56838. Accuracy 0.852.
Using normal model
LBFGS training took [417] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125217
Train loss (w/o reg) on all data: 0.0708682
Test loss (w/o reg) on all data: 1.07068
Train acc on all data:  0.991539763113
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 3.36343e-05
Norm of the params: 32.9695
                Loss: fixed 507 labels. Loss 1.07068. Accuracy 0.772.
Using normal model
LBFGS training took [479] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26907
Train loss (w/o reg) on all data: 0.213678
Test loss (w/o reg) on all data: 0.999491
Train acc on all data:  0.928208846991
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 2.20889e-05
Norm of the params: 33.2842
              Random: fixed 302 labels. Loss 0.99949. Accuracy 0.763.
### Flips: 1236, rs: 28, checks: 1236
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174118
Train loss (w/o reg) on all data: 0.134505
Test loss (w/o reg) on all data: 0.507892
Train acc on all data:  0.95358955765
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 4.77652e-05
Norm of the params: 28.147
     Influence (LOO): fixed 633 labels. Loss 0.50789. Accuracy 0.868.
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11349
Train loss (w/o reg) on all data: 0.062551
Test loss (w/o reg) on all data: 1.04444
Train acc on all data:  0.997582789461
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 1.32652e-05
Norm of the params: 31.9184
                Loss: fixed 564 labels. Loss 1.04444. Accuracy 0.789.
Using normal model
LBFGS training took [474] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260214
Train loss (w/o reg) on all data: 0.206466
Test loss (w/o reg) on all data: 0.938673
Train acc on all data:  0.932076383853
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 5.94524e-05
Norm of the params: 32.7866
              Random: fixed 361 labels. Loss 0.93867. Accuracy 0.778.
Using normal model
LBFGS training took [737] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.292708
Train loss (w/o reg) on all data: 0.232274
Test loss (w/o reg) on all data: 1.05638
Train acc on all data:  0.919023446942
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 6.04539e-05
Norm of the params: 34.7661
Flipped loss: 1.05638. Accuracy: 0.735
### Flips: 1236, rs: 29, checks: 206
Using normal model
LBFGS training took [484] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.233167
Train loss (w/o reg) on all data: 0.179308
Test loss (w/o reg) on all data: 0.895644
Train acc on all data:  0.941503504955
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 2.83864e-05
Norm of the params: 32.8204
     Influence (LOO): fixed 157 labels. Loss 0.89564. Accuracy 0.784.
Using normal model
LBFGS training took [607] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214799
Train loss (w/o reg) on all data: 0.149653
Test loss (w/o reg) on all data: 1.13344
Train acc on all data:  0.963741841914
Test acc on all data:   0.742028985507
Norm of the mean of gradients: 2.51819e-05
Norm of the params: 36.0961
                Loss: fixed 189 labels. Loss 1.13344. Accuracy 0.742.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.285332
Train loss (w/o reg) on all data: 0.225657
Test loss (w/o reg) on all data: 1.01012
Train acc on all data:  0.92385786802
Test acc on all data:   0.747826086957
Norm of the mean of gradients: 7.77789e-05
Norm of the params: 34.5472
              Random: fixed  65 labels. Loss 1.01012. Accuracy 0.748.
### Flips: 1236, rs: 29, checks: 412
Using normal model
LBFGS training took [422] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20941
Train loss (w/o reg) on all data: 0.160879
Test loss (w/o reg) on all data: 0.880161
Train acc on all data:  0.946337926033
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 8.50868e-05
Norm of the params: 31.1548
     Influence (LOO): fixed 273 labels. Loss 0.88016. Accuracy 0.808.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174604
Train loss (w/o reg) on all data: 0.111997
Test loss (w/o reg) on all data: 1.1969
Train acc on all data:  0.981387478849
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 5.22238e-05
Norm of the params: 35.3855
                Loss: fixed 309 labels. Loss 1.19690. Accuracy 0.755.
Using normal model
LBFGS training took [612] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275788
Train loss (w/o reg) on all data: 0.216826
Test loss (w/o reg) on all data: 0.93615
Train acc on all data:  0.927725404883
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 0.000111486
Norm of the params: 34.3402
              Random: fixed 137 labels. Loss 0.93615. Accuracy 0.764.
### Flips: 1236, rs: 29, checks: 618
Using normal model
LBFGS training took [388] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195604
Train loss (w/o reg) on all data: 0.150146
Test loss (w/o reg) on all data: 0.736807
Train acc on all data:  0.948271694465
Test acc on all data:   0.815458937198
Norm of the mean of gradients: 3.96876e-05
Norm of the params: 30.1523
     Influence (LOO): fixed 377 labels. Loss 0.73681. Accuracy 0.815.
Using normal model
LBFGS training took [488] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.149725
Train loss (w/o reg) on all data: 0.0899588
Test loss (w/o reg) on all data: 1.10668
Train acc on all data:  0.989364273628
Test acc on all data:   0.772946859903
Norm of the mean of gradients: 9.55454e-05
Norm of the params: 34.5735
                Loss: fixed 387 labels. Loss 1.10668. Accuracy 0.773.
Using normal model
LBFGS training took [618] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.267833
Train loss (w/o reg) on all data: 0.210006
Test loss (w/o reg) on all data: 0.963355
Train acc on all data:  0.92941745226
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 4.786e-05
Norm of the params: 34.0077
              Random: fixed 190 labels. Loss 0.96336. Accuracy 0.772.
### Flips: 1236, rs: 29, checks: 824
Using normal model
LBFGS training took [412] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185709
Train loss (w/o reg) on all data: 0.1428
Test loss (w/o reg) on all data: 0.602757
Train acc on all data:  0.950205462896
Test acc on all data:   0.842512077295
Norm of the mean of gradients: 3.61014e-05
Norm of the params: 29.2948
     Influence (LOO): fixed 463 labels. Loss 0.60276. Accuracy 0.843.
Using normal model
LBFGS training took [439] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136299
Train loss (w/o reg) on all data: 0.0792307
Test loss (w/o reg) on all data: 1.0657
Train acc on all data:  0.991056321006
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 6.13214e-05
Norm of the params: 33.7841
                Loss: fixed 445 labels. Loss 1.06570. Accuracy 0.783.
Using normal model
LBFGS training took [710] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.259284
Train loss (w/o reg) on all data: 0.202466
Test loss (w/o reg) on all data: 0.908779
Train acc on all data:  0.932076383853
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 7.7045e-05
Norm of the params: 33.71
              Random: fixed 242 labels. Loss 0.90878. Accuracy 0.779.
### Flips: 1236, rs: 29, checks: 1030
Using normal model
LBFGS training took [376] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176575
Train loss (w/o reg) on all data: 0.136577
Test loss (w/o reg) on all data: 0.528367
Train acc on all data:  0.953831278704
Test acc on all data:   0.862801932367
Norm of the mean of gradients: 1.97163e-05
Norm of the params: 28.2836
     Influence (LOO): fixed 550 labels. Loss 0.52837. Accuracy 0.863.
Using normal model
LBFGS training took [463] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125613
Train loss (w/o reg) on all data: 0.0717364
Test loss (w/o reg) on all data: 1.02612
Train acc on all data:  0.992506647329
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 2.44308e-05
Norm of the params: 32.8258
                Loss: fixed 500 labels. Loss 1.02612. Accuracy 0.794.
Using normal model
LBFGS training took [648] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246373
Train loss (w/o reg) on all data: 0.190665
Test loss (w/o reg) on all data: 0.863133
Train acc on all data:  0.938361131255
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 2.52927e-05
Norm of the params: 33.3789
              Random: fixed 312 labels. Loss 0.86313. Accuracy 0.789.
### Flips: 1236, rs: 29, checks: 1236
Using normal model
LBFGS training took [344] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169381
Train loss (w/o reg) on all data: 0.131625
Test loss (w/o reg) on all data: 0.456568
Train acc on all data:  0.954072999758
Test acc on all data:   0.867632850242
Norm of the mean of gradients: 1.42086e-05
Norm of the params: 27.4794
     Influence (LOO): fixed 619 labels. Loss 0.45657. Accuracy 0.868.
Using normal model
LBFGS training took [396] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115081
Train loss (w/o reg) on all data: 0.0644794
Test loss (w/o reg) on all data: 0.998208
Train acc on all data:  0.992990089437
Test acc on all data:   0.807729468599
Norm of the mean of gradients: 8.30559e-06
Norm of the params: 31.8124
                Loss: fixed 563 labels. Loss 0.99821. Accuracy 0.808.
Using normal model
LBFGS training took [601] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.237129
Train loss (w/o reg) on all data: 0.183351
Test loss (w/o reg) on all data: 0.776844
Train acc on all data:  0.941745226009
Test acc on all data:   0.802898550725
Norm of the mean of gradients: 4.42468e-05
Norm of the params: 32.7959
              Random: fixed 379 labels. Loss 0.77684. Accuracy 0.803.
Using normal model
LBFGS training took [632] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299967
Train loss (w/o reg) on all data: 0.239739
Test loss (w/o reg) on all data: 1.3069
Train acc on all data:  0.912738699541
Test acc on all data:   0.689855072464
Norm of the mean of gradients: 6.1506e-05
Norm of the params: 34.7068
Flipped loss: 1.30690. Accuracy: 0.690
### Flips: 1236, rs: 30, checks: 206
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.241736
Train loss (w/o reg) on all data: 0.187435
Test loss (w/o reg) on all data: 1.26456
Train acc on all data:  0.936669083877
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 4.41398e-05
Norm of the params: 32.9547
     Influence (LOO): fixed 152 labels. Loss 1.26456. Accuracy 0.733.
Using normal model
LBFGS training took [575] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22161
Train loss (w/o reg) on all data: 0.156329
Test loss (w/o reg) on all data: 1.34569
Train acc on all data:  0.961808073483
Test acc on all data:   0.702415458937
Norm of the mean of gradients: 9.3064e-05
Norm of the params: 36.1333
                Loss: fixed 191 labels. Loss 1.34569. Accuracy 0.702.
Using normal model
LBFGS training took [571] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.294125
Train loss (w/o reg) on all data: 0.235119
Test loss (w/o reg) on all data: 1.13088
Train acc on all data:  0.917814841673
Test acc on all data:   0.706280193237
Norm of the mean of gradients: 4.15641e-05
Norm of the params: 34.3529
              Random: fixed  70 labels. Loss 1.13088. Accuracy 0.706.
### Flips: 1236, rs: 30, checks: 412
Using normal model
LBFGS training took [522] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215499
Train loss (w/o reg) on all data: 0.166959
Test loss (w/o reg) on all data: 1.17525
Train acc on all data:  0.944162436548
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 4.59188e-05
Norm of the params: 31.1578
     Influence (LOO): fixed 283 labels. Loss 1.17525. Accuracy 0.749.
Using normal model
LBFGS training took [541] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.177886
Train loss (w/o reg) on all data: 0.113772
Test loss (w/o reg) on all data: 1.38538
Train acc on all data:  0.982596084119
Test acc on all data:   0.726570048309
Norm of the mean of gradients: 8.5091e-05
Norm of the params: 35.8089
                Loss: fixed 321 labels. Loss 1.38538. Accuracy 0.727.
Using normal model
LBFGS training took [627] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.286421
Train loss (w/o reg) on all data: 0.227366
Test loss (w/o reg) on all data: 1.07754
Train acc on all data:  0.921440657481
Test acc on all data:   0.727536231884
Norm of the mean of gradients: 2.80882e-05
Norm of the params: 34.367
              Random: fixed 129 labels. Loss 1.07754. Accuracy 0.728.
### Flips: 1236, rs: 30, checks: 618
Using normal model
LBFGS training took [408] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203147
Train loss (w/o reg) on all data: 0.158012
Test loss (w/o reg) on all data: 1.18865
Train acc on all data:  0.945371041818
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 1.19807e-05
Norm of the params: 30.045
     Influence (LOO): fixed 376 labels. Loss 1.18865. Accuracy 0.779.
Using normal model
LBFGS training took [512] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155379
Train loss (w/o reg) on all data: 0.0946608
Test loss (w/o reg) on all data: 1.48196
Train acc on all data:  0.989605994682
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 1.82363e-05
Norm of the params: 34.8478
                Loss: fixed 398 labels. Loss 1.48196. Accuracy 0.741.
Using normal model
LBFGS training took [611] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279907
Train loss (w/o reg) on all data: 0.222124
Test loss (w/o reg) on all data: 1.07685
Train acc on all data:  0.925308194344
Test acc on all data:   0.725603864734
Norm of the mean of gradients: 2.81759e-05
Norm of the params: 33.995
              Random: fixed 187 labels. Loss 1.07685. Accuracy 0.726.
### Flips: 1236, rs: 30, checks: 824
Using normal model
LBFGS training took [426] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192057
Train loss (w/o reg) on all data: 0.149855
Test loss (w/o reg) on all data: 1.05946
Train acc on all data:  0.947304810249
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 1.24669e-05
Norm of the params: 29.0527
     Influence (LOO): fixed 461 labels. Loss 1.05946. Accuracy 0.807.
Using normal model
LBFGS training took [539] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139871
Train loss (w/o reg) on all data: 0.0820633
Test loss (w/o reg) on all data: 1.38679
Train acc on all data:  0.991781484167
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 2.1032e-05
Norm of the params: 34.0023
                Loss: fixed 465 labels. Loss 1.38679. Accuracy 0.769.
Using normal model
LBFGS training took [591] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273045
Train loss (w/o reg) on all data: 0.216176
Test loss (w/o reg) on all data: 1.01174
Train acc on all data:  0.925791636452
Test acc on all data:   0.739130434783
Norm of the mean of gradients: 0.00011426
Norm of the params: 33.7249
              Random: fixed 242 labels. Loss 1.01174. Accuracy 0.739.
### Flips: 1236, rs: 30, checks: 1030
Using normal model
LBFGS training took [366] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183881
Train loss (w/o reg) on all data: 0.143737
Test loss (w/o reg) on all data: 0.886358
Train acc on all data:  0.948755136572
Test acc on all data:   0.836714975845
Norm of the mean of gradients: 2.05443e-05
Norm of the params: 28.3351
     Influence (LOO): fixed 544 labels. Loss 0.88636. Accuracy 0.837.
Using normal model
LBFGS training took [452] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12642
Train loss (w/o reg) on all data: 0.071603
Test loss (w/o reg) on all data: 1.35451
Train acc on all data:  0.991539763113
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 2.1967e-05
Norm of the params: 33.1109
                Loss: fixed 529 labels. Loss 1.35451. Accuracy 0.790.
Using normal model
LBFGS training took [584] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260648
Train loss (w/o reg) on all data: 0.204761
Test loss (w/o reg) on all data: 0.987008
Train acc on all data:  0.932318104907
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 3.21084e-05
Norm of the params: 33.4326
              Random: fixed 303 labels. Loss 0.98701. Accuracy 0.763.
### Flips: 1236, rs: 30, checks: 1236
Using normal model
LBFGS training took [370] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175583
Train loss (w/o reg) on all data: 0.137627
Test loss (w/o reg) on all data: 0.789049
Train acc on all data:  0.949963741842
Test acc on all data:   0.845410628019
Norm of the mean of gradients: 5.92943e-05
Norm of the params: 27.5522
     Influence (LOO): fixed 625 labels. Loss 0.78905. Accuracy 0.845.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116029
Train loss (w/o reg) on all data: 0.0639194
Test loss (w/o reg) on all data: 1.22312
Train acc on all data:  0.996857626299
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 4.6717e-05
Norm of the params: 32.2829
                Loss: fixed 583 labels. Loss 1.22312. Accuracy 0.807.
Using normal model
LBFGS training took [591] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249502
Train loss (w/o reg) on all data: 0.194716
Test loss (w/o reg) on all data: 0.936933
Train acc on all data:  0.936427362823
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 3.15559e-05
Norm of the params: 33.1016
              Random: fixed 361 labels. Loss 0.93693. Accuracy 0.767.
Using normal model
LBFGS training took [636] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.30174
Train loss (w/o reg) on all data: 0.243192
Test loss (w/o reg) on all data: 0.904876
Train acc on all data:  0.912496978487
Test acc on all data:   0.719806763285
Norm of the mean of gradients: 6.62807e-05
Norm of the params: 34.2192
Flipped loss: 0.90488. Accuracy: 0.720
### Flips: 1236, rs: 31, checks: 206
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.244606
Train loss (w/o reg) on all data: 0.191381
Test loss (w/o reg) on all data: 0.829765
Train acc on all data:  0.934010152284
Test acc on all data:   0.765217391304
Norm of the mean of gradients: 1.4544e-05
Norm of the params: 32.6267
     Influence (LOO): fixed 147 labels. Loss 0.82976. Accuracy 0.765.
Using normal model
LBFGS training took [559] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219528
Train loss (w/o reg) on all data: 0.155517
Test loss (w/o reg) on all data: 0.893401
Train acc on all data:  0.956731931351
Test acc on all data:   0.746859903382
Norm of the mean of gradients: 2.64269e-05
Norm of the params: 35.7802
                Loss: fixed 186 labels. Loss 0.89340. Accuracy 0.747.
Using normal model
LBFGS training took [513] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.294267
Train loss (w/o reg) on all data: 0.236479
Test loss (w/o reg) on all data: 0.832799
Train acc on all data:  0.916847957457
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 6.51576e-05
Norm of the params: 33.9962
              Random: fixed  79 labels. Loss 0.83280. Accuracy 0.741.
### Flips: 1236, rs: 31, checks: 412
Using normal model
LBFGS training took [384] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219707
Train loss (w/o reg) on all data: 0.170929
Test loss (w/o reg) on all data: 0.89738
Train acc on all data:  0.942470389171
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 7.12423e-05
Norm of the params: 31.2342
     Influence (LOO): fixed 269 labels. Loss 0.89738. Accuracy 0.785.
Using normal model
LBFGS training took [460] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180575
Train loss (w/o reg) on all data: 0.115712
Test loss (w/o reg) on all data: 0.889324
Train acc on all data:  0.979937152526
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 2.47229e-05
Norm of the params: 36.0176
                Loss: fixed 311 labels. Loss 0.88932. Accuracy 0.768.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.286212
Train loss (w/o reg) on all data: 0.229492
Test loss (w/o reg) on all data: 0.905751
Train acc on all data:  0.921440657481
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 0.000139303
Norm of the params: 33.681
              Random: fixed 141 labels. Loss 0.90575. Accuracy 0.757.
### Flips: 1236, rs: 31, checks: 618
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.205773
Train loss (w/o reg) on all data: 0.159665
Test loss (w/o reg) on all data: 0.680411
Train acc on all data:  0.945612762872
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 2.07523e-05
Norm of the params: 30.3672
     Influence (LOO): fixed 364 labels. Loss 0.68041. Accuracy 0.812.
Using normal model
LBFGS training took [510] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.155708
Train loss (w/o reg) on all data: 0.0939873
Test loss (w/o reg) on all data: 0.966559
Train acc on all data:  0.987913947305
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 6.39882e-05
Norm of the params: 35.1342
                Loss: fixed 393 labels. Loss 0.96656. Accuracy 0.771.
Using normal model
LBFGS training took [571] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.278268
Train loss (w/o reg) on all data: 0.222394
Test loss (w/o reg) on all data: 0.771808
Train acc on all data:  0.922890983805
Test acc on all data:   0.767149758454
Norm of the mean of gradients: 3.83128e-05
Norm of the params: 33.4286
              Random: fixed 197 labels. Loss 0.77181. Accuracy 0.767.
### Flips: 1236, rs: 31, checks: 824
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196052
Train loss (w/o reg) on all data: 0.152222
Test loss (w/o reg) on all data: 0.546258
Train acc on all data:  0.945854483926
Test acc on all data:   0.830917874396
Norm of the mean of gradients: 3.97895e-05
Norm of the params: 29.6074
     Influence (LOO): fixed 446 labels. Loss 0.54626. Accuracy 0.831.
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139803
Train loss (w/o reg) on all data: 0.0809042
Test loss (w/o reg) on all data: 0.907302
Train acc on all data:  0.994923857868
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 2.27881e-05
Norm of the params: 34.3217
                Loss: fixed 453 labels. Loss 0.90730. Accuracy 0.788.
Using normal model
LBFGS training took [581] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.270387
Train loss (w/o reg) on all data: 0.215875
Test loss (w/o reg) on all data: 0.71925
Train acc on all data:  0.927000241721
Test acc on all data:   0.773913043478
Norm of the mean of gradients: 4.19255e-05
Norm of the params: 33.0188
              Random: fixed 266 labels. Loss 0.71925. Accuracy 0.774.
### Flips: 1236, rs: 31, checks: 1030
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.186997
Train loss (w/o reg) on all data: 0.14565
Test loss (w/o reg) on all data: 0.558106
Train acc on all data:  0.948996857626
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 1.43718e-05
Norm of the params: 28.7564
     Influence (LOO): fixed 534 labels. Loss 0.55811. Accuracy 0.860.
Using normal model
LBFGS training took [483] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126756
Train loss (w/o reg) on all data: 0.0713815
Test loss (w/o reg) on all data: 0.853867
Train acc on all data:  0.996374184191
Test acc on all data:   0.804830917874
Norm of the mean of gradients: 1.53017e-05
Norm of the params: 33.2789
                Loss: fixed 511 labels. Loss 0.85387. Accuracy 0.805.
Using normal model
LBFGS training took [582] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.260197
Train loss (w/o reg) on all data: 0.206277
Test loss (w/o reg) on all data: 0.680704
Train acc on all data:  0.932801547015
Test acc on all data:   0.777777777778
Norm of the mean of gradients: 2.69295e-05
Norm of the params: 32.8392
              Random: fixed 335 labels. Loss 0.68070. Accuracy 0.778.
### Flips: 1236, rs: 31, checks: 1236
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176751
Train loss (w/o reg) on all data: 0.137974
Test loss (w/o reg) on all data: 0.798299
Train acc on all data:  0.950930626058
Test acc on all data:   0.875362318841
Norm of the mean of gradients: 5.96526e-06
Norm of the params: 27.8486
     Influence (LOO): fixed 625 labels. Loss 0.79830. Accuracy 0.875.
Using normal model
LBFGS training took [403] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116542
Train loss (w/o reg) on all data: 0.063926
Test loss (w/o reg) on all data: 0.803492
Train acc on all data:  0.997099347353
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 3.48394e-05
Norm of the params: 32.4395
                Loss: fixed 565 labels. Loss 0.80349. Accuracy 0.818.
Using normal model
LBFGS training took [542] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252199
Train loss (w/o reg) on all data: 0.199472
Test loss (w/o reg) on all data: 0.650106
Train acc on all data:  0.934735315446
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 2.46091e-05
Norm of the params: 32.4737
              Random: fixed 398 labels. Loss 0.65011. Accuracy 0.789.
Using normal model
LBFGS training took [652] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.304017
Train loss (w/o reg) on all data: 0.2464
Test loss (w/o reg) on all data: 0.873836
Train acc on all data:  0.907904278463
Test acc on all data:   0.713043478261
Norm of the mean of gradients: 7.0567e-05
Norm of the params: 33.9462
Flipped loss: 0.87384. Accuracy: 0.713
### Flips: 1236, rs: 32, checks: 206
Using normal model
LBFGS training took [468] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.243299
Train loss (w/o reg) on all data: 0.191366
Test loss (w/o reg) on all data: 0.855092
Train acc on all data:  0.933043268069
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 3.53319e-05
Norm of the params: 32.2283
     Influence (LOO): fixed 155 labels. Loss 0.85509. Accuracy 0.769.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221227
Train loss (w/o reg) on all data: 0.15915
Test loss (w/o reg) on all data: 0.923188
Train acc on all data:  0.95600676819
Test acc on all data:   0.730434782609
Norm of the mean of gradients: 0.000115122
Norm of the params: 35.2356
                Loss: fixed 190 labels. Loss 0.92319. Accuracy 0.730.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298162
Train loss (w/o reg) on all data: 0.241167
Test loss (w/o reg) on all data: 0.8644
Train acc on all data:  0.910079767948
Test acc on all data:   0.725603864734
Norm of the mean of gradients: 3.20887e-05
Norm of the params: 33.7624
              Random: fixed  50 labels. Loss 0.86440. Accuracy 0.726.
### Flips: 1236, rs: 32, checks: 412
Using normal model
LBFGS training took [462] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2214
Train loss (w/o reg) on all data: 0.174187
Test loss (w/o reg) on all data: 0.698877
Train acc on all data:  0.938844573362
Test acc on all data:   0.789371980676
Norm of the mean of gradients: 6.28087e-05
Norm of the params: 30.7288
     Influence (LOO): fixed 287 labels. Loss 0.69888. Accuracy 0.789.
Using normal model
LBFGS training took [617] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.171383
Train loss (w/o reg) on all data: 0.109467
Test loss (w/o reg) on all data: 0.838068
Train acc on all data:  0.982112642011
Test acc on all data:   0.765217391304
Norm of the mean of gradients: 9.25816e-05
Norm of the params: 35.1897
                Loss: fixed 323 labels. Loss 0.83807. Accuracy 0.765.
Using normal model
LBFGS training took [652] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.289917
Train loss (w/o reg) on all data: 0.233872
Test loss (w/o reg) on all data: 0.81291
Train acc on all data:  0.917331399565
Test acc on all data:   0.734299516908
Norm of the mean of gradients: 6.01065e-05
Norm of the params: 33.4799
              Random: fixed 112 labels. Loss 0.81291. Accuracy 0.734.
### Flips: 1236, rs: 32, checks: 618
Using normal model
LBFGS training took [401] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20747
Train loss (w/o reg) on all data: 0.162946
Test loss (w/o reg) on all data: 0.537234
Train acc on all data:  0.943195552333
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 1.387e-05
Norm of the params: 29.8409
     Influence (LOO): fixed 381 labels. Loss 0.53723. Accuracy 0.814.
Using normal model
LBFGS training took [555] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.150752
Train loss (w/o reg) on all data: 0.0907121
Test loss (w/o reg) on all data: 0.810171
Train acc on all data:  0.988155668359
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 4.35621e-05
Norm of the params: 34.6525
                Loss: fixed 397 labels. Loss 0.81017. Accuracy 0.776.
Using normal model
LBFGS training took [634] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.283264
Train loss (w/o reg) on all data: 0.227869
Test loss (w/o reg) on all data: 0.774422
Train acc on all data:  0.919990331158
Test acc on all data:   0.75652173913
Norm of the mean of gradients: 0.000138962
Norm of the params: 33.2854
              Random: fixed 174 labels. Loss 0.77442. Accuracy 0.757.
### Flips: 1236, rs: 32, checks: 824
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196188
Train loss (w/o reg) on all data: 0.154232
Test loss (w/o reg) on all data: 0.479124
Train acc on all data:  0.944645878656
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 7.79918e-06
Norm of the params: 28.9675
     Influence (LOO): fixed 469 labels. Loss 0.47912. Accuracy 0.841.
Using normal model
LBFGS training took [568] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.141495
Train loss (w/o reg) on all data: 0.0832343
Test loss (w/o reg) on all data: 0.773581
Train acc on all data:  0.989847715736
Test acc on all data:   0.779710144928
Norm of the mean of gradients: 3.57061e-05
Norm of the params: 34.1351
                Loss: fixed 445 labels. Loss 0.77358. Accuracy 0.780.
Using normal model
LBFGS training took [643] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27619
Train loss (w/o reg) on all data: 0.221688
Test loss (w/o reg) on all data: 0.737083
Train acc on all data:  0.923616146966
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 4.48501e-05
Norm of the params: 33.0156
              Random: fixed 238 labels. Loss 0.73708. Accuracy 0.763.
### Flips: 1236, rs: 32, checks: 1030
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182732
Train loss (w/o reg) on all data: 0.143494
Test loss (w/o reg) on all data: 0.429718
Train acc on all data:  0.949963741842
Test acc on all data:   0.857971014493
Norm of the mean of gradients: 7.16785e-06
Norm of the params: 28.0136
     Influence (LOO): fixed 544 labels. Loss 0.42972. Accuracy 0.858.
Using normal model
LBFGS training took [480] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.125864
Train loss (w/o reg) on all data: 0.0713502
Test loss (w/o reg) on all data: 0.770483
Train acc on all data:  0.994682136814
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 2.66342e-05
Norm of the params: 33.0193
                Loss: fixed 503 labels. Loss 0.77048. Accuracy 0.798.
Using normal model
LBFGS training took [622] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263677
Train loss (w/o reg) on all data: 0.209984
Test loss (w/o reg) on all data: 0.750087
Train acc on all data:  0.926758520667
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 2.41298e-05
Norm of the params: 32.7696
              Random: fixed 303 labels. Loss 0.75009. Accuracy 0.777.
### Flips: 1236, rs: 32, checks: 1236
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17368
Train loss (w/o reg) on all data: 0.13727
Test loss (w/o reg) on all data: 0.399968
Train acc on all data:  0.95044718395
Test acc on all data:   0.872463768116
Norm of the mean of gradients: 4.51666e-05
Norm of the params: 26.9852
     Influence (LOO): fixed 619 labels. Loss 0.39997. Accuracy 0.872.
Using normal model
LBFGS training took [474] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114775
Train loss (w/o reg) on all data: 0.0631214
Test loss (w/o reg) on all data: 0.810299
Train acc on all data:  0.996857626299
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 7.00546e-05
Norm of the params: 32.1415
                Loss: fixed 568 labels. Loss 0.81030. Accuracy 0.814.
Using normal model
LBFGS training took [624] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25367
Train loss (w/o reg) on all data: 0.20158
Test loss (w/o reg) on all data: 0.713943
Train acc on all data:  0.930384336476
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 2.6765e-05
Norm of the params: 32.2769
              Random: fixed 368 labels. Loss 0.71394. Accuracy 0.798.
Using normal model
LBFGS training took [680] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.305802
Train loss (w/o reg) on all data: 0.247528
Test loss (w/o reg) on all data: 0.878985
Train acc on all data:  0.910804931109
Test acc on all data:   0.712077294686
Norm of the mean of gradients: 0.000145422
Norm of the params: 34.1392
Flipped loss: 0.87898. Accuracy: 0.712
### Flips: 1236, rs: 33, checks: 206
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.242429
Train loss (w/o reg) on all data: 0.188889
Test loss (w/o reg) on all data: 1.02513
Train acc on all data:  0.93376843123
Test acc on all data:   0.757487922705
Norm of the mean of gradients: 4.80155e-05
Norm of the params: 32.723
     Influence (LOO): fixed 161 labels. Loss 1.02513. Accuracy 0.757.
Using normal model
LBFGS training took [619] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.221894
Train loss (w/o reg) on all data: 0.158761
Test loss (w/o reg) on all data: 0.931475
Train acc on all data:  0.955281605028
Test acc on all data:   0.725603864734
Norm of the mean of gradients: 3.06845e-05
Norm of the params: 35.5339
                Loss: fixed 188 labels. Loss 0.93147. Accuracy 0.726.
Using normal model
LBFGS training took [516] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298906
Train loss (w/o reg) on all data: 0.241173
Test loss (w/o reg) on all data: 0.832
Train acc on all data:  0.913463862702
Test acc on all data:   0.729468599034
Norm of the mean of gradients: 4.01962e-05
Norm of the params: 33.9805
              Random: fixed  65 labels. Loss 0.83200. Accuracy 0.729.
### Flips: 1236, rs: 33, checks: 412
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.214416
Train loss (w/o reg) on all data: 0.166328
Test loss (w/o reg) on all data: 0.947795
Train acc on all data:  0.941261783901
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 8.04246e-05
Norm of the params: 31.0122
     Influence (LOO): fixed 293 labels. Loss 0.94779. Accuracy 0.786.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175639
Train loss (w/o reg) on all data: 0.114017
Test loss (w/o reg) on all data: 0.910479
Train acc on all data:  0.979695431472
Test acc on all data:   0.737198067633
Norm of the mean of gradients: 3.51433e-05
Norm of the params: 35.1061
                Loss: fixed 314 labels. Loss 0.91048. Accuracy 0.737.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29283
Train loss (w/o reg) on all data: 0.235999
Test loss (w/o reg) on all data: 0.762745
Train acc on all data:  0.914430746918
Test acc on all data:   0.754589371981
Norm of the mean of gradients: 8.08732e-05
Norm of the params: 33.7137
              Random: fixed 136 labels. Loss 0.76275. Accuracy 0.755.
### Flips: 1236, rs: 33, checks: 618
Using normal model
LBFGS training took [432] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201599
Train loss (w/o reg) on all data: 0.156311
Test loss (w/o reg) on all data: 0.809081
Train acc on all data:  0.94367899444
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 3.13225e-05
Norm of the params: 30.0957
     Influence (LOO): fixed 396 labels. Loss 0.80908. Accuracy 0.804.
Using normal model
LBFGS training took [576] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.152097
Train loss (w/o reg) on all data: 0.092172
Test loss (w/o reg) on all data: 0.835594
Train acc on all data:  0.987430505197
Test acc on all data:   0.750724637681
Norm of the mean of gradients: 5.70909e-05
Norm of the params: 34.6194
                Loss: fixed 400 labels. Loss 0.83559. Accuracy 0.751.
Using normal model
LBFGS training took [574] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282927
Train loss (w/o reg) on all data: 0.227414
Test loss (w/o reg) on all data: 0.719712
Train acc on all data:  0.919748610104
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 5.04522e-05
Norm of the params: 33.3207
              Random: fixed 201 labels. Loss 0.71971. Accuracy 0.771.
### Flips: 1236, rs: 33, checks: 824
Using normal model
LBFGS training took [386] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187913
Train loss (w/o reg) on all data: 0.146174
Test loss (w/o reg) on all data: 0.774606
Train acc on all data:  0.947304810249
Test acc on all data:   0.816425120773
Norm of the mean of gradients: 7.29242e-05
Norm of the params: 28.8928
     Influence (LOO): fixed 496 labels. Loss 0.77461. Accuracy 0.816.
Using normal model
LBFGS training took [566] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131185
Train loss (w/o reg) on all data: 0.075337
Test loss (w/o reg) on all data: 0.825836
Train acc on all data:  0.991539763113
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 3.46477e-05
Norm of the params: 33.421
                Loss: fixed 477 labels. Loss 0.82584. Accuracy 0.770.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.273671
Train loss (w/o reg) on all data: 0.219143
Test loss (w/o reg) on all data: 0.655615
Train acc on all data:  0.923616146966
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 3.12136e-05
Norm of the params: 33.0238
              Random: fixed 262 labels. Loss 0.65562. Accuracy 0.786.
### Flips: 1236, rs: 33, checks: 1030
Using normal model
LBFGS training took [381] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.180181
Train loss (w/o reg) on all data: 0.14014
Test loss (w/o reg) on all data: 0.713916
Train acc on all data:  0.948996857626
Test acc on all data:   0.840579710145
Norm of the mean of gradients: 1.12631e-05
Norm of the params: 28.299
     Influence (LOO): fixed 568 labels. Loss 0.71392. Accuracy 0.841.
Using normal model
LBFGS training took [577] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121773
Train loss (w/o reg) on all data: 0.0683405
Test loss (w/o reg) on all data: 0.905514
Train acc on all data:  0.992748368383
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 2.75433e-05
Norm of the params: 32.6901
                Loss: fixed 523 labels. Loss 0.90551. Accuracy 0.785.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262082
Train loss (w/o reg) on all data: 0.209324
Test loss (w/o reg) on all data: 0.63164
Train acc on all data:  0.926758520667
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 5.63077e-05
Norm of the params: 32.4832
              Random: fixed 332 labels. Loss 0.63164. Accuracy 0.796.
### Flips: 1236, rs: 33, checks: 1236
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173042
Train loss (w/o reg) on all data: 0.134752
Test loss (w/o reg) on all data: 0.632063
Train acc on all data:  0.953106115543
Test acc on all data:   0.859903381643
Norm of the mean of gradients: 9.04782e-06
Norm of the params: 27.6733
     Influence (LOO): fixed 627 labels. Loss 0.63206. Accuracy 0.860.
Using normal model
LBFGS training took [639] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110379
Train loss (w/o reg) on all data: 0.0599704
Test loss (w/o reg) on all data: 0.814659
Train acc on all data:  0.998066231569
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 6.41995e-05
Norm of the params: 31.7517
                Loss: fixed 567 labels. Loss 0.81466. Accuracy 0.812.
Using normal model
LBFGS training took [514] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252009
Train loss (w/o reg) on all data: 0.200135
Test loss (w/o reg) on all data: 0.631768
Train acc on all data:  0.933284989123
Test acc on all data:   0.8038647343
Norm of the mean of gradients: 8.1085e-05
Norm of the params: 32.2098
              Random: fixed 395 labels. Loss 0.63177. Accuracy 0.804.
Using normal model
LBFGS training took [624] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.296025
Train loss (w/o reg) on all data: 0.237505
Test loss (w/o reg) on all data: 0.905135
Train acc on all data:  0.920473773266
Test acc on all data:   0.732367149758
Norm of the mean of gradients: 0.000105423
Norm of the params: 34.211
Flipped loss: 0.90513. Accuracy: 0.732
### Flips: 1236, rs: 34, checks: 206
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.235685
Train loss (w/o reg) on all data: 0.181606
Test loss (w/o reg) on all data: 0.868525
Train acc on all data:  0.941745226009
Test acc on all data:   0.775845410628
Norm of the mean of gradients: 2.57538e-05
Norm of the params: 32.8875
     Influence (LOO): fixed 149 labels. Loss 0.86853. Accuracy 0.776.
Using normal model
LBFGS training took [558] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207322
Train loss (w/o reg) on all data: 0.14374
Test loss (w/o reg) on all data: 0.925351
Train acc on all data:  0.963258399807
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 1.72111e-05
Norm of the params: 35.6601
                Loss: fixed 188 labels. Loss 0.92535. Accuracy 0.749.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.290076
Train loss (w/o reg) on all data: 0.232846
Test loss (w/o reg) on all data: 0.855143
Train acc on all data:  0.925549915398
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 2.27607e-05
Norm of the params: 33.832
              Random: fixed  60 labels. Loss 0.85514. Accuracy 0.738.
### Flips: 1236, rs: 34, checks: 412
Using normal model
LBFGS training took [439] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.213457
Train loss (w/o reg) on all data: 0.163902
Test loss (w/o reg) on all data: 0.771559
Train acc on all data:  0.947063089195
Test acc on all data:   0.8
Norm of the mean of gradients: 2.15509e-05
Norm of the params: 31.4819
     Influence (LOO): fixed 268 labels. Loss 0.77156. Accuracy 0.800.
Using normal model
LBFGS training took [546] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.164636
Train loss (w/o reg) on all data: 0.102769
Test loss (w/o reg) on all data: 0.905329
Train acc on all data:  0.984771573604
Test acc on all data:   0.749758454106
Norm of the mean of gradients: 1.99178e-05
Norm of the params: 35.1759
                Loss: fixed 310 labels. Loss 0.90533. Accuracy 0.750.
Using normal model
LBFGS training took [582] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.283358
Train loss (w/o reg) on all data: 0.226897
Test loss (w/o reg) on all data: 0.847001
Train acc on all data:  0.926516799613
Test acc on all data:   0.741062801932
Norm of the mean of gradients: 2.15501e-05
Norm of the params: 33.6041
              Random: fixed 122 labels. Loss 0.84700. Accuracy 0.741.
### Flips: 1236, rs: 34, checks: 618
Using normal model
LBFGS training took [466] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.196462
Train loss (w/o reg) on all data: 0.150619
Test loss (w/o reg) on all data: 0.694473
Train acc on all data:  0.950688905004
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 1.05136e-05
Norm of the params: 30.2796
     Influence (LOO): fixed 384 labels. Loss 0.69447. Accuracy 0.822.
Using normal model
LBFGS training took [546] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143008
Train loss (w/o reg) on all data: 0.0842359
Test loss (w/o reg) on all data: 0.861831
Train acc on all data:  0.990814599952
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 2.95178e-05
Norm of the params: 34.2849
                Loss: fixed 390 labels. Loss 0.86183. Accuracy 0.768.
Using normal model
LBFGS training took [675] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275294
Train loss (w/o reg) on all data: 0.219304
Test loss (w/o reg) on all data: 0.829155
Train acc on all data:  0.92941745226
Test acc on all data:   0.737198067633
Norm of the mean of gradients: 8.29799e-05
Norm of the params: 33.4633
              Random: fixed 178 labels. Loss 0.82916. Accuracy 0.737.
### Flips: 1236, rs: 34, checks: 824
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18405
Train loss (w/o reg) on all data: 0.141211
Test loss (w/o reg) on all data: 0.639413
Train acc on all data:  0.951655789219
Test acc on all data:   0.831884057971
Norm of the mean of gradients: 3.25507e-05
Norm of the params: 29.2707
     Influence (LOO): fixed 481 labels. Loss 0.63941. Accuracy 0.832.
Using normal model
LBFGS training took [504] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.131079
Train loss (w/o reg) on all data: 0.074433
Test loss (w/o reg) on all data: 0.848505
Train acc on all data:  0.992023205221
Test acc on all data:   0.782608695652
Norm of the mean of gradients: 1.49264e-05
Norm of the params: 33.6588
                Loss: fixed 449 labels. Loss 0.84851. Accuracy 0.783.
Using normal model
LBFGS training took [605] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.268211
Train loss (w/o reg) on all data: 0.212772
Test loss (w/o reg) on all data: 0.832083
Train acc on all data:  0.932801547015
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 0.000188507
Norm of the params: 33.2984
              Random: fixed 234 labels. Loss 0.83208. Accuracy 0.749.
### Flips: 1236, rs: 34, checks: 1030
Using normal model
LBFGS training took [457] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173877
Train loss (w/o reg) on all data: 0.133453
Test loss (w/o reg) on all data: 0.545319
Train acc on all data:  0.95479816292
Test acc on all data:   0.850241545894
Norm of the mean of gradients: 5.3114e-05
Norm of the params: 28.4337
     Influence (LOO): fixed 573 labels. Loss 0.54532. Accuracy 0.850.
Using normal model
LBFGS training took [467] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118645
Train loss (w/o reg) on all data: 0.0651459
Test loss (w/o reg) on all data: 0.85083
Train acc on all data:  0.997341068407
Test acc on all data:   0.810628019324
Norm of the mean of gradients: 2.74941e-05
Norm of the params: 32.7106
                Loss: fixed 510 labels. Loss 0.85083. Accuracy 0.811.
Using normal model
LBFGS training took [678] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.258577
Train loss (w/o reg) on all data: 0.203507
Test loss (w/o reg) on all data: 0.793796
Train acc on all data:  0.936427362823
Test acc on all data:   0.773913043478
Norm of the mean of gradients: 0.000103313
Norm of the params: 33.1872
              Random: fixed 297 labels. Loss 0.79380. Accuracy 0.774.
### Flips: 1236, rs: 34, checks: 1236
Using normal model
LBFGS training took [446] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.166194
Train loss (w/o reg) on all data: 0.127888
Test loss (w/o reg) on all data: 0.503604
Train acc on all data:  0.957940536621
Test acc on all data:   0.864734299517
Norm of the mean of gradients: 3.37643e-05
Norm of the params: 27.679
     Influence (LOO): fixed 645 labels. Loss 0.50360. Accuracy 0.865.
Using normal model
LBFGS training took [473] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11019
Train loss (w/o reg) on all data: 0.0593577
Test loss (w/o reg) on all data: 0.789238
Train acc on all data:  0.997582789461
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 5.16171e-05
Norm of the params: 31.8847
                Loss: fixed 564 labels. Loss 0.78924. Accuracy 0.818.
Using normal model
LBFGS training took [638] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.249252
Train loss (w/o reg) on all data: 0.195379
Test loss (w/o reg) on all data: 0.824835
Train acc on all data:  0.939811457578
Test acc on all data:   0.776811594203
Norm of the mean of gradients: 7.31529e-05
Norm of the params: 32.8246
              Random: fixed 367 labels. Loss 0.82484. Accuracy 0.777.
Using normal model
LBFGS training took [726] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.309834
Train loss (w/o reg) on all data: 0.251752
Test loss (w/o reg) on all data: 1.09131
Train acc on all data:  0.910563210056
Test acc on all data:   0.676328502415
Norm of the mean of gradients: 9.18847e-05
Norm of the params: 34.0828
Flipped loss: 1.09131. Accuracy: 0.676
### Flips: 1236, rs: 35, checks: 206
Using normal model
LBFGS training took [567] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252025
Train loss (w/o reg) on all data: 0.19795
Test loss (w/o reg) on all data: 1.13611
Train acc on all data:  0.931351220691
Test acc on all data:   0.72077294686
Norm of the mean of gradients: 3.6118e-05
Norm of the params: 32.8862
     Influence (LOO): fixed 139 labels. Loss 1.13611. Accuracy 0.721.
Using normal model
LBFGS training took [599] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.230693
Train loss (w/o reg) on all data: 0.166506
Test loss (w/o reg) on all data: 1.16209
Train acc on all data:  0.953347836597
Test acc on all data:   0.686956521739
Norm of the mean of gradients: 3.29981e-05
Norm of the params: 35.8294
                Loss: fixed 182 labels. Loss 1.16209. Accuracy 0.687.
Using normal model
LBFGS training took [625] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.302426
Train loss (w/o reg) on all data: 0.245052
Test loss (w/o reg) on all data: 1.11923
Train acc on all data:  0.908629441624
Test acc on all data:   0.687922705314
Norm of the mean of gradients: 2.32189e-05
Norm of the params: 33.8745
              Random: fixed  63 labels. Loss 1.11923. Accuracy 0.688.
### Flips: 1236, rs: 35, checks: 412
Using normal model
LBFGS training took [459] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222767
Train loss (w/o reg) on all data: 0.173443
Test loss (w/o reg) on all data: 1.05699
Train acc on all data:  0.937635968093
Test acc on all data:   0.763285024155
Norm of the mean of gradients: 2.38612e-05
Norm of the params: 31.4085
     Influence (LOO): fixed 266 labels. Loss 1.05699. Accuracy 0.763.
Using normal model
LBFGS training took [584] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191709
Train loss (w/o reg) on all data: 0.128463
Test loss (w/o reg) on all data: 1.1582
Train acc on all data:  0.973410684071
Test acc on all data:   0.703381642512
Norm of the mean of gradients: 7.58416e-05
Norm of the params: 35.5657
                Loss: fixed 296 labels. Loss 1.15820. Accuracy 0.703.
Using normal model
LBFGS training took [631] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.295629
Train loss (w/o reg) on all data: 0.238918
Test loss (w/o reg) on all data: 1.01262
Train acc on all data:  0.912738699541
Test acc on all data:   0.710144927536
Norm of the mean of gradients: 0.000108196
Norm of the params: 33.6783
              Random: fixed 123 labels. Loss 1.01262. Accuracy 0.710.
### Flips: 1236, rs: 35, checks: 618
Using normal model
LBFGS training took [402] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.207813
Train loss (w/o reg) on all data: 0.161552
Test loss (w/o reg) on all data: 0.861941
Train acc on all data:  0.94053662074
Test acc on all data:   0.790338164251
Norm of the mean of gradients: 2.48637e-05
Norm of the params: 30.4176
     Influence (LOO): fixed 378 labels. Loss 0.86194. Accuracy 0.790.
Using normal model
LBFGS training took [579] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.160157
Train loss (w/o reg) on all data: 0.0987637
Test loss (w/o reg) on all data: 1.09135
Train acc on all data:  0.987188784143
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 8.7934e-05
Norm of the params: 35.0408
                Loss: fixed 396 labels. Loss 1.09135. Accuracy 0.738.
Using normal model
LBFGS training took [622] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.286816
Train loss (w/o reg) on all data: 0.23139
Test loss (w/o reg) on all data: 0.960708
Train acc on all data:  0.916847957457
Test acc on all data:   0.723671497585
Norm of the mean of gradients: 8.83458e-05
Norm of the params: 33.2944
              Random: fixed 176 labels. Loss 0.96071. Accuracy 0.724.
### Flips: 1236, rs: 35, checks: 824
Using normal model
LBFGS training took [382] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193256
Train loss (w/o reg) on all data: 0.149805
Test loss (w/o reg) on all data: 0.776336
Train acc on all data:  0.945612762872
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 4.38323e-05
Norm of the params: 29.4791
     Influence (LOO): fixed 483 labels. Loss 0.77634. Accuracy 0.812.
Using normal model
LBFGS training took [487] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139889
Train loss (w/o reg) on all data: 0.081692
Test loss (w/o reg) on all data: 1.06213
Train acc on all data:  0.994682136814
Test acc on all data:   0.75845410628
Norm of the mean of gradients: 7.29775e-05
Norm of the params: 34.1167
                Loss: fixed 474 labels. Loss 1.06213. Accuracy 0.758.
Using normal model
LBFGS training took [653] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.280852
Train loss (w/o reg) on all data: 0.226245
Test loss (w/o reg) on all data: 0.909131
Train acc on all data:  0.920232052212
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 4.19555e-05
Norm of the params: 33.0476
              Random: fixed 234 labels. Loss 0.90913. Accuracy 0.735.
### Flips: 1236, rs: 35, checks: 1030
Using normal model
LBFGS training took [360] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183618
Train loss (w/o reg) on all data: 0.142886
Test loss (w/o reg) on all data: 0.772708
Train acc on all data:  0.947546531303
Test acc on all data:   0.820289855072
Norm of the mean of gradients: 2.89475e-05
Norm of the params: 28.5416
     Influence (LOO): fixed 560 labels. Loss 0.77271. Accuracy 0.820.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.128884
Train loss (w/o reg) on all data: 0.0729587
Test loss (w/o reg) on all data: 1.05234
Train acc on all data:  0.995890742084
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 3.4569e-05
Norm of the params: 33.4441
                Loss: fixed 525 labels. Loss 1.05234. Accuracy 0.785.
Using normal model
LBFGS training took [694] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271237
Train loss (w/o reg) on all data: 0.218097
Test loss (w/o reg) on all data: 0.893504
Train acc on all data:  0.923616146966
Test acc on all data:   0.748792270531
Norm of the mean of gradients: 2.52262e-05
Norm of the params: 32.6005
              Random: fixed 298 labels. Loss 0.89350. Accuracy 0.749.
### Flips: 1236, rs: 35, checks: 1236
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.173466
Train loss (w/o reg) on all data: 0.134944
Test loss (w/o reg) on all data: 0.641592
Train acc on all data:  0.949480299734
Test acc on all data:   0.832850241546
Norm of the mean of gradients: 1.22407e-05
Norm of the params: 27.7569
     Influence (LOO): fixed 630 labels. Loss 0.64159. Accuracy 0.833.
Using normal model
LBFGS training took [464] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117726
Train loss (w/o reg) on all data: 0.0650892
Test loss (w/o reg) on all data: 0.952629
Train acc on all data:  0.997099347353
Test acc on all data:   0.809661835749
Norm of the mean of gradients: 1.57224e-05
Norm of the params: 32.4458
                Loss: fixed 575 labels. Loss 0.95263. Accuracy 0.810.
Using normal model
LBFGS training took [704] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.261151
Train loss (w/o reg) on all data: 0.208867
Test loss (w/o reg) on all data: 0.89071
Train acc on all data:  0.927241962775
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 8.27538e-05
Norm of the params: 32.337
              Random: fixed 356 labels. Loss 0.89071. Accuracy 0.770.
Using normal model
LBFGS training took [719] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.301856
Train loss (w/o reg) on all data: 0.243807
Test loss (w/o reg) on all data: 0.971465
Train acc on all data:  0.91394730481
Test acc on all data:   0.71690821256
Norm of the mean of gradients: 0.000114571
Norm of the params: 34.073
Flipped loss: 0.97146. Accuracy: 0.717
### Flips: 1236, rs: 36, checks: 206
Using normal model
LBFGS training took [490] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.246006
Train loss (w/o reg) on all data: 0.192718
Test loss (w/o reg) on all data: 0.922125
Train acc on all data:  0.932318104907
Test acc on all data:   0.729468599034
Norm of the mean of gradients: 2.0632e-05
Norm of the params: 32.646
     Influence (LOO): fixed 145 labels. Loss 0.92213. Accuracy 0.729.
Using normal model
LBFGS training took [663] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.227069
Train loss (w/o reg) on all data: 0.163682
Test loss (w/o reg) on all data: 0.982885
Train acc on all data:  0.955039883974
Test acc on all data:   0.733333333333
Norm of the mean of gradients: 3.45746e-05
Norm of the params: 35.6054
                Loss: fixed 180 labels. Loss 0.98289. Accuracy 0.733.
Using normal model
LBFGS training took [597] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.292073
Train loss (w/o reg) on all data: 0.234378
Test loss (w/o reg) on all data: 0.959406
Train acc on all data:  0.917814841673
Test acc on all data:   0.738164251208
Norm of the mean of gradients: 8.87708e-05
Norm of the params: 33.9689
              Random: fixed  69 labels. Loss 0.95941. Accuracy 0.738.
### Flips: 1236, rs: 36, checks: 412
Using normal model
LBFGS training took [464] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223335
Train loss (w/o reg) on all data: 0.174726
Test loss (w/o reg) on all data: 0.836634
Train acc on all data:  0.937152525985
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 1.86065e-05
Norm of the params: 31.18
     Influence (LOO): fixed 254 labels. Loss 0.83663. Accuracy 0.759.
Using normal model
LBFGS training took [656] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192067
Train loss (w/o reg) on all data: 0.128304
Test loss (w/o reg) on all data: 0.983469
Train acc on all data:  0.975344452502
Test acc on all data:   0.744927536232
Norm of the mean of gradients: 4.81634e-05
Norm of the params: 35.7109
                Loss: fixed 287 labels. Loss 0.98347. Accuracy 0.745.
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.284538
Train loss (w/o reg) on all data: 0.227972
Test loss (w/o reg) on all data: 0.866605
Train acc on all data:  0.923374425912
Test acc on all data:   0.757487922705
Norm of the mean of gradients: 8.14506e-05
Norm of the params: 33.6352
              Random: fixed 137 labels. Loss 0.86661. Accuracy 0.757.
### Flips: 1236, rs: 36, checks: 618
Using normal model
LBFGS training took [353] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.209355
Train loss (w/o reg) on all data: 0.16453
Test loss (w/o reg) on all data: 0.721468
Train acc on all data:  0.94053662074
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 9.94167e-06
Norm of the params: 29.9417
     Influence (LOO): fixed 349 labels. Loss 0.72147. Accuracy 0.787.
Using normal model
LBFGS training took [533] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.163064
Train loss (w/o reg) on all data: 0.101517
Test loss (w/o reg) on all data: 0.977942
Train acc on all data:  0.985013294658
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 3.31622e-05
Norm of the params: 35.0847
                Loss: fixed 385 labels. Loss 0.97794. Accuracy 0.770.
Using normal model
LBFGS training took [641] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.277319
Train loss (w/o reg) on all data: 0.221829
Test loss (w/o reg) on all data: 0.858832
Train acc on all data:  0.924099589074
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 8.46151e-05
Norm of the params: 33.3138
              Random: fixed 198 labels. Loss 0.85883. Accuracy 0.770.
### Flips: 1236, rs: 36, checks: 824
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193644
Train loss (w/o reg) on all data: 0.152073
Test loss (w/o reg) on all data: 0.663635
Train acc on all data:  0.946337926033
Test acc on all data:   0.818357487923
Norm of the mean of gradients: 1.0375e-05
Norm of the params: 28.8346
     Influence (LOO): fixed 458 labels. Loss 0.66363. Accuracy 0.818.
Using normal model
LBFGS training took [547] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148539
Train loss (w/o reg) on all data: 0.089275
Test loss (w/o reg) on all data: 0.953071
Train acc on all data:  0.989122552574
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 1.66618e-05
Norm of the params: 34.4279
                Loss: fixed 441 labels. Loss 0.95307. Accuracy 0.785.
Using normal model
LBFGS training took [591] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.265191
Train loss (w/o reg) on all data: 0.210186
Test loss (w/o reg) on all data: 0.823122
Train acc on all data:  0.928692289098
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 6.32231e-05
Norm of the params: 33.1676
              Random: fixed 267 labels. Loss 0.82312. Accuracy 0.775.
### Flips: 1236, rs: 36, checks: 1030
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183062
Train loss (w/o reg) on all data: 0.143647
Test loss (w/o reg) on all data: 0.639909
Train acc on all data:  0.949722020788
Test acc on all data:   0.834782608696
Norm of the mean of gradients: 1.86157e-05
Norm of the params: 28.0768
     Influence (LOO): fixed 547 labels. Loss 0.63991. Accuracy 0.835.
Using normal model
LBFGS training took [511] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.132717
Train loss (w/o reg) on all data: 0.077124
Test loss (w/o reg) on all data: 0.892412
Train acc on all data:  0.990331157844
Test acc on all data:   0.794202898551
Norm of the mean of gradients: 1.15769e-05
Norm of the params: 33.3445
                Loss: fixed 498 labels. Loss 0.89241. Accuracy 0.794.
Using normal model
LBFGS training took [605] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.254899
Train loss (w/o reg) on all data: 0.200192
Test loss (w/o reg) on all data: 0.824937
Train acc on all data:  0.932801547015
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 5.01276e-05
Norm of the params: 33.0778
              Random: fixed 333 labels. Loss 0.82494. Accuracy 0.784.
### Flips: 1236, rs: 36, checks: 1236
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.174122
Train loss (w/o reg) on all data: 0.136263
Test loss (w/o reg) on all data: 0.546491
Train acc on all data:  0.95044718395
Test acc on all data:   0.852173913043
Norm of the mean of gradients: 1.09984e-05
Norm of the params: 27.5172
     Influence (LOO): fixed 617 labels. Loss 0.54649. Accuracy 0.852.
Using normal model
LBFGS training took [474] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11713
Train loss (w/o reg) on all data: 0.0654069
Test loss (w/o reg) on all data: 0.85654
Train acc on all data:  0.992023205221
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 1.53745e-05
Norm of the params: 32.1629
                Loss: fixed 573 labels. Loss 0.85654. Accuracy 0.807.
Using normal model
LBFGS training took [597] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240845
Train loss (w/o reg) on all data: 0.187638
Test loss (w/o reg) on all data: 0.775322
Train acc on all data:  0.94053662074
Test acc on all data:   0.79806763285
Norm of the mean of gradients: 0.000112584
Norm of the params: 32.6212
              Random: fixed 404 labels. Loss 0.77532. Accuracy 0.798.
Using normal model
LBFGS training took [572] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.303991
Train loss (w/o reg) on all data: 0.245043
Test loss (w/o reg) on all data: 0.958127
Train acc on all data:  0.911046652163
Test acc on all data:   0.729468599034
Norm of the mean of gradients: 7.06281e-05
Norm of the params: 34.3361
Flipped loss: 0.95813. Accuracy: 0.729
### Flips: 1236, rs: 37, checks: 206
Using normal model
LBFGS training took [421] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24554
Train loss (w/o reg) on all data: 0.192491
Test loss (w/o reg) on all data: 0.855426
Train acc on all data:  0.932559825961
Test acc on all data:   0.770048309179
Norm of the mean of gradients: 9.66212e-05
Norm of the params: 32.5727
     Influence (LOO): fixed 147 labels. Loss 0.85543. Accuracy 0.770.
Using normal model
LBFGS training took [532] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.223065
Train loss (w/o reg) on all data: 0.158449
Test loss (w/o reg) on all data: 0.999395
Train acc on all data:  0.955523326082
Test acc on all data:   0.739130434783
Norm of the mean of gradients: 2.36421e-05
Norm of the params: 35.9488
                Loss: fixed 187 labels. Loss 0.99939. Accuracy 0.739.
Using normal model
LBFGS training took [529] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.295953
Train loss (w/o reg) on all data: 0.237242
Test loss (w/o reg) on all data: 0.923628
Train acc on all data:  0.914914189026
Test acc on all data:   0.727536231884
Norm of the mean of gradients: 4.09451e-05
Norm of the params: 34.2669
              Random: fixed  57 labels. Loss 0.92363. Accuracy 0.728.
### Flips: 1236, rs: 37, checks: 412
Using normal model
LBFGS training took [395] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216483
Train loss (w/o reg) on all data: 0.168385
Test loss (w/o reg) on all data: 0.750202
Train acc on all data:  0.941745226009
Test acc on all data:   0.783574879227
Norm of the mean of gradients: 1.48422e-05
Norm of the params: 31.0155
     Influence (LOO): fixed 275 labels. Loss 0.75020. Accuracy 0.784.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.176366
Train loss (w/o reg) on all data: 0.112565
Test loss (w/o reg) on all data: 0.959441
Train acc on all data:  0.979937152526
Test acc on all data:   0.76038647343
Norm of the mean of gradients: 2.6976e-05
Norm of the params: 35.7214
                Loss: fixed 318 labels. Loss 0.95944. Accuracy 0.760.
Using normal model
LBFGS training took [497] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.289737
Train loss (w/o reg) on all data: 0.231694
Test loss (w/o reg) on all data: 0.862155
Train acc on all data:  0.917573120619
Test acc on all data:   0.743961352657
Norm of the mean of gradients: 3.39744e-05
Norm of the params: 34.0714
              Random: fixed 117 labels. Loss 0.86215. Accuracy 0.744.
### Flips: 1236, rs: 37, checks: 618
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.201807
Train loss (w/o reg) on all data: 0.156716
Test loss (w/o reg) on all data: 0.663122
Train acc on all data:  0.945612762872
Test acc on all data:   0.806763285024
Norm of the mean of gradients: 1.80442e-05
Norm of the params: 30.0301
     Influence (LOO): fixed 391 labels. Loss 0.66312. Accuracy 0.807.
Using normal model
LBFGS training took [428] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15118
Train loss (w/o reg) on all data: 0.0904315
Test loss (w/o reg) on all data: 0.911629
Train acc on all data:  0.991781484167
Test acc on all data:   0.774879227053
Norm of the mean of gradients: 1.48562e-05
Norm of the params: 34.8566
                Loss: fixed 395 labels. Loss 0.91163. Accuracy 0.775.
Using normal model
LBFGS training took [572] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.282573
Train loss (w/o reg) on all data: 0.225399
Test loss (w/o reg) on all data: 0.790337
Train acc on all data:  0.92071549432
Test acc on all data:   0.75845410628
Norm of the mean of gradients: 2.16312e-05
Norm of the params: 33.8156
              Random: fixed 174 labels. Loss 0.79034. Accuracy 0.758.
### Flips: 1236, rs: 37, checks: 824
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.191118
Train loss (w/o reg) on all data: 0.148089
Test loss (w/o reg) on all data: 0.597371
Train acc on all data:  0.949722020788
Test acc on all data:   0.819323671498
Norm of the mean of gradients: 1.26595e-05
Norm of the params: 29.3356
     Influence (LOO): fixed 476 labels. Loss 0.59737. Accuracy 0.819.
Using normal model
LBFGS training took [437] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.134162
Train loss (w/o reg) on all data: 0.0775206
Test loss (w/o reg) on all data: 0.844383
Train acc on all data:  0.993956973652
Test acc on all data:   0.8
Norm of the mean of gradients: 3.35352e-05
Norm of the params: 33.6575
                Loss: fixed 463 labels. Loss 0.84438. Accuracy 0.800.
Using normal model
LBFGS training took [519] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271558
Train loss (w/o reg) on all data: 0.215499
Test loss (w/o reg) on all data: 0.733879
Train acc on all data:  0.924824752236
Test acc on all data:   0.771014492754
Norm of the mean of gradients: 3.05229e-05
Norm of the params: 33.484
              Random: fixed 239 labels. Loss 0.73388. Accuracy 0.771.
### Flips: 1236, rs: 37, checks: 1030
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.179007
Train loss (w/o reg) on all data: 0.138864
Test loss (w/o reg) on all data: 0.529224
Train acc on all data:  0.951414068165
Test acc on all data:   0.835748792271
Norm of the mean of gradients: 1.37691e-05
Norm of the params: 28.335
     Influence (LOO): fixed 571 labels. Loss 0.52922. Accuracy 0.836.
Using normal model
LBFGS training took [407] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119119
Train loss (w/o reg) on all data: 0.0663842
Test loss (w/o reg) on all data: 0.819503
Train acc on all data:  0.996615905245
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 3.57452e-05
Norm of the params: 32.4761
                Loss: fixed 524 labels. Loss 0.81950. Accuracy 0.814.
Using normal model
LBFGS training took [435] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262202
Train loss (w/o reg) on all data: 0.207029
Test loss (w/o reg) on all data: 0.682108
Train acc on all data:  0.927483683829
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 7.41519e-05
Norm of the params: 33.2186
              Random: fixed 297 labels. Loss 0.68211. Accuracy 0.788.
### Flips: 1236, rs: 37, checks: 1236
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.170019
Train loss (w/o reg) on all data: 0.132193
Test loss (w/o reg) on all data: 0.479145
Train acc on all data:  0.954072999758
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 7.86735e-06
Norm of the params: 27.5049
     Influence (LOO): fixed 637 labels. Loss 0.47914. Accuracy 0.853.
Using normal model
LBFGS training took [424] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10889
Train loss (w/o reg) on all data: 0.0593171
Test loss (w/o reg) on all data: 0.780006
Train acc on all data:  0.996615905245
Test acc on all data:   0.822222222222
Norm of the mean of gradients: 1.53982e-05
Norm of the params: 31.4874
                Loss: fixed 571 labels. Loss 0.78001. Accuracy 0.822.
Using normal model
LBFGS training took [494] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252841
Train loss (w/o reg) on all data: 0.198971
Test loss (w/o reg) on all data: 0.673462
Train acc on all data:  0.931351220691
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 3.62817e-05
Norm of the params: 32.8238
              Random: fixed 360 labels. Loss 0.67346. Accuracy 0.801.
Using normal model
LBFGS training took [649] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.299906
Train loss (w/o reg) on all data: 0.241067
Test loss (w/o reg) on all data: 0.915611
Train acc on all data:  0.91515591008
Test acc on all data:   0.735265700483
Norm of the mean of gradients: 2.56348e-05
Norm of the params: 34.3043
Flipped loss: 0.91561. Accuracy: 0.735
### Flips: 1236, rs: 38, checks: 206
Using normal model
LBFGS training took [436] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24015
Train loss (w/o reg) on all data: 0.187663
Test loss (w/o reg) on all data: 0.841484
Train acc on all data:  0.938361131255
Test acc on all data:   0.764251207729
Norm of the mean of gradients: 2.29905e-05
Norm of the params: 32.3998
     Influence (LOO): fixed 156 labels. Loss 0.84148. Accuracy 0.764.
Using normal model
LBFGS training took [604] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.222355
Train loss (w/o reg) on all data: 0.157947
Test loss (w/o reg) on all data: 0.922153
Train acc on all data:  0.958907420836
Test acc on all data:   0.732367149758
Norm of the mean of gradients: 6.38578e-05
Norm of the params: 35.8909
                Loss: fixed 182 labels. Loss 0.92215. Accuracy 0.732.
Using normal model
LBFGS training took [579] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.294138
Train loss (w/o reg) on all data: 0.235524
Test loss (w/o reg) on all data: 0.885212
Train acc on all data:  0.917814841673
Test acc on all data:   0.742028985507
Norm of the mean of gradients: 0.000105479
Norm of the params: 34.2387
              Random: fixed  60 labels. Loss 0.88521. Accuracy 0.742.
### Flips: 1236, rs: 38, checks: 412
Using normal model
LBFGS training took [362] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215275
Train loss (w/o reg) on all data: 0.167617
Test loss (w/o reg) on all data: 0.741239
Train acc on all data:  0.941986947063
Test acc on all data:   0.7961352657
Norm of the mean of gradients: 1.13912e-05
Norm of the params: 30.8733
     Influence (LOO): fixed 273 labels. Loss 0.74124. Accuracy 0.796.
Using normal model
LBFGS training took [477] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.175057
Train loss (w/o reg) on all data: 0.111308
Test loss (w/o reg) on all data: 0.93076
Train acc on all data:  0.983079526227
Test acc on all data:   0.750724637681
Norm of the mean of gradients: 4.55898e-05
Norm of the params: 35.7069
                Loss: fixed 310 labels. Loss 0.93076. Accuracy 0.751.
Using normal model
LBFGS training took [601] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.286912
Train loss (w/o reg) on all data: 0.228958
Test loss (w/o reg) on all data: 0.782862
Train acc on all data:  0.923616146966
Test acc on all data:   0.759420289855
Norm of the mean of gradients: 5.2777e-05
Norm of the params: 34.0452
              Random: fixed 122 labels. Loss 0.78286. Accuracy 0.759.
### Flips: 1236, rs: 38, checks: 618
Using normal model
LBFGS training took [352] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.202915
Train loss (w/o reg) on all data: 0.158132
Test loss (w/o reg) on all data: 0.658703
Train acc on all data:  0.946579647087
Test acc on all data:   0.814492753623
Norm of the mean of gradients: 2.3854e-05
Norm of the params: 29.9275
     Influence (LOO): fixed 366 labels. Loss 0.65870. Accuracy 0.814.
Using normal model
LBFGS training took [472] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153237
Train loss (w/o reg) on all data: 0.0921663
Test loss (w/o reg) on all data: 0.857597
Train acc on all data:  0.989364273628
Test acc on all data:   0.771980676329
Norm of the mean of gradients: 3.82e-05
Norm of the params: 34.9488
                Loss: fixed 388 labels. Loss 0.85760. Accuracy 0.772.
Using normal model
LBFGS training took [581] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.279268
Train loss (w/o reg) on all data: 0.222085
Test loss (w/o reg) on all data: 0.781878
Train acc on all data:  0.927725404883
Test acc on all data:   0.768115942029
Norm of the mean of gradients: 7.27375e-05
Norm of the params: 33.8181
              Random: fixed 178 labels. Loss 0.78188. Accuracy 0.768.
### Flips: 1236, rs: 38, checks: 824
Using normal model
LBFGS training took [325] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.193435
Train loss (w/o reg) on all data: 0.151489
Test loss (w/o reg) on all data: 0.580237
Train acc on all data:  0.946096204979
Test acc on all data:   0.833816425121
Norm of the mean of gradients: 1.49861e-05
Norm of the params: 28.964
     Influence (LOO): fixed 461 labels. Loss 0.58024. Accuracy 0.834.
Using normal model
LBFGS training took [486] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.139485
Train loss (w/o reg) on all data: 0.0812466
Test loss (w/o reg) on all data: 0.818835
Train acc on all data:  0.990814599952
Test acc on all data:   0.787439613527
Norm of the mean of gradients: 3.04117e-05
Norm of the params: 34.1288
                Loss: fixed 452 labels. Loss 0.81883. Accuracy 0.787.
Using normal model
LBFGS training took [587] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.269052
Train loss (w/o reg) on all data: 0.212528
Test loss (w/o reg) on all data: 0.749724
Train acc on all data:  0.92941745226
Test acc on all data:   0.784541062802
Norm of the mean of gradients: 2.4859e-05
Norm of the params: 33.6225
              Random: fixed 243 labels. Loss 0.74972. Accuracy 0.785.
### Flips: 1236, rs: 38, checks: 1030
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.182008
Train loss (w/o reg) on all data: 0.142093
Test loss (w/o reg) on all data: 0.490845
Train acc on all data:  0.95044718395
Test acc on all data:   0.853140096618
Norm of the mean of gradients: 4.0053e-05
Norm of the params: 28.2544
     Influence (LOO): fixed 540 labels. Loss 0.49084. Accuracy 0.853.
Using normal model
LBFGS training took [478] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129167
Train loss (w/o reg) on all data: 0.0738228
Test loss (w/o reg) on all data: 0.859603
Train acc on all data:  0.992023205221
Test acc on all data:   0.797101449275
Norm of the mean of gradients: 1.81392e-05
Norm of the params: 33.27
                Loss: fixed 505 labels. Loss 0.85960. Accuracy 0.797.
Using normal model
LBFGS training took [530] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.264163
Train loss (w/o reg) on all data: 0.208759
Test loss (w/o reg) on all data: 0.731231
Train acc on all data:  0.930142615422
Test acc on all data:   0.788405797101
Norm of the mean of gradients: 2.26893e-05
Norm of the params: 33.2878
              Random: fixed 295 labels. Loss 0.73123. Accuracy 0.788.
### Flips: 1236, rs: 38, checks: 1236
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172297
Train loss (w/o reg) on all data: 0.135026
Test loss (w/o reg) on all data: 0.454664
Train acc on all data:  0.951897510273
Test acc on all data:   0.849275362319
Norm of the mean of gradients: 7.33982e-06
Norm of the params: 27.3024
     Influence (LOO): fixed 616 labels. Loss 0.45466. Accuracy 0.849.
Using normal model
LBFGS training took [438] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117814
Train loss (w/o reg) on all data: 0.0662343
Test loss (w/o reg) on all data: 0.822335
Train acc on all data:  0.992264926275
Test acc on all data:   0.808695652174
Norm of the mean of gradients: 9.27026e-06
Norm of the params: 32.1185
                Loss: fixed 566 labels. Loss 0.82233. Accuracy 0.809.
Using normal model
LBFGS training took [544] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256323
Train loss (w/o reg) on all data: 0.202352
Test loss (w/o reg) on all data: 0.685721
Train acc on all data:  0.932801547015
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 7.54281e-05
Norm of the params: 32.8544
              Random: fixed 356 labels. Loss 0.68572. Accuracy 0.791.
Using normal model
LBFGS training took [656] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.298681
Train loss (w/o reg) on all data: 0.240993
Test loss (w/o reg) on all data: 1.01581
Train acc on all data:  0.911771815325
Test acc on all data:   0.713043478261
Norm of the mean of gradients: 3.70776e-05
Norm of the params: 33.9671
Flipped loss: 1.01581. Accuracy: 0.713
### Flips: 1236, rs: 39, checks: 206
Using normal model
LBFGS training took [429] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240597
Train loss (w/o reg) on all data: 0.188002
Test loss (w/o reg) on all data: 0.944921
Train acc on all data:  0.933043268069
Test acc on all data:   0.752657004831
Norm of the mean of gradients: 1.19857e-05
Norm of the params: 32.4329
     Influence (LOO): fixed 144 labels. Loss 0.94492. Accuracy 0.753.
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.215426
Train loss (w/o reg) on all data: 0.152837
Test loss (w/o reg) on all data: 1.07985
Train acc on all data:  0.958907420836
Test acc on all data:   0.727536231884
Norm of the mean of gradients: 2.34292e-05
Norm of the params: 35.3807
                Loss: fixed 188 labels. Loss 1.07985. Accuracy 0.728.
Using normal model
LBFGS training took [527] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.290978
Train loss (w/o reg) on all data: 0.234101
Test loss (w/o reg) on all data: 0.930293
Train acc on all data:  0.911771815325
Test acc on all data:   0.729468599034
Norm of the mean of gradients: 2.06596e-05
Norm of the params: 33.7274
              Random: fixed  71 labels. Loss 0.93029. Accuracy 0.729.
### Flips: 1236, rs: 39, checks: 412
Using normal model
LBFGS training took [357] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.216697
Train loss (w/o reg) on all data: 0.168904
Test loss (w/o reg) on all data: 0.758908
Train acc on all data:  0.940053178632
Test acc on all data:   0.785507246377
Norm of the mean of gradients: 1.08402e-05
Norm of the params: 30.9173
     Influence (LOO): fixed 266 labels. Loss 0.75891. Accuracy 0.786.
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172561
Train loss (w/o reg) on all data: 0.110608
Test loss (w/o reg) on all data: 1.01454
Train acc on all data:  0.98017887358
Test acc on all data:   0.772946859903
Norm of the mean of gradients: 4.05578e-05
Norm of the params: 35.2005
                Loss: fixed 310 labels. Loss 1.01454. Accuracy 0.773.
Using normal model
LBFGS training took [553] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2847
Train loss (w/o reg) on all data: 0.228523
Test loss (w/o reg) on all data: 0.875482
Train acc on all data:  0.914430746918
Test acc on all data:   0.743961352657
Norm of the mean of gradients: 0.000107001
Norm of the params: 33.5193
              Random: fixed 135 labels. Loss 0.87548. Accuracy 0.744.
### Flips: 1236, rs: 39, checks: 618
Using normal model
LBFGS training took [337] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.199833
Train loss (w/o reg) on all data: 0.155701
Test loss (w/o reg) on all data: 0.6465
Train acc on all data:  0.94367899444
Test acc on all data:   0.813526570048
Norm of the mean of gradients: 2.6496e-05
Norm of the params: 29.7094
     Influence (LOO): fixed 386 labels. Loss 0.64650. Accuracy 0.814.
Using normal model
LBFGS training took [552] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.148443
Train loss (w/o reg) on all data: 0.0891573
Test loss (w/o reg) on all data: 0.861513
Train acc on all data:  0.987672226251
Test acc on all data:   0.778743961353
Norm of the mean of gradients: 3.72264e-05
Norm of the params: 34.4343
                Loss: fixed 395 labels. Loss 0.86151. Accuracy 0.779.
Using normal model
LBFGS training took [557] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.278635
Train loss (w/o reg) on all data: 0.223377
Test loss (w/o reg) on all data: 0.866111
Train acc on all data:  0.916847957457
Test acc on all data:   0.761352657005
Norm of the mean of gradients: 0.000127521
Norm of the params: 33.2442
              Random: fixed 191 labels. Loss 0.86611. Accuracy 0.761.
### Flips: 1236, rs: 39, checks: 824
Using normal model
LBFGS training took [347] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19177
Train loss (w/o reg) on all data: 0.150069
Test loss (w/o reg) on all data: 0.587332
Train acc on all data:  0.944162436548
Test acc on all data:   0.827053140097
Norm of the mean of gradients: 1.58493e-05
Norm of the params: 28.8793
     Influence (LOO): fixed 467 labels. Loss 0.58733. Accuracy 0.827.
Using normal model
LBFGS training took [520] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.135169
Train loss (w/o reg) on all data: 0.0785237
Test loss (w/o reg) on all data: 0.849389
Train acc on all data:  0.99008943679
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 4.13797e-05
Norm of the params: 33.6586
                Loss: fixed 452 labels. Loss 0.84939. Accuracy 0.791.
Using normal model
LBFGS training took [578] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.271968
Train loss (w/o reg) on all data: 0.217507
Test loss (w/o reg) on all data: 0.803842
Train acc on all data:  0.920473773266
Test acc on all data:   0.769082125604
Norm of the mean of gradients: 9.45592e-05
Norm of the params: 33.0035
              Random: fixed 259 labels. Loss 0.80384. Accuracy 0.769.
### Flips: 1236, rs: 39, checks: 1030
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.183012
Train loss (w/o reg) on all data: 0.143301
Test loss (w/o reg) on all data: 0.583071
Train acc on all data:  0.946821368141
Test acc on all data:   0.846376811594
Norm of the mean of gradients: 1.39197e-05
Norm of the params: 28.1819
     Influence (LOO): fixed 549 labels. Loss 0.58307. Accuracy 0.846.
Using normal model
LBFGS training took [506] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.126313
Train loss (w/o reg) on all data: 0.0716789
Test loss (w/o reg) on all data: 0.845008
Train acc on all data:  0.991781484167
Test acc on all data:   0.800966183575
Norm of the mean of gradients: 4.05769e-05
Norm of the params: 33.0558
                Loss: fixed 500 labels. Loss 0.84501. Accuracy 0.801.
Using normal model
LBFGS training took [495] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.263365
Train loss (w/o reg) on all data: 0.209659
Test loss (w/o reg) on all data: 0.77524
Train acc on all data:  0.926758520667
Test acc on all data:   0.781642512077
Norm of the mean of gradients: 0.000105709
Norm of the params: 32.7739
              Random: fixed 322 labels. Loss 0.77524. Accuracy 0.782.
### Flips: 1236, rs: 39, checks: 1236
Using normal model
LBFGS training took [329] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.172418
Train loss (w/o reg) on all data: 0.135069
Test loss (w/o reg) on all data: 0.494198
Train acc on all data:  0.950205462896
Test acc on all data:   0.855072463768
Norm of the mean of gradients: 1.20427e-05
Norm of the params: 27.3312
     Influence (LOO): fixed 629 labels. Loss 0.49420. Accuracy 0.855.
Using normal model
LBFGS training took [515] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118844
Train loss (w/o reg) on all data: 0.0666325
Test loss (w/o reg) on all data: 0.78018
Train acc on all data:  0.991298042059
Test acc on all data:   0.811594202899
Norm of the mean of gradients: 5.25112e-05
Norm of the params: 32.3145
                Loss: fixed 551 labels. Loss 0.78018. Accuracy 0.812.
Using normal model
LBFGS training took [534] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.252666
Train loss (w/o reg) on all data: 0.199924
Test loss (w/o reg) on all data: 0.766
Train acc on all data:  0.932076383853
Test acc on all data:   0.791304347826
Norm of the mean of gradients: 8.3506e-05
Norm of the params: 32.4783
              Random: fixed 389 labels. Loss 0.76600. Accuracy 0.791.
